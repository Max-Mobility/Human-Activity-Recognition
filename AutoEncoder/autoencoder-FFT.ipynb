{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Conv1D,Activation,MaxPooling1D,Dense,Flatten,UpSampling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model,load_model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import plot\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sub_model(myinputs,channel=3,sensors=1):\n",
    "    x     = Conv1D(filters =8*sensors, kernel_size=5,strides = 1, padding = 'same')(myinputs)\n",
    "    x     = BatchNormalization()(x) \n",
    "    x     = Activation('relu')(x)\n",
    "    x     = MaxPooling1D(pool_size=3,strides = 2,padding='same')(x)\n",
    "    #  x shape is 64X16 =3072\n",
    "    x     = Conv1D(filters =12*sensors, kernel_size=3,strides = 1, padding = 'same')(x)\n",
    "    x     = BatchNormalization()(x) \n",
    "    x     = Activation('relu')(x)\n",
    "    x     = MaxPooling1D(pool_size=2,strides = 2,padding='same')(x)\n",
    "    #  x shape is 32X24 = 2048\n",
    "    x     = Conv1D(filters =16*sensors, kernel_size=3,strides = 1, padding = 'same')(x)\n",
    "    x     = BatchNormalization()(x) \n",
    "    x     = Activation('relu')(x)\n",
    "    x     = MaxPooling1D(pool_size=2,strides = 2,padding='same')(x)\n",
    "    # x shape is 16*32 = 1536\n",
    "    x     = Conv1D(filters =20*sensors, kernel_size=3,strides = 1, padding = 'same')(x)\n",
    "    x     = BatchNormalization()(x) \n",
    "    x     = Activation('relu')(x)\n",
    "    x     = MaxPooling1D(pool_size=2,strides = 2,padding='same')(x)\n",
    "    # x shape is 8*32 = 1024\n",
    "    x     = Conv1D(filters =24, kernel_size=3,strides = 2, padding = 'same')(x)\n",
    "    # x shape is 4*32 = 512\n",
    "\n",
    "    latent_vector = Flatten()(x)\n",
    "    # decoder x = 4*128\n",
    "    x     = Conv1D(filters =24*sensors, kernel_size=3,strides = 1, padding = 'same')(x)\n",
    "    x     = BatchNormalization()(x) \n",
    "    x     = Activation('relu')(x)\n",
    "    x     = UpSampling1D(2)(x)\n",
    "    # x shape is 8*128\n",
    "    x     = Conv1D(filters =20*sensors, kernel_size=3,strides = 1, padding = 'same')(x)\n",
    "    x     = BatchNormalization()(x) \n",
    "    x     = Activation('relu')(x)\n",
    "    x     = UpSampling1D(2)(x)\n",
    "    # x shape is 16*96\n",
    "    x     = Conv1D(filters =16*sensors, kernel_size=3,strides = 1, padding = 'same')(x)\n",
    "    x     = BatchNormalization()(x) \n",
    "    x     = Activation('relu')(x)\n",
    "    x     = UpSampling1D(2)(x)\n",
    "    # x shape is 32*64\n",
    "    x     = Conv1D(filters =12*sensors, kernel_size=3,strides = 1, padding = 'same')(x)\n",
    "    x     = BatchNormalization()(x) \n",
    "    x     = Activation('relu')(x)\n",
    "    x     = UpSampling1D(2)(x)\n",
    "    # x shape is 64*48\n",
    "    x     = Conv1D(filters =8*sensors, kernel_size=3,strides = 1, padding = 'same')(x)\n",
    "    x     = BatchNormalization()(x) \n",
    "    x     = Activation('relu')(x)\n",
    "    x     = UpSampling1D(2)(x)\n",
    "    # x shape is 128*48\n",
    "    y     = Conv1D(filters =channel, kernel_size=3,strides = 1, padding = 'same')(x)\n",
    "    #y     = Activation('relu')(x)\n",
    "    return latent_vector,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(name=\"push_detect\",channel=3,sensors=1,inputLength=128):\n",
    "    # input shape 128X26 =3382\n",
    "    with tf.name_scope(name):\n",
    "        myInputs = Input(shape=(inputLength,channel))\n",
    "                \n",
    "        LV,Y = get_sub_model(myInputs,channel=channel,sensors=sensors)\n",
    "        \n",
    "        autoencoder = Model (inputs=myInputs,\n",
    "                             outputs =Y)\n",
    "    \n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputLength=64\n",
    "gyroModel=build_model(sensors=3,inputLength=inputLength)\n",
    "linearAccModel=build_model(sensors=3,inputLength=inputLength)\n",
    "gravityModel=build_model(sensors=3,inputLength=inputLength)\n",
    "gameVecModel=build_model(channel=4,sensors=3,inputLength=inputLength)\n",
    "opt = keras.optimizers.SGD(lr=0.1, momentum=0.0, decay=0.0, nesterov=False)\n",
    "gyroModel.compile(optimizer=opt,loss='mae',metrics=['accuracy'])\n",
    "opt = keras.optimizers.SGD(lr=.1, momentum=0.0, decay=0.0, nesterov=False)\n",
    "linearAccModel.compile(optimizer=opt,loss='mae',metrics=['accuracy'])\n",
    "opt = keras.optimizers.SGD(lr=.1, momentum=0.0, decay=0.0, nesterov=False)\n",
    "gravityModel.compile(optimizer=opt,loss='mae',metrics=['accuracy'])\n",
    "opt = keras.optimizers.SGD(lr=.1, momentum=0.0, decay=0.0, nesterov=False)\n",
    "gameVecModel.compile(optimizer=opt,loss='mae',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= np.load('watch_norm_64.npy')\n",
    "np.random.shuffle(data)\n",
    "p=0.85\n",
    "train_num =int(data.shape[0]*p)\n",
    "gyro_train=data[:train_num,:,13:16]\n",
    "linearAcc_train=data[:train_num,:,16:19]\n",
    "gravity_train=data[:train_num,:,19:22]\n",
    "gameVec_train=data[:train_num,:,22:26]\n",
    "\n",
    "gyro_test =data[train_num:,:,13:16]\n",
    "linearAcc_test =data[train_num:,:,16:19]\n",
    "gravity_test =data[train_num:,:,19:22]\n",
    "gameVec_test =data[train_num:,:,22:26]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training round 0\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 1/20\n",
      "10976/10976 [==============================] - 28s 3ms/step - loss: 0.1456 - acc: 0.3480 - val_loss: 0.1318 - val_acc: 0.3459\n",
      "Epoch 2/20\n",
      "10976/10976 [==============================] - 6s 519us/step - loss: 0.0659 - acc: 0.3626 - val_loss: 0.0775 - val_acc: 0.3684\n",
      "Epoch 3/20\n",
      "10976/10976 [==============================] - 6s 524us/step - loss: 0.0532 - acc: 0.3748 - val_loss: 0.0510 - val_acc: 0.3801\n",
      "Epoch 4/20\n",
      "10976/10976 [==============================] - 6s 555us/step - loss: 0.0425 - acc: 0.3834 - val_loss: 0.0467 - val_acc: 0.3851\n",
      "Epoch 5/20\n",
      "10976/10976 [==============================] - 6s 521us/step - loss: 0.0369 - acc: 0.3910 - val_loss: 0.0365 - val_acc: 0.3949\n",
      "Epoch 6/20\n",
      "10976/10976 [==============================] - 6s 537us/step - loss: 0.0360 - acc: 0.3974 - val_loss: 0.0331 - val_acc: 0.4095\n",
      "Epoch 7/20\n",
      "10976/10976 [==============================] - 6s 526us/step - loss: 0.0342 - acc: 0.3935 - val_loss: 0.0322 - val_acc: 0.4129\n",
      "Epoch 8/20\n",
      "10976/10976 [==============================] - 5s 500us/step - loss: 0.0332 - acc: 0.4062 - val_loss: 0.0381 - val_acc: 0.4180\n",
      "Epoch 9/20\n",
      "10976/10976 [==============================] - 6s 509us/step - loss: 0.0320 - acc: 0.4089 - val_loss: 0.0409 - val_acc: 0.4210\n",
      "Epoch 10/20\n",
      "10976/10976 [==============================] - 6s 531us/step - loss: 0.0307 - acc: 0.4121 - val_loss: 0.0294 - val_acc: 0.4173\n",
      "Epoch 11/20\n",
      "10976/10976 [==============================] - 6s 542us/step - loss: 0.0290 - acc: 0.4169 - val_loss: 0.0278 - val_acc: 0.4308\n",
      "Epoch 12/20\n",
      "10976/10976 [==============================] - 6s 515us/step - loss: 0.0288 - acc: 0.4187 - val_loss: 0.0257 - val_acc: 0.3920\n",
      "Epoch 13/20\n",
      "10976/10976 [==============================] - 6s 522us/step - loss: 0.0278 - acc: 0.4224 - val_loss: 0.0254 - val_acc: 0.4390\n",
      "Epoch 14/20\n",
      "10976/10976 [==============================] - 6s 530us/step - loss: 0.0278 - acc: 0.4252 - val_loss: 0.0283 - val_acc: 0.4222\n",
      "Epoch 15/20\n",
      "10976/10976 [==============================] - 6s 530us/step - loss: 0.0276 - acc: 0.4269 - val_loss: 0.0246 - val_acc: 0.4499\n",
      "Epoch 16/20\n",
      "10976/10976 [==============================] - 6s 537us/step - loss: 0.0261 - acc: 0.4285 - val_loss: 0.0250 - val_acc: 0.4522\n",
      "Epoch 17/20\n",
      "10976/10976 [==============================] - 6s 532us/step - loss: 0.0258 - acc: 0.4213 - val_loss: 0.0240 - val_acc: 0.3651\n",
      "Epoch 18/20\n",
      "10976/10976 [==============================] - 6s 514us/step - loss: 0.0261 - acc: 0.4212 - val_loss: 0.0224 - val_acc: 0.4459\n",
      "Epoch 19/20\n",
      "10976/10976 [==============================] - 6s 529us/step - loss: 0.0256 - acc: 0.4306 - val_loss: 0.0225 - val_acc: 0.4127\n",
      "Epoch 20/20\n",
      "10976/10976 [==============================] - 6s 541us/step - loss: 0.0272 - acc: 0.4309 - val_loss: 0.0270 - val_acc: 0.4094\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 1/20\n",
      "10976/10976 [==============================] - 23s 2ms/step - loss: 0.1840 - acc: 0.3273 - val_loss: 0.2632 - val_acc: 0.3270\n",
      "Epoch 2/20\n",
      "10976/10976 [==============================] - 6s 545us/step - loss: 0.1110 - acc: 0.3361 - val_loss: 0.1609 - val_acc: 0.3254\n",
      "Epoch 3/20\n",
      "10976/10976 [==============================] - 6s 550us/step - loss: 0.0915 - acc: 0.3383 - val_loss: 0.1115 - val_acc: 0.3393\n",
      "Epoch 4/20\n",
      "10976/10976 [==============================] - 6s 528us/step - loss: 0.0778 - acc: 0.3436 - val_loss: 0.0883 - val_acc: 0.3411\n",
      "Epoch 5/20\n",
      "10976/10976 [==============================] - 6s 505us/step - loss: 0.0676 - acc: 0.3511 - val_loss: 0.0681 - val_acc: 0.3545\n",
      "Epoch 6/20\n",
      "10976/10976 [==============================] - 6s 536us/step - loss: 0.0590 - acc: 0.3534 - val_loss: 0.0582 - val_acc: 0.4026\n",
      "Epoch 7/20\n",
      "10976/10976 [==============================] - 6s 542us/step - loss: 0.0544 - acc: 0.3626 - val_loss: 0.0538 - val_acc: 0.3467\n",
      "Epoch 8/20\n",
      "10976/10976 [==============================] - 6s 560us/step - loss: 0.0501 - acc: 0.3826 - val_loss: 0.0486 - val_acc: 0.3956\n",
      "Epoch 9/20\n",
      "10976/10976 [==============================] - 6s 535us/step - loss: 0.0470 - acc: 0.3700 - val_loss: 0.0440 - val_acc: 0.3601\n",
      "Epoch 10/20\n",
      "10976/10976 [==============================] - 6s 525us/step - loss: 0.0445 - acc: 0.3812 - val_loss: 0.0404 - val_acc: 0.4154\n",
      "Epoch 11/20\n",
      "10976/10976 [==============================] - 6s 547us/step - loss: 0.0423 - acc: 0.3790 - val_loss: 0.0381 - val_acc: 0.4101\n",
      "Epoch 12/20\n",
      "10976/10976 [==============================] - 6s 514us/step - loss: 0.0405 - acc: 0.4012 - val_loss: 0.0388 - val_acc: 0.3826\n",
      "Epoch 13/20\n",
      "10976/10976 [==============================] - 6s 539us/step - loss: 0.0393 - acc: 0.3997 - val_loss: 0.0369 - val_acc: 0.4352\n",
      "Epoch 14/20\n",
      "10976/10976 [==============================] - 5s 492us/step - loss: 0.0382 - acc: 0.3909 - val_loss: 0.0371 - val_acc: 0.3911\n",
      "Epoch 15/20\n",
      "10976/10976 [==============================] - 6s 533us/step - loss: 0.0377 - acc: 0.3842 - val_loss: 0.0357 - val_acc: 0.3668\n",
      "Epoch 16/20\n",
      "10976/10976 [==============================] - 6s 524us/step - loss: 0.0364 - acc: 0.4080 - val_loss: 0.0355 - val_acc: 0.3778\n",
      "Epoch 17/20\n",
      "10976/10976 [==============================] - 6s 538us/step - loss: 0.0360 - acc: 0.4061 - val_loss: 0.0362 - val_acc: 0.3611\n",
      "Epoch 18/20\n",
      "10976/10976 [==============================] - 6s 529us/step - loss: 0.0350 - acc: 0.4005 - val_loss: 0.0394 - val_acc: 0.4940\n",
      "Epoch 19/20\n",
      "10976/10976 [==============================] - 6s 548us/step - loss: 0.0356 - acc: 0.4175 - val_loss: 0.0325 - val_acc: 0.3577\n",
      "Epoch 20/20\n",
      "10976/10976 [==============================] - 6s 518us/step - loss: 0.0342 - acc: 0.4150 - val_loss: 0.0324 - val_acc: 0.4984\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 1/20\n",
      "10976/10976 [==============================] - 24s 2ms/step - loss: 0.1405 - acc: 0.3407 - val_loss: 0.0966 - val_acc: 0.2495\n",
      "Epoch 2/20\n",
      "10976/10976 [==============================] - 6s 530us/step - loss: 0.0647 - acc: 0.3638 - val_loss: 0.0727 - val_acc: 0.3179\n",
      "Epoch 3/20\n",
      "10976/10976 [==============================] - 6s 580us/step - loss: 0.0514 - acc: 0.3512 - val_loss: 0.0559 - val_acc: 0.3314\n",
      "Epoch 4/20\n",
      "10976/10976 [==============================] - 6s 566us/step - loss: 0.0439 - acc: 0.3767 - val_loss: 0.0452 - val_acc: 0.3362\n",
      "Epoch 5/20\n",
      "10976/10976 [==============================] - 6s 562us/step - loss: 0.0377 - acc: 0.3735 - val_loss: 0.0368 - val_acc: 0.2392\n",
      "Epoch 6/20\n",
      "10976/10976 [==============================] - 6s 534us/step - loss: 0.0343 - acc: 0.3736 - val_loss: 0.0390 - val_acc: 0.4779\n",
      "Epoch 7/20\n",
      "10976/10976 [==============================] - 6s 561us/step - loss: 0.0322 - acc: 0.3382 - val_loss: 0.0345 - val_acc: 0.4112\n",
      "Epoch 8/20\n",
      "10976/10976 [==============================] - 6s 533us/step - loss: 0.0317 - acc: 0.3789 - val_loss: 0.0338 - val_acc: 0.3967\n",
      "Epoch 9/20\n",
      "10976/10976 [==============================] - 6s 555us/step - loss: 0.0303 - acc: 0.3808 - val_loss: 0.0286 - val_acc: 0.4374\n",
      "Epoch 10/20\n",
      "10976/10976 [==============================] - 6s 518us/step - loss: 0.0280 - acc: 0.3738 - val_loss: 0.0286 - val_acc: 0.4070\n",
      "Epoch 11/20\n",
      "10976/10976 [==============================] - 6s 551us/step - loss: 0.0277 - acc: 0.3783 - val_loss: 0.0260 - val_acc: 0.4285\n",
      "Epoch 12/20\n",
      "10976/10976 [==============================] - 6s 551us/step - loss: 0.0269 - acc: 0.3094 - val_loss: 0.0263 - val_acc: 0.4533\n",
      "Epoch 13/20\n",
      "10976/10976 [==============================] - 6s 540us/step - loss: 0.0255 - acc: 0.3162 - val_loss: 0.0213 - val_acc: 0.2355\n",
      "Epoch 14/20\n",
      "10976/10976 [==============================] - 6s 529us/step - loss: 0.0254 - acc: 0.3095 - val_loss: 0.0255 - val_acc: 0.4427\n",
      "Epoch 15/20\n",
      "10976/10976 [==============================] - 6s 564us/step - loss: 0.0252 - acc: 0.3205 - val_loss: 0.0281 - val_acc: 0.1874\n",
      "Epoch 16/20\n",
      "10976/10976 [==============================] - 6s 545us/step - loss: 0.0248 - acc: 0.3126 - val_loss: 0.0258 - val_acc: 0.4345\n",
      "Epoch 17/20\n",
      "10976/10976 [==============================] - 6s 560us/step - loss: 0.0240 - acc: 0.3184 - val_loss: 0.0243 - val_acc: 0.1878\n",
      "Epoch 18/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 6s 517us/step - loss: 0.0238 - acc: 0.3347 - val_loss: 0.0236 - val_acc: 0.3681\n",
      "Epoch 19/20\n",
      "10976/10976 [==============================] - 6s 513us/step - loss: 0.0240 - acc: 0.3551 - val_loss: 0.0217 - val_acc: 0.3793\n",
      "Epoch 20/20\n",
      "10976/10976 [==============================] - 6s 533us/step - loss: 0.0235 - acc: 0.3548 - val_loss: 0.0222 - val_acc: 0.2906\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 1/20\n",
      "10976/10976 [==============================] - 27s 2ms/step - loss: 0.1246 - acc: 0.2601 - val_loss: 0.0803 - val_acc: 0.3519\n",
      "Epoch 2/20\n",
      "10976/10976 [==============================] - 6s 548us/step - loss: 0.0496 - acc: 0.2676 - val_loss: 0.0523 - val_acc: 0.2283\n",
      "Epoch 3/20\n",
      "10976/10976 [==============================] - 6s 545us/step - loss: 0.0418 - acc: 0.2663 - val_loss: 0.0463 - val_acc: 0.3548\n",
      "Epoch 4/20\n",
      "10976/10976 [==============================] - 6s 572us/step - loss: 0.0393 - acc: 0.2894 - val_loss: 0.0335 - val_acc: 0.2628\n",
      "Epoch 5/20\n",
      "10976/10976 [==============================] - 6s 532us/step - loss: 0.0365 - acc: 0.2962 - val_loss: 0.0416 - val_acc: 0.2451\n",
      "Epoch 6/20\n",
      "10976/10976 [==============================] - 6s 572us/step - loss: 0.0351 - acc: 0.2926 - val_loss: 0.0406 - val_acc: 0.3180\n",
      "Epoch 7/20\n",
      "10976/10976 [==============================] - 6s 538us/step - loss: 0.0330 - acc: 0.3011 - val_loss: 0.0328 - val_acc: 0.3612\n",
      "Epoch 8/20\n",
      "10976/10976 [==============================] - 6s 532us/step - loss: 0.0308 - acc: 0.3082 - val_loss: 0.0317 - val_acc: 0.2504\n",
      "Epoch 9/20\n",
      "10976/10976 [==============================] - 6s 553us/step - loss: 0.0307 - acc: 0.3146 - val_loss: 0.0315 - val_acc: 0.3164\n",
      "Epoch 10/20\n",
      "10976/10976 [==============================] - 6s 541us/step - loss: 0.0297 - acc: 0.3312 - val_loss: 0.0241 - val_acc: 0.3174\n",
      "Epoch 11/20\n",
      "10976/10976 [==============================] - 6s 557us/step - loss: 0.0284 - acc: 0.3291 - val_loss: 0.0247 - val_acc: 0.2464\n",
      "Epoch 12/20\n",
      "10976/10976 [==============================] - 6s 538us/step - loss: 0.0275 - acc: 0.2830 - val_loss: 0.0320 - val_acc: 0.3553\n",
      "Epoch 13/20\n",
      "10976/10976 [==============================] - 6s 554us/step - loss: 0.0268 - acc: 0.2865 - val_loss: 0.0291 - val_acc: 0.2598\n",
      "Epoch 14/20\n",
      "10976/10976 [==============================] - 6s 568us/step - loss: 0.0269 - acc: 0.2555 - val_loss: 0.0258 - val_acc: 0.3264\n",
      "Epoch 15/20\n",
      "10976/10976 [==============================] - 6s 530us/step - loss: 0.0256 - acc: 0.2785 - val_loss: 0.0292 - val_acc: 0.3684\n",
      "Epoch 16/20\n",
      "10976/10976 [==============================] - 6s 544us/step - loss: 0.0253 - acc: 0.2630 - val_loss: 0.0210 - val_acc: 0.2832\n",
      "Epoch 17/20\n",
      "10976/10976 [==============================] - 6s 566us/step - loss: 0.0246 - acc: 0.2866 - val_loss: 0.0251 - val_acc: 0.3253\n",
      "Epoch 18/20\n",
      "10976/10976 [==============================] - 6s 576us/step - loss: 0.0241 - acc: 0.2887 - val_loss: 0.0223 - val_acc: 0.2569\n",
      "Epoch 19/20\n",
      "10976/10976 [==============================] - 6s 562us/step - loss: 0.0230 - acc: 0.2982 - val_loss: 0.0227 - val_acc: 0.3323\n",
      "Epoch 20/20\n",
      "10976/10976 [==============================] - 6s 556us/step - loss: 0.0231 - acc: 0.2825 - val_loss: 0.0232 - val_acc: 0.2499\n",
      "start training round 1\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 21/40\n",
      "10976/10976 [==============================] - 6s 548us/step - loss: 0.0261 - acc: 0.4105 - val_loss: 0.0238 - val_acc: 0.4657\n",
      "Epoch 22/40\n",
      "10976/10976 [==============================] - 6s 547us/step - loss: 0.0255 - acc: 0.4084 - val_loss: 0.0234 - val_acc: 0.3674\n",
      "Epoch 23/40\n",
      "10976/10976 [==============================] - 6s 538us/step - loss: 0.0253 - acc: 0.4067 - val_loss: 0.0255 - val_acc: 0.4635\n",
      "Epoch 24/40\n",
      "10976/10976 [==============================] - 6s 554us/step - loss: 0.0252 - acc: 0.4101 - val_loss: 0.0257 - val_acc: 0.4430\n",
      "Epoch 25/40\n",
      "10976/10976 [==============================] - 6s 562us/step - loss: 0.0264 - acc: 0.4159 - val_loss: 0.0228 - val_acc: 0.3755\n",
      "Epoch 26/40\n",
      "10976/10976 [==============================] - 6s 550us/step - loss: 0.0254 - acc: 0.4062 - val_loss: 0.0256 - val_acc: 0.4398\n",
      "Epoch 27/40\n",
      "10976/10976 [==============================] - 6s 553us/step - loss: 0.0249 - acc: 0.4085 - val_loss: 0.0263 - val_acc: 0.3618\n",
      "Epoch 28/40\n",
      "10976/10976 [==============================] - 6s 519us/step - loss: 0.0251 - acc: 0.4125 - val_loss: 0.0228 - val_acc: 0.4678\n",
      "Epoch 29/40\n",
      "10976/10976 [==============================] - 6s 570us/step - loss: 0.0242 - acc: 0.4140 - val_loss: 0.0213 - val_acc: 0.3680\n",
      "Epoch 30/40\n",
      "10976/10976 [==============================] - 6s 560us/step - loss: 0.0248 - acc: 0.4143 - val_loss: 0.0254 - val_acc: 0.4685\n",
      "Epoch 31/40\n",
      "10976/10976 [==============================] - 6s 548us/step - loss: 0.0247 - acc: 0.4102 - val_loss: 0.0243 - val_acc: 0.3699\n",
      "Epoch 32/40\n",
      "10976/10976 [==============================] - 6s 540us/step - loss: 0.0254 - acc: 0.4092 - val_loss: 0.0243 - val_acc: 0.4669\n",
      "Epoch 33/40\n",
      "10976/10976 [==============================] - 6s 554us/step - loss: 0.0254 - acc: 0.4114 - val_loss: 0.0276 - val_acc: 0.3579\n",
      "Epoch 34/40\n",
      "10976/10976 [==============================] - 6s 523us/step - loss: 0.0254 - acc: 0.4134 - val_loss: 0.0250 - val_acc: 0.4658\n",
      "Epoch 35/40\n",
      "10976/10976 [==============================] - 6s 537us/step - loss: 0.0250 - acc: 0.4169 - val_loss: 0.0248 - val_acc: 0.3905\n",
      "Epoch 36/40\n",
      "10976/10976 [==============================] - 6s 572us/step - loss: 0.0250 - acc: 0.4159 - val_loss: 0.0213 - val_acc: 0.4665\n",
      "Epoch 37/40\n",
      "10976/10976 [==============================] - 6s 546us/step - loss: 0.0247 - acc: 0.4285 - val_loss: 0.0246 - val_acc: 0.4461\n",
      "Epoch 38/40\n",
      "10976/10976 [==============================] - 6s 555us/step - loss: 0.0244 - acc: 0.4430 - val_loss: 0.0239 - val_acc: 0.4323\n",
      "Epoch 39/40\n",
      "10976/10976 [==============================] - 6s 574us/step - loss: 0.0239 - acc: 0.4413 - val_loss: 0.0206 - val_acc: 0.4551\n",
      "Epoch 40/40\n",
      "10976/10976 [==============================] - 6s 537us/step - loss: 0.0246 - acc: 0.4429 - val_loss: 0.0276 - val_acc: 0.4338\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 21/40\n",
      "10976/10976 [==============================] - 6s 545us/step - loss: 0.0342 - acc: 0.4155 - val_loss: 0.0342 - val_acc: 0.3487\n",
      "Epoch 22/40\n",
      "10976/10976 [==============================] - 6s 547us/step - loss: 0.0334 - acc: 0.4126 - val_loss: 0.0327 - val_acc: 0.4957\n",
      "Epoch 23/40\n",
      "10976/10976 [==============================] - 6s 538us/step - loss: 0.0328 - acc: 0.4195 - val_loss: 0.0325 - val_acc: 0.3351\n",
      "Epoch 24/40\n",
      "10976/10976 [==============================] - 6s 575us/step - loss: 0.0324 - acc: 0.4181 - val_loss: 0.0332 - val_acc: 0.5000\n",
      "Epoch 25/40\n",
      "10976/10976 [==============================] - 6s 570us/step - loss: 0.0322 - acc: 0.4240 - val_loss: 0.0312 - val_acc: 0.3632\n",
      "Epoch 26/40\n",
      "10976/10976 [==============================] - 6s 541us/step - loss: 0.0324 - acc: 0.4179 - val_loss: 0.0351 - val_acc: 0.4935\n",
      "Epoch 27/40\n",
      "10976/10976 [==============================] - 6s 554us/step - loss: 0.0315 - acc: 0.4207 - val_loss: 0.0328 - val_acc: 0.3679\n",
      "Epoch 28/40\n",
      "10976/10976 [==============================] - 6s 542us/step - loss: 0.0312 - acc: 0.4224 - val_loss: 0.0316 - val_acc: 0.5021\n",
      "Epoch 29/40\n",
      "10976/10976 [==============================] - 6s 549us/step - loss: 0.0310 - acc: 0.4229 - val_loss: 0.0292 - val_acc: 0.3649\n",
      "Epoch 30/40\n",
      "10976/10976 [==============================] - 6s 554us/step - loss: 0.0309 - acc: 0.4254 - val_loss: 0.0298 - val_acc: 0.5009\n",
      "Epoch 31/40\n",
      "10976/10976 [==============================] - 6s 543us/step - loss: 0.0306 - acc: 0.4258 - val_loss: 0.0315 - val_acc: 0.4322\n",
      "Epoch 32/40\n",
      "10976/10976 [==============================] - 6s 536us/step - loss: 0.0300 - acc: 0.4259 - val_loss: 0.0296 - val_acc: 0.5028\n",
      "Epoch 33/40\n",
      "10976/10976 [==============================] - 6s 562us/step - loss: 0.0302 - acc: 0.4302 - val_loss: 0.0300 - val_acc: 0.3890\n",
      "Epoch 34/40\n",
      "10976/10976 [==============================] - 6s 577us/step - loss: 0.0301 - acc: 0.4250 - val_loss: 0.0309 - val_acc: 0.5040\n",
      "Epoch 35/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 7s 594us/step - loss: 0.0297 - acc: 0.4289 - val_loss: 0.0307 - val_acc: 0.3909\n",
      "Epoch 36/40\n",
      "10976/10976 [==============================] - 7s 611us/step - loss: 0.0295 - acc: 0.4309 - val_loss: 0.0281 - val_acc: 0.5012\n",
      "Epoch 37/40\n",
      "10976/10976 [==============================] - 6s 562us/step - loss: 0.0296 - acc: 0.4357 - val_loss: 0.0311 - val_acc: 0.3414\n",
      "Epoch 38/40\n",
      "10976/10976 [==============================] - 6s 576us/step - loss: 0.0294 - acc: 0.4308 - val_loss: 0.0296 - val_acc: 0.5061\n",
      "Epoch 39/40\n",
      "10976/10976 [==============================] - 7s 611us/step - loss: 0.0290 - acc: 0.4281 - val_loss: 0.0299 - val_acc: 0.4156\n",
      "Epoch 40/40\n",
      "10976/10976 [==============================] - 6s 567us/step - loss: 0.0290 - acc: 0.4406 - val_loss: 0.0294 - val_acc: 0.5067\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 21/40\n",
      "10976/10976 [==============================] - 6s 585us/step - loss: 0.0232 - acc: 0.3449 - val_loss: 0.0227 - val_acc: 0.3765\n",
      "Epoch 22/40\n",
      "10976/10976 [==============================] - 6s 517us/step - loss: 0.0231 - acc: 0.3573 - val_loss: 0.0255 - val_acc: 0.2643\n",
      "Epoch 23/40\n",
      "10976/10976 [==============================] - 6s 545us/step - loss: 0.0230 - acc: 0.3432 - val_loss: 0.0230 - val_acc: 0.3774\n",
      "Epoch 24/40\n",
      "10976/10976 [==============================] - 6s 545us/step - loss: 0.0228 - acc: 0.3501 - val_loss: 0.0241 - val_acc: 0.3311\n",
      "Epoch 25/40\n",
      "10976/10976 [==============================] - 6s 563us/step - loss: 0.0227 - acc: 0.3505 - val_loss: 0.0222 - val_acc: 0.3789\n",
      "Epoch 26/40\n",
      "10976/10976 [==============================] - 6s 565us/step - loss: 0.0223 - acc: 0.3530 - val_loss: 0.0257 - val_acc: 0.3255\n",
      "Epoch 27/40\n",
      "10976/10976 [==============================] - 6s 542us/step - loss: 0.0224 - acc: 0.3461 - val_loss: 0.0215 - val_acc: 0.3818\n",
      "Epoch 28/40\n",
      "10976/10976 [==============================] - 6s 578us/step - loss: 0.0225 - acc: 0.3551 - val_loss: 0.0241 - val_acc: 0.3138\n",
      "Epoch 29/40\n",
      "10976/10976 [==============================] - 6s 558us/step - loss: 0.0222 - acc: 0.3578 - val_loss: 0.0225 - val_acc: 0.3809\n",
      "Epoch 30/40\n",
      "10976/10976 [==============================] - 6s 547us/step - loss: 0.0225 - acc: 0.3481 - val_loss: 0.0229 - val_acc: 0.3966\n",
      "Epoch 31/40\n",
      "10976/10976 [==============================] - 6s 581us/step - loss: 0.0221 - acc: 0.3477 - val_loss: 0.0255 - val_acc: 0.3011\n",
      "Epoch 32/40\n",
      "10976/10976 [==============================] - 6s 552us/step - loss: 0.0224 - acc: 0.3436 - val_loss: 0.0206 - val_acc: 0.3803\n",
      "Epoch 33/40\n",
      "10976/10976 [==============================] - 6s 592us/step - loss: 0.0218 - acc: 0.3530 - val_loss: 0.0234 - val_acc: 0.2942\n",
      "Epoch 34/40\n",
      "10976/10976 [==============================] - 6s 563us/step - loss: 0.0217 - acc: 0.3515 - val_loss: 0.0201 - val_acc: 0.3804\n",
      "Epoch 35/40\n",
      "10976/10976 [==============================] - 6s 564us/step - loss: 0.0215 - acc: 0.3636 - val_loss: 0.0239 - val_acc: 0.2857\n",
      "Epoch 36/40\n",
      "10976/10976 [==============================] - 6s 574us/step - loss: 0.0219 - acc: 0.3477 - val_loss: 0.0211 - val_acc: 0.3804\n",
      "Epoch 37/40\n",
      "10976/10976 [==============================] - 6s 566us/step - loss: 0.0213 - acc: 0.3717 - val_loss: 0.0228 - val_acc: 0.3880\n",
      "Epoch 38/40\n",
      "10976/10976 [==============================] - 7s 602us/step - loss: 0.0215 - acc: 0.3665 - val_loss: 0.0209 - val_acc: 0.3800\n",
      "Epoch 39/40\n",
      "10976/10976 [==============================] - 6s 574us/step - loss: 0.0213 - acc: 0.3829 - val_loss: 0.0243 - val_acc: 0.4134\n",
      "Epoch 40/40\n",
      "10976/10976 [==============================] - 6s 539us/step - loss: 0.0220 - acc: 0.3637 - val_loss: 0.0249 - val_acc: 0.4357\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 21/40\n",
      "10976/10976 [==============================] - 6s 546us/step - loss: 0.0226 - acc: 0.2852 - val_loss: 0.0230 - val_acc: 0.3166\n",
      "Epoch 22/40\n",
      "10976/10976 [==============================] - 7s 598us/step - loss: 0.0221 - acc: 0.2925 - val_loss: 0.0208 - val_acc: 0.2410\n",
      "Epoch 23/40\n",
      "10976/10976 [==============================] - 6s 532us/step - loss: 0.0228 - acc: 0.2991 - val_loss: 0.0233 - val_acc: 0.3277\n",
      "Epoch 24/40\n",
      "10976/10976 [==============================] - 6s 563us/step - loss: 0.0228 - acc: 0.2955 - val_loss: 0.0228 - val_acc: 0.2384\n",
      "Epoch 25/40\n",
      "10976/10976 [==============================] - 6s 547us/step - loss: 0.0224 - acc: 0.2962 - val_loss: 0.0219 - val_acc: 0.3685\n",
      "Epoch 26/40\n",
      "10976/10976 [==============================] - 6s 583us/step - loss: 0.0220 - acc: 0.2981 - val_loss: 0.0214 - val_acc: 0.2453\n",
      "Epoch 27/40\n",
      "10976/10976 [==============================] - 6s 548us/step - loss: 0.0217 - acc: 0.2886 - val_loss: 0.0239 - val_acc: 0.3369\n",
      "Epoch 28/40\n",
      "10976/10976 [==============================] - 6s 561us/step - loss: 0.0218 - acc: 0.3070 - val_loss: 0.0198 - val_acc: 0.3628\n",
      "Epoch 29/40\n",
      "10976/10976 [==============================] - 6s 539us/step - loss: 0.0209 - acc: 0.3447 - val_loss: 0.0232 - val_acc: 0.3219\n",
      "Epoch 30/40\n",
      "10976/10976 [==============================] - 6s 509us/step - loss: 0.0211 - acc: 0.3425 - val_loss: 0.0211 - val_acc: 0.3655\n",
      "Epoch 31/40\n",
      "10976/10976 [==============================] - 6s 565us/step - loss: 0.0211 - acc: 0.3398 - val_loss: 0.0232 - val_acc: 0.3629\n",
      "Epoch 32/40\n",
      "10976/10976 [==============================] - 6s 560us/step - loss: 0.0213 - acc: 0.3386 - val_loss: 0.0182 - val_acc: 0.3694\n",
      "Epoch 33/40\n",
      "10976/10976 [==============================] - 6s 561us/step - loss: 0.0207 - acc: 0.3403 - val_loss: 0.0228 - val_acc: 0.3404\n",
      "Epoch 34/40\n",
      "10976/10976 [==============================] - 6s 553us/step - loss: 0.0207 - acc: 0.3436 - val_loss: 0.0184 - val_acc: 0.3743\n",
      "Epoch 35/40\n",
      "10976/10976 [==============================] - 6s 548us/step - loss: 0.0200 - acc: 0.3441 - val_loss: 0.0233 - val_acc: 0.3341\n",
      "Epoch 36/40\n",
      "10976/10976 [==============================] - 6s 555us/step - loss: 0.0203 - acc: 0.3406 - val_loss: 0.0204 - val_acc: 0.3638\n",
      "Epoch 37/40\n",
      "10976/10976 [==============================] - 6s 566us/step - loss: 0.0208 - acc: 0.2651 - val_loss: 0.0216 - val_acc: 0.3568\n",
      "Epoch 38/40\n",
      "10976/10976 [==============================] - 6s 533us/step - loss: 0.0202 - acc: 0.2415 - val_loss: 0.0181 - val_acc: 0.1317\n",
      "Epoch 39/40\n",
      "10976/10976 [==============================] - 6s 558us/step - loss: 0.0202 - acc: 0.2367 - val_loss: 0.0211 - val_acc: 0.3504\n",
      "Epoch 40/40\n",
      "10976/10976 [==============================] - 6s 556us/step - loss: 0.0200 - acc: 0.2451 - val_loss: 0.0185 - val_acc: 0.1302\n",
      "start training round 2\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 41/60\n",
      "10976/10976 [==============================] - 6s 540us/step - loss: 0.0242 - acc: 0.4416 - val_loss: 0.0213 - val_acc: 0.4512\n",
      "Epoch 42/60\n",
      "10976/10976 [==============================] - 6s 555us/step - loss: 0.0245 - acc: 0.4130 - val_loss: 0.0227 - val_acc: 0.4586\n",
      "Epoch 43/60\n",
      "10976/10976 [==============================] - 6s 546us/step - loss: 0.0245 - acc: 0.4272 - val_loss: 0.0249 - val_acc: 0.4382\n",
      "Epoch 44/60\n",
      "10976/10976 [==============================] - 6s 553us/step - loss: 0.0243 - acc: 0.4409 - val_loss: 0.0214 - val_acc: 0.4581\n",
      "Epoch 45/60\n",
      "10976/10976 [==============================] - 6s 559us/step - loss: 0.0243 - acc: 0.4399 - val_loss: 0.0266 - val_acc: 0.4412\n",
      "Epoch 46/60\n",
      "10976/10976 [==============================] - 6s 545us/step - loss: 0.0249 - acc: 0.4437 - val_loss: 0.0203 - val_acc: 0.4464\n",
      "Epoch 47/60\n",
      "10976/10976 [==============================] - 6s 560us/step - loss: 0.0235 - acc: 0.4343 - val_loss: 0.0205 - val_acc: 0.3960\n",
      "Epoch 48/60\n",
      "10976/10976 [==============================] - 6s 570us/step - loss: 0.0238 - acc: 0.4405 - val_loss: 0.0215 - val_acc: 0.4493\n",
      "Epoch 49/60\n",
      "10976/10976 [==============================] - 6s 541us/step - loss: 0.0233 - acc: 0.4472 - val_loss: 0.0250 - val_acc: 0.4416\n",
      "Epoch 50/60\n",
      "10976/10976 [==============================] - 6s 551us/step - loss: 0.0239 - acc: 0.4336 - val_loss: 0.0252 - val_acc: 0.4483\n",
      "Epoch 51/60\n",
      "10976/10976 [==============================] - 6s 544us/step - loss: 0.0234 - acc: 0.4119 - val_loss: 0.0212 - val_acc: 0.3661\n",
      "Epoch 52/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 6s 561us/step - loss: 0.0234 - acc: 0.4098 - val_loss: 0.0236 - val_acc: 0.4588\n",
      "Epoch 53/60\n",
      "10976/10976 [==============================] - 6s 566us/step - loss: 0.0240 - acc: 0.4242 - val_loss: 0.0204 - val_acc: 0.4618\n",
      "Epoch 54/60\n",
      "10976/10976 [==============================] - 6s 550us/step - loss: 0.0240 - acc: 0.4317 - val_loss: 0.0267 - val_acc: 0.3687\n",
      "Epoch 55/60\n",
      "10976/10976 [==============================] - 6s 572us/step - loss: 0.0236 - acc: 0.4149 - val_loss: 0.0211 - val_acc: 0.4683\n",
      "Epoch 56/60\n",
      "10976/10976 [==============================] - 6s 556us/step - loss: 0.0236 - acc: 0.4168 - val_loss: 0.0244 - val_acc: 0.3687\n",
      "Epoch 57/60\n",
      "10976/10976 [==============================] - 6s 514us/step - loss: 0.0235 - acc: 0.4172 - val_loss: 0.0195 - val_acc: 0.4701\n",
      "Epoch 58/60\n",
      "10976/10976 [==============================] - 6s 549us/step - loss: 0.0235 - acc: 0.4113 - val_loss: 0.0241 - val_acc: 0.3772\n",
      "Epoch 59/60\n",
      "10976/10976 [==============================] - 6s 553us/step - loss: 0.0234 - acc: 0.4140 - val_loss: 0.0259 - val_acc: 0.4602\n",
      "Epoch 60/60\n",
      "10976/10976 [==============================] - 6s 563us/step - loss: 0.0236 - acc: 0.4104 - val_loss: 0.0215 - val_acc: 0.3713\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 41/60\n",
      "10976/10976 [==============================] - 6s 574us/step - loss: 0.0289 - acc: 0.4398 - val_loss: 0.0283 - val_acc: 0.3549\n",
      "Epoch 42/60\n",
      "10976/10976 [==============================] - 6s 521us/step - loss: 0.0288 - acc: 0.4278 - val_loss: 0.0259 - val_acc: 0.5052\n",
      "Epoch 43/60\n",
      "10976/10976 [==============================] - 6s 566us/step - loss: 0.0284 - acc: 0.4293 - val_loss: 0.0279 - val_acc: 0.3735\n",
      "Epoch 44/60\n",
      "10976/10976 [==============================] - 6s 582us/step - loss: 0.0284 - acc: 0.4319 - val_loss: 0.0286 - val_acc: 0.4979\n",
      "Epoch 45/60\n",
      "10976/10976 [==============================] - 6s 549us/step - loss: 0.0284 - acc: 0.4392 - val_loss: 0.0282 - val_acc: 0.3399\n",
      "Epoch 46/60\n",
      "10976/10976 [==============================] - 6s 533us/step - loss: 0.0283 - acc: 0.4279 - val_loss: 0.0287 - val_acc: 0.5086\n",
      "Epoch 47/60\n",
      "10976/10976 [==============================] - 6s 567us/step - loss: 0.0284 - acc: 0.4274 - val_loss: 0.0287 - val_acc: 0.3821\n",
      "Epoch 48/60\n",
      "10976/10976 [==============================] - 6s 552us/step - loss: 0.0280 - acc: 0.4361 - val_loss: 0.0282 - val_acc: 0.5041\n",
      "Epoch 49/60\n",
      "10976/10976 [==============================] - 6s 548us/step - loss: 0.0280 - acc: 0.4368 - val_loss: 0.0276 - val_acc: 0.3834\n",
      "Epoch 50/60\n",
      "10976/10976 [==============================] - 6s 522us/step - loss: 0.0278 - acc: 0.4329 - val_loss: 0.0257 - val_acc: 0.5078\n",
      "Epoch 51/60\n",
      "10976/10976 [==============================] - 6s 572us/step - loss: 0.0277 - acc: 0.4491 - val_loss: 0.0280 - val_acc: 0.3407\n",
      "Epoch 52/60\n",
      "10976/10976 [==============================] - 6s 556us/step - loss: 0.0280 - acc: 0.4260 - val_loss: 0.0272 - val_acc: 0.5102\n",
      "Epoch 53/60\n",
      "10976/10976 [==============================] - 6s 537us/step - loss: 0.0279 - acc: 0.4478 - val_loss: 0.0268 - val_acc: 0.3757\n",
      "Epoch 54/60\n",
      "10976/10976 [==============================] - 6s 558us/step - loss: 0.0273 - acc: 0.4198 - val_loss: 0.0281 - val_acc: 0.5069\n",
      "Epoch 55/60\n",
      "10976/10976 [==============================] - 6s 568us/step - loss: 0.0280 - acc: 0.4437 - val_loss: 0.0287 - val_acc: 0.3886\n",
      "Epoch 56/60\n",
      "10976/10976 [==============================] - 6s 548us/step - loss: 0.0282 - acc: 0.4692 - val_loss: 0.0292 - val_acc: 0.4830\n",
      "Epoch 57/60\n",
      "10976/10976 [==============================] - 6s 557us/step - loss: 0.0274 - acc: 0.4628 - val_loss: 0.0289 - val_acc: 0.4457\n",
      "Epoch 58/60\n",
      "10976/10976 [==============================] - 6s 542us/step - loss: 0.0276 - acc: 0.4609 - val_loss: 0.0279 - val_acc: 0.4866\n",
      "Epoch 59/60\n",
      "10976/10976 [==============================] - 6s 559us/step - loss: 0.0285 - acc: 0.4692 - val_loss: 0.0296 - val_acc: 0.4615\n",
      "Epoch 60/60\n",
      "10976/10976 [==============================] - 6s 540us/step - loss: 0.0282 - acc: 0.4618 - val_loss: 0.0253 - val_acc: 0.4504\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 41/60\n",
      "10976/10976 [==============================] - 6s 545us/step - loss: 0.0220 - acc: 0.3199 - val_loss: 0.0214 - val_acc: 0.2042\n",
      "Epoch 42/60\n",
      "10976/10976 [==============================] - 6s 548us/step - loss: 0.0212 - acc: 0.3210 - val_loss: 0.0207 - val_acc: 0.3987\n",
      "Epoch 43/60\n",
      "10976/10976 [==============================] - 6s 529us/step - loss: 0.0214 - acc: 0.3191 - val_loss: 0.0217 - val_acc: 0.2074\n",
      "Epoch 44/60\n",
      "10976/10976 [==============================] - 6s 570us/step - loss: 0.0211 - acc: 0.3215 - val_loss: 0.0224 - val_acc: 0.4521\n",
      "Epoch 45/60\n",
      "10976/10976 [==============================] - 6s 507us/step - loss: 0.0210 - acc: 0.3239 - val_loss: 0.0200 - val_acc: 0.2109\n",
      "Epoch 46/60\n",
      "10976/10976 [==============================] - 6s 554us/step - loss: 0.0208 - acc: 0.3207 - val_loss: 0.0230 - val_acc: 0.4261\n",
      "Epoch 47/60\n",
      "10976/10976 [==============================] - 6s 577us/step - loss: 0.0207 - acc: 0.3277 - val_loss: 0.0180 - val_acc: 0.2099\n",
      "Epoch 48/60\n",
      "10976/10976 [==============================] - 6s 555us/step - loss: 0.0207 - acc: 0.3133 - val_loss: 0.0212 - val_acc: 0.4131\n",
      "Epoch 49/60\n",
      "10976/10976 [==============================] - 6s 533us/step - loss: 0.0207 - acc: 0.3216 - val_loss: 0.0176 - val_acc: 0.2100\n",
      "Epoch 50/60\n",
      "10976/10976 [==============================] - 6s 566us/step - loss: 0.0204 - acc: 0.3234 - val_loss: 0.0207 - val_acc: 0.4130\n",
      "Epoch 51/60\n",
      "10976/10976 [==============================] - 6s 553us/step - loss: 0.0203 - acc: 0.3258 - val_loss: 0.0199 - val_acc: 0.2077\n",
      "Epoch 52/60\n",
      "10976/10976 [==============================] - 6s 557us/step - loss: 0.0205 - acc: 0.3180 - val_loss: 0.0241 - val_acc: 0.4142\n",
      "Epoch 53/60\n",
      "10976/10976 [==============================] - 6s 536us/step - loss: 0.0207 - acc: 0.3175 - val_loss: 0.0181 - val_acc: 0.2080\n",
      "Epoch 54/60\n",
      "10976/10976 [==============================] - 6s 569us/step - loss: 0.0206 - acc: 0.3151 - val_loss: 0.0204 - val_acc: 0.4111\n",
      "Epoch 55/60\n",
      "10976/10976 [==============================] - 6s 573us/step - loss: 0.0202 - acc: 0.3240 - val_loss: 0.0174 - val_acc: 0.2155\n",
      "Epoch 56/60\n",
      "10976/10976 [==============================] - 6s 545us/step - loss: 0.0202 - acc: 0.3169 - val_loss: 0.0212 - val_acc: 0.4086\n",
      "Epoch 57/60\n",
      "10976/10976 [==============================] - 6s 532us/step - loss: 0.0207 - acc: 0.3266 - val_loss: 0.0193 - val_acc: 0.2125\n",
      "Epoch 58/60\n",
      "10976/10976 [==============================] - 6s 574us/step - loss: 0.0205 - acc: 0.3195 - val_loss: 0.0243 - val_acc: 0.4126\n",
      "Epoch 59/60\n",
      "10976/10976 [==============================] - 6s 538us/step - loss: 0.0204 - acc: 0.3311 - val_loss: 0.0178 - val_acc: 0.2127\n",
      "Epoch 60/60\n",
      "10976/10976 [==============================] - 6s 579us/step - loss: 0.0203 - acc: 0.3184 - val_loss: 0.0221 - val_acc: 0.4126\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 41/60\n",
      "10976/10976 [==============================] - 6s 555us/step - loss: 0.0194 - acc: 0.2421 - val_loss: 0.0193 - val_acc: 0.3768\n",
      "Epoch 42/60\n",
      "10976/10976 [==============================] - 6s 540us/step - loss: 0.0192 - acc: 0.2434 - val_loss: 0.0185 - val_acc: 0.1271\n",
      "Epoch 43/60\n",
      "10976/10976 [==============================] - 6s 551us/step - loss: 0.0197 - acc: 0.2426 - val_loss: 0.0205 - val_acc: 0.3689\n",
      "Epoch 44/60\n",
      "10976/10976 [==============================] - 6s 551us/step - loss: 0.0194 - acc: 0.2456 - val_loss: 0.0177 - val_acc: 0.1266\n",
      "Epoch 45/60\n",
      "10976/10976 [==============================] - 6s 549us/step - loss: 0.0192 - acc: 0.2423 - val_loss: 0.0200 - val_acc: 0.3635\n",
      "Epoch 46/60\n",
      "10976/10976 [==============================] - 6s 541us/step - loss: 0.0193 - acc: 0.2440 - val_loss: 0.0163 - val_acc: 0.1321\n",
      "Epoch 47/60\n",
      "10976/10976 [==============================] - 6s 570us/step - loss: 0.0184 - acc: 0.2418 - val_loss: 0.0196 - val_acc: 0.3754\n",
      "Epoch 48/60\n",
      "10976/10976 [==============================] - 6s 562us/step - loss: 0.0191 - acc: 0.2465 - val_loss: 0.0189 - val_acc: 0.1248\n",
      "Epoch 49/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 6s 561us/step - loss: 0.0189 - acc: 0.2437 - val_loss: 0.0197 - val_acc: 0.3744\n",
      "Epoch 50/60\n",
      "10976/10976 [==============================] - 6s 544us/step - loss: 0.0189 - acc: 0.2474 - val_loss: 0.0187 - val_acc: 0.1289\n",
      "Epoch 51/60\n",
      "10976/10976 [==============================] - 6s 559us/step - loss: 0.0186 - acc: 0.2671 - val_loss: 0.0172 - val_acc: 0.3307\n",
      "Epoch 52/60\n",
      "10976/10976 [==============================] - 6s 562us/step - loss: 0.0183 - acc: 0.2994 - val_loss: 0.0169 - val_acc: 0.2826\n",
      "Epoch 53/60\n",
      "10976/10976 [==============================] - 6s 517us/step - loss: 0.0182 - acc: 0.2997 - val_loss: 0.0187 - val_acc: 0.3227\n",
      "Epoch 54/60\n",
      "10976/10976 [==============================] - 6s 563us/step - loss: 0.0184 - acc: 0.3102 - val_loss: 0.0186 - val_acc: 0.2334\n",
      "Epoch 55/60\n",
      "10976/10976 [==============================] - 6s 570us/step - loss: 0.0182 - acc: 0.2875 - val_loss: 0.0194 - val_acc: 0.3654\n",
      "Epoch 56/60\n",
      "10976/10976 [==============================] - 6s 544us/step - loss: 0.0186 - acc: 0.2427 - val_loss: 0.0147 - val_acc: 0.1368\n",
      "Epoch 57/60\n",
      "10976/10976 [==============================] - 6s 575us/step - loss: 0.0181 - acc: 0.2400 - val_loss: 0.0189 - val_acc: 0.3704\n",
      "Epoch 58/60\n",
      "10976/10976 [==============================] - 6s 559us/step - loss: 0.0184 - acc: 0.2486 - val_loss: 0.0159 - val_acc: 0.1380\n",
      "Epoch 59/60\n",
      "10976/10976 [==============================] - 6s 568us/step - loss: 0.0182 - acc: 0.2421 - val_loss: 0.0182 - val_acc: 0.3731\n",
      "Epoch 60/60\n",
      "10976/10976 [==============================] - 6s 535us/step - loss: 0.0181 - acc: 0.2483 - val_loss: 0.0167 - val_acc: 0.1350\n",
      "start training round 3\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 61/80\n",
      "10976/10976 [==============================] - 6s 553us/step - loss: 0.0232 - acc: 0.4080 - val_loss: 0.0224 - val_acc: 0.4587\n",
      "Epoch 62/80\n",
      "10976/10976 [==============================] - 6s 545us/step - loss: 0.0235 - acc: 0.4088 - val_loss: 0.0216 - val_acc: 0.3657\n",
      "Epoch 63/80\n",
      "10976/10976 [==============================] - 6s 556us/step - loss: 0.0234 - acc: 0.4055 - val_loss: 0.0215 - val_acc: 0.4337\n",
      "Epoch 64/80\n",
      "10976/10976 [==============================] - 6s 532us/step - loss: 0.0232 - acc: 0.4058 - val_loss: 0.0216 - val_acc: 0.3669\n",
      "Epoch 65/80\n",
      "10976/10976 [==============================] - 6s 555us/step - loss: 0.0236 - acc: 0.4099 - val_loss: 0.0255 - val_acc: 0.4566\n",
      "Epoch 66/80\n",
      "10976/10976 [==============================] - 6s 556us/step - loss: 0.0233 - acc: 0.4084 - val_loss: 0.0214 - val_acc: 0.3690\n",
      "Epoch 67/80\n",
      "10976/10976 [==============================] - 6s 548us/step - loss: 0.0225 - acc: 0.4100 - val_loss: 0.0249 - val_acc: 0.4579\n",
      "Epoch 68/80\n",
      "10976/10976 [==============================] - 6s 573us/step - loss: 0.0229 - acc: 0.4103 - val_loss: 0.0230 - val_acc: 0.3644\n",
      "Epoch 69/80\n",
      "10976/10976 [==============================] - 6s 551us/step - loss: 0.0228 - acc: 0.4107 - val_loss: 0.0225 - val_acc: 0.3775\n",
      "Epoch 70/80\n",
      "10976/10976 [==============================] - 6s 564us/step - loss: 0.0232 - acc: 0.4142 - val_loss: 0.0222 - val_acc: 0.4565\n",
      "Epoch 71/80\n",
      "10976/10976 [==============================] - 6s 547us/step - loss: 0.0228 - acc: 0.4105 - val_loss: 0.0224 - val_acc: 0.3718\n",
      "Epoch 72/80\n",
      "10976/10976 [==============================] - 6s 575us/step - loss: 0.0227 - acc: 0.4105 - val_loss: 0.0261 - val_acc: 0.3900\n",
      "Epoch 73/80\n",
      "10976/10976 [==============================] - 6s 550us/step - loss: 0.0228 - acc: 0.4140 - val_loss: 0.0259 - val_acc: 0.4486\n",
      "Epoch 74/80\n",
      "10976/10976 [==============================] - 6s 532us/step - loss: 0.0228 - acc: 0.4127 - val_loss: 0.0205 - val_acc: 0.3843\n",
      "Epoch 75/80\n",
      "10976/10976 [==============================] - 6s 564us/step - loss: 0.0227 - acc: 0.4164 - val_loss: 0.0248 - val_acc: 0.4435\n",
      "Epoch 76/80\n",
      "10976/10976 [==============================] - 6s 574us/step - loss: 0.0225 - acc: 0.4153 - val_loss: 0.0210 - val_acc: 0.4097\n",
      "Epoch 77/80\n",
      "10976/10976 [==============================] - 6s 539us/step - loss: 0.0224 - acc: 0.4156 - val_loss: 0.0217 - val_acc: 0.4547\n",
      "Epoch 78/80\n",
      "10976/10976 [==============================] - 6s 558us/step - loss: 0.0228 - acc: 0.4159 - val_loss: 0.0218 - val_acc: 0.3814\n",
      "Epoch 79/80\n",
      "10976/10976 [==============================] - 6s 545us/step - loss: 0.0226 - acc: 0.4138 - val_loss: 0.0203 - val_acc: 0.3966\n",
      "Epoch 80/80\n",
      "10976/10976 [==============================] - 6s 511us/step - loss: 0.0223 - acc: 0.4180 - val_loss: 0.0197 - val_acc: 0.3834\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 61/80\n",
      "10976/10976 [==============================] - 6s 575us/step - loss: 0.0267 - acc: 0.4653 - val_loss: 0.0263 - val_acc: 0.4701\n",
      "Epoch 62/80\n",
      "10976/10976 [==============================] - 6s 551us/step - loss: 0.0273 - acc: 0.4637 - val_loss: 0.0288 - val_acc: 0.4963\n",
      "Epoch 63/80\n",
      "10976/10976 [==============================] - 6s 564us/step - loss: 0.0274 - acc: 0.4684 - val_loss: 0.0266 - val_acc: 0.4060\n",
      "Epoch 64/80\n",
      "10976/10976 [==============================] - 6s 544us/step - loss: 0.0276 - acc: 0.4619 - val_loss: 0.0265 - val_acc: 0.4909\n",
      "Epoch 65/80\n",
      "10976/10976 [==============================] - 6s 554us/step - loss: 0.0272 - acc: 0.4637 - val_loss: 0.0281 - val_acc: 0.4630\n",
      "Epoch 66/80\n",
      "10976/10976 [==============================] - 6s 559us/step - loss: 0.0271 - acc: 0.4645 - val_loss: 0.0273 - val_acc: 0.4900\n",
      "Epoch 67/80\n",
      "10976/10976 [==============================] - 6s 556us/step - loss: 0.0271 - acc: 0.4663 - val_loss: 0.0286 - val_acc: 0.4527\n",
      "Epoch 68/80\n",
      "10976/10976 [==============================] - 6s 574us/step - loss: 0.0271 - acc: 0.4625 - val_loss: 0.0253 - val_acc: 0.4945\n",
      "Epoch 69/80\n",
      "10976/10976 [==============================] - 6s 579us/step - loss: 0.0261 - acc: 0.4579 - val_loss: 0.0271 - val_acc: 0.4555\n",
      "Epoch 70/80\n",
      "10976/10976 [==============================] - 6s 538us/step - loss: 0.0266 - acc: 0.4602 - val_loss: 0.0283 - val_acc: 0.4824\n",
      "Epoch 71/80\n",
      "10976/10976 [==============================] - 6s 548us/step - loss: 0.0267 - acc: 0.4634 - val_loss: 0.0271 - val_acc: 0.4234\n",
      "Epoch 72/80\n",
      "10976/10976 [==============================] - 6s 538us/step - loss: 0.0260 - acc: 0.4253 - val_loss: 0.0261 - val_acc: 0.4143\n",
      "Epoch 73/80\n",
      "10976/10976 [==============================] - 6s 566us/step - loss: 0.0265 - acc: 0.4535 - val_loss: 0.0284 - val_acc: 0.4368\n",
      "Epoch 74/80\n",
      "10976/10976 [==============================] - 6s 550us/step - loss: 0.0266 - acc: 0.4704 - val_loss: 0.0263 - val_acc: 0.4881\n",
      "Epoch 75/80\n",
      "10976/10976 [==============================] - 6s 552us/step - loss: 0.0267 - acc: 0.4682 - val_loss: 0.0257 - val_acc: 0.4940\n",
      "Epoch 76/80\n",
      "10976/10976 [==============================] - 6s 563us/step - loss: 0.0264 - acc: 0.4612 - val_loss: 0.0250 - val_acc: 0.4598\n",
      "Epoch 77/80\n",
      "10976/10976 [==============================] - 6s 573us/step - loss: 0.0260 - acc: 0.4644 - val_loss: 0.0274 - val_acc: 0.4815\n",
      "Epoch 78/80\n",
      "10976/10976 [==============================] - 6s 552us/step - loss: 0.0264 - acc: 0.4602 - val_loss: 0.0255 - val_acc: 0.4151\n",
      "Epoch 79/80\n",
      "10976/10976 [==============================] - 6s 548us/step - loss: 0.0268 - acc: 0.4656 - val_loss: 0.0259 - val_acc: 0.5038\n",
      "Epoch 80/80\n",
      "10976/10976 [==============================] - 6s 542us/step - loss: 0.0263 - acc: 0.4713 - val_loss: 0.0256 - val_acc: 0.4117\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 61/80\n",
      "10976/10976 [==============================] - 6s 554us/step - loss: 0.0201 - acc: 0.3217 - val_loss: 0.0237 - val_acc: 0.4211\n",
      "Epoch 62/80\n",
      "10976/10976 [==============================] - 6s 561us/step - loss: 0.0205 - acc: 0.3217 - val_loss: 0.0212 - val_acc: 0.4199\n",
      "Epoch 63/80\n",
      "10976/10976 [==============================] - 6s 555us/step - loss: 0.0200 - acc: 0.3269 - val_loss: 0.0186 - val_acc: 0.2086\n",
      "Epoch 64/80\n",
      "10976/10976 [==============================] - 6s 567us/step - loss: 0.0202 - acc: 0.3244 - val_loss: 0.0228 - val_acc: 0.4847\n",
      "Epoch 65/80\n",
      "10976/10976 [==============================] - 6s 551us/step - loss: 0.0205 - acc: 0.3266 - val_loss: 0.0198 - val_acc: 0.2099\n",
      "Epoch 66/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 6s 566us/step - loss: 0.0205 - acc: 0.3219 - val_loss: 0.0214 - val_acc: 0.4118\n",
      "Epoch 67/80\n",
      "10976/10976 [==============================] - 6s 515us/step - loss: 0.0203 - acc: 0.3195 - val_loss: 0.0179 - val_acc: 0.2165\n",
      "Epoch 68/80\n",
      "10976/10976 [==============================] - 6s 550us/step - loss: 0.0202 - acc: 0.3244 - val_loss: 0.0212 - val_acc: 0.4159\n",
      "Epoch 69/80\n",
      "10976/10976 [==============================] - 7s 604us/step - loss: 0.0202 - acc: 0.3194 - val_loss: 0.0187 - val_acc: 0.2095\n",
      "Epoch 70/80\n",
      "10976/10976 [==============================] - 6s 582us/step - loss: 0.0199 - acc: 0.3917 - val_loss: 0.0182 - val_acc: 0.4154\n",
      "Epoch 71/80\n",
      "10976/10976 [==============================] - 6s 575us/step - loss: 0.0202 - acc: 0.4358 - val_loss: 0.0248 - val_acc: 0.4786\n",
      "Epoch 72/80\n",
      "10976/10976 [==============================] - 6s 569us/step - loss: 0.0202 - acc: 0.4234 - val_loss: 0.0142 - val_acc: 0.4895\n",
      "Epoch 73/80\n",
      "10976/10976 [==============================] - 7s 608us/step - loss: 0.0202 - acc: 0.4136 - val_loss: 0.0260 - val_acc: 0.4100\n",
      "Epoch 74/80\n",
      "10976/10976 [==============================] - 7s 605us/step - loss: 0.0203 - acc: 0.4208 - val_loss: 0.0152 - val_acc: 0.4900\n",
      "Epoch 75/80\n",
      "10976/10976 [==============================] - 7s 598us/step - loss: 0.0202 - acc: 0.3971 - val_loss: 0.0245 - val_acc: 0.4088\n",
      "Epoch 76/80\n",
      "10976/10976 [==============================] - 7s 602us/step - loss: 0.0202 - acc: 0.4122 - val_loss: 0.0160 - val_acc: 0.4048\n",
      "Epoch 77/80\n",
      "10976/10976 [==============================] - 7s 601us/step - loss: 0.0202 - acc: 0.4157 - val_loss: 0.0226 - val_acc: 0.3940\n",
      "Epoch 78/80\n",
      "10976/10976 [==============================] - 6s 578us/step - loss: 0.0202 - acc: 0.4399 - val_loss: 0.0160 - val_acc: 0.4898\n",
      "Epoch 79/80\n",
      "10976/10976 [==============================] - 6s 582us/step - loss: 0.0201 - acc: 0.4446 - val_loss: 0.0193 - val_acc: 0.4122\n",
      "Epoch 80/80\n",
      "10976/10976 [==============================] - 7s 597us/step - loss: 0.0200 - acc: 0.4696 - val_loss: 0.0142 - val_acc: 0.4136\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 61/80\n",
      "10976/10976 [==============================] - 6s 589us/step - loss: 0.0180 - acc: 0.2438 - val_loss: 0.0192 - val_acc: 0.3767\n",
      "Epoch 62/80\n",
      "10976/10976 [==============================] - 7s 598us/step - loss: 0.0183 - acc: 0.2520 - val_loss: 0.0169 - val_acc: 0.1295\n",
      "Epoch 63/80\n",
      "10976/10976 [==============================] - 7s 599us/step - loss: 0.0178 - acc: 0.2648 - val_loss: 0.0173 - val_acc: 0.2894\n",
      "Epoch 64/80\n",
      "10976/10976 [==============================] - 6s 576us/step - loss: 0.0179 - acc: 0.3026 - val_loss: 0.0169 - val_acc: 0.4210\n",
      "Epoch 65/80\n",
      "10976/10976 [==============================] - 6s 581us/step - loss: 0.0180 - acc: 0.3127 - val_loss: 0.0163 - val_acc: 0.3386\n",
      "Epoch 66/80\n",
      "10976/10976 [==============================] - 7s 595us/step - loss: 0.0177 - acc: 0.3285 - val_loss: 0.0180 - val_acc: 0.1952\n",
      "Epoch 67/80\n",
      "10976/10976 [==============================] - 6s 570us/step - loss: 0.0177 - acc: 0.3027 - val_loss: 0.0171 - val_acc: 0.3324\n",
      "Epoch 68/80\n",
      "10976/10976 [==============================] - 6s 571us/step - loss: 0.0176 - acc: 0.3206 - val_loss: 0.0179 - val_acc: 0.2443\n",
      "Epoch 69/80\n",
      "10976/10976 [==============================] - 7s 593us/step - loss: 0.0177 - acc: 0.3028 - val_loss: 0.0178 - val_acc: 0.3288\n",
      "Epoch 70/80\n",
      "10976/10976 [==============================] - 6s 592us/step - loss: 0.0178 - acc: 0.2710 - val_loss: 0.0165 - val_acc: 0.1346\n",
      "Epoch 71/80\n",
      "10976/10976 [==============================] - 6s 562us/step - loss: 0.0177 - acc: 0.2473 - val_loss: 0.0186 - val_acc: 0.3711\n",
      "Epoch 72/80\n",
      "10976/10976 [==============================] - 6s 588us/step - loss: 0.0178 - acc: 0.2510 - val_loss: 0.0157 - val_acc: 0.1366\n",
      "Epoch 73/80\n",
      "10976/10976 [==============================] - 7s 595us/step - loss: 0.0179 - acc: 0.2467 - val_loss: 0.0173 - val_acc: 0.3298\n",
      "Epoch 74/80\n",
      "10976/10976 [==============================] - 6s 557us/step - loss: 0.0172 - acc: 0.2757 - val_loss: 0.0171 - val_acc: 0.2565\n",
      "Epoch 75/80\n",
      "10976/10976 [==============================] - 6s 550us/step - loss: 0.0174 - acc: 0.3019 - val_loss: 0.0164 - val_acc: 0.3370\n",
      "Epoch 76/80\n",
      "10976/10976 [==============================] - 6s 577us/step - loss: 0.0175 - acc: 0.2864 - val_loss: 0.0165 - val_acc: 0.3620\n",
      "Epoch 77/80\n",
      "10976/10976 [==============================] - 7s 596us/step - loss: 0.0174 - acc: 0.2891 - val_loss: 0.0161 - val_acc: 0.3359\n",
      "Epoch 78/80\n",
      "10976/10976 [==============================] - 7s 595us/step - loss: 0.0174 - acc: 0.2950 - val_loss: 0.0172 - val_acc: 0.3876\n",
      "Epoch 79/80\n",
      "10976/10976 [==============================] - 6s 578us/step - loss: 0.0173 - acc: 0.3066 - val_loss: 0.0169 - val_acc: 0.3371\n",
      "Epoch 80/80\n",
      "10976/10976 [==============================] - 6s 577us/step - loss: 0.0175 - acc: 0.2975 - val_loss: 0.0167 - val_acc: 0.1326\n",
      "start training round 4\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 81/100\n",
      "10976/10976 [==============================] - 6s 582us/step - loss: 0.0222 - acc: 0.4164 - val_loss: 0.0264 - val_acc: 0.4597\n",
      "Epoch 82/100\n",
      "10976/10976 [==============================] - 6s 582us/step - loss: 0.0223 - acc: 0.4142 - val_loss: 0.0235 - val_acc: 0.3751\n",
      "Epoch 83/100\n",
      "10976/10976 [==============================] - 6s 578us/step - loss: 0.0230 - acc: 0.4102 - val_loss: 0.0276 - val_acc: 0.4309\n",
      "Epoch 84/100\n",
      "10976/10976 [==============================] - 6s 573us/step - loss: 0.0237 - acc: 0.4124 - val_loss: 0.0236 - val_acc: 0.3795\n",
      "Epoch 85/100\n",
      "10976/10976 [==============================] - 6s 584us/step - loss: 0.0231 - acc: 0.4103 - val_loss: 0.0223 - val_acc: 0.4397\n",
      "Epoch 86/100\n",
      "10976/10976 [==============================] - 6s 584us/step - loss: 0.0226 - acc: 0.4147 - val_loss: 0.0199 - val_acc: 0.3839\n",
      "Epoch 87/100\n",
      "10976/10976 [==============================] - 6s 579us/step - loss: 0.0224 - acc: 0.4156 - val_loss: 0.0236 - val_acc: 0.4509\n",
      "Epoch 88/100\n",
      "10976/10976 [==============================] - 6s 563us/step - loss: 0.0222 - acc: 0.4197 - val_loss: 0.0226 - val_acc: 0.3792\n",
      "Epoch 89/100\n",
      "10976/10976 [==============================] - 6s 586us/step - loss: 0.0222 - acc: 0.4144 - val_loss: 0.0228 - val_acc: 0.4412\n",
      "Epoch 90/100\n",
      "10976/10976 [==============================] - 6s 581us/step - loss: 0.0226 - acc: 0.4140 - val_loss: 0.0204 - val_acc: 0.3864\n",
      "Epoch 91/100\n",
      "10976/10976 [==============================] - 6s 581us/step - loss: 0.0219 - acc: 0.4161 - val_loss: 0.0232 - val_acc: 0.4317\n",
      "Epoch 92/100\n",
      "10976/10976 [==============================] - 6s 581us/step - loss: 0.0219 - acc: 0.4134 - val_loss: 0.0206 - val_acc: 0.3852\n",
      "Epoch 93/100\n",
      "10976/10976 [==============================] - 6s 590us/step - loss: 0.0223 - acc: 0.4175 - val_loss: 0.0223 - val_acc: 0.4475\n",
      "Epoch 94/100\n",
      "10976/10976 [==============================] - 6s 565us/step - loss: 0.0225 - acc: 0.4171 - val_loss: 0.0232 - val_acc: 0.3826\n",
      "Epoch 95/100\n",
      "10976/10976 [==============================] - 6s 573us/step - loss: 0.0222 - acc: 0.4124 - val_loss: 0.0209 - val_acc: 0.4459\n",
      "Epoch 96/100\n",
      "10976/10976 [==============================] - 7s 594us/step - loss: 0.0222 - acc: 0.4054 - val_loss: 0.0224 - val_acc: 0.3643\n",
      "Epoch 97/100\n",
      "10976/10976 [==============================] - 7s 595us/step - loss: 0.0218 - acc: 0.4082 - val_loss: 0.0223 - val_acc: 0.4622\n",
      "Epoch 98/100\n",
      "10976/10976 [==============================] - 7s 592us/step - loss: 0.0217 - acc: 0.4098 - val_loss: 0.0225 - val_acc: 0.3568\n",
      "Epoch 99/100\n",
      "10976/10976 [==============================] - 6s 579us/step - loss: 0.0219 - acc: 0.4106 - val_loss: 0.0217 - val_acc: 0.4397\n",
      "Epoch 100/100\n",
      "10976/10976 [==============================] - 6s 576us/step - loss: 0.0225 - acc: 0.4101 - val_loss: 0.0230 - val_acc: 0.4543\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 81/100\n",
      "10976/10976 [==============================] - 6s 566us/step - loss: 0.0263 - acc: 0.4620 - val_loss: 0.0251 - val_acc: 0.5059\n",
      "Epoch 82/100\n",
      "10976/10976 [==============================] - 6s 579us/step - loss: 0.0264 - acc: 0.4623 - val_loss: 0.0262 - val_acc: 0.4297\n",
      "Epoch 83/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 6s 570us/step - loss: 0.0263 - acc: 0.4703 - val_loss: 0.0259 - val_acc: 0.4899\n",
      "Epoch 84/100\n",
      "10976/10976 [==============================] - 6s 544us/step - loss: 0.0259 - acc: 0.4575 - val_loss: 0.0262 - val_acc: 0.3850\n",
      "Epoch 85/100\n",
      "10976/10976 [==============================] - 6s 577us/step - loss: 0.0258 - acc: 0.4417 - val_loss: 0.0266 - val_acc: 0.5152\n",
      "Epoch 86/100\n",
      "10976/10976 [==============================] - 6s 550us/step - loss: 0.0260 - acc: 0.4476 - val_loss: 0.0238 - val_acc: 0.4624\n",
      "Epoch 87/100\n",
      "10976/10976 [==============================] - 6s 556us/step - loss: 0.0260 - acc: 0.4501 - val_loss: 0.0243 - val_acc: 0.5068\n",
      "Epoch 88/100\n",
      "10976/10976 [==============================] - 6s 574us/step - loss: 0.0260 - acc: 0.4695 - val_loss: 0.0272 - val_acc: 0.5163\n",
      "Epoch 89/100\n",
      "10976/10976 [==============================] - 6s 540us/step - loss: 0.0263 - acc: 0.4639 - val_loss: 0.0264 - val_acc: 0.4984\n",
      "Epoch 90/100\n",
      "10976/10976 [==============================] - 6s 530us/step - loss: 0.0264 - acc: 0.4739 - val_loss: 0.0256 - val_acc: 0.4875\n",
      "Epoch 91/100\n",
      "10976/10976 [==============================] - 6s 558us/step - loss: 0.0261 - acc: 0.4700 - val_loss: 0.0255 - val_acc: 0.5143\n",
      "Epoch 92/100\n",
      "10976/10976 [==============================] - 6s 532us/step - loss: 0.0256 - acc: 0.4554 - val_loss: 0.0259 - val_acc: 0.4808\n",
      "Epoch 93/100\n",
      "10976/10976 [==============================] - 6s 537us/step - loss: 0.0255 - acc: 0.4591 - val_loss: 0.0254 - val_acc: 0.5077\n",
      "Epoch 94/100\n",
      "10976/10976 [==============================] - 6s 549us/step - loss: 0.0256 - acc: 0.4622 - val_loss: 0.0263 - val_acc: 0.4411\n",
      "Epoch 95/100\n",
      "10976/10976 [==============================] - 6s 541us/step - loss: 0.0258 - acc: 0.4732 - val_loss: 0.0249 - val_acc: 0.5052\n",
      "Epoch 96/100\n",
      "10976/10976 [==============================] - 6s 549us/step - loss: 0.0259 - acc: 0.4708 - val_loss: 0.0256 - val_acc: 0.4553\n",
      "Epoch 97/100\n",
      "10976/10976 [==============================] - 6s 555us/step - loss: 0.0256 - acc: 0.4739 - val_loss: 0.0269 - val_acc: 0.5018\n",
      "Epoch 98/100\n",
      "10976/10976 [==============================] - 6s 569us/step - loss: 0.0253 - acc: 0.4664 - val_loss: 0.0259 - val_acc: 0.4436\n",
      "Epoch 99/100\n",
      "10976/10976 [==============================] - 6s 564us/step - loss: 0.0259 - acc: 0.4740 - val_loss: 0.0247 - val_acc: 0.4993\n",
      "Epoch 100/100\n",
      "10976/10976 [==============================] - 6s 565us/step - loss: 0.0254 - acc: 0.4624 - val_loss: 0.0269 - val_acc: 0.4579\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 81/100\n",
      "10976/10976 [==============================] - 6s 560us/step - loss: 0.0199 - acc: 0.4472 - val_loss: 0.0253 - val_acc: 0.2628\n",
      "Epoch 82/100\n",
      "10976/10976 [==============================] - 6s 539us/step - loss: 0.0202 - acc: 0.4230 - val_loss: 0.0159 - val_acc: 0.4881\n",
      "Epoch 83/100\n",
      "10976/10976 [==============================] - 6s 567us/step - loss: 0.0197 - acc: 0.4203 - val_loss: 0.0256 - val_acc: 0.3997\n",
      "Epoch 84/100\n",
      "10976/10976 [==============================] - 6s 548us/step - loss: 0.0200 - acc: 0.4177 - val_loss: 0.0157 - val_acc: 0.4885\n",
      "Epoch 85/100\n",
      "10976/10976 [==============================] - 6s 568us/step - loss: 0.0195 - acc: 0.3739 - val_loss: 0.0181 - val_acc: 0.2086\n",
      "Epoch 86/100\n",
      "10976/10976 [==============================] - 6s 578us/step - loss: 0.0196 - acc: 0.3330 - val_loss: 0.0179 - val_acc: 0.4859\n",
      "Epoch 87/100\n",
      "10976/10976 [==============================] - 6s 559us/step - loss: 0.0199 - acc: 0.3275 - val_loss: 0.0174 - val_acc: 0.2137\n",
      "Epoch 88/100\n",
      "10976/10976 [==============================] - 6s 538us/step - loss: 0.0198 - acc: 0.3144 - val_loss: 0.0221 - val_acc: 0.4156\n",
      "Epoch 89/100\n",
      "10976/10976 [==============================] - 6s 561us/step - loss: 0.0198 - acc: 0.3242 - val_loss: 0.0171 - val_acc: 0.2140\n",
      "Epoch 90/100\n",
      "10976/10976 [==============================] - 6s 556us/step - loss: 0.0201 - acc: 0.3235 - val_loss: 0.0260 - val_acc: 0.4059\n",
      "Epoch 91/100\n",
      "10976/10976 [==============================] - 6s 544us/step - loss: 0.0202 - acc: 0.4233 - val_loss: 0.0133 - val_acc: 0.4180\n",
      "Epoch 92/100\n",
      "10976/10976 [==============================] - 6s 541us/step - loss: 0.0199 - acc: 0.4332 - val_loss: 0.0262 - val_acc: 0.2786\n",
      "Epoch 93/100\n",
      "10976/10976 [==============================] - 6s 569us/step - loss: 0.0199 - acc: 0.4373 - val_loss: 0.0158 - val_acc: 0.4156\n",
      "Epoch 94/100\n",
      "10976/10976 [==============================] - 6s 561us/step - loss: 0.0197 - acc: 0.4459 - val_loss: 0.0255 - val_acc: 0.4081\n",
      "Epoch 95/100\n",
      "10976/10976 [==============================] - 6s 554us/step - loss: 0.0200 - acc: 0.4186 - val_loss: 0.0193 - val_acc: 0.4178\n",
      "Epoch 96/100\n",
      "10976/10976 [==============================] - 6s 548us/step - loss: 0.0200 - acc: 0.4330 - val_loss: 0.0250 - val_acc: 0.4116\n",
      "Epoch 97/100\n",
      "10976/10976 [==============================] - 6s 558us/step - loss: 0.0197 - acc: 0.4324 - val_loss: 0.0142 - val_acc: 0.4202\n",
      "Epoch 98/100\n",
      "10976/10976 [==============================] - 6s 549us/step - loss: 0.0195 - acc: 0.4188 - val_loss: 0.0250 - val_acc: 0.4132\n",
      "Epoch 99/100\n",
      "10976/10976 [==============================] - 6s 544us/step - loss: 0.0196 - acc: 0.4193 - val_loss: 0.0151 - val_acc: 0.4169\n",
      "Epoch 100/100\n",
      "10976/10976 [==============================] - 6s 550us/step - loss: 0.0197 - acc: 0.4369 - val_loss: 0.0227 - val_acc: 0.4774\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 81/100\n",
      "10976/10976 [==============================] - 6s 541us/step - loss: 0.0178 - acc: 0.2471 - val_loss: 0.0208 - val_acc: 0.3751\n",
      "Epoch 82/100\n",
      "10976/10976 [==============================] - 6s 581us/step - loss: 0.0179 - acc: 0.2540 - val_loss: 0.0159 - val_acc: 0.1365\n",
      "Epoch 83/100\n",
      "10976/10976 [==============================] - 6s 554us/step - loss: 0.0175 - acc: 0.2482 - val_loss: 0.0191 - val_acc: 0.3808\n",
      "Epoch 84/100\n",
      "10976/10976 [==============================] - 6s 539us/step - loss: 0.0176 - acc: 0.2547 - val_loss: 0.0159 - val_acc: 0.1340\n",
      "Epoch 85/100\n",
      "10976/10976 [==============================] - 6s 560us/step - loss: 0.0175 - acc: 0.2511 - val_loss: 0.0174 - val_acc: 0.3752\n",
      "Epoch 86/100\n",
      "10976/10976 [==============================] - 6s 566us/step - loss: 0.0174 - acc: 0.2498 - val_loss: 0.0155 - val_acc: 0.1357\n",
      "Epoch 87/100\n",
      "10976/10976 [==============================] - 6s 559us/step - loss: 0.0170 - acc: 0.2491 - val_loss: 0.0187 - val_acc: 0.3580\n",
      "Epoch 88/100\n",
      "10976/10976 [==============================] - 6s 568us/step - loss: 0.0171 - acc: 0.2570 - val_loss: 0.0153 - val_acc: 0.1359\n",
      "Epoch 89/100\n",
      "10976/10976 [==============================] - 6s 571us/step - loss: 0.0175 - acc: 0.2489 - val_loss: 0.0157 - val_acc: 0.3641\n",
      "Epoch 90/100\n",
      "10976/10976 [==============================] - 6s 541us/step - loss: 0.0170 - acc: 0.2926 - val_loss: 0.0163 - val_acc: 0.2957\n",
      "Epoch 91/100\n",
      "10976/10976 [==============================] - 6s 557us/step - loss: 0.0172 - acc: 0.3177 - val_loss: 0.0157 - val_acc: 0.3364\n",
      "Epoch 92/100\n",
      "10976/10976 [==============================] - 6s 554us/step - loss: 0.0171 - acc: 0.3200 - val_loss: 0.0168 - val_acc: 0.3532\n",
      "Epoch 93/100\n",
      "10976/10976 [==============================] - 6s 536us/step - loss: 0.0171 - acc: 0.3189 - val_loss: 0.0173 - val_acc: 0.3090\n",
      "Epoch 94/100\n",
      "10976/10976 [==============================] - 6s 573us/step - loss: 0.0169 - acc: 0.3208 - val_loss: 0.0163 - val_acc: 0.3507\n",
      "Epoch 95/100\n",
      "10976/10976 [==============================] - 6s 558us/step - loss: 0.0170 - acc: 0.3251 - val_loss: 0.0172 - val_acc: 0.3289\n",
      "Epoch 96/100\n",
      "10976/10976 [==============================] - 6s 575us/step - loss: 0.0171 - acc: 0.3317 - val_loss: 0.0167 - val_acc: 0.3254\n",
      "Epoch 97/100\n",
      "10976/10976 [==============================] - 6s 540us/step - loss: 0.0170 - acc: 0.3216 - val_loss: 0.0160 - val_acc: 0.2308\n",
      "Epoch 98/100\n",
      "10976/10976 [==============================] - 6s 540us/step - loss: 0.0169 - acc: 0.3221 - val_loss: 0.0153 - val_acc: 0.2662\n",
      "Epoch 99/100\n",
      "10976/10976 [==============================] - 6s 522us/step - loss: 0.0170 - acc: 0.3205 - val_loss: 0.0176 - val_acc: 0.3871\n",
      "Epoch 100/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 6s 557us/step - loss: 0.0171 - acc: 0.3072 - val_loss: 0.0168 - val_acc: 0.2662\n",
      "start training round 5\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 101/120\n",
      "10976/10976 [==============================] - 6s 548us/step - loss: 0.0216 - acc: 0.4184 - val_loss: 0.0204 - val_acc: 0.3879\n",
      "Epoch 102/120\n",
      "10976/10976 [==============================] - 6s 522us/step - loss: 0.0219 - acc: 0.4141 - val_loss: 0.0226 - val_acc: 0.4514\n",
      "Epoch 103/120\n",
      "10976/10976 [==============================] - 6s 535us/step - loss: 0.0225 - acc: 0.4171 - val_loss: 0.0218 - val_acc: 0.4287\n",
      "Epoch 104/120\n",
      "10976/10976 [==============================] - 6s 548us/step - loss: 0.0222 - acc: 0.4174 - val_loss: 0.0208 - val_acc: 0.3882\n",
      "Epoch 105/120\n",
      "10976/10976 [==============================] - 6s 529us/step - loss: 0.0219 - acc: 0.4169 - val_loss: 0.0234 - val_acc: 0.4396\n",
      "Epoch 106/120\n",
      "10976/10976 [==============================] - 6s 548us/step - loss: 0.0222 - acc: 0.4187 - val_loss: 0.0218 - val_acc: 0.3852\n",
      "Epoch 107/120\n",
      "10976/10976 [==============================] - 6s 564us/step - loss: 0.0223 - acc: 0.4155 - val_loss: 0.0226 - val_acc: 0.4505\n",
      "Epoch 108/120\n",
      "10976/10976 [==============================] - 6s 549us/step - loss: 0.0223 - acc: 0.4161 - val_loss: 0.0210 - val_acc: 0.3875\n",
      "Epoch 109/120\n",
      "10976/10976 [==============================] - 6s 531us/step - loss: 0.0218 - acc: 0.4197 - val_loss: 0.0231 - val_acc: 0.4490\n",
      "Epoch 110/120\n",
      "10976/10976 [==============================] - 6s 530us/step - loss: 0.0216 - acc: 0.4224 - val_loss: 0.0195 - val_acc: 0.3867\n",
      "Epoch 111/120\n",
      "10976/10976 [==============================] - 6s 545us/step - loss: 0.0214 - acc: 0.4211 - val_loss: 0.0226 - val_acc: 0.4610\n",
      "Epoch 112/120\n",
      "10976/10976 [==============================] - 6s 537us/step - loss: 0.0214 - acc: 0.4194 - val_loss: 0.0209 - val_acc: 0.3968\n",
      "Epoch 113/120\n",
      "10976/10976 [==============================] - 6s 544us/step - loss: 0.0214 - acc: 0.4184 - val_loss: 0.0237 - val_acc: 0.4387\n",
      "Epoch 114/120\n",
      "10976/10976 [==============================] - 6s 535us/step - loss: 0.0218 - acc: 0.4215 - val_loss: 0.0226 - val_acc: 0.3877\n",
      "Epoch 115/120\n",
      "10976/10976 [==============================] - 6s 533us/step - loss: 0.0214 - acc: 0.4225 - val_loss: 0.0226 - val_acc: 0.4589\n",
      "Epoch 116/120\n",
      "10976/10976 [==============================] - 6s 531us/step - loss: 0.0217 - acc: 0.4183 - val_loss: 0.0210 - val_acc: 0.3916\n",
      "Epoch 117/120\n",
      "10976/10976 [==============================] - 6s 533us/step - loss: 0.0214 - acc: 0.4201 - val_loss: 0.0202 - val_acc: 0.4319\n",
      "Epoch 118/120\n",
      "10976/10976 [==============================] - 6s 542us/step - loss: 0.0215 - acc: 0.4201 - val_loss: 0.0203 - val_acc: 0.3955\n",
      "Epoch 119/120\n",
      "10976/10976 [==============================] - 6s 537us/step - loss: 0.0216 - acc: 0.4196 - val_loss: 0.0233 - val_acc: 0.4562\n",
      "Epoch 120/120\n",
      "10976/10976 [==============================] - 6s 534us/step - loss: 0.0216 - acc: 0.4205 - val_loss: 0.0212 - val_acc: 0.3907\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 101/120\n",
      "10976/10976 [==============================] - 6s 525us/step - loss: 0.0254 - acc: 0.4287 - val_loss: 0.0247 - val_acc: 0.5051\n",
      "Epoch 102/120\n",
      "10976/10976 [==============================] - 6s 528us/step - loss: 0.0252 - acc: 0.4729 - val_loss: 0.0268 - val_acc: 0.5202\n",
      "Epoch 103/120\n",
      "10976/10976 [==============================] - 6s 541us/step - loss: 0.0254 - acc: 0.4684 - val_loss: 0.0256 - val_acc: 0.3739\n",
      "Epoch 104/120\n",
      "10976/10976 [==============================] - 6s 524us/step - loss: 0.0254 - acc: 0.4653 - val_loss: 0.0257 - val_acc: 0.4701\n",
      "Epoch 105/120\n",
      "10976/10976 [==============================] - 6s 545us/step - loss: 0.0256 - acc: 0.4321 - val_loss: 0.0251 - val_acc: 0.4004\n",
      "Epoch 106/120\n",
      "10976/10976 [==============================] - 6s 549us/step - loss: 0.0252 - acc: 0.4277 - val_loss: 0.0265 - val_acc: 0.4538\n",
      "Epoch 107/120\n",
      "10976/10976 [==============================] - 6s 508us/step - loss: 0.0253 - acc: 0.4149 - val_loss: 0.0247 - val_acc: 0.4794\n",
      "Epoch 108/120\n",
      "10976/10976 [==============================] - 6s 543us/step - loss: 0.0252 - acc: 0.4635 - val_loss: 0.0239 - val_acc: 0.4572\n",
      "Epoch 109/120\n",
      "10976/10976 [==============================] - 6s 558us/step - loss: 0.0250 - acc: 0.4661 - val_loss: 0.0282 - val_acc: 0.4827\n",
      "Epoch 110/120\n",
      "10976/10976 [==============================] - 6s 536us/step - loss: 0.0251 - acc: 0.4426 - val_loss: 0.0261 - val_acc: 0.4636\n",
      "Epoch 111/120\n",
      "10976/10976 [==============================] - 6s 533us/step - loss: 0.0256 - acc: 0.4166 - val_loss: 0.0257 - val_acc: 0.4949\n",
      "Epoch 112/120\n",
      "10976/10976 [==============================] - 6s 528us/step - loss: 0.0255 - acc: 0.4173 - val_loss: 0.0257 - val_acc: 0.3507\n",
      "Epoch 113/120\n",
      "10976/10976 [==============================] - 6s 519us/step - loss: 0.0252 - acc: 0.4092 - val_loss: 0.0256 - val_acc: 0.4763\n",
      "Epoch 114/120\n",
      "10976/10976 [==============================] - 6s 526us/step - loss: 0.0251 - acc: 0.4174 - val_loss: 0.0243 - val_acc: 0.4211\n",
      "Epoch 115/120\n",
      "10976/10976 [==============================] - 6s 531us/step - loss: 0.0246 - acc: 0.4426 - val_loss: 0.0244 - val_acc: 0.5219\n",
      "Epoch 116/120\n",
      "10976/10976 [==============================] - 6s 520us/step - loss: 0.0252 - acc: 0.4260 - val_loss: 0.0246 - val_acc: 0.3667\n",
      "Epoch 117/120\n",
      "10976/10976 [==============================] - 6s 531us/step - loss: 0.0249 - acc: 0.4161 - val_loss: 0.0248 - val_acc: 0.4293\n",
      "Epoch 118/120\n",
      "10976/10976 [==============================] - 6s 536us/step - loss: 0.0249 - acc: 0.4150 - val_loss: 0.0251 - val_acc: 0.4559\n",
      "Epoch 119/120\n",
      "10976/10976 [==============================] - 6s 511us/step - loss: 0.0245 - acc: 0.4517 - val_loss: 0.0253 - val_acc: 0.4711\n",
      "Epoch 120/120\n",
      "10976/10976 [==============================] - 6s 523us/step - loss: 0.0248 - acc: 0.4233 - val_loss: 0.0256 - val_acc: 0.4503\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 101/120\n",
      "10976/10976 [==============================] - 6s 536us/step - loss: 0.0195 - acc: 0.4115 - val_loss: 0.0141 - val_acc: 0.4872\n",
      "Epoch 102/120\n",
      "10976/10976 [==============================] - 6s 538us/step - loss: 0.0196 - acc: 0.4461 - val_loss: 0.0242 - val_acc: 0.4061\n",
      "Epoch 103/120\n",
      "10976/10976 [==============================] - 6s 511us/step - loss: 0.0195 - acc: 0.4176 - val_loss: 0.0149 - val_acc: 0.4128\n",
      "Epoch 104/120\n",
      "10976/10976 [==============================] - 6s 542us/step - loss: 0.0196 - acc: 0.4231 - val_loss: 0.0247 - val_acc: 0.2653\n",
      "Epoch 105/120\n",
      "10976/10976 [==============================] - 6s 530us/step - loss: 0.0196 - acc: 0.4218 - val_loss: 0.0137 - val_acc: 0.4904\n",
      "Epoch 106/120\n",
      "10976/10976 [==============================] - 6s 525us/step - loss: 0.0196 - acc: 0.4422 - val_loss: 0.0250 - val_acc: 0.4046\n",
      "Epoch 107/120\n",
      "10976/10976 [==============================] - 6s 552us/step - loss: 0.0198 - acc: 0.4267 - val_loss: 0.0179 - val_acc: 0.4166\n",
      "Epoch 108/120\n",
      "10976/10976 [==============================] - 6s 521us/step - loss: 0.0196 - acc: 0.4297 - val_loss: 0.0159 - val_acc: 0.4189\n",
      "Epoch 109/120\n",
      "10976/10976 [==============================] - 6s 522us/step - loss: 0.0198 - acc: 0.4519 - val_loss: 0.0172 - val_acc: 0.4069\n",
      "Epoch 110/120\n",
      "10976/10976 [==============================] - 6s 539us/step - loss: 0.0195 - acc: 0.4389 - val_loss: 0.0266 - val_acc: 0.4063\n",
      "Epoch 111/120\n",
      "10976/10976 [==============================] - 6s 524us/step - loss: 0.0199 - acc: 0.4281 - val_loss: 0.0159 - val_acc: 0.4880\n",
      "Epoch 112/120\n",
      "10976/10976 [==============================] - 5s 500us/step - loss: 0.0196 - acc: 0.4266 - val_loss: 0.0249 - val_acc: 0.4170\n",
      "Epoch 113/120\n",
      "10976/10976 [==============================] - 6s 530us/step - loss: 0.0193 - acc: 0.4262 - val_loss: 0.0145 - val_acc: 0.4904\n",
      "Epoch 114/120\n",
      "10976/10976 [==============================] - 6s 521us/step - loss: 0.0189 - acc: 0.4319 - val_loss: 0.0229 - val_acc: 0.4792\n",
      "Epoch 115/120\n",
      "10976/10976 [==============================] - 6s 526us/step - loss: 0.0194 - acc: 0.4466 - val_loss: 0.0140 - val_acc: 0.4902\n",
      "Epoch 116/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 5s 489us/step - loss: 0.0194 - acc: 0.3942 - val_loss: 0.0259 - val_acc: 0.4043\n",
      "Epoch 117/120\n",
      "10976/10976 [==============================] - 6s 519us/step - loss: 0.0196 - acc: 0.4165 - val_loss: 0.0146 - val_acc: 0.4896\n",
      "Epoch 118/120\n",
      "10976/10976 [==============================] - 6s 501us/step - loss: 0.0193 - acc: 0.4310 - val_loss: 0.0252 - val_acc: 0.4117\n",
      "Epoch 119/120\n",
      "10976/10976 [==============================] - 6s 526us/step - loss: 0.0196 - acc: 0.4267 - val_loss: 0.0135 - val_acc: 0.4921\n",
      "Epoch 120/120\n",
      "10976/10976 [==============================] - 6s 540us/step - loss: 0.0192 - acc: 0.4394 - val_loss: 0.0259 - val_acc: 0.4901\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 101/120\n",
      "10976/10976 [==============================] - 5s 498us/step - loss: 0.0168 - acc: 0.3173 - val_loss: 0.0177 - val_acc: 0.3900\n",
      "Epoch 102/120\n",
      "10976/10976 [==============================] - 6s 504us/step - loss: 0.0169 - acc: 0.3109 - val_loss: 0.0142 - val_acc: 0.2686\n",
      "Epoch 103/120\n",
      "10976/10976 [==============================] - 6s 527us/step - loss: 0.0170 - acc: 0.3155 - val_loss: 0.0157 - val_acc: 0.3374\n",
      "Epoch 104/120\n",
      "10976/10976 [==============================] - 6s 512us/step - loss: 0.0167 - acc: 0.3228 - val_loss: 0.0163 - val_acc: 0.3831\n",
      "Epoch 105/120\n",
      "10976/10976 [==============================] - 6s 516us/step - loss: 0.0169 - acc: 0.3288 - val_loss: 0.0163 - val_acc: 0.3326\n",
      "Epoch 106/120\n",
      "10976/10976 [==============================] - 6s 522us/step - loss: 0.0167 - acc: 0.3222 - val_loss: 0.0161 - val_acc: 0.3463\n",
      "Epoch 107/120\n",
      "10976/10976 [==============================] - 6s 535us/step - loss: 0.0169 - acc: 0.3227 - val_loss: 0.0165 - val_acc: 0.3082\n",
      "Epoch 108/120\n",
      "10976/10976 [==============================] - 6s 510us/step - loss: 0.0167 - acc: 0.3145 - val_loss: 0.0166 - val_acc: 0.4072\n",
      "Epoch 109/120\n",
      "10976/10976 [==============================] - 6s 509us/step - loss: 0.0169 - acc: 0.3260 - val_loss: 0.0163 - val_acc: 0.3337\n",
      "Epoch 110/120\n",
      "10976/10976 [==============================] - 6s 562us/step - loss: 0.0167 - acc: 0.3267 - val_loss: 0.0160 - val_acc: 0.3766\n",
      "Epoch 111/120\n",
      "10976/10976 [==============================] - 6s 512us/step - loss: 0.0168 - acc: 0.3208 - val_loss: 0.0165 - val_acc: 0.3011\n",
      "Epoch 112/120\n",
      "10976/10976 [==============================] - 6s 532us/step - loss: 0.0167 - acc: 0.3279 - val_loss: 0.0145 - val_acc: 0.3079\n",
      "Epoch 113/120\n",
      "10976/10976 [==============================] - 6s 526us/step - loss: 0.0166 - acc: 0.3245 - val_loss: 0.0157 - val_acc: 0.3149\n",
      "Epoch 114/120\n",
      "10976/10976 [==============================] - 6s 517us/step - loss: 0.0167 - acc: 0.3240 - val_loss: 0.0145 - val_acc: 0.3421\n",
      "Epoch 115/120\n",
      "10976/10976 [==============================] - 6s 523us/step - loss: 0.0167 - acc: 0.3324 - val_loss: 0.0157 - val_acc: 0.3345\n",
      "Epoch 116/120\n",
      "10976/10976 [==============================] - 6s 546us/step - loss: 0.0166 - acc: 0.3327 - val_loss: 0.0163 - val_acc: 0.3877\n",
      "Epoch 117/120\n",
      "10976/10976 [==============================] - 6s 525us/step - loss: 0.0167 - acc: 0.3299 - val_loss: 0.0162 - val_acc: 0.3369\n",
      "Epoch 118/120\n",
      "10976/10976 [==============================] - 6s 505us/step - loss: 0.0167 - acc: 0.3333 - val_loss: 0.0123 - val_acc: 0.2997\n",
      "Epoch 119/120\n",
      "10976/10976 [==============================] - 6s 548us/step - loss: 0.0166 - acc: 0.3360 - val_loss: 0.0160 - val_acc: 0.3281\n",
      "Epoch 120/120\n",
      "10976/10976 [==============================] - 6s 547us/step - loss: 0.0168 - acc: 0.3317 - val_loss: 0.0121 - val_acc: 0.3084\n",
      "start training round 6\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 121/140\n",
      "10976/10976 [==============================] - 6s 543us/step - loss: 0.0221 - acc: 0.4191 - val_loss: 0.0220 - val_acc: 0.4539\n",
      "Epoch 122/140\n",
      "10976/10976 [==============================] - 6s 516us/step - loss: 0.0218 - acc: 0.4174 - val_loss: 0.0225 - val_acc: 0.3887\n",
      "Epoch 123/140\n",
      "10976/10976 [==============================] - 6s 510us/step - loss: 0.0212 - acc: 0.4194 - val_loss: 0.0224 - val_acc: 0.4391\n",
      "Epoch 124/140\n",
      "10976/10976 [==============================] - 6s 531us/step - loss: 0.0212 - acc: 0.4228 - val_loss: 0.0208 - val_acc: 0.3916\n",
      "Epoch 125/140\n",
      "10976/10976 [==============================] - 5s 482us/step - loss: 0.0213 - acc: 0.4209 - val_loss: 0.0221 - val_acc: 0.4502\n",
      "Epoch 126/140\n",
      "10976/10976 [==============================] - 6s 506us/step - loss: 0.0215 - acc: 0.4187 - val_loss: 0.0195 - val_acc: 0.3933\n",
      "Epoch 127/140\n",
      "10976/10976 [==============================] - 6s 521us/step - loss: 0.0213 - acc: 0.4257 - val_loss: 0.0206 - val_acc: 0.4160\n",
      "Epoch 128/140\n",
      "10976/10976 [==============================] - 6s 509us/step - loss: 0.0213 - acc: 0.4176 - val_loss: 0.0220 - val_acc: 0.3905\n",
      "Epoch 129/140\n",
      "10976/10976 [==============================] - 6s 522us/step - loss: 0.0220 - acc: 0.4192 - val_loss: 0.0217 - val_acc: 0.4459\n",
      "Epoch 130/140\n",
      "10976/10976 [==============================] - 6s 514us/step - loss: 0.0213 - acc: 0.4204 - val_loss: 0.0197 - val_acc: 0.3952\n",
      "Epoch 131/140\n",
      "10976/10976 [==============================] - 6s 514us/step - loss: 0.0213 - acc: 0.4192 - val_loss: 0.0216 - val_acc: 0.4501\n",
      "Epoch 132/140\n",
      "10976/10976 [==============================] - 6s 505us/step - loss: 0.0211 - acc: 0.4172 - val_loss: 0.0218 - val_acc: 0.3879\n",
      "Epoch 133/140\n",
      "10976/10976 [==============================] - 6s 530us/step - loss: 0.0213 - acc: 0.4246 - val_loss: 0.0223 - val_acc: 0.4501\n",
      "Epoch 134/140\n",
      "10976/10976 [==============================] - 6s 505us/step - loss: 0.0214 - acc: 0.4142 - val_loss: 0.0228 - val_acc: 0.3893\n",
      "Epoch 135/140\n",
      "10976/10976 [==============================] - 6s 514us/step - loss: 0.0214 - acc: 0.4186 - val_loss: 0.0232 - val_acc: 0.4551\n",
      "Epoch 136/140\n",
      "10976/10976 [==============================] - 5s 492us/step - loss: 0.0214 - acc: 0.4211 - val_loss: 0.0196 - val_acc: 0.3947\n",
      "Epoch 137/140\n",
      "10976/10976 [==============================] - 6s 503us/step - loss: 0.0213 - acc: 0.4205 - val_loss: 0.0207 - val_acc: 0.4515\n",
      "Epoch 138/140\n",
      "10976/10976 [==============================] - 6s 504us/step - loss: 0.0213 - acc: 0.4240 - val_loss: 0.0221 - val_acc: 0.3866\n",
      "Epoch 139/140\n",
      "10976/10976 [==============================] - 5s 495us/step - loss: 0.0211 - acc: 0.4214 - val_loss: 0.0224 - val_acc: 0.4526\n",
      "Epoch 140/140\n",
      "10976/10976 [==============================] - 5s 499us/step - loss: 0.0211 - acc: 0.4193 - val_loss: 0.0199 - val_acc: 0.3914\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 121/140\n",
      "10976/10976 [==============================] - 6s 521us/step - loss: 0.0250 - acc: 0.4278 - val_loss: 0.0240 - val_acc: 0.4265\n",
      "Epoch 122/140\n",
      "10976/10976 [==============================] - 5s 489us/step - loss: 0.0248 - acc: 0.4198 - val_loss: 0.0254 - val_acc: 0.4771\n",
      "Epoch 123/140\n",
      "10976/10976 [==============================] - 6s 521us/step - loss: 0.0250 - acc: 0.4147 - val_loss: 0.0250 - val_acc: 0.4680\n",
      "Epoch 124/140\n",
      "10976/10976 [==============================] - 6s 505us/step - loss: 0.0249 - acc: 0.4205 - val_loss: 0.0259 - val_acc: 0.4276\n",
      "Epoch 125/140\n",
      "10976/10976 [==============================] - 5s 494us/step - loss: 0.0252 - acc: 0.4735 - val_loss: 0.0247 - val_acc: 0.5043\n",
      "Epoch 126/140\n",
      "10976/10976 [==============================] - 6s 532us/step - loss: 0.0250 - acc: 0.4742 - val_loss: 0.0267 - val_acc: 0.4683\n",
      "Epoch 127/140\n",
      "10976/10976 [==============================] - 6s 510us/step - loss: 0.0248 - acc: 0.4663 - val_loss: 0.0241 - val_acc: 0.5065\n",
      "Epoch 128/140\n",
      "10976/10976 [==============================] - 6s 508us/step - loss: 0.0251 - acc: 0.4797 - val_loss: 0.0260 - val_acc: 0.4832\n",
      "Epoch 129/140\n",
      "10976/10976 [==============================] - 6s 546us/step - loss: 0.0252 - acc: 0.4772 - val_loss: 0.0256 - val_acc: 0.5073\n",
      "Epoch 130/140\n",
      "10976/10976 [==============================] - 6s 507us/step - loss: 0.0250 - acc: 0.4789 - val_loss: 0.0245 - val_acc: 0.4856\n",
      "Epoch 131/140\n",
      "10976/10976 [==============================] - 6s 516us/step - loss: 0.0248 - acc: 0.4773 - val_loss: 0.0235 - val_acc: 0.5200\n",
      "Epoch 132/140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 6s 514us/step - loss: 0.0249 - acc: 0.4758 - val_loss: 0.0259 - val_acc: 0.4675\n",
      "Epoch 133/140\n",
      "10976/10976 [==============================] - 6s 518us/step - loss: 0.0253 - acc: 0.4748 - val_loss: 0.0257 - val_acc: 0.5230\n",
      "Epoch 134/140\n",
      "10976/10976 [==============================] - 5s 497us/step - loss: 0.0250 - acc: 0.4811 - val_loss: 0.0267 - val_acc: 0.4683\n",
      "Epoch 135/140\n",
      "10976/10976 [==============================] - 6s 506us/step - loss: 0.0252 - acc: 0.4765 - val_loss: 0.0245 - val_acc: 0.5152\n",
      "Epoch 136/140\n",
      "10976/10976 [==============================] - 5s 490us/step - loss: 0.0244 - acc: 0.4731 - val_loss: 0.0255 - val_acc: 0.4845\n",
      "Epoch 137/140\n",
      "10976/10976 [==============================] - 6s 523us/step - loss: 0.0244 - acc: 0.4514 - val_loss: 0.0250 - val_acc: 0.4120\n",
      "Epoch 138/140\n",
      "10976/10976 [==============================] - 6s 531us/step - loss: 0.0250 - acc: 0.4492 - val_loss: 0.0255 - val_acc: 0.4415\n",
      "Epoch 139/140\n",
      "10976/10976 [==============================] - 6s 502us/step - loss: 0.0246 - acc: 0.4262 - val_loss: 0.0242 - val_acc: 0.5049\n",
      "Epoch 140/140\n",
      "10976/10976 [==============================] - 6s 512us/step - loss: 0.0245 - acc: 0.4255 - val_loss: 0.0255 - val_acc: 0.3487\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 121/140\n",
      "10976/10976 [==============================] - 6s 512us/step - loss: 0.0197 - acc: 0.4427 - val_loss: 0.0146 - val_acc: 0.4155\n",
      "Epoch 122/140\n",
      "10976/10976 [==============================] - 6s 515us/step - loss: 0.0191 - acc: 0.4073 - val_loss: 0.0252 - val_acc: 0.4084\n",
      "Epoch 123/140\n",
      "10976/10976 [==============================] - 6s 520us/step - loss: 0.0193 - acc: 0.4400 - val_loss: 0.0141 - val_acc: 0.4894\n",
      "Epoch 124/140\n",
      "10976/10976 [==============================] - 5s 492us/step - loss: 0.0193 - acc: 0.4222 - val_loss: 0.0250 - val_acc: 0.2706\n",
      "Epoch 125/140\n",
      "10976/10976 [==============================] - 6s 517us/step - loss: 0.0193 - acc: 0.4101 - val_loss: 0.0143 - val_acc: 0.4855\n",
      "Epoch 126/140\n",
      "10976/10976 [==============================] - 6s 508us/step - loss: 0.0191 - acc: 0.4270 - val_loss: 0.0261 - val_acc: 0.2726\n",
      "Epoch 127/140\n",
      "10976/10976 [==============================] - 6s 502us/step - loss: 0.0194 - acc: 0.3969 - val_loss: 0.0135 - val_acc: 0.4875\n",
      "Epoch 128/140\n",
      "10976/10976 [==============================] - 6s 524us/step - loss: 0.0193 - acc: 0.4045 - val_loss: 0.0252 - val_acc: 0.4130\n",
      "Epoch 129/140\n",
      "10976/10976 [==============================] - 6s 504us/step - loss: 0.0195 - acc: 0.4356 - val_loss: 0.0144 - val_acc: 0.4936\n",
      "Epoch 130/140\n",
      "10976/10976 [==============================] - 6s 512us/step - loss: 0.0191 - acc: 0.4455 - val_loss: 0.0255 - val_acc: 0.4845\n",
      "Epoch 131/140\n",
      "10976/10976 [==============================] - 5s 494us/step - loss: 0.0195 - acc: 0.4251 - val_loss: 0.0152 - val_acc: 0.4934\n",
      "Epoch 132/140\n",
      "10976/10976 [==============================] - 6s 515us/step - loss: 0.0192 - acc: 0.4611 - val_loss: 0.0241 - val_acc: 0.4117\n",
      "Epoch 133/140\n",
      "10976/10976 [==============================] - 6s 525us/step - loss: 0.0192 - acc: 0.4379 - val_loss: 0.0152 - val_acc: 0.4162\n",
      "Epoch 134/140\n",
      "10976/10976 [==============================] - 5s 496us/step - loss: 0.0192 - acc: 0.4455 - val_loss: 0.0247 - val_acc: 0.4122\n",
      "Epoch 135/140\n",
      "10976/10976 [==============================] - 6s 527us/step - loss: 0.0195 - acc: 0.4306 - val_loss: 0.0145 - val_acc: 0.4218\n",
      "Epoch 136/140\n",
      "10976/10976 [==============================] - 6s 505us/step - loss: 0.0194 - acc: 0.4364 - val_loss: 0.0248 - val_acc: 0.4133\n",
      "Epoch 137/140\n",
      "10976/10976 [==============================] - 6s 523us/step - loss: 0.0197 - acc: 0.4422 - val_loss: 0.0146 - val_acc: 0.4160\n",
      "Epoch 138/140\n",
      "10976/10976 [==============================] - 6s 513us/step - loss: 0.0191 - acc: 0.4452 - val_loss: 0.0253 - val_acc: 0.4155\n",
      "Epoch 139/140\n",
      "10976/10976 [==============================] - 6s 521us/step - loss: 0.0192 - acc: 0.4436 - val_loss: 0.0142 - val_acc: 0.4190\n",
      "Epoch 140/140\n",
      "10976/10976 [==============================] - 5s 484us/step - loss: 0.0191 - acc: 0.4260 - val_loss: 0.0247 - val_acc: 0.4865\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 121/140\n",
      "10976/10976 [==============================] - 6s 510us/step - loss: 0.0162 - acc: 0.3145 - val_loss: 0.0171 - val_acc: 0.3934\n",
      "Epoch 122/140\n",
      "10976/10976 [==============================] - 6s 519us/step - loss: 0.0167 - acc: 0.3173 - val_loss: 0.0150 - val_acc: 0.2664\n",
      "Epoch 123/140\n",
      "10976/10976 [==============================] - 6s 531us/step - loss: 0.0166 - acc: 0.3101 - val_loss: 0.0183 - val_acc: 0.3659\n",
      "Epoch 124/140\n",
      "10976/10976 [==============================] - 6s 504us/step - loss: 0.0166 - acc: 0.3046 - val_loss: 0.0142 - val_acc: 0.2696\n",
      "Epoch 125/140\n",
      "10976/10976 [==============================] - 6s 510us/step - loss: 0.0164 - acc: 0.3108 - val_loss: 0.0178 - val_acc: 0.3695\n",
      "Epoch 126/140\n",
      "10976/10976 [==============================] - 5s 477us/step - loss: 0.0166 - acc: 0.3174 - val_loss: 0.0136 - val_acc: 0.2775\n",
      "Epoch 127/140\n",
      "10976/10976 [==============================] - 6s 544us/step - loss: 0.0163 - acc: 0.3131 - val_loss: 0.0172 - val_acc: 0.3599\n",
      "Epoch 128/140\n",
      "10976/10976 [==============================] - 6s 502us/step - loss: 0.0164 - acc: 0.3165 - val_loss: 0.0139 - val_acc: 0.2694\n",
      "Epoch 129/140\n",
      "10976/10976 [==============================] - 6s 532us/step - loss: 0.0164 - acc: 0.3189 - val_loss: 0.0180 - val_acc: 0.3976\n",
      "Epoch 130/140\n",
      "10976/10976 [==============================] - 5s 496us/step - loss: 0.0165 - acc: 0.3268 - val_loss: 0.0135 - val_acc: 0.2723\n",
      "Epoch 131/140\n",
      "10976/10976 [==============================] - 5s 491us/step - loss: 0.0163 - acc: 0.3202 - val_loss: 0.0176 - val_acc: 0.3618\n",
      "Epoch 132/140\n",
      "10976/10976 [==============================] - 6s 529us/step - loss: 0.0167 - acc: 0.3128 - val_loss: 0.0147 - val_acc: 0.3372\n",
      "Epoch 133/140\n",
      "10976/10976 [==============================] - 5s 498us/step - loss: 0.0166 - acc: 0.3104 - val_loss: 0.0167 - val_acc: 0.2282\n",
      "Epoch 134/140\n",
      "10976/10976 [==============================] - 6s 518us/step - loss: 0.0164 - acc: 0.2836 - val_loss: 0.0152 - val_acc: 0.3381\n",
      "Epoch 135/140\n",
      "10976/10976 [==============================] - 6s 538us/step - loss: 0.0164 - acc: 0.2973 - val_loss: 0.0176 - val_acc: 0.2741\n",
      "Epoch 136/140\n",
      "10976/10976 [==============================] - 6s 522us/step - loss: 0.0167 - acc: 0.3043 - val_loss: 0.0148 - val_acc: 0.3331\n",
      "Epoch 137/140\n",
      "10976/10976 [==============================] - 6s 512us/step - loss: 0.0165 - acc: 0.2966 - val_loss: 0.0178 - val_acc: 0.2779\n",
      "Epoch 138/140\n",
      "10976/10976 [==============================] - 6s 506us/step - loss: 0.0165 - acc: 0.2971 - val_loss: 0.0137 - val_acc: 0.3367\n",
      "Epoch 139/140\n",
      "10976/10976 [==============================] - 6s 517us/step - loss: 0.0164 - acc: 0.3043 - val_loss: 0.0175 - val_acc: 0.2482\n",
      "Epoch 140/140\n",
      "10976/10976 [==============================] - 6s 528us/step - loss: 0.0166 - acc: 0.3096 - val_loss: 0.0139 - val_acc: 0.3386\n",
      "start training round 7\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 141/160\n",
      "10976/10976 [==============================] - 6s 526us/step - loss: 0.0215 - acc: 0.4217 - val_loss: 0.0233 - val_acc: 0.4492\n",
      "Epoch 142/160\n",
      "10976/10976 [==============================] - 6s 512us/step - loss: 0.0210 - acc: 0.4238 - val_loss: 0.0185 - val_acc: 0.3921\n",
      "Epoch 143/160\n",
      "10976/10976 [==============================] - 5s 494us/step - loss: 0.0211 - acc: 0.4188 - val_loss: 0.0203 - val_acc: 0.4526\n",
      "Epoch 144/160\n",
      "10976/10976 [==============================] - 6s 527us/step - loss: 0.0210 - acc: 0.4216 - val_loss: 0.0223 - val_acc: 0.3895\n",
      "Epoch 145/160\n",
      "10976/10976 [==============================] - 6s 550us/step - loss: 0.0215 - acc: 0.4184 - val_loss: 0.0222 - val_acc: 0.4518\n",
      "Epoch 146/160\n",
      "10976/10976 [==============================] - 6s 532us/step - loss: 0.0213 - acc: 0.4208 - val_loss: 0.0199 - val_acc: 0.3868\n",
      "Epoch 147/160\n",
      "10976/10976 [==============================] - 6s 550us/step - loss: 0.0208 - acc: 0.4186 - val_loss: 0.0211 - val_acc: 0.4588\n",
      "Epoch 148/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 6s 550us/step - loss: 0.0209 - acc: 0.4269 - val_loss: 0.0209 - val_acc: 0.3935\n",
      "Epoch 149/160\n",
      "10976/10976 [==============================] - 6s 543us/step - loss: 0.0212 - acc: 0.4223 - val_loss: 0.0223 - val_acc: 0.4438\n",
      "Epoch 150/160\n",
      "10976/10976 [==============================] - 6s 524us/step - loss: 0.0212 - acc: 0.4234 - val_loss: 0.0192 - val_acc: 0.3939\n",
      "Epoch 151/160\n",
      "10976/10976 [==============================] - 6s 506us/step - loss: 0.0211 - acc: 0.4158 - val_loss: 0.0191 - val_acc: 0.3798\n",
      "Epoch 152/160\n",
      "10976/10976 [==============================] - 5s 488us/step - loss: 0.0210 - acc: 0.4161 - val_loss: 0.0191 - val_acc: 0.3993\n",
      "Epoch 153/160\n",
      "10976/10976 [==============================] - 6s 530us/step - loss: 0.0209 - acc: 0.4199 - val_loss: 0.0233 - val_acc: 0.4617\n",
      "Epoch 154/160\n",
      "10976/10976 [==============================] - 6s 551us/step - loss: 0.0209 - acc: 0.4247 - val_loss: 0.0177 - val_acc: 0.3999\n",
      "Epoch 155/160\n",
      "10976/10976 [==============================] - 6s 539us/step - loss: 0.0206 - acc: 0.4224 - val_loss: 0.0220 - val_acc: 0.4614\n",
      "Epoch 156/160\n",
      "10976/10976 [==============================] - 6s 529us/step - loss: 0.0209 - acc: 0.4216 - val_loss: 0.0190 - val_acc: 0.3937\n",
      "Epoch 157/160\n",
      "10976/10976 [==============================] - 5s 498us/step - loss: 0.0212 - acc: 0.4243 - val_loss: 0.0213 - val_acc: 0.4213\n",
      "Epoch 158/160\n",
      "10976/10976 [==============================] - 6s 523us/step - loss: 0.0216 - acc: 0.4227 - val_loss: 0.0213 - val_acc: 0.3911\n",
      "Epoch 159/160\n",
      "10976/10976 [==============================] - 6s 528us/step - loss: 0.0213 - acc: 0.4215 - val_loss: 0.0212 - val_acc: 0.4343\n",
      "Epoch 160/160\n",
      "10976/10976 [==============================] - 6s 528us/step - loss: 0.0209 - acc: 0.4127 - val_loss: 0.0205 - val_acc: 0.4641\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 141/160\n",
      "10976/10976 [==============================] - 6s 507us/step - loss: 0.0244 - acc: 0.4286 - val_loss: 0.0243 - val_acc: 0.4805\n",
      "Epoch 142/160\n",
      "10976/10976 [==============================] - 6s 511us/step - loss: 0.0245 - acc: 0.4491 - val_loss: 0.0240 - val_acc: 0.3754\n",
      "Epoch 143/160\n",
      "10976/10976 [==============================] - 6s 530us/step - loss: 0.0244 - acc: 0.4332 - val_loss: 0.0240 - val_acc: 0.4834\n",
      "Epoch 144/160\n",
      "10976/10976 [==============================] - 6s 513us/step - loss: 0.0241 - acc: 0.4811 - val_loss: 0.0243 - val_acc: 0.5218\n",
      "Epoch 145/160\n",
      "10976/10976 [==============================] - 6s 526us/step - loss: 0.0240 - acc: 0.4487 - val_loss: 0.0242 - val_acc: 0.3711\n",
      "Epoch 146/160\n",
      "10976/10976 [==============================] - 6s 521us/step - loss: 0.0244 - acc: 0.4240 - val_loss: 0.0247 - val_acc: 0.4070\n",
      "Epoch 147/160\n",
      "10976/10976 [==============================] - 6s 516us/step - loss: 0.0245 - acc: 0.4442 - val_loss: 0.0243 - val_acc: 0.5244\n",
      "Epoch 148/160\n",
      "10976/10976 [==============================] - 6s 534us/step - loss: 0.0248 - acc: 0.4817 - val_loss: 0.0262 - val_acc: 0.4776\n",
      "Epoch 149/160\n",
      "10976/10976 [==============================] - 6s 526us/step - loss: 0.0244 - acc: 0.4827 - val_loss: 0.0239 - val_acc: 0.4973\n",
      "Epoch 150/160\n",
      "10976/10976 [==============================] - 6s 523us/step - loss: 0.0245 - acc: 0.4835 - val_loss: 0.0255 - val_acc: 0.4415\n",
      "Epoch 151/160\n",
      "10976/10976 [==============================] - 6s 521us/step - loss: 0.0243 - acc: 0.4761 - val_loss: 0.0231 - val_acc: 0.4958\n",
      "Epoch 152/160\n",
      "10976/10976 [==============================] - 6s 520us/step - loss: 0.0243 - acc: 0.4795 - val_loss: 0.0266 - val_acc: 0.4474\n",
      "Epoch 153/160\n",
      "10976/10976 [==============================] - 6s 514us/step - loss: 0.0248 - acc: 0.4791 - val_loss: 0.0256 - val_acc: 0.5093\n",
      "Epoch 154/160\n",
      "10976/10976 [==============================] - 5s 490us/step - loss: 0.0244 - acc: 0.4721 - val_loss: 0.0248 - val_acc: 0.3998\n",
      "Epoch 155/160\n",
      "10976/10976 [==============================] - 6s 514us/step - loss: 0.0240 - acc: 0.4435 - val_loss: 0.0256 - val_acc: 0.4890\n",
      "Epoch 156/160\n",
      "10976/10976 [==============================] - 6s 532us/step - loss: 0.0247 - acc: 0.4786 - val_loss: 0.0260 - val_acc: 0.5009\n",
      "Epoch 157/160\n",
      "10976/10976 [==============================] - 6s 559us/step - loss: 0.0245 - acc: 0.4461 - val_loss: 0.0248 - val_acc: 0.4433\n",
      "Epoch 158/160\n",
      "10976/10976 [==============================] - 6s 506us/step - loss: 0.0244 - acc: 0.4234 - val_loss: 0.0233 - val_acc: 0.4353\n",
      "Epoch 159/160\n",
      "10976/10976 [==============================] - 6s 504us/step - loss: 0.0239 - acc: 0.4747 - val_loss: 0.0244 - val_acc: 0.4794\n",
      "Epoch 160/160\n",
      "10976/10976 [==============================] - 5s 493us/step - loss: 0.0244 - acc: 0.4818 - val_loss: 0.0240 - val_acc: 0.5191\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 141/160\n",
      "10976/10976 [==============================] - 6s 513us/step - loss: 0.0193 - acc: 0.4320 - val_loss: 0.0144 - val_acc: 0.4910\n",
      "Epoch 142/160\n",
      "10976/10976 [==============================] - 6s 506us/step - loss: 0.0188 - acc: 0.4335 - val_loss: 0.0248 - val_acc: 0.4170\n",
      "Epoch 143/160\n",
      "10976/10976 [==============================] - 5s 500us/step - loss: 0.0193 - acc: 0.4238 - val_loss: 0.0137 - val_acc: 0.4159\n",
      "Epoch 144/160\n",
      "10976/10976 [==============================] - 6s 506us/step - loss: 0.0191 - acc: 0.4181 - val_loss: 0.0252 - val_acc: 0.2777\n",
      "Epoch 145/160\n",
      "10976/10976 [==============================] - 6s 508us/step - loss: 0.0193 - acc: 0.4283 - val_loss: 0.0135 - val_acc: 0.4183\n",
      "Epoch 146/160\n",
      "10976/10976 [==============================] - 6s 502us/step - loss: 0.0189 - acc: 0.4038 - val_loss: 0.0255 - val_acc: 0.2743\n",
      "Epoch 147/160\n",
      "10976/10976 [==============================] - 5s 487us/step - loss: 0.0192 - acc: 0.4386 - val_loss: 0.0132 - val_acc: 0.4939\n",
      "Epoch 148/160\n",
      "10976/10976 [==============================] - 5s 496us/step - loss: 0.0189 - acc: 0.4157 - val_loss: 0.0258 - val_acc: 0.4909\n",
      "Epoch 149/160\n",
      "10976/10976 [==============================] - 5s 495us/step - loss: 0.0193 - acc: 0.4243 - val_loss: 0.0137 - val_acc: 0.4935\n",
      "Epoch 150/160\n",
      "10976/10976 [==============================] - 5s 492us/step - loss: 0.0190 - acc: 0.4458 - val_loss: 0.0252 - val_acc: 0.4936\n",
      "Epoch 151/160\n",
      "10976/10976 [==============================] - 6s 528us/step - loss: 0.0192 - acc: 0.4195 - val_loss: 0.0142 - val_acc: 0.4206\n",
      "Epoch 152/160\n",
      "10976/10976 [==============================] - 5s 493us/step - loss: 0.0189 - acc: 0.4252 - val_loss: 0.0244 - val_acc: 0.4104\n",
      "Epoch 153/160\n",
      "10976/10976 [==============================] - 6s 527us/step - loss: 0.0191 - acc: 0.4289 - val_loss: 0.0139 - val_acc: 0.4949\n",
      "Epoch 154/160\n",
      "10976/10976 [==============================] - 5s 497us/step - loss: 0.0190 - acc: 0.4328 - val_loss: 0.0243 - val_acc: 0.2692\n",
      "Epoch 155/160\n",
      "10976/10976 [==============================] - 6s 507us/step - loss: 0.0193 - acc: 0.4351 - val_loss: 0.0149 - val_acc: 0.4209\n",
      "Epoch 156/160\n",
      "10976/10976 [==============================] - 5s 500us/step - loss: 0.0190 - acc: 0.4358 - val_loss: 0.0249 - val_acc: 0.4144\n",
      "Epoch 157/160\n",
      "10976/10976 [==============================] - 6s 537us/step - loss: 0.0191 - acc: 0.4406 - val_loss: 0.0150 - val_acc: 0.4154\n",
      "Epoch 158/160\n",
      "10976/10976 [==============================] - 6s 520us/step - loss: 0.0189 - acc: 0.4165 - val_loss: 0.0256 - val_acc: 0.4111\n",
      "Epoch 159/160\n",
      "10976/10976 [==============================] - 6s 513us/step - loss: 0.0192 - acc: 0.4404 - val_loss: 0.0130 - val_acc: 0.4200\n",
      "Epoch 160/160\n",
      "10976/10976 [==============================] - 6s 506us/step - loss: 0.0188 - acc: 0.4140 - val_loss: 0.0258 - val_acc: 0.2733\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 141/160\n",
      "10976/10976 [==============================] - 6s 523us/step - loss: 0.0164 - acc: 0.2937 - val_loss: 0.0173 - val_acc: 0.3001\n",
      "Epoch 142/160\n",
      "10976/10976 [==============================] - 6s 507us/step - loss: 0.0164 - acc: 0.3005 - val_loss: 0.0144 - val_acc: 0.3357\n",
      "Epoch 143/160\n",
      "10976/10976 [==============================] - 6s 525us/step - loss: 0.0163 - acc: 0.2868 - val_loss: 0.0174 - val_acc: 0.2569\n",
      "Epoch 144/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 6s 520us/step - loss: 0.0163 - acc: 0.2914 - val_loss: 0.0133 - val_acc: 0.3407\n",
      "Epoch 145/160\n",
      "10976/10976 [==============================] - 6s 534us/step - loss: 0.0162 - acc: 0.2915 - val_loss: 0.0172 - val_acc: 0.2677\n",
      "Epoch 146/160\n",
      "10976/10976 [==============================] - 6s 528us/step - loss: 0.0164 - acc: 0.2895 - val_loss: 0.0155 - val_acc: 0.2537\n",
      "Epoch 147/160\n",
      "10976/10976 [==============================] - 6s 521us/step - loss: 0.0162 - acc: 0.2950 - val_loss: 0.0158 - val_acc: 0.3964\n",
      "Epoch 148/160\n",
      "10976/10976 [==============================] - 6s 509us/step - loss: 0.0165 - acc: 0.3097 - val_loss: 0.0121 - val_acc: 0.3639\n",
      "Epoch 149/160\n",
      "10976/10976 [==============================] - 6s 520us/step - loss: 0.0161 - acc: 0.3088 - val_loss: 0.0145 - val_acc: 0.2742\n",
      "Epoch 150/160\n",
      "10976/10976 [==============================] - 6s 513us/step - loss: 0.0160 - acc: 0.3007 - val_loss: 0.0156 - val_acc: 0.2728\n",
      "Epoch 151/160\n",
      "10976/10976 [==============================] - 6s 502us/step - loss: 0.0163 - acc: 0.3088 - val_loss: 0.0140 - val_acc: 0.2740\n",
      "Epoch 152/160\n",
      "10976/10976 [==============================] - 6s 530us/step - loss: 0.0161 - acc: 0.3289 - val_loss: 0.0172 - val_acc: 0.3987\n",
      "Epoch 153/160\n",
      "10976/10976 [==============================] - 6s 509us/step - loss: 0.0162 - acc: 0.3184 - val_loss: 0.0150 - val_acc: 0.2689\n",
      "Epoch 154/160\n",
      "10976/10976 [==============================] - 6s 516us/step - loss: 0.0160 - acc: 0.3235 - val_loss: 0.0170 - val_acc: 0.3516\n",
      "Epoch 155/160\n",
      "10976/10976 [==============================] - 6s 530us/step - loss: 0.0164 - acc: 0.3151 - val_loss: 0.0132 - val_acc: 0.2791\n",
      "Epoch 156/160\n",
      "10976/10976 [==============================] - 6s 567us/step - loss: 0.0160 - acc: 0.3176 - val_loss: 0.0177 - val_acc: 0.4015\n",
      "Epoch 157/160\n",
      "10976/10976 [==============================] - 6s 530us/step - loss: 0.0163 - acc: 0.3199 - val_loss: 0.0134 - val_acc: 0.2762\n",
      "Epoch 158/160\n",
      "10976/10976 [==============================] - 6s 526us/step - loss: 0.0159 - acc: 0.3122 - val_loss: 0.0173 - val_acc: 0.3722\n",
      "Epoch 159/160\n",
      "10976/10976 [==============================] - 6s 548us/step - loss: 0.0162 - acc: 0.3224 - val_loss: 0.0140 - val_acc: 0.2728\n",
      "Epoch 160/160\n",
      "10976/10976 [==============================] - 6s 506us/step - loss: 0.0161 - acc: 0.3192 - val_loss: 0.0175 - val_acc: 0.3877\n",
      "start training round 8\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 161/180\n",
      "10976/10976 [==============================] - 6s 528us/step - loss: 0.0209 - acc: 0.4127 - val_loss: 0.0190 - val_acc: 0.3721\n",
      "Epoch 162/180\n",
      "10976/10976 [==============================] - 6s 525us/step - loss: 0.0207 - acc: 0.4122 - val_loss: 0.0216 - val_acc: 0.4652\n",
      "Epoch 163/180\n",
      "10976/10976 [==============================] - 6s 520us/step - loss: 0.0220 - acc: 0.4209 - val_loss: 0.0228 - val_acc: 0.4475\n",
      "Epoch 164/180\n",
      "10976/10976 [==============================] - 6s 519us/step - loss: 0.0210 - acc: 0.4238 - val_loss: 0.0200 - val_acc: 0.3981\n",
      "Epoch 165/180\n",
      "10976/10976 [==============================] - 6s 505us/step - loss: 0.0213 - acc: 0.4246 - val_loss: 0.0217 - val_acc: 0.4525\n",
      "Epoch 166/180\n",
      "10976/10976 [==============================] - 6s 526us/step - loss: 0.0212 - acc: 0.4226 - val_loss: 0.0197 - val_acc: 0.3943\n",
      "Epoch 167/180\n",
      "10976/10976 [==============================] - 6s 509us/step - loss: 0.0209 - acc: 0.4231 - val_loss: 0.0208 - val_acc: 0.4572\n",
      "Epoch 168/180\n",
      "10976/10976 [==============================] - 6s 519us/step - loss: 0.0208 - acc: 0.4244 - val_loss: 0.0195 - val_acc: 0.3960\n",
      "Epoch 169/180\n",
      "10976/10976 [==============================] - 6s 517us/step - loss: 0.0207 - acc: 0.4205 - val_loss: 0.0211 - val_acc: 0.4611\n",
      "Epoch 170/180\n",
      "10976/10976 [==============================] - 6s 503us/step - loss: 0.0210 - acc: 0.4244 - val_loss: 0.0204 - val_acc: 0.3939\n",
      "Epoch 171/180\n",
      "10976/10976 [==============================] - 6s 515us/step - loss: 0.0213 - acc: 0.4258 - val_loss: 0.0218 - val_acc: 0.4381\n",
      "Epoch 172/180\n",
      "10976/10976 [==============================] - 6s 514us/step - loss: 0.0209 - acc: 0.4248 - val_loss: 0.0188 - val_acc: 0.4014\n",
      "Epoch 173/180\n",
      "10976/10976 [==============================] - 6s 514us/step - loss: 0.0208 - acc: 0.4380 - val_loss: 0.0208 - val_acc: 0.4634\n",
      "Epoch 174/180\n",
      "10976/10976 [==============================] - 6s 524us/step - loss: 0.0213 - acc: 0.4534 - val_loss: 0.0249 - val_acc: 0.4604\n",
      "Epoch 175/180\n",
      "10976/10976 [==============================] - 6s 537us/step - loss: 0.0209 - acc: 0.4566 - val_loss: 0.0184 - val_acc: 0.4625\n",
      "Epoch 176/180\n",
      "10976/10976 [==============================] - 6s 550us/step - loss: 0.0219 - acc: 0.4368 - val_loss: 0.0227 - val_acc: 0.4696\n",
      "Epoch 177/180\n",
      "10976/10976 [==============================] - 6s 506us/step - loss: 0.0213 - acc: 0.4276 - val_loss: 0.0216 - val_acc: 0.3791\n",
      "Epoch 178/180\n",
      "10976/10976 [==============================] - 6s 538us/step - loss: 0.0209 - acc: 0.4281 - val_loss: 0.0185 - val_acc: 0.4762\n",
      "Epoch 179/180\n",
      "10976/10976 [==============================] - 6s 520us/step - loss: 0.0205 - acc: 0.4248 - val_loss: 0.0228 - val_acc: 0.3738\n",
      "Epoch 180/180\n",
      "10976/10976 [==============================] - 6s 529us/step - loss: 0.0208 - acc: 0.4242 - val_loss: 0.0202 - val_acc: 0.4776\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 161/180\n",
      "10976/10976 [==============================] - 6s 546us/step - loss: 0.0244 - acc: 0.4754 - val_loss: 0.0236 - val_acc: 0.4155\n",
      "Epoch 162/180\n",
      "10976/10976 [==============================] - 6s 507us/step - loss: 0.0240 - acc: 0.4398 - val_loss: 0.0250 - val_acc: 0.4844\n",
      "Epoch 163/180\n",
      "10976/10976 [==============================] - 6s 540us/step - loss: 0.0244 - acc: 0.4258 - val_loss: 0.0241 - val_acc: 0.3617\n",
      "Epoch 164/180\n",
      "10976/10976 [==============================] - 6s 510us/step - loss: 0.0242 - acc: 0.4238 - val_loss: 0.0241 - val_acc: 0.4900\n",
      "Epoch 165/180\n",
      "10976/10976 [==============================] - 6s 518us/step - loss: 0.0242 - acc: 0.4218 - val_loss: 0.0240 - val_acc: 0.4174\n",
      "Epoch 166/180\n",
      "10976/10976 [==============================] - 6s 537us/step - loss: 0.0239 - acc: 0.4219 - val_loss: 0.0236 - val_acc: 0.4578\n",
      "Epoch 167/180\n",
      "10976/10976 [==============================] - 5s 494us/step - loss: 0.0243 - acc: 0.4480 - val_loss: 0.0239 - val_acc: 0.5241\n",
      "Epoch 168/180\n",
      "10976/10976 [==============================] - 5s 498us/step - loss: 0.0245 - acc: 0.4629 - val_loss: 0.0254 - val_acc: 0.5060\n",
      "Epoch 169/180\n",
      "10976/10976 [==============================] - 5s 500us/step - loss: 0.0243 - acc: 0.4250 - val_loss: 0.0253 - val_acc: 0.3498\n",
      "Epoch 170/180\n",
      "10976/10976 [==============================] - 6s 508us/step - loss: 0.0244 - acc: 0.4213 - val_loss: 0.0240 - val_acc: 0.4833\n",
      "Epoch 171/180\n",
      "10976/10976 [==============================] - 6s 512us/step - loss: 0.0242 - acc: 0.4351 - val_loss: 0.0245 - val_acc: 0.4118\n",
      "Epoch 172/180\n",
      "10976/10976 [==============================] - 6s 533us/step - loss: 0.0241 - acc: 0.4653 - val_loss: 0.0232 - val_acc: 0.5283\n",
      "Epoch 173/180\n",
      "10976/10976 [==============================] - 6s 510us/step - loss: 0.0241 - acc: 0.4768 - val_loss: 0.0235 - val_acc: 0.4960\n",
      "Epoch 174/180\n",
      "10976/10976 [==============================] - 6s 541us/step - loss: 0.0240 - acc: 0.4854 - val_loss: 0.0252 - val_acc: 0.4566\n",
      "Epoch 175/180\n",
      "10976/10976 [==============================] - 6s 513us/step - loss: 0.0247 - acc: 0.4821 - val_loss: 0.0225 - val_acc: 0.5175\n",
      "Epoch 176/180\n",
      "10976/10976 [==============================] - 6s 515us/step - loss: 0.0235 - acc: 0.4404 - val_loss: 0.0248 - val_acc: 0.4464\n",
      "Epoch 177/180\n",
      "10976/10976 [==============================] - 6s 506us/step - loss: 0.0246 - acc: 0.4459 - val_loss: 0.0244 - val_acc: 0.5266\n",
      "Epoch 178/180\n",
      "10976/10976 [==============================] - 6s 503us/step - loss: 0.0249 - acc: 0.4845 - val_loss: 0.0254 - val_acc: 0.4623\n",
      "Epoch 179/180\n",
      "10976/10976 [==============================] - 6s 509us/step - loss: 0.0238 - acc: 0.4591 - val_loss: 0.0251 - val_acc: 0.3897\n",
      "Epoch 180/180\n",
      "10976/10976 [==============================] - 6s 508us/step - loss: 0.0241 - acc: 0.4564 - val_loss: 0.0246 - val_acc: 0.5292\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 161/180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 6s 531us/step - loss: 0.0192 - acc: 0.4182 - val_loss: 0.0140 - val_acc: 0.4954\n",
      "Epoch 162/180\n",
      "10976/10976 [==============================] - 6s 507us/step - loss: 0.0189 - acc: 0.4164 - val_loss: 0.0231 - val_acc: 0.4058\n",
      "Epoch 163/180\n",
      "10976/10976 [==============================] - 6s 536us/step - loss: 0.0188 - acc: 0.4240 - val_loss: 0.0139 - val_acc: 0.4979\n",
      "Epoch 164/180\n",
      "10976/10976 [==============================] - 6s 514us/step - loss: 0.0186 - acc: 0.4173 - val_loss: 0.0254 - val_acc: 0.4146\n",
      "Epoch 165/180\n",
      "10976/10976 [==============================] - 6s 516us/step - loss: 0.0189 - acc: 0.4437 - val_loss: 0.0138 - val_acc: 0.4934\n",
      "Epoch 166/180\n",
      "10976/10976 [==============================] - 6s 503us/step - loss: 0.0189 - acc: 0.4091 - val_loss: 0.0246 - val_acc: 0.4168\n",
      "Epoch 167/180\n",
      "10976/10976 [==============================] - 6s 527us/step - loss: 0.0192 - acc: 0.4266 - val_loss: 0.0126 - val_acc: 0.4919\n",
      "Epoch 168/180\n",
      "10976/10976 [==============================] - 6s 522us/step - loss: 0.0186 - acc: 0.4297 - val_loss: 0.0232 - val_acc: 0.2736\n",
      "Epoch 169/180\n",
      "10976/10976 [==============================] - 5s 487us/step - loss: 0.0191 - acc: 0.4288 - val_loss: 0.0131 - val_acc: 0.4977\n",
      "Epoch 170/180\n",
      "10976/10976 [==============================] - 6s 503us/step - loss: 0.0185 - acc: 0.4298 - val_loss: 0.0256 - val_acc: 0.4222\n",
      "Epoch 171/180\n",
      "10976/10976 [==============================] - 6s 531us/step - loss: 0.0189 - acc: 0.4304 - val_loss: 0.0152 - val_acc: 0.4135\n",
      "Epoch 172/180\n",
      "10976/10976 [==============================] - 6s 513us/step - loss: 0.0188 - acc: 0.4233 - val_loss: 0.0254 - val_acc: 0.4214\n",
      "Epoch 173/180\n",
      "10976/10976 [==============================] - 6s 520us/step - loss: 0.0187 - acc: 0.4362 - val_loss: 0.0142 - val_acc: 0.4948\n",
      "Epoch 174/180\n",
      "10976/10976 [==============================] - 6s 503us/step - loss: 0.0185 - acc: 0.4272 - val_loss: 0.0253 - val_acc: 0.2760\n",
      "Epoch 175/180\n",
      "10976/10976 [==============================] - 6s 512us/step - loss: 0.0191 - acc: 0.4411 - val_loss: 0.0140 - val_acc: 0.4976\n",
      "Epoch 176/180\n",
      "10976/10976 [==============================] - 6s 510us/step - loss: 0.0187 - acc: 0.4353 - val_loss: 0.0197 - val_acc: 0.4234\n",
      "Epoch 177/180\n",
      "10976/10976 [==============================] - 6s 503us/step - loss: 0.0190 - acc: 0.4271 - val_loss: 0.0145 - val_acc: 0.4208\n",
      "Epoch 178/180\n",
      "10976/10976 [==============================] - 5s 485us/step - loss: 0.0185 - acc: 0.4356 - val_loss: 0.0243 - val_acc: 0.4171\n",
      "Epoch 179/180\n",
      "10976/10976 [==============================] - 6s 503us/step - loss: 0.0189 - acc: 0.4451 - val_loss: 0.0147 - val_acc: 0.4965\n",
      "Epoch 180/180\n",
      "10976/10976 [==============================] - 6s 511us/step - loss: 0.0189 - acc: 0.4525 - val_loss: 0.0245 - val_acc: 0.4915\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 161/180\n",
      "10976/10976 [==============================] - 6s 522us/step - loss: 0.0161 - acc: 0.3152 - val_loss: 0.0144 - val_acc: 0.2732\n",
      "Epoch 162/180\n",
      "10976/10976 [==============================] - 6s 526us/step - loss: 0.0162 - acc: 0.3184 - val_loss: 0.0173 - val_acc: 0.3747\n",
      "Epoch 163/180\n",
      "10976/10976 [==============================] - 6s 525us/step - loss: 0.0162 - acc: 0.3259 - val_loss: 0.0132 - val_acc: 0.2776\n",
      "Epoch 164/180\n",
      "10976/10976 [==============================] - 6s 511us/step - loss: 0.0159 - acc: 0.3217 - val_loss: 0.0173 - val_acc: 0.3785\n",
      "Epoch 165/180\n",
      "10976/10976 [==============================] - 6s 533us/step - loss: 0.0161 - acc: 0.3190 - val_loss: 0.0138 - val_acc: 0.2730\n",
      "Epoch 166/180\n",
      "10976/10976 [==============================] - 6s 523us/step - loss: 0.0159 - acc: 0.3213 - val_loss: 0.0171 - val_acc: 0.3886\n",
      "Epoch 167/180\n",
      "10976/10976 [==============================] - 6s 542us/step - loss: 0.0161 - acc: 0.3233 - val_loss: 0.0133 - val_acc: 0.2820\n",
      "Epoch 168/180\n",
      "10976/10976 [==============================] - 6s 523us/step - loss: 0.0161 - acc: 0.3313 - val_loss: 0.0155 - val_acc: 0.3675\n",
      "Epoch 169/180\n",
      "10976/10976 [==============================] - 6s 522us/step - loss: 0.0161 - acc: 0.3257 - val_loss: 0.0137 - val_acc: 0.2782\n",
      "Epoch 170/180\n",
      "10976/10976 [==============================] - 6s 534us/step - loss: 0.0159 - acc: 0.3141 - val_loss: 0.0175 - val_acc: 0.3276\n",
      "Epoch 171/180\n",
      "10976/10976 [==============================] - 6s 510us/step - loss: 0.0160 - acc: 0.3222 - val_loss: 0.0136 - val_acc: 0.2764\n",
      "Epoch 172/180\n",
      "10976/10976 [==============================] - 6s 535us/step - loss: 0.0157 - acc: 0.3111 - val_loss: 0.0161 - val_acc: 0.3273\n",
      "Epoch 173/180\n",
      "10976/10976 [==============================] - 6s 535us/step - loss: 0.0160 - acc: 0.3263 - val_loss: 0.0155 - val_acc: 0.3712\n",
      "Epoch 174/180\n",
      "10976/10976 [==============================] - 6s 515us/step - loss: 0.0159 - acc: 0.3307 - val_loss: 0.0156 - val_acc: 0.2918\n",
      "Epoch 175/180\n",
      "10976/10976 [==============================] - 6s 525us/step - loss: 0.0159 - acc: 0.3376 - val_loss: 0.0140 - val_acc: 0.4071\n",
      "Epoch 176/180\n",
      "10976/10976 [==============================] - 6s 513us/step - loss: 0.0159 - acc: 0.3327 - val_loss: 0.0156 - val_acc: 0.3196\n",
      "Epoch 177/180\n",
      "10976/10976 [==============================] - 6s 514us/step - loss: 0.0158 - acc: 0.3359 - val_loss: 0.0157 - val_acc: 0.3320\n",
      "Epoch 178/180\n",
      "10976/10976 [==============================] - 6s 539us/step - loss: 0.0158 - acc: 0.3233 - val_loss: 0.0155 - val_acc: 0.3243\n",
      "Epoch 179/180\n",
      "10976/10976 [==============================] - 6s 515us/step - loss: 0.0159 - acc: 0.3301 - val_loss: 0.0155 - val_acc: 0.3432\n",
      "Epoch 180/180\n",
      "10976/10976 [==============================] - 6s 533us/step - loss: 0.0159 - acc: 0.3328 - val_loss: 0.0153 - val_acc: 0.3139\n",
      "start training round 9\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 181/200\n",
      "10976/10976 [==============================] - 6s 517us/step - loss: 0.0214 - acc: 0.4253 - val_loss: 0.0218 - val_acc: 0.3889\n",
      "Epoch 182/200\n",
      "10976/10976 [==============================] - 6s 514us/step - loss: 0.0204 - acc: 0.4235 - val_loss: 0.0214 - val_acc: 0.4777\n",
      "Epoch 183/200\n",
      "10976/10976 [==============================] - 6s 520us/step - loss: 0.0209 - acc: 0.4246 - val_loss: 0.0210 - val_acc: 0.3719\n",
      "Epoch 184/200\n",
      "10976/10976 [==============================] - 6s 520us/step - loss: 0.0207 - acc: 0.4244 - val_loss: 0.0210 - val_acc: 0.4781\n",
      "Epoch 185/200\n",
      "10976/10976 [==============================] - 5s 498us/step - loss: 0.0212 - acc: 0.4362 - val_loss: 0.0249 - val_acc: 0.4674\n",
      "Epoch 186/200\n",
      "10976/10976 [==============================] - 6s 523us/step - loss: 0.0213 - acc: 0.4614 - val_loss: 0.0157 - val_acc: 0.4806\n",
      "Epoch 187/200\n",
      "10976/10976 [==============================] - 5s 485us/step - loss: 0.0212 - acc: 0.4642 - val_loss: 0.0250 - val_acc: 0.4727\n",
      "Epoch 188/200\n",
      "10976/10976 [==============================] - 6s 521us/step - loss: 0.0208 - acc: 0.4621 - val_loss: 0.0175 - val_acc: 0.4795\n",
      "Epoch 189/200\n",
      "10976/10976 [==============================] - 6s 528us/step - loss: 0.0212 - acc: 0.4620 - val_loss: 0.0200 - val_acc: 0.4479\n",
      "Epoch 190/200\n",
      "10976/10976 [==============================] - 6s 511us/step - loss: 0.0210 - acc: 0.4629 - val_loss: 0.0174 - val_acc: 0.4756\n",
      "Epoch 191/200\n",
      "10976/10976 [==============================] - 6s 516us/step - loss: 0.0207 - acc: 0.4584 - val_loss: 0.0240 - val_acc: 0.4338\n",
      "Epoch 192/200\n",
      "10976/10976 [==============================] - 6s 521us/step - loss: 0.0210 - acc: 0.4607 - val_loss: 0.0173 - val_acc: 0.4733\n",
      "Epoch 193/200\n",
      "10976/10976 [==============================] - 6s 514us/step - loss: 0.0206 - acc: 0.4613 - val_loss: 0.0236 - val_acc: 0.4549\n",
      "Epoch 194/200\n",
      "10976/10976 [==============================] - 6s 534us/step - loss: 0.0208 - acc: 0.4616 - val_loss: 0.0186 - val_acc: 0.4718\n",
      "Epoch 195/200\n",
      "10976/10976 [==============================] - 6s 506us/step - loss: 0.0209 - acc: 0.4614 - val_loss: 0.0231 - val_acc: 0.4653\n",
      "Epoch 196/200\n",
      "10976/10976 [==============================] - 6s 526us/step - loss: 0.0210 - acc: 0.4639 - val_loss: 0.0186 - val_acc: 0.4758\n",
      "Epoch 197/200\n",
      "10976/10976 [==============================] - 5s 494us/step - loss: 0.0211 - acc: 0.4631 - val_loss: 0.0239 - val_acc: 0.4661\n",
      "Epoch 198/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 6s 510us/step - loss: 0.0205 - acc: 0.4606 - val_loss: 0.0194 - val_acc: 0.4795\n",
      "Epoch 199/200\n",
      "10976/10976 [==============================] - 6s 534us/step - loss: 0.0205 - acc: 0.4623 - val_loss: 0.0249 - val_acc: 0.4689\n",
      "Epoch 200/200\n",
      "10976/10976 [==============================] - 6s 512us/step - loss: 0.0202 - acc: 0.4624 - val_loss: 0.0178 - val_acc: 0.4654\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 181/200\n",
      "10976/10976 [==============================] - 6s 508us/step - loss: 0.0241 - acc: 0.4835 - val_loss: 0.0240 - val_acc: 0.4802\n",
      "Epoch 182/200\n",
      "10976/10976 [==============================] - 6s 516us/step - loss: 0.0241 - acc: 0.4826 - val_loss: 0.0230 - val_acc: 0.5062\n",
      "Epoch 183/200\n",
      "10976/10976 [==============================] - 6s 503us/step - loss: 0.0240 - acc: 0.4845 - val_loss: 0.0227 - val_acc: 0.4185\n",
      "Epoch 184/200\n",
      "10976/10976 [==============================] - 6s 509us/step - loss: 0.0245 - acc: 0.4753 - val_loss: 0.0258 - val_acc: 0.5275\n",
      "Epoch 185/200\n",
      "10976/10976 [==============================] - 6s 510us/step - loss: 0.0242 - acc: 0.4880 - val_loss: 0.0243 - val_acc: 0.4898\n",
      "Epoch 186/200\n",
      "10976/10976 [==============================] - 6s 529us/step - loss: 0.0243 - acc: 0.4864 - val_loss: 0.0245 - val_acc: 0.5148\n",
      "Epoch 187/200\n",
      "10976/10976 [==============================] - 6s 506us/step - loss: 0.0239 - acc: 0.4850 - val_loss: 0.0219 - val_acc: 0.5243\n",
      "Epoch 188/200\n",
      "10976/10976 [==============================] - 6s 525us/step - loss: 0.0242 - acc: 0.4865 - val_loss: 0.0222 - val_acc: 0.4631\n",
      "Epoch 189/200\n",
      "10976/10976 [==============================] - 6s 536us/step - loss: 0.0235 - acc: 0.4869 - val_loss: 0.0241 - val_acc: 0.5245\n",
      "Epoch 190/200\n",
      "10976/10976 [==============================] - 6s 505us/step - loss: 0.0235 - acc: 0.4829 - val_loss: 0.0237 - val_acc: 0.4737\n",
      "Epoch 191/200\n",
      "10976/10976 [==============================] - 6s 509us/step - loss: 0.0242 - acc: 0.4848 - val_loss: 0.0241 - val_acc: 0.4814\n",
      "Epoch 192/200\n",
      "10976/10976 [==============================] - 6s 524us/step - loss: 0.0245 - acc: 0.4671 - val_loss: 0.0250 - val_acc: 0.3983\n",
      "Epoch 193/200\n",
      "10976/10976 [==============================] - 6s 515us/step - loss: 0.0240 - acc: 0.4509 - val_loss: 0.0245 - val_acc: 0.5319\n",
      "Epoch 194/200\n",
      "10976/10976 [==============================] - 6s 523us/step - loss: 0.0234 - acc: 0.4530 - val_loss: 0.0237 - val_acc: 0.3952\n",
      "Epoch 195/200\n",
      "10976/10976 [==============================] - 6s 533us/step - loss: 0.0239 - acc: 0.4214 - val_loss: 0.0243 - val_acc: 0.4869\n",
      "Epoch 196/200\n",
      "10976/10976 [==============================] - 6s 527us/step - loss: 0.0240 - acc: 0.4276 - val_loss: 0.0246 - val_acc: 0.3580\n",
      "Epoch 197/200\n",
      "10976/10976 [==============================] - 5s 499us/step - loss: 0.0240 - acc: 0.4223 - val_loss: 0.0238 - val_acc: 0.4814\n",
      "Epoch 198/200\n",
      "10976/10976 [==============================] - 5s 473us/step - loss: 0.0239 - acc: 0.4315 - val_loss: 0.0236 - val_acc: 0.3886\n",
      "Epoch 199/200\n",
      "10976/10976 [==============================] - 6s 507us/step - loss: 0.0237 - acc: 0.4305 - val_loss: 0.0231 - val_acc: 0.4952\n",
      "Epoch 200/200\n",
      "10976/10976 [==============================] - 6s 516us/step - loss: 0.0237 - acc: 0.4595 - val_loss: 0.0237 - val_acc: 0.4899\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 181/200\n",
      "10976/10976 [==============================] - 6s 515us/step - loss: 0.0190 - acc: 0.4169 - val_loss: 0.0144 - val_acc: 0.4947\n",
      "Epoch 182/200\n",
      "10976/10976 [==============================] - 6s 549us/step - loss: 0.0187 - acc: 0.4344 - val_loss: 0.0245 - val_acc: 0.4937\n",
      "Epoch 183/200\n",
      "10976/10976 [==============================] - 6s 531us/step - loss: 0.0190 - acc: 0.4529 - val_loss: 0.0136 - val_acc: 0.4950\n",
      "Epoch 184/200\n",
      "10976/10976 [==============================] - 6s 519us/step - loss: 0.0186 - acc: 0.4177 - val_loss: 0.0251 - val_acc: 0.4934\n",
      "Epoch 185/200\n",
      "10976/10976 [==============================] - 6s 545us/step - loss: 0.0190 - acc: 0.4289 - val_loss: 0.0136 - val_acc: 0.4262\n",
      "Epoch 186/200\n",
      "10976/10976 [==============================] - 5s 499us/step - loss: 0.0184 - acc: 0.4147 - val_loss: 0.0207 - val_acc: 0.2805\n",
      "Epoch 187/200\n",
      "10976/10976 [==============================] - 6s 519us/step - loss: 0.0189 - acc: 0.4522 - val_loss: 0.0124 - val_acc: 0.4244\n",
      "Epoch 188/200\n",
      "10976/10976 [==============================] - 6s 514us/step - loss: 0.0188 - acc: 0.4344 - val_loss: 0.0242 - val_acc: 0.4246\n",
      "Epoch 189/200\n",
      "10976/10976 [==============================] - 6s 521us/step - loss: 0.0187 - acc: 0.4295 - val_loss: 0.0128 - val_acc: 0.4999\n",
      "Epoch 190/200\n",
      "10976/10976 [==============================] - 6s 541us/step - loss: 0.0184 - acc: 0.4383 - val_loss: 0.0231 - val_acc: 0.4164\n",
      "Epoch 191/200\n",
      "10976/10976 [==============================] - 6s 505us/step - loss: 0.0189 - acc: 0.4150 - val_loss: 0.0130 - val_acc: 0.5002\n",
      "Epoch 192/200\n",
      "10976/10976 [==============================] - 6s 518us/step - loss: 0.0188 - acc: 0.4102 - val_loss: 0.0248 - val_acc: 0.4195\n",
      "Epoch 193/200\n",
      "10976/10976 [==============================] - 6s 531us/step - loss: 0.0184 - acc: 0.4170 - val_loss: 0.0182 - val_acc: 0.4927\n",
      "Epoch 194/200\n",
      "10976/10976 [==============================] - 6s 516us/step - loss: 0.0186 - acc: 0.3361 - val_loss: 0.0177 - val_acc: 0.2175\n",
      "Epoch 195/200\n",
      "10976/10976 [==============================] - 6s 511us/step - loss: 0.0187 - acc: 0.3330 - val_loss: 0.0205 - val_acc: 0.4275\n",
      "Epoch 196/200\n",
      "10976/10976 [==============================] - 6s 533us/step - loss: 0.0185 - acc: 0.3317 - val_loss: 0.0167 - val_acc: 0.2172\n",
      "Epoch 197/200\n",
      "10976/10976 [==============================] - 6s 531us/step - loss: 0.0186 - acc: 0.3349 - val_loss: 0.0202 - val_acc: 0.4968\n",
      "Epoch 198/200\n",
      "10976/10976 [==============================] - 6s 516us/step - loss: 0.0188 - acc: 0.3419 - val_loss: 0.0173 - val_acc: 0.2172\n",
      "Epoch 199/200\n",
      "10976/10976 [==============================] - 6s 513us/step - loss: 0.0185 - acc: 0.3249 - val_loss: 0.0204 - val_acc: 0.4960\n",
      "Epoch 200/200\n",
      "10976/10976 [==============================] - 6s 520us/step - loss: 0.0186 - acc: 0.3445 - val_loss: 0.0168 - val_acc: 0.2170\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 181/200\n",
      "10976/10976 [==============================] - 6s 520us/step - loss: 0.0158 - acc: 0.3348 - val_loss: 0.0156 - val_acc: 0.3866\n",
      "Epoch 182/200\n",
      "10976/10976 [==============================] - 6s 518us/step - loss: 0.0159 - acc: 0.3378 - val_loss: 0.0148 - val_acc: 0.3393\n",
      "Epoch 183/200\n",
      "10976/10976 [==============================] - 6s 539us/step - loss: 0.0158 - acc: 0.3410 - val_loss: 0.0147 - val_acc: 0.3535\n",
      "Epoch 184/200\n",
      "10976/10976 [==============================] - 5s 498us/step - loss: 0.0157 - acc: 0.3339 - val_loss: 0.0152 - val_acc: 0.3348\n",
      "Epoch 185/200\n",
      "10976/10976 [==============================] - 6s 526us/step - loss: 0.0158 - acc: 0.3392 - val_loss: 0.0154 - val_acc: 0.3384\n",
      "Epoch 186/200\n",
      "10976/10976 [==============================] - 6s 539us/step - loss: 0.0158 - acc: 0.3420 - val_loss: 0.0155 - val_acc: 0.3309\n",
      "Epoch 187/200\n",
      "10976/10976 [==============================] - 6s 538us/step - loss: 0.0157 - acc: 0.3328 - val_loss: 0.0139 - val_acc: 0.4209\n",
      "Epoch 188/200\n",
      "10976/10976 [==============================] - 5s 489us/step - loss: 0.0157 - acc: 0.3420 - val_loss: 0.0152 - val_acc: 0.2655\n",
      "Epoch 189/200\n",
      "10976/10976 [==============================] - 6s 527us/step - loss: 0.0157 - acc: 0.3304 - val_loss: 0.0156 - val_acc: 0.3383\n",
      "Epoch 190/200\n",
      "10976/10976 [==============================] - 6s 529us/step - loss: 0.0157 - acc: 0.3375 - val_loss: 0.0155 - val_acc: 0.3276\n",
      "Epoch 191/200\n",
      "10976/10976 [==============================] - 6s 511us/step - loss: 0.0157 - acc: 0.3376 - val_loss: 0.0143 - val_acc: 0.3447\n",
      "Epoch 192/200\n",
      "10976/10976 [==============================] - 6s 533us/step - loss: 0.0156 - acc: 0.3307 - val_loss: 0.0162 - val_acc: 0.3271\n",
      "Epoch 193/200\n",
      "10976/10976 [==============================] - 6s 508us/step - loss: 0.0158 - acc: 0.3345 - val_loss: 0.0157 - val_acc: 0.3488\n",
      "Epoch 194/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 6s 526us/step - loss: 0.0157 - acc: 0.3328 - val_loss: 0.0152 - val_acc: 0.3397\n",
      "Epoch 195/200\n",
      "10976/10976 [==============================] - 6s 539us/step - loss: 0.0157 - acc: 0.3413 - val_loss: 0.0157 - val_acc: 0.3371\n",
      "Epoch 196/200\n",
      "10976/10976 [==============================] - 6s 522us/step - loss: 0.0157 - acc: 0.3389 - val_loss: 0.0153 - val_acc: 0.3366\n",
      "Epoch 197/200\n",
      "10976/10976 [==============================] - 6s 513us/step - loss: 0.0157 - acc: 0.3391 - val_loss: 0.0154 - val_acc: 0.3852\n",
      "Epoch 198/200\n",
      "10976/10976 [==============================] - 6s 523us/step - loss: 0.0158 - acc: 0.3471 - val_loss: 0.0155 - val_acc: 0.3421\n",
      "Epoch 199/200\n",
      "10976/10976 [==============================] - 6s 516us/step - loss: 0.0157 - acc: 0.3452 - val_loss: 0.0146 - val_acc: 0.3351\n",
      "Epoch 200/200\n",
      "10976/10976 [==============================] - 6s 512us/step - loss: 0.0156 - acc: 0.3397 - val_loss: 0.0149 - val_acc: 0.3439\n",
      "start training round 10\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 201/220\n",
      "10976/10976 [==============================] - 6s 531us/step - loss: 0.0203 - acc: 0.4612 - val_loss: 0.0244 - val_acc: 0.4566\n",
      "Epoch 202/220\n",
      "10976/10976 [==============================] - 6s 510us/step - loss: 0.0206 - acc: 0.4613 - val_loss: 0.0174 - val_acc: 0.4782\n",
      "Epoch 203/220\n",
      "10976/10976 [==============================] - 6s 523us/step - loss: 0.0205 - acc: 0.4623 - val_loss: 0.0243 - val_acc: 0.4785\n",
      "Epoch 204/220\n",
      "10976/10976 [==============================] - 6s 517us/step - loss: 0.0207 - acc: 0.4646 - val_loss: 0.0158 - val_acc: 0.4688\n",
      "Epoch 205/220\n",
      "10976/10976 [==============================] - 5s 499us/step - loss: 0.0199 - acc: 0.4628 - val_loss: 0.0248 - val_acc: 0.4678\n",
      "Epoch 206/220\n",
      "10976/10976 [==============================] - 6s 532us/step - loss: 0.0206 - acc: 0.4643 - val_loss: 0.0176 - val_acc: 0.4725\n",
      "Epoch 207/220\n",
      "10976/10976 [==============================] - 6s 515us/step - loss: 0.0207 - acc: 0.4642 - val_loss: 0.0231 - val_acc: 0.4710\n",
      "Epoch 208/220\n",
      "10976/10976 [==============================] - 6s 517us/step - loss: 0.0205 - acc: 0.4633 - val_loss: 0.0173 - val_acc: 0.4821\n",
      "Epoch 209/220\n",
      "10976/10976 [==============================] - 5s 500us/step - loss: 0.0202 - acc: 0.4661 - val_loss: 0.0231 - val_acc: 0.4534\n",
      "Epoch 210/220\n",
      "10976/10976 [==============================] - 6s 533us/step - loss: 0.0204 - acc: 0.4640 - val_loss: 0.0168 - val_acc: 0.4823\n",
      "Epoch 211/220\n",
      "10976/10976 [==============================] - 6s 518us/step - loss: 0.0207 - acc: 0.4638 - val_loss: 0.0234 - val_acc: 0.4583\n",
      "Epoch 212/220\n",
      "10976/10976 [==============================] - 6s 521us/step - loss: 0.0210 - acc: 0.4664 - val_loss: 0.0197 - val_acc: 0.4842\n",
      "Epoch 213/220\n",
      "10976/10976 [==============================] - 6s 503us/step - loss: 0.0207 - acc: 0.4584 - val_loss: 0.0229 - val_acc: 0.4544\n",
      "Epoch 214/220\n",
      "10976/10976 [==============================] - 6s 510us/step - loss: 0.0204 - acc: 0.4620 - val_loss: 0.0183 - val_acc: 0.4724\n",
      "Epoch 215/220\n",
      "10976/10976 [==============================] - 6s 505us/step - loss: 0.0207 - acc: 0.4638 - val_loss: 0.0251 - val_acc: 0.4727\n",
      "Epoch 216/220\n",
      "10976/10976 [==============================] - 6s 521us/step - loss: 0.0205 - acc: 0.4630 - val_loss: 0.0158 - val_acc: 0.4795\n",
      "Epoch 217/220\n",
      "10976/10976 [==============================] - 6s 507us/step - loss: 0.0207 - acc: 0.4633 - val_loss: 0.0243 - val_acc: 0.4597\n",
      "Epoch 218/220\n",
      "10976/10976 [==============================] - 5s 484us/step - loss: 0.0203 - acc: 0.4657 - val_loss: 0.0149 - val_acc: 0.4782\n",
      "Epoch 219/220\n",
      "10976/10976 [==============================] - 5s 497us/step - loss: 0.0202 - acc: 0.4627 - val_loss: 0.0200 - val_acc: 0.4535\n",
      "Epoch 220/220\n",
      "10976/10976 [==============================] - 6s 516us/step - loss: 0.0202 - acc: 0.4642 - val_loss: 0.0195 - val_acc: 0.4738\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 201/220\n",
      "10976/10976 [==============================] - 6s 509us/step - loss: 0.0241 - acc: 0.4919 - val_loss: 0.0231 - val_acc: 0.5286\n",
      "Epoch 202/220\n",
      "10976/10976 [==============================] - 5s 495us/step - loss: 0.0248 - acc: 0.4915 - val_loss: 0.0229 - val_acc: 0.4389\n",
      "Epoch 203/220\n",
      "10976/10976 [==============================] - 6s 529us/step - loss: 0.0235 - acc: 0.4777 - val_loss: 0.0242 - val_acc: 0.4166\n",
      "Epoch 204/220\n",
      "10976/10976 [==============================] - 6s 513us/step - loss: 0.0235 - acc: 0.4314 - val_loss: 0.0241 - val_acc: 0.5295\n",
      "Epoch 205/220\n",
      "10976/10976 [==============================] - 6s 524us/step - loss: 0.0239 - acc: 0.4766 - val_loss: 0.0234 - val_acc: 0.4741\n",
      "Epoch 206/220\n",
      "10976/10976 [==============================] - 6s 506us/step - loss: 0.0237 - acc: 0.4841 - val_loss: 0.0228 - val_acc: 0.5345\n",
      "Epoch 207/220\n",
      "10976/10976 [==============================] - 6s 504us/step - loss: 0.0239 - acc: 0.4878 - val_loss: 0.0255 - val_acc: 0.4480\n",
      "Epoch 208/220\n",
      "10976/10976 [==============================] - 6s 505us/step - loss: 0.0240 - acc: 0.4843 - val_loss: 0.0235 - val_acc: 0.5056\n",
      "Epoch 209/220\n",
      "10976/10976 [==============================] - 6s 523us/step - loss: 0.0235 - acc: 0.4873 - val_loss: 0.0218 - val_acc: 0.5294\n",
      "Epoch 210/220\n",
      "10976/10976 [==============================] - 6s 502us/step - loss: 0.0236 - acc: 0.4802 - val_loss: 0.0243 - val_acc: 0.3552\n",
      "Epoch 211/220\n",
      "10976/10976 [==============================] - 6s 521us/step - loss: 0.0235 - acc: 0.4710 - val_loss: 0.0226 - val_acc: 0.4786\n",
      "Epoch 212/220\n",
      "10976/10976 [==============================] - 6s 527us/step - loss: 0.0237 - acc: 0.4912 - val_loss: 0.0227 - val_acc: 0.5152\n",
      "Epoch 213/220\n",
      "10976/10976 [==============================] - 5s 496us/step - loss: 0.0237 - acc: 0.4914 - val_loss: 0.0237 - val_acc: 0.4368\n",
      "Epoch 214/220\n",
      "10976/10976 [==============================] - 6s 532us/step - loss: 0.0232 - acc: 0.4677 - val_loss: 0.0231 - val_acc: 0.5288\n",
      "Epoch 215/220\n",
      "10976/10976 [==============================] - 6s 512us/step - loss: 0.0236 - acc: 0.4316 - val_loss: 0.0238 - val_acc: 0.3751\n",
      "Epoch 216/220\n",
      "10976/10976 [==============================] - 6s 512us/step - loss: 0.0237 - acc: 0.4296 - val_loss: 0.0234 - val_acc: 0.4867\n",
      "Epoch 217/220\n",
      "10976/10976 [==============================] - 5s 488us/step - loss: 0.0234 - acc: 0.4352 - val_loss: 0.0239 - val_acc: 0.3888\n",
      "Epoch 218/220\n",
      "10976/10976 [==============================] - 6s 526us/step - loss: 0.0236 - acc: 0.4298 - val_loss: 0.0236 - val_acc: 0.4978\n",
      "Epoch 219/220\n",
      "10976/10976 [==============================] - 6s 508us/step - loss: 0.0233 - acc: 0.4456 - val_loss: 0.0237 - val_acc: 0.4629\n",
      "Epoch 220/220\n",
      "10976/10976 [==============================] - 6s 520us/step - loss: 0.0241 - acc: 0.4911 - val_loss: 0.0231 - val_acc: 0.5243\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 201/220\n",
      "10976/10976 [==============================] - 6s 518us/step - loss: 0.0185 - acc: 0.3382 - val_loss: 0.0215 - val_acc: 0.4328\n",
      "Epoch 202/220\n",
      "10976/10976 [==============================] - 6s 522us/step - loss: 0.0187 - acc: 0.3370 - val_loss: 0.0162 - val_acc: 0.2200\n",
      "Epoch 203/220\n",
      "10976/10976 [==============================] - 6s 511us/step - loss: 0.0184 - acc: 0.3319 - val_loss: 0.0199 - val_acc: 0.4265\n",
      "Epoch 204/220\n",
      "10976/10976 [==============================] - 6s 519us/step - loss: 0.0186 - acc: 0.3324 - val_loss: 0.0182 - val_acc: 0.2184\n",
      "Epoch 205/220\n",
      "10976/10976 [==============================] - 6s 512us/step - loss: 0.0185 - acc: 0.3371 - val_loss: 0.0206 - val_acc: 0.4936\n",
      "Epoch 206/220\n",
      "10976/10976 [==============================] - 6s 506us/step - loss: 0.0186 - acc: 0.3406 - val_loss: 0.0148 - val_acc: 0.2238\n",
      "Epoch 207/220\n",
      "10976/10976 [==============================] - 5s 498us/step - loss: 0.0184 - acc: 0.3414 - val_loss: 0.0212 - val_acc: 0.4334\n",
      "Epoch 208/220\n",
      "10976/10976 [==============================] - 5s 482us/step - loss: 0.0185 - acc: 0.3403 - val_loss: 0.0176 - val_acc: 0.2167\n",
      "Epoch 209/220\n",
      "10976/10976 [==============================] - 6s 502us/step - loss: 0.0188 - acc: 0.3291 - val_loss: 0.0217 - val_acc: 0.4341\n",
      "Epoch 210/220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 6s 528us/step - loss: 0.0186 - acc: 0.3387 - val_loss: 0.0184 - val_acc: 0.2166\n",
      "Epoch 211/220\n",
      "10976/10976 [==============================] - 6s 513us/step - loss: 0.0186 - acc: 0.3379 - val_loss: 0.0207 - val_acc: 0.4966\n",
      "Epoch 212/220\n",
      "10976/10976 [==============================] - 6s 535us/step - loss: 0.0184 - acc: 0.3438 - val_loss: 0.0160 - val_acc: 0.2208\n",
      "Epoch 213/220\n",
      "10976/10976 [==============================] - 6s 531us/step - loss: 0.0185 - acc: 0.3375 - val_loss: 0.0215 - val_acc: 0.4956\n",
      "Epoch 214/220\n",
      "10976/10976 [==============================] - 5s 491us/step - loss: 0.0185 - acc: 0.3332 - val_loss: 0.0201 - val_acc: 0.2129\n",
      "Epoch 215/220\n",
      "10976/10976 [==============================] - 6s 502us/step - loss: 0.0186 - acc: 0.3326 - val_loss: 0.0210 - val_acc: 0.4321\n",
      "Epoch 216/220\n",
      "10976/10976 [==============================] - 6s 515us/step - loss: 0.0185 - acc: 0.3408 - val_loss: 0.0172 - val_acc: 0.2191\n",
      "Epoch 217/220\n",
      "10976/10976 [==============================] - 6s 537us/step - loss: 0.0184 - acc: 0.3314 - val_loss: 0.0207 - val_acc: 0.4329\n",
      "Epoch 218/220\n",
      "10976/10976 [==============================] - 6s 532us/step - loss: 0.0183 - acc: 0.3488 - val_loss: 0.0169 - val_acc: 0.2210\n",
      "Epoch 219/220\n",
      "10976/10976 [==============================] - 6s 511us/step - loss: 0.0182 - acc: 0.3373 - val_loss: 0.0204 - val_acc: 0.4965\n",
      "Epoch 220/220\n",
      "10976/10976 [==============================] - 5s 500us/step - loss: 0.0185 - acc: 0.3460 - val_loss: 0.0178 - val_acc: 0.2188\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 201/220\n",
      "10976/10976 [==============================] - 6s 523us/step - loss: 0.0155 - acc: 0.3390 - val_loss: 0.0149 - val_acc: 0.4042\n",
      "Epoch 202/220\n",
      "10976/10976 [==============================] - 6s 515us/step - loss: 0.0157 - acc: 0.3464 - val_loss: 0.0148 - val_acc: 0.3448\n",
      "Epoch 203/220\n",
      "10976/10976 [==============================] - 6s 526us/step - loss: 0.0157 - acc: 0.3441 - val_loss: 0.0163 - val_acc: 0.3569\n",
      "Epoch 204/220\n",
      "10976/10976 [==============================] - 6s 530us/step - loss: 0.0157 - acc: 0.3369 - val_loss: 0.0157 - val_acc: 0.3226\n",
      "Epoch 205/220\n",
      "10976/10976 [==============================] - 6s 540us/step - loss: 0.0155 - acc: 0.3392 - val_loss: 0.0151 - val_acc: 0.4071\n",
      "Epoch 206/220\n",
      "10976/10976 [==============================] - 6s 511us/step - loss: 0.0156 - acc: 0.3452 - val_loss: 0.0147 - val_acc: 0.2332\n",
      "Epoch 207/220\n",
      "10976/10976 [==============================] - 5s 497us/step - loss: 0.0155 - acc: 0.3464 - val_loss: 0.0156 - val_acc: 0.4006\n",
      "Epoch 208/220\n",
      "10976/10976 [==============================] - 6s 527us/step - loss: 0.0156 - acc: 0.3493 - val_loss: 0.0147 - val_acc: 0.3475\n",
      "Epoch 209/220\n",
      "10976/10976 [==============================] - 6s 552us/step - loss: 0.0159 - acc: 0.3812 - val_loss: 0.0177 - val_acc: 0.3904\n",
      "Epoch 210/220\n",
      "10976/10976 [==============================] - 6s 524us/step - loss: 0.0158 - acc: 0.3941 - val_loss: 0.0117 - val_acc: 0.3925\n",
      "Epoch 211/220\n",
      "10976/10976 [==============================] - 6s 509us/step - loss: 0.0158 - acc: 0.3871 - val_loss: 0.0154 - val_acc: 0.2348\n",
      "Epoch 212/220\n",
      "10976/10976 [==============================] - 5s 501us/step - loss: 0.0159 - acc: 0.3937 - val_loss: 0.0119 - val_acc: 0.4357\n",
      "Epoch 213/220\n",
      "10976/10976 [==============================] - 6s 514us/step - loss: 0.0158 - acc: 0.3939 - val_loss: 0.0190 - val_acc: 0.4171\n",
      "Epoch 214/220\n",
      "10976/10976 [==============================] - 6s 537us/step - loss: 0.0158 - acc: 0.3811 - val_loss: 0.0129 - val_acc: 0.4326\n",
      "Epoch 215/220\n",
      "10976/10976 [==============================] - 6s 516us/step - loss: 0.0156 - acc: 0.3895 - val_loss: 0.0190 - val_acc: 0.4070\n",
      "Epoch 216/220\n",
      "10976/10976 [==============================] - 6s 507us/step - loss: 0.0159 - acc: 0.3946 - val_loss: 0.0122 - val_acc: 0.3769\n",
      "Epoch 217/220\n",
      "10976/10976 [==============================] - 6s 508us/step - loss: 0.0155 - acc: 0.3991 - val_loss: 0.0173 - val_acc: 0.3577\n",
      "Epoch 218/220\n",
      "10976/10976 [==============================] - 5s 478us/step - loss: 0.0156 - acc: 0.3915 - val_loss: 0.0113 - val_acc: 0.4423\n",
      "Epoch 219/220\n",
      "10976/10976 [==============================] - 6s 531us/step - loss: 0.0155 - acc: 0.4024 - val_loss: 0.0186 - val_acc: 0.4097\n",
      "Epoch 220/220\n",
      "10976/10976 [==============================] - 5s 498us/step - loss: 0.0156 - acc: 0.3929 - val_loss: 0.0114 - val_acc: 0.4113\n",
      "start training round 11\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 221/240\n",
      "10976/10976 [==============================] - 6s 532us/step - loss: 0.0200 - acc: 0.4655 - val_loss: 0.0243 - val_acc: 0.4700\n",
      "Epoch 222/240\n",
      "10976/10976 [==============================] - 6s 523us/step - loss: 0.0208 - acc: 0.4673 - val_loss: 0.0175 - val_acc: 0.4808\n",
      "Epoch 223/240\n",
      "10976/10976 [==============================] - 6s 542us/step - loss: 0.0201 - acc: 0.4671 - val_loss: 0.0226 - val_acc: 0.4446\n",
      "Epoch 224/240\n",
      "10976/10976 [==============================] - 6s 510us/step - loss: 0.0201 - acc: 0.4618 - val_loss: 0.0180 - val_acc: 0.4768\n",
      "Epoch 225/240\n",
      "10976/10976 [==============================] - 6s 517us/step - loss: 0.0205 - acc: 0.4662 - val_loss: 0.0225 - val_acc: 0.4597\n",
      "Epoch 226/240\n",
      "10976/10976 [==============================] - 6s 543us/step - loss: 0.0207 - acc: 0.4646 - val_loss: 0.0178 - val_acc: 0.4789\n",
      "Epoch 227/240\n",
      "10976/10976 [==============================] - 6s 512us/step - loss: 0.0204 - acc: 0.4612 - val_loss: 0.0248 - val_acc: 0.4564\n",
      "Epoch 228/240\n",
      "10976/10976 [==============================] - 6s 525us/step - loss: 0.0203 - acc: 0.4617 - val_loss: 0.0176 - val_acc: 0.4818\n",
      "Epoch 229/240\n",
      "10976/10976 [==============================] - 6s 516us/step - loss: 0.0205 - acc: 0.4674 - val_loss: 0.0206 - val_acc: 0.4288\n",
      "Epoch 230/240\n",
      "10976/10976 [==============================] - 6s 512us/step - loss: 0.0206 - acc: 0.4650 - val_loss: 0.0166 - val_acc: 0.4821\n",
      "Epoch 231/240\n",
      "10976/10976 [==============================] - 6s 514us/step - loss: 0.0203 - acc: 0.4681 - val_loss: 0.0222 - val_acc: 0.4350\n",
      "Epoch 232/240\n",
      "10976/10976 [==============================] - 6s 534us/step - loss: 0.0206 - acc: 0.4636 - val_loss: 0.0177 - val_acc: 0.4807\n",
      "Epoch 233/240\n",
      "10976/10976 [==============================] - 6s 520us/step - loss: 0.0203 - acc: 0.4684 - val_loss: 0.0240 - val_acc: 0.4618\n",
      "Epoch 234/240\n",
      "10976/10976 [==============================] - 5s 494us/step - loss: 0.0204 - acc: 0.4666 - val_loss: 0.0168 - val_acc: 0.4795\n",
      "Epoch 235/240\n",
      "10976/10976 [==============================] - 6s 509us/step - loss: 0.0205 - acc: 0.4653 - val_loss: 0.0251 - val_acc: 0.4693\n",
      "Epoch 236/240\n",
      "10976/10976 [==============================] - 5s 499us/step - loss: 0.0207 - acc: 0.4646 - val_loss: 0.0182 - val_acc: 0.4729\n",
      "Epoch 237/240\n",
      "10976/10976 [==============================] - 6s 510us/step - loss: 0.0202 - acc: 0.4642 - val_loss: 0.0229 - val_acc: 0.4671\n",
      "Epoch 238/240\n",
      "10976/10976 [==============================] - 5s 498us/step - loss: 0.0201 - acc: 0.4623 - val_loss: 0.0163 - val_acc: 0.4778\n",
      "Epoch 239/240\n",
      "10976/10976 [==============================] - 6s 512us/step - loss: 0.0207 - acc: 0.4688 - val_loss: 0.0196 - val_acc: 0.4341\n",
      "Epoch 240/240\n",
      "10976/10976 [==============================] - 6s 534us/step - loss: 0.0207 - acc: 0.4646 - val_loss: 0.0176 - val_acc: 0.4810\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 221/240\n",
      "10976/10976 [==============================] - 6s 509us/step - loss: 0.0238 - acc: 0.4899 - val_loss: 0.0246 - val_acc: 0.4789\n",
      "Epoch 222/240\n",
      "10976/10976 [==============================] - 6s 524us/step - loss: 0.0233 - acc: 0.4812 - val_loss: 0.0226 - val_acc: 0.5371\n",
      "Epoch 223/240\n",
      "10976/10976 [==============================] - 6s 514us/step - loss: 0.0237 - acc: 0.4821 - val_loss: 0.0250 - val_acc: 0.4721\n",
      "Epoch 224/240\n",
      "10976/10976 [==============================] - 6s 515us/step - loss: 0.0236 - acc: 0.4332 - val_loss: 0.0243 - val_acc: 0.4890\n",
      "Epoch 225/240\n",
      "10976/10976 [==============================] - 6s 507us/step - loss: 0.0235 - acc: 0.4323 - val_loss: 0.0237 - val_acc: 0.3701\n",
      "Epoch 226/240\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 6s 527us/step - loss: 0.0236 - acc: 0.4280 - val_loss: 0.0240 - val_acc: 0.4961\n",
      "Epoch 227/240\n",
      "10976/10976 [==============================] - 5s 489us/step - loss: 0.0234 - acc: 0.4349 - val_loss: 0.0233 - val_acc: 0.3766\n",
      "Epoch 228/240\n",
      "10976/10976 [==============================] - 5s 489us/step - loss: 0.0238 - acc: 0.4782 - val_loss: 0.0237 - val_acc: 0.5241\n",
      "Epoch 229/240\n",
      "10976/10976 [==============================] - 6s 506us/step - loss: 0.0236 - acc: 0.4897 - val_loss: 0.0238 - val_acc: 0.4390\n",
      "Epoch 230/240\n",
      "10976/10976 [==============================] - 6s 524us/step - loss: 0.0237 - acc: 0.4887 - val_loss: 0.0233 - val_acc: 0.5395\n",
      "Epoch 231/240\n",
      "10976/10976 [==============================] - 6s 513us/step - loss: 0.0236 - acc: 0.4908 - val_loss: 0.0241 - val_acc: 0.4880\n",
      "Epoch 232/240\n",
      "10976/10976 [==============================] - 6s 511us/step - loss: 0.0238 - acc: 0.4905 - val_loss: 0.0239 - val_acc: 0.5089\n",
      "Epoch 233/240\n",
      "10976/10976 [==============================] - 6s 512us/step - loss: 0.0237 - acc: 0.4907 - val_loss: 0.0235 - val_acc: 0.4515\n",
      "Epoch 234/240\n",
      "10976/10976 [==============================] - 6s 540us/step - loss: 0.0232 - acc: 0.4793 - val_loss: 0.0235 - val_acc: 0.5356\n",
      "Epoch 235/240\n",
      "10976/10976 [==============================] - 6s 525us/step - loss: 0.0236 - acc: 0.4866 - val_loss: 0.0224 - val_acc: 0.4371\n",
      "Epoch 236/240\n",
      "10976/10976 [==============================] - 6s 513us/step - loss: 0.0233 - acc: 0.4815 - val_loss: 0.0256 - val_acc: 0.5265\n",
      "Epoch 237/240\n",
      "10976/10976 [==============================] - 6s 518us/step - loss: 0.0239 - acc: 0.4939 - val_loss: 0.0249 - val_acc: 0.4935\n",
      "Epoch 238/240\n",
      "10976/10976 [==============================] - 5s 501us/step - loss: 0.0239 - acc: 0.4686 - val_loss: 0.0230 - val_acc: 0.5097\n",
      "Epoch 239/240\n",
      "10976/10976 [==============================] - 6s 507us/step - loss: 0.0234 - acc: 0.4361 - val_loss: 0.0233 - val_acc: 0.3983\n",
      "Epoch 240/240\n",
      "10976/10976 [==============================] - 6s 516us/step - loss: 0.0233 - acc: 0.4382 - val_loss: 0.0253 - val_acc: 0.5026\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 221/240\n",
      "10976/10976 [==============================] - 6s 523us/step - loss: 0.0185 - acc: 0.3307 - val_loss: 0.0216 - val_acc: 0.4345\n",
      "Epoch 222/240\n",
      "10976/10976 [==============================] - 6s 513us/step - loss: 0.0184 - acc: 0.3316 - val_loss: 0.0169 - val_acc: 0.2182\n",
      "Epoch 223/240\n",
      "10976/10976 [==============================] - 6s 533us/step - loss: 0.0184 - acc: 0.3322 - val_loss: 0.0176 - val_acc: 0.4983\n",
      "Epoch 224/240\n",
      "10976/10976 [==============================] - 5s 501us/step - loss: 0.0185 - acc: 0.3372 - val_loss: 0.0170 - val_acc: 0.2215\n",
      "Epoch 225/240\n",
      "10976/10976 [==============================] - 6s 513us/step - loss: 0.0184 - acc: 0.3423 - val_loss: 0.0206 - val_acc: 0.4362\n",
      "Epoch 226/240\n",
      "10976/10976 [==============================] - 6s 513us/step - loss: 0.0183 - acc: 0.3502 - val_loss: 0.0159 - val_acc: 0.2202\n",
      "Epoch 227/240\n",
      "10976/10976 [==============================] - 6s 507us/step - loss: 0.0182 - acc: 0.3413 - val_loss: 0.0208 - val_acc: 0.4998\n",
      "Epoch 228/240\n",
      "10976/10976 [==============================] - 6s 532us/step - loss: 0.0185 - acc: 0.3465 - val_loss: 0.0169 - val_acc: 0.2166\n",
      "Epoch 229/240\n",
      "10976/10976 [==============================] - 6s 507us/step - loss: 0.0182 - acc: 0.3352 - val_loss: 0.0200 - val_acc: 0.4987\n",
      "Epoch 230/240\n",
      "10976/10976 [==============================] - 6s 502us/step - loss: 0.0183 - acc: 0.3469 - val_loss: 0.0167 - val_acc: 0.2193\n",
      "Epoch 231/240\n",
      "10976/10976 [==============================] - 6s 518us/step - loss: 0.0183 - acc: 0.3445 - val_loss: 0.0221 - val_acc: 0.4968\n",
      "Epoch 232/240\n",
      "10976/10976 [==============================] - 6s 519us/step - loss: 0.0182 - acc: 0.3481 - val_loss: 0.0161 - val_acc: 0.2182\n",
      "Epoch 233/240\n",
      "10976/10976 [==============================] - 6s 534us/step - loss: 0.0184 - acc: 0.3364 - val_loss: 0.0212 - val_acc: 0.4988\n",
      "Epoch 234/240\n",
      "10976/10976 [==============================] - 6s 522us/step - loss: 0.0182 - acc: 0.3400 - val_loss: 0.0177 - val_acc: 0.2139\n",
      "Epoch 235/240\n",
      "10976/10976 [==============================] - 6s 528us/step - loss: 0.0182 - acc: 0.3408 - val_loss: 0.0211 - val_acc: 0.4436\n",
      "Epoch 236/240\n",
      "10976/10976 [==============================] - 5s 495us/step - loss: 0.0182 - acc: 0.3458 - val_loss: 0.0158 - val_acc: 0.2201\n",
      "Epoch 237/240\n",
      "10976/10976 [==============================] - 5s 499us/step - loss: 0.0183 - acc: 0.3346 - val_loss: 0.0200 - val_acc: 0.4417\n",
      "Epoch 238/240\n",
      "10976/10976 [==============================] - 5s 495us/step - loss: 0.0183 - acc: 0.3462 - val_loss: 0.0174 - val_acc: 0.2164\n",
      "Epoch 239/240\n",
      "10976/10976 [==============================] - 6s 519us/step - loss: 0.0182 - acc: 0.3432 - val_loss: 0.0214 - val_acc: 0.4975\n",
      "Epoch 240/240\n",
      "10976/10976 [==============================] - 6s 519us/step - loss: 0.0183 - acc: 0.3530 - val_loss: 0.0154 - val_acc: 0.2200\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 221/240\n",
      "10976/10976 [==============================] - 5s 493us/step - loss: 0.0157 - acc: 0.3962 - val_loss: 0.0188 - val_acc: 0.4014\n",
      "Epoch 222/240\n",
      "10976/10976 [==============================] - 6s 532us/step - loss: 0.0156 - acc: 0.3909 - val_loss: 0.0129 - val_acc: 0.4160\n",
      "Epoch 223/240\n",
      "10976/10976 [==============================] - 6s 547us/step - loss: 0.0155 - acc: 0.3793 - val_loss: 0.0182 - val_acc: 0.3626\n",
      "Epoch 224/240\n",
      "10976/10976 [==============================] - 6s 549us/step - loss: 0.0157 - acc: 0.3807 - val_loss: 0.0115 - val_acc: 0.4361\n",
      "Epoch 225/240\n",
      "10976/10976 [==============================] - 6s 539us/step - loss: 0.0153 - acc: 0.3748 - val_loss: 0.0182 - val_acc: 0.3872\n",
      "Epoch 226/240\n",
      "10976/10976 [==============================] - 6s 509us/step - loss: 0.0156 - acc: 0.3875 - val_loss: 0.0120 - val_acc: 0.4219\n",
      "Epoch 227/240\n",
      "10976/10976 [==============================] - 6s 526us/step - loss: 0.0155 - acc: 0.3796 - val_loss: 0.0180 - val_acc: 0.3746\n",
      "Epoch 228/240\n",
      "10976/10976 [==============================] - 6s 511us/step - loss: 0.0157 - acc: 0.3819 - val_loss: 0.0124 - val_acc: 0.3708\n",
      "Epoch 229/240\n",
      "10976/10976 [==============================] - 6s 512us/step - loss: 0.0154 - acc: 0.3274 - val_loss: 0.0163 - val_acc: 0.4094\n",
      "Epoch 230/240\n",
      "10976/10976 [==============================] - 6s 533us/step - loss: 0.0157 - acc: 0.2759 - val_loss: 0.0138 - val_acc: 0.1586\n",
      "Epoch 231/240\n",
      "10976/10976 [==============================] - 6s 530us/step - loss: 0.0154 - acc: 0.2766 - val_loss: 0.0172 - val_acc: 0.4064\n",
      "Epoch 232/240\n",
      "10976/10976 [==============================] - 6s 517us/step - loss: 0.0156 - acc: 0.2833 - val_loss: 0.0153 - val_acc: 0.1502\n",
      "Epoch 233/240\n",
      "10976/10976 [==============================] - 6s 520us/step - loss: 0.0156 - acc: 0.2737 - val_loss: 0.0171 - val_acc: 0.4134\n",
      "Epoch 234/240\n",
      "10976/10976 [==============================] - 6s 505us/step - loss: 0.0154 - acc: 0.2793 - val_loss: 0.0135 - val_acc: 0.1613\n",
      "Epoch 235/240\n",
      "10976/10976 [==============================] - 6s 521us/step - loss: 0.0154 - acc: 0.2717 - val_loss: 0.0170 - val_acc: 0.4091\n",
      "Epoch 236/240\n",
      "10976/10976 [==============================] - 6s 514us/step - loss: 0.0156 - acc: 0.2730 - val_loss: 0.0132 - val_acc: 0.1597\n",
      "Epoch 237/240\n",
      "10976/10976 [==============================] - 6s 519us/step - loss: 0.0155 - acc: 0.2757 - val_loss: 0.0169 - val_acc: 0.4070\n",
      "Epoch 238/240\n",
      "10976/10976 [==============================] - 6s 527us/step - loss: 0.0154 - acc: 0.2825 - val_loss: 0.0149 - val_acc: 0.1579\n",
      "Epoch 239/240\n",
      "10976/10976 [==============================] - 6s 512us/step - loss: 0.0155 - acc: 0.2739 - val_loss: 0.0160 - val_acc: 0.4095\n",
      "Epoch 240/240\n",
      "10976/10976 [==============================] - 6s 528us/step - loss: 0.0155 - acc: 0.3801 - val_loss: 0.0119 - val_acc: 0.4194\n",
      "start training round 12\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 241/260\n",
      "10976/10976 [==============================] - 6s 528us/step - loss: 0.0202 - acc: 0.4636 - val_loss: 0.0231 - val_acc: 0.4684\n",
      "Epoch 242/260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 6s 537us/step - loss: 0.0199 - acc: 0.4617 - val_loss: 0.0177 - val_acc: 0.4670\n",
      "Epoch 243/260\n",
      "10976/10976 [==============================] - 6s 522us/step - loss: 0.0202 - acc: 0.4629 - val_loss: 0.0233 - val_acc: 0.4644\n",
      "Epoch 244/260\n",
      "10976/10976 [==============================] - 5s 497us/step - loss: 0.0204 - acc: 0.4671 - val_loss: 0.0151 - val_acc: 0.4848\n",
      "Epoch 245/260\n",
      "10976/10976 [==============================] - 6s 520us/step - loss: 0.0196 - acc: 0.4588 - val_loss: 0.0239 - val_acc: 0.4741\n",
      "Epoch 246/260\n",
      "10976/10976 [==============================] - 5s 495us/step - loss: 0.0204 - acc: 0.4672 - val_loss: 0.0169 - val_acc: 0.4790\n",
      "Epoch 247/260\n",
      "10976/10976 [==============================] - 5s 476us/step - loss: 0.0199 - acc: 0.4650 - val_loss: 0.0225 - val_acc: 0.4652\n",
      "Epoch 248/260\n",
      "10976/10976 [==============================] - 6s 506us/step - loss: 0.0208 - acc: 0.4619 - val_loss: 0.0235 - val_acc: 0.4566\n",
      "Epoch 249/260\n",
      "10976/10976 [==============================] - 6s 507us/step - loss: 0.0208 - acc: 0.4644 - val_loss: 0.0164 - val_acc: 0.4855\n",
      "Epoch 250/260\n",
      "10976/10976 [==============================] - 6s 504us/step - loss: 0.0202 - acc: 0.4607 - val_loss: 0.0229 - val_acc: 0.4473\n",
      "Epoch 251/260\n",
      "10976/10976 [==============================] - 6s 530us/step - loss: 0.0200 - acc: 0.4568 - val_loss: 0.0174 - val_acc: 0.4813\n",
      "Epoch 252/260\n",
      "10976/10976 [==============================] - 6s 526us/step - loss: 0.0203 - acc: 0.4632 - val_loss: 0.0236 - val_acc: 0.4704\n",
      "Epoch 253/260\n",
      "10976/10976 [==============================] - 6s 509us/step - loss: 0.0206 - acc: 0.4656 - val_loss: 0.0179 - val_acc: 0.4816\n",
      "Epoch 254/260\n",
      "10976/10976 [==============================] - 6s 517us/step - loss: 0.0203 - acc: 0.4636 - val_loss: 0.0243 - val_acc: 0.4717\n",
      "Epoch 255/260\n",
      "10976/10976 [==============================] - 6s 506us/step - loss: 0.0203 - acc: 0.4705 - val_loss: 0.0171 - val_acc: 0.4822\n",
      "Epoch 256/260\n",
      "10976/10976 [==============================] - 6s 523us/step - loss: 0.0201 - acc: 0.4672 - val_loss: 0.0236 - val_acc: 0.4670\n",
      "Epoch 257/260\n",
      "10976/10976 [==============================] - 6s 527us/step - loss: 0.0200 - acc: 0.4642 - val_loss: 0.0168 - val_acc: 0.4839\n",
      "Epoch 258/260\n",
      "10976/10976 [==============================] - 6s 515us/step - loss: 0.0197 - acc: 0.4657 - val_loss: 0.0243 - val_acc: 0.4587\n",
      "Epoch 259/260\n",
      "10976/10976 [==============================] - 6s 502us/step - loss: 0.0205 - acc: 0.4683 - val_loss: 0.0162 - val_acc: 0.4699\n",
      "Epoch 260/260\n",
      "10976/10976 [==============================] - 6s 506us/step - loss: 0.0200 - acc: 0.4682 - val_loss: 0.0244 - val_acc: 0.4660\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 241/260\n",
      "10976/10976 [==============================] - 6s 517us/step - loss: 0.0237 - acc: 0.4912 - val_loss: 0.0240 - val_acc: 0.4619\n",
      "Epoch 242/260\n",
      "10976/10976 [==============================] - 6s 510us/step - loss: 0.0238 - acc: 0.4886 - val_loss: 0.0243 - val_acc: 0.5441\n",
      "Epoch 243/260\n",
      "10976/10976 [==============================] - 6s 530us/step - loss: 0.0236 - acc: 0.4968 - val_loss: 0.0234 - val_acc: 0.4410\n",
      "Epoch 244/260\n",
      "10976/10976 [==============================] - 6s 502us/step - loss: 0.0233 - acc: 0.4915 - val_loss: 0.0242 - val_acc: 0.5384\n",
      "Epoch 245/260\n",
      "10976/10976 [==============================] - 5s 498us/step - loss: 0.0236 - acc: 0.4936 - val_loss: 0.0242 - val_acc: 0.3785\n",
      "Epoch 246/260\n",
      "10976/10976 [==============================] - 6s 507us/step - loss: 0.0232 - acc: 0.4612 - val_loss: 0.0234 - val_acc: 0.5400\n",
      "Epoch 247/260\n",
      "10976/10976 [==============================] - 6s 520us/step - loss: 0.0234 - acc: 0.4473 - val_loss: 0.0242 - val_acc: 0.3707\n",
      "Epoch 248/260\n",
      "10976/10976 [==============================] - 6s 501us/step - loss: 0.0235 - acc: 0.4302 - val_loss: 0.0239 - val_acc: 0.4999\n",
      "Epoch 249/260\n",
      "10976/10976 [==============================] - 6s 504us/step - loss: 0.0233 - acc: 0.4329 - val_loss: 0.0237 - val_acc: 0.4555\n",
      "Epoch 250/260\n",
      "10976/10976 [==============================] - 6s 525us/step - loss: 0.0234 - acc: 0.4386 - val_loss: 0.0237 - val_acc: 0.4373\n",
      "Epoch 251/260\n",
      "10976/10976 [==============================] - 6s 525us/step - loss: 0.0233 - acc: 0.4347 - val_loss: 0.0237 - val_acc: 0.3863\n",
      "Epoch 252/260\n",
      "10976/10976 [==============================] - 6s 536us/step - loss: 0.0233 - acc: 0.4533 - val_loss: 0.0229 - val_acc: 0.5205\n",
      "Epoch 253/260\n",
      "10976/10976 [==============================] - 6s 502us/step - loss: 0.0233 - acc: 0.4971 - val_loss: 0.0249 - val_acc: 0.4884\n",
      "Epoch 254/260\n",
      "10976/10976 [==============================] - 5s 501us/step - loss: 0.0229 - acc: 0.4624 - val_loss: 0.0234 - val_acc: 0.5114\n",
      "Epoch 255/260\n",
      "10976/10976 [==============================] - 6s 521us/step - loss: 0.0232 - acc: 0.4300 - val_loss: 0.0232 - val_acc: 0.3822\n",
      "Epoch 256/260\n",
      "10976/10976 [==============================] - 6s 516us/step - loss: 0.0232 - acc: 0.4394 - val_loss: 0.0242 - val_acc: 0.4973\n",
      "Epoch 257/260\n",
      "10976/10976 [==============================] - 5s 474us/step - loss: 0.0234 - acc: 0.4359 - val_loss: 0.0234 - val_acc: 0.3768\n",
      "Epoch 258/260\n",
      "10976/10976 [==============================] - 6s 523us/step - loss: 0.0230 - acc: 0.4377 - val_loss: 0.0235 - val_acc: 0.4298\n",
      "Epoch 259/260\n",
      "10976/10976 [==============================] - 6s 516us/step - loss: 0.0232 - acc: 0.4396 - val_loss: 0.0253 - val_acc: 0.5137\n",
      "Epoch 260/260\n",
      "10976/10976 [==============================] - 5s 494us/step - loss: 0.0231 - acc: 0.4765 - val_loss: 0.0232 - val_acc: 0.5350\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 241/260\n",
      "10976/10976 [==============================] - 6s 526us/step - loss: 0.0182 - acc: 0.3345 - val_loss: 0.0203 - val_acc: 0.4353\n",
      "Epoch 242/260\n",
      "10976/10976 [==============================] - 5s 499us/step - loss: 0.0182 - acc: 0.3450 - val_loss: 0.0160 - val_acc: 0.2210\n",
      "Epoch 243/260\n",
      "10976/10976 [==============================] - 6s 518us/step - loss: 0.0181 - acc: 0.3455 - val_loss: 0.0203 - val_acc: 0.4392\n",
      "Epoch 244/260\n",
      "10976/10976 [==============================] - 6s 521us/step - loss: 0.0183 - acc: 0.3427 - val_loss: 0.0167 - val_acc: 0.2186\n",
      "Epoch 245/260\n",
      "10976/10976 [==============================] - 6s 523us/step - loss: 0.0182 - acc: 0.3401 - val_loss: 0.0205 - val_acc: 0.5010\n",
      "Epoch 246/260\n",
      "10976/10976 [==============================] - 6s 509us/step - loss: 0.0183 - acc: 0.3453 - val_loss: 0.0164 - val_acc: 0.2190\n",
      "Epoch 247/260\n",
      "10976/10976 [==============================] - 6s 506us/step - loss: 0.0181 - acc: 0.3429 - val_loss: 0.0204 - val_acc: 0.4323\n",
      "Epoch 248/260\n",
      "10976/10976 [==============================] - 5s 498us/step - loss: 0.0184 - acc: 0.3457 - val_loss: 0.0176 - val_acc: 0.2188\n",
      "Epoch 249/260\n",
      "10976/10976 [==============================] - 6s 525us/step - loss: 0.0182 - acc: 0.3444 - val_loss: 0.0195 - val_acc: 0.4344\n",
      "Epoch 250/260\n",
      "10976/10976 [==============================] - 6s 533us/step - loss: 0.0183 - acc: 0.3353 - val_loss: 0.0167 - val_acc: 0.2190\n",
      "Epoch 251/260\n",
      "10976/10976 [==============================] - 6s 558us/step - loss: 0.0182 - acc: 0.3381 - val_loss: 0.0204 - val_acc: 0.5014\n",
      "Epoch 252/260\n",
      "10976/10976 [==============================] - 6s 507us/step - loss: 0.0183 - acc: 0.3473 - val_loss: 0.0165 - val_acc: 0.2194\n",
      "Epoch 253/260\n",
      "10976/10976 [==============================] - 6s 526us/step - loss: 0.0182 - acc: 0.3485 - val_loss: 0.0208 - val_acc: 0.5028\n",
      "Epoch 254/260\n",
      "10976/10976 [==============================] - 6s 517us/step - loss: 0.0183 - acc: 0.3454 - val_loss: 0.0165 - val_acc: 0.2200\n",
      "Epoch 255/260\n",
      "10976/10976 [==============================] - 6s 512us/step - loss: 0.0180 - acc: 0.3328 - val_loss: 0.0208 - val_acc: 0.4352\n",
      "Epoch 256/260\n",
      "10976/10976 [==============================] - 6s 536us/step - loss: 0.0179 - acc: 0.3462 - val_loss: 0.0177 - val_acc: 0.2178\n",
      "Epoch 257/260\n",
      "10976/10976 [==============================] - 6s 515us/step - loss: 0.0182 - acc: 0.3353 - val_loss: 0.0206 - val_acc: 0.4365\n",
      "Epoch 258/260\n",
      "10976/10976 [==============================] - 6s 542us/step - loss: 0.0182 - acc: 0.3377 - val_loss: 0.0171 - val_acc: 0.2205\n",
      "Epoch 259/260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 6s 518us/step - loss: 0.0183 - acc: 0.3319 - val_loss: 0.0208 - val_acc: 0.5040\n",
      "Epoch 260/260\n",
      "10976/10976 [==============================] - 6s 509us/step - loss: 0.0182 - acc: 0.3389 - val_loss: 0.0168 - val_acc: 0.2183\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 241/260\n",
      "10976/10976 [==============================] - 6s 535us/step - loss: 0.0155 - acc: 0.4020 - val_loss: 0.0182 - val_acc: 0.4234\n",
      "Epoch 242/260\n",
      "10976/10976 [==============================] - 6s 542us/step - loss: 0.0155 - acc: 0.3957 - val_loss: 0.0127 - val_acc: 0.4257\n",
      "Epoch 243/260\n",
      "10976/10976 [==============================] - 6s 508us/step - loss: 0.0153 - acc: 0.3980 - val_loss: 0.0182 - val_acc: 0.4081\n",
      "Epoch 244/260\n",
      "10976/10976 [==============================] - 6s 529us/step - loss: 0.0154 - acc: 0.3983 - val_loss: 0.0128 - val_acc: 0.4482\n",
      "Epoch 245/260\n",
      "10976/10976 [==============================] - 6s 530us/step - loss: 0.0153 - acc: 0.4036 - val_loss: 0.0176 - val_acc: 0.3459\n",
      "Epoch 246/260\n",
      "10976/10976 [==============================] - 5s 486us/step - loss: 0.0153 - acc: 0.4022 - val_loss: 0.0127 - val_acc: 0.4183\n",
      "Epoch 247/260\n",
      "10976/10976 [==============================] - 6s 503us/step - loss: 0.0153 - acc: 0.3958 - val_loss: 0.0167 - val_acc: 0.3644\n",
      "Epoch 248/260\n",
      "10976/10976 [==============================] - 6s 515us/step - loss: 0.0154 - acc: 0.3988 - val_loss: 0.0131 - val_acc: 0.4188\n",
      "Epoch 249/260\n",
      "10976/10976 [==============================] - 6s 544us/step - loss: 0.0154 - acc: 0.4024 - val_loss: 0.0189 - val_acc: 0.4246\n",
      "Epoch 250/260\n",
      "10976/10976 [==============================] - 6s 509us/step - loss: 0.0152 - acc: 0.4129 - val_loss: 0.0113 - val_acc: 0.4347\n",
      "Epoch 251/260\n",
      "10976/10976 [==============================] - 6s 535us/step - loss: 0.0151 - acc: 0.3621 - val_loss: 0.0164 - val_acc: 0.4270\n",
      "Epoch 252/260\n",
      "10976/10976 [==============================] - 6s 516us/step - loss: 0.0154 - acc: 0.3539 - val_loss: 0.0132 - val_acc: 0.3015\n",
      "Epoch 253/260\n",
      "10976/10976 [==============================] - 6s 530us/step - loss: 0.0153 - acc: 0.3735 - val_loss: 0.0187 - val_acc: 0.4178\n",
      "Epoch 254/260\n",
      "10976/10976 [==============================] - 6s 510us/step - loss: 0.0154 - acc: 0.4075 - val_loss: 0.0124 - val_acc: 0.4531\n",
      "Epoch 255/260\n",
      "10976/10976 [==============================] - 6s 513us/step - loss: 0.0155 - acc: 0.4102 - val_loss: 0.0183 - val_acc: 0.3732\n",
      "Epoch 256/260\n",
      "10976/10976 [==============================] - 6s 524us/step - loss: 0.0154 - acc: 0.4091 - val_loss: 0.0117 - val_acc: 0.3662\n",
      "Epoch 257/260\n",
      "10976/10976 [==============================] - 6s 518us/step - loss: 0.0153 - acc: 0.4045 - val_loss: 0.0184 - val_acc: 0.4165\n",
      "Epoch 258/260\n",
      "10976/10976 [==============================] - 6s 543us/step - loss: 0.0152 - acc: 0.4025 - val_loss: 0.0136 - val_acc: 0.3669\n",
      "Epoch 259/260\n",
      "10976/10976 [==============================] - 6s 548us/step - loss: 0.0153 - acc: 0.4190 - val_loss: 0.0172 - val_acc: 0.3593\n",
      "Epoch 260/260\n",
      "10976/10976 [==============================] - 6s 515us/step - loss: 0.0153 - acc: 0.4105 - val_loss: 0.0124 - val_acc: 0.4462\n",
      "start training round 13\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 261/280\n",
      "10976/10976 [==============================] - 6s 525us/step - loss: 0.0201 - acc: 0.4675 - val_loss: 0.0155 - val_acc: 0.4706\n",
      "Epoch 262/280\n",
      "10976/10976 [==============================] - 6s 510us/step - loss: 0.0199 - acc: 0.4659 - val_loss: 0.0232 - val_acc: 0.4681\n",
      "Epoch 263/280\n",
      "10976/10976 [==============================] - 6s 514us/step - loss: 0.0204 - acc: 0.4653 - val_loss: 0.0180 - val_acc: 0.4698\n",
      "Epoch 264/280\n",
      "10976/10976 [==============================] - 6s 521us/step - loss: 0.0205 - acc: 0.4654 - val_loss: 0.0235 - val_acc: 0.4699\n",
      "Epoch 265/280\n",
      "10976/10976 [==============================] - 6s 537us/step - loss: 0.0201 - acc: 0.4682 - val_loss: 0.0161 - val_acc: 0.4848\n",
      "Epoch 266/280\n",
      "10976/10976 [==============================] - 5s 500us/step - loss: 0.0198 - acc: 0.4647 - val_loss: 0.0206 - val_acc: 0.4309\n",
      "Epoch 267/280\n",
      "10976/10976 [==============================] - 6s 524us/step - loss: 0.0203 - acc: 0.4691 - val_loss: 0.0177 - val_acc: 0.4832\n",
      "Epoch 268/280\n",
      "10976/10976 [==============================] - 6s 519us/step - loss: 0.0204 - acc: 0.4632 - val_loss: 0.0228 - val_acc: 0.4653\n",
      "Epoch 269/280\n",
      "10976/10976 [==============================] - 6s 514us/step - loss: 0.0198 - acc: 0.4675 - val_loss: 0.0163 - val_acc: 0.4811\n",
      "Epoch 270/280\n",
      "10976/10976 [==============================] - 6s 504us/step - loss: 0.0199 - acc: 0.4669 - val_loss: 0.0226 - val_acc: 0.4785\n",
      "Epoch 271/280\n",
      "10976/10976 [==============================] - 6s 504us/step - loss: 0.0203 - acc: 0.4664 - val_loss: 0.0164 - val_acc: 0.4742\n",
      "Epoch 272/280\n",
      "10976/10976 [==============================] - 6s 505us/step - loss: 0.0199 - acc: 0.4682 - val_loss: 0.0231 - val_acc: 0.4635\n",
      "Epoch 273/280\n",
      "10976/10976 [==============================] - 6s 522us/step - loss: 0.0199 - acc: 0.4659 - val_loss: 0.0168 - val_acc: 0.4828\n",
      "Epoch 274/280\n",
      "10976/10976 [==============================] - 6s 520us/step - loss: 0.0200 - acc: 0.4610 - val_loss: 0.0208 - val_acc: 0.4835\n",
      "Epoch 275/280\n",
      "10976/10976 [==============================] - 6s 503us/step - loss: 0.0197 - acc: 0.4687 - val_loss: 0.0165 - val_acc: 0.4830\n",
      "Epoch 276/280\n",
      "10976/10976 [==============================] - 5s 493us/step - loss: 0.0197 - acc: 0.4667 - val_loss: 0.0215 - val_acc: 0.4804\n",
      "Epoch 277/280\n",
      "10976/10976 [==============================] - 6s 511us/step - loss: 0.0204 - acc: 0.4680 - val_loss: 0.0180 - val_acc: 0.4702\n",
      "Epoch 278/280\n",
      "10976/10976 [==============================] - 6s 505us/step - loss: 0.0203 - acc: 0.4655 - val_loss: 0.0240 - val_acc: 0.4724\n",
      "Epoch 279/280\n",
      "10976/10976 [==============================] - 6s 503us/step - loss: 0.0201 - acc: 0.4675 - val_loss: 0.0167 - val_acc: 0.4711\n",
      "Epoch 280/280\n",
      "10976/10976 [==============================] - 6s 516us/step - loss: 0.0199 - acc: 0.4670 - val_loss: 0.0156 - val_acc: 0.4839\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 261/280\n",
      "10976/10976 [==============================] - 6s 528us/step - loss: 0.0233 - acc: 0.4631 - val_loss: 0.0244 - val_acc: 0.3666\n",
      "Epoch 262/280\n",
      "10976/10976 [==============================] - 6s 517us/step - loss: 0.0232 - acc: 0.4397 - val_loss: 0.0229 - val_acc: 0.4416\n",
      "Epoch 263/280\n",
      "10976/10976 [==============================] - 6s 523us/step - loss: 0.0233 - acc: 0.4368 - val_loss: 0.0236 - val_acc: 0.3757\n",
      "Epoch 264/280\n",
      "10976/10976 [==============================] - 6s 527us/step - loss: 0.0232 - acc: 0.4321 - val_loss: 0.0233 - val_acc: 0.4923\n",
      "Epoch 265/280\n",
      "10976/10976 [==============================] - 6s 514us/step - loss: 0.0231 - acc: 0.4394 - val_loss: 0.0232 - val_acc: 0.4557\n",
      "Epoch 266/280\n",
      "10976/10976 [==============================] - 6s 524us/step - loss: 0.0231 - acc: 0.4637 - val_loss: 0.0222 - val_acc: 0.5458\n",
      "Epoch 267/280\n",
      "10976/10976 [==============================] - 6s 505us/step - loss: 0.0232 - acc: 0.4964 - val_loss: 0.0234 - val_acc: 0.4903\n",
      "Epoch 268/280\n",
      "10976/10976 [==============================] - 6s 517us/step - loss: 0.0237 - acc: 0.5035 - val_loss: 0.0256 - val_acc: 0.5274\n",
      "Epoch 269/280\n",
      "10976/10976 [==============================] - 6s 526us/step - loss: 0.0235 - acc: 0.5038 - val_loss: 0.0241 - val_acc: 0.4950\n",
      "Epoch 270/280\n",
      "10976/10976 [==============================] - 6s 537us/step - loss: 0.0233 - acc: 0.5012 - val_loss: 0.0226 - val_acc: 0.5414\n",
      "Epoch 271/280\n",
      "10976/10976 [==============================] - 6s 506us/step - loss: 0.0229 - acc: 0.4914 - val_loss: 0.0228 - val_acc: 0.3878\n",
      "Epoch 272/280\n",
      "10976/10976 [==============================] - 6s 512us/step - loss: 0.0231 - acc: 0.4356 - val_loss: 0.0227 - val_acc: 0.4407\n",
      "Epoch 273/280\n",
      "10976/10976 [==============================] - 6s 505us/step - loss: 0.0228 - acc: 0.4444 - val_loss: 0.0230 - val_acc: 0.3970\n",
      "Epoch 274/280\n",
      "10976/10976 [==============================] - 6s 540us/step - loss: 0.0233 - acc: 0.4350 - val_loss: 0.0240 - val_acc: 0.5083\n",
      "Epoch 275/280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 5s 492us/step - loss: 0.0233 - acc: 0.4362 - val_loss: 0.0235 - val_acc: 0.3751\n",
      "Epoch 276/280\n",
      "10976/10976 [==============================] - 6s 524us/step - loss: 0.0231 - acc: 0.4429 - val_loss: 0.0236 - val_acc: 0.4247\n",
      "Epoch 277/280\n",
      "10976/10976 [==============================] - 6s 521us/step - loss: 0.0232 - acc: 0.4425 - val_loss: 0.0232 - val_acc: 0.4008\n",
      "Epoch 278/280\n",
      "10976/10976 [==============================] - 6s 520us/step - loss: 0.0230 - acc: 0.4389 - val_loss: 0.0223 - val_acc: 0.4591\n",
      "Epoch 279/280\n",
      "10976/10976 [==============================] - 6s 518us/step - loss: 0.0234 - acc: 0.5032 - val_loss: 0.0217 - val_acc: 0.5352\n",
      "Epoch 280/280\n",
      "10976/10976 [==============================] - 6s 525us/step - loss: 0.0234 - acc: 0.5034 - val_loss: 0.0239 - val_acc: 0.4686\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 261/280\n",
      "10976/10976 [==============================] - 6s 504us/step - loss: 0.0180 - acc: 0.3352 - val_loss: 0.0205 - val_acc: 0.4366\n",
      "Epoch 262/280\n",
      "10976/10976 [==============================] - 6s 507us/step - loss: 0.0183 - acc: 0.3359 - val_loss: 0.0172 - val_acc: 0.2202\n",
      "Epoch 263/280\n",
      "10976/10976 [==============================] - 6s 527us/step - loss: 0.0182 - acc: 0.3369 - val_loss: 0.0212 - val_acc: 0.4354\n",
      "Epoch 264/280\n",
      "10976/10976 [==============================] - 6s 508us/step - loss: 0.0183 - acc: 0.3503 - val_loss: 0.0157 - val_acc: 0.2230\n",
      "Epoch 265/280\n",
      "10976/10976 [==============================] - 5s 468us/step - loss: 0.0182 - acc: 0.3437 - val_loss: 0.0201 - val_acc: 0.4399\n",
      "Epoch 266/280\n",
      "10976/10976 [==============================] - 6s 510us/step - loss: 0.0179 - acc: 0.3422 - val_loss: 0.0162 - val_acc: 0.2254\n",
      "Epoch 267/280\n",
      "10976/10976 [==============================] - 6s 526us/step - loss: 0.0182 - acc: 0.3345 - val_loss: 0.0197 - val_acc: 0.4393\n",
      "Epoch 268/280\n",
      "10976/10976 [==============================] - 6s 531us/step - loss: 0.0181 - acc: 0.3429 - val_loss: 0.0170 - val_acc: 0.2212\n",
      "Epoch 269/280\n",
      "10976/10976 [==============================] - 6s 524us/step - loss: 0.0182 - acc: 0.3532 - val_loss: 0.0199 - val_acc: 0.5056\n",
      "Epoch 270/280\n",
      "10976/10976 [==============================] - 6s 517us/step - loss: 0.0180 - acc: 0.3493 - val_loss: 0.0176 - val_acc: 0.2195\n",
      "Epoch 271/280\n",
      "10976/10976 [==============================] - 5s 499us/step - loss: 0.0180 - acc: 0.3526 - val_loss: 0.0201 - val_acc: 0.4379\n",
      "Epoch 272/280\n",
      "10976/10976 [==============================] - 6s 504us/step - loss: 0.0181 - acc: 0.3511 - val_loss: 0.0168 - val_acc: 0.2196\n",
      "Epoch 273/280\n",
      "10976/10976 [==============================] - 6s 555us/step - loss: 0.0179 - acc: 0.3473 - val_loss: 0.0202 - val_acc: 0.4385\n",
      "Epoch 274/280\n",
      "10976/10976 [==============================] - 6s 521us/step - loss: 0.0183 - acc: 0.3470 - val_loss: 0.0173 - val_acc: 0.2191\n",
      "Epoch 275/280\n",
      "10976/10976 [==============================] - 5s 496us/step - loss: 0.0182 - acc: 0.3426 - val_loss: 0.0196 - val_acc: 0.5064\n",
      "Epoch 276/280\n",
      "10976/10976 [==============================] - 6s 529us/step - loss: 0.0181 - acc: 0.3499 - val_loss: 0.0165 - val_acc: 0.2196\n",
      "Epoch 277/280\n",
      "10976/10976 [==============================] - 6s 524us/step - loss: 0.0181 - acc: 0.3415 - val_loss: 0.0202 - val_acc: 0.4398\n",
      "Epoch 278/280\n",
      "10976/10976 [==============================] - 6s 513us/step - loss: 0.0180 - acc: 0.3471 - val_loss: 0.0165 - val_acc: 0.2209\n",
      "Epoch 279/280\n",
      "10976/10976 [==============================] - 6s 512us/step - loss: 0.0180 - acc: 0.3341 - val_loss: 0.0211 - val_acc: 0.4434\n",
      "Epoch 280/280\n",
      "10976/10976 [==============================] - 6s 504us/step - loss: 0.0180 - acc: 0.3523 - val_loss: 0.0157 - val_acc: 0.2233\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 261/280\n",
      "10976/10976 [==============================] - 6s 532us/step - loss: 0.0152 - acc: 0.4130 - val_loss: 0.0175 - val_acc: 0.4003\n",
      "Epoch 262/280\n",
      "10976/10976 [==============================] - 6s 522us/step - loss: 0.0153 - acc: 0.4091 - val_loss: 0.0120 - val_acc: 0.4260\n",
      "Epoch 263/280\n",
      "10976/10976 [==============================] - 6s 529us/step - loss: 0.0154 - acc: 0.4106 - val_loss: 0.0184 - val_acc: 0.3609\n",
      "Epoch 264/280\n",
      "10976/10976 [==============================] - 6s 522us/step - loss: 0.0153 - acc: 0.4014 - val_loss: 0.0116 - val_acc: 0.3806\n",
      "Epoch 265/280\n",
      "10976/10976 [==============================] - 6s 506us/step - loss: 0.0149 - acc: 0.4045 - val_loss: 0.0188 - val_acc: 0.4227\n",
      "Epoch 266/280\n",
      "10976/10976 [==============================] - 6s 526us/step - loss: 0.0152 - acc: 0.4005 - val_loss: 0.0120 - val_acc: 0.3843\n",
      "Epoch 267/280\n",
      "10976/10976 [==============================] - 6s 519us/step - loss: 0.0152 - acc: 0.4117 - val_loss: 0.0186 - val_acc: 0.4316\n",
      "Epoch 268/280\n",
      "10976/10976 [==============================] - 6s 526us/step - loss: 0.0152 - acc: 0.4096 - val_loss: 0.0108 - val_acc: 0.4284\n",
      "Epoch 269/280\n",
      "10976/10976 [==============================] - 6s 534us/step - loss: 0.0151 - acc: 0.4049 - val_loss: 0.0189 - val_acc: 0.4253\n",
      "Epoch 270/280\n",
      "10976/10976 [==============================] - 6s 524us/step - loss: 0.0154 - acc: 0.4070 - val_loss: 0.0117 - val_acc: 0.4332\n",
      "Epoch 271/280\n",
      "10976/10976 [==============================] - 6s 527us/step - loss: 0.0153 - acc: 0.4075 - val_loss: 0.0178 - val_acc: 0.4161\n",
      "Epoch 272/280\n",
      "10976/10976 [==============================] - 6s 502us/step - loss: 0.0152 - acc: 0.4073 - val_loss: 0.0122 - val_acc: 0.4280\n",
      "Epoch 273/280\n",
      "10976/10976 [==============================] - 6s 527us/step - loss: 0.0151 - acc: 0.4094 - val_loss: 0.0178 - val_acc: 0.3553\n",
      "Epoch 274/280\n",
      "10976/10976 [==============================] - 6s 518us/step - loss: 0.0151 - acc: 0.4053 - val_loss: 0.0137 - val_acc: 0.3798\n",
      "Epoch 275/280\n",
      "10976/10976 [==============================] - 6s 503us/step - loss: 0.0153 - acc: 0.4113 - val_loss: 0.0180 - val_acc: 0.4016\n",
      "Epoch 276/280\n",
      "10976/10976 [==============================] - 6s 529us/step - loss: 0.0152 - acc: 0.4110 - val_loss: 0.0118 - val_acc: 0.3736\n",
      "Epoch 277/280\n",
      "10976/10976 [==============================] - 6s 520us/step - loss: 0.0147 - acc: 0.4225 - val_loss: 0.0154 - val_acc: 0.4238\n",
      "Epoch 278/280\n",
      "10976/10976 [==============================] - 6s 528us/step - loss: 0.0153 - acc: 0.4355 - val_loss: 0.0110 - val_acc: 0.4604\n",
      "Epoch 279/280\n",
      "10976/10976 [==============================] - 6s 507us/step - loss: 0.0149 - acc: 0.4264 - val_loss: 0.0178 - val_acc: 0.4623\n",
      "Epoch 280/280\n",
      "10976/10976 [==============================] - 6s 538us/step - loss: 0.0153 - acc: 0.4343 - val_loss: 0.0115 - val_acc: 0.4628\n",
      "start training round 14\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 281/300\n",
      "10976/10976 [==============================] - 6s 512us/step - loss: 0.0199 - acc: 0.4246 - val_loss: 0.0192 - val_acc: 0.3802\n",
      "Epoch 282/300\n",
      "10976/10976 [==============================] - 6s 505us/step - loss: 0.0201 - acc: 0.4254 - val_loss: 0.0213 - val_acc: 0.4801\n",
      "Epoch 283/300\n",
      "10976/10976 [==============================] - 6s 529us/step - loss: 0.0198 - acc: 0.4253 - val_loss: 0.0192 - val_acc: 0.3804\n",
      "Epoch 284/300\n",
      "10976/10976 [==============================] - 6s 527us/step - loss: 0.0198 - acc: 0.4213 - val_loss: 0.0201 - val_acc: 0.4797\n",
      "Epoch 285/300\n",
      "10976/10976 [==============================] - 6s 514us/step - loss: 0.0195 - acc: 0.4313 - val_loss: 0.0206 - val_acc: 0.3806\n",
      "Epoch 286/300\n",
      "10976/10976 [==============================] - 6s 520us/step - loss: 0.0196 - acc: 0.4246 - val_loss: 0.0201 - val_acc: 0.4812\n",
      "Epoch 287/300\n",
      "10976/10976 [==============================] - 6s 515us/step - loss: 0.0200 - acc: 0.4274 - val_loss: 0.0193 - val_acc: 0.3811\n",
      "Epoch 288/300\n",
      "10976/10976 [==============================] - 6s 530us/step - loss: 0.0200 - acc: 0.4216 - val_loss: 0.0184 - val_acc: 0.4806\n",
      "Epoch 289/300\n",
      "10976/10976 [==============================] - 6s 527us/step - loss: 0.0199 - acc: 0.4320 - val_loss: 0.0182 - val_acc: 0.3816\n",
      "Epoch 290/300\n",
      "10976/10976 [==============================] - 6s 506us/step - loss: 0.0196 - acc: 0.4235 - val_loss: 0.0176 - val_acc: 0.4739\n",
      "Epoch 291/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 6s 509us/step - loss: 0.0197 - acc: 0.4301 - val_loss: 0.0189 - val_acc: 0.3810\n",
      "Epoch 292/300\n",
      "10976/10976 [==============================] - 6s 520us/step - loss: 0.0201 - acc: 0.4240 - val_loss: 0.0207 - val_acc: 0.4794\n",
      "Epoch 293/300\n",
      "10976/10976 [==============================] - 6s 523us/step - loss: 0.0195 - acc: 0.4245 - val_loss: 0.0194 - val_acc: 0.3790\n",
      "Epoch 294/300\n",
      "10976/10976 [==============================] - 6s 528us/step - loss: 0.0201 - acc: 0.4238 - val_loss: 0.0212 - val_acc: 0.4801\n",
      "Epoch 295/300\n",
      "10976/10976 [==============================] - 6s 519us/step - loss: 0.0197 - acc: 0.4267 - val_loss: 0.0204 - val_acc: 0.3739\n",
      "Epoch 296/300\n",
      "10976/10976 [==============================] - 6s 515us/step - loss: 0.0195 - acc: 0.4247 - val_loss: 0.0210 - val_acc: 0.4807\n",
      "Epoch 297/300\n",
      "10976/10976 [==============================] - 6s 503us/step - loss: 0.0195 - acc: 0.4305 - val_loss: 0.0187 - val_acc: 0.3775\n",
      "Epoch 298/300\n",
      "10976/10976 [==============================] - 6s 529us/step - loss: 0.0198 - acc: 0.4246 - val_loss: 0.0210 - val_acc: 0.4807\n",
      "Epoch 299/300\n",
      "10976/10976 [==============================] - 6s 525us/step - loss: 0.0197 - acc: 0.4297 - val_loss: 0.0187 - val_acc: 0.3811\n",
      "Epoch 300/300\n",
      "10976/10976 [==============================] - 5s 501us/step - loss: 0.0197 - acc: 0.4267 - val_loss: 0.0173 - val_acc: 0.4823\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 281/300\n",
      "10976/10976 [==============================] - 6s 528us/step - loss: 0.0230 - acc: 0.4988 - val_loss: 0.0235 - val_acc: 0.5264\n",
      "Epoch 282/300\n",
      "10976/10976 [==============================] - 6s 537us/step - loss: 0.0229 - acc: 0.4955 - val_loss: 0.0236 - val_acc: 0.4675\n",
      "Epoch 283/300\n",
      "10976/10976 [==============================] - 6s 515us/step - loss: 0.0231 - acc: 0.4462 - val_loss: 0.0231 - val_acc: 0.4282\n",
      "Epoch 284/300\n",
      "10976/10976 [==============================] - 5s 490us/step - loss: 0.0232 - acc: 0.4457 - val_loss: 0.0235 - val_acc: 0.4647\n",
      "Epoch 285/300\n",
      "10976/10976 [==============================] - 5s 494us/step - loss: 0.0232 - acc: 0.4450 - val_loss: 0.0225 - val_acc: 0.4355\n",
      "Epoch 286/300\n",
      "10976/10976 [==============================] - 6s 505us/step - loss: 0.0229 - acc: 0.4507 - val_loss: 0.0223 - val_acc: 0.5199\n",
      "Epoch 287/300\n",
      "10976/10976 [==============================] - 6s 505us/step - loss: 0.0228 - acc: 0.4632 - val_loss: 0.0238 - val_acc: 0.4744\n",
      "Epoch 288/300\n",
      "10976/10976 [==============================] - 6s 522us/step - loss: 0.0231 - acc: 0.5014 - val_loss: 0.0227 - val_acc: 0.5239\n",
      "Epoch 289/300\n",
      "10976/10976 [==============================] - 6s 527us/step - loss: 0.0235 - acc: 0.5023 - val_loss: 0.0243 - val_acc: 0.4834\n",
      "Epoch 290/300\n",
      "10976/10976 [==============================] - 6s 513us/step - loss: 0.0229 - acc: 0.5000 - val_loss: 0.0223 - val_acc: 0.5436\n",
      "Epoch 291/300\n",
      "10976/10976 [==============================] - 6s 510us/step - loss: 0.0230 - acc: 0.4873 - val_loss: 0.0252 - val_acc: 0.4698\n",
      "Epoch 292/300\n",
      "10976/10976 [==============================] - 6s 502us/step - loss: 0.0237 - acc: 0.5063 - val_loss: 0.0219 - val_acc: 0.5391\n",
      "Epoch 293/300\n",
      "10976/10976 [==============================] - 6s 524us/step - loss: 0.0229 - acc: 0.5034 - val_loss: 0.0246 - val_acc: 0.4931\n",
      "Epoch 294/300\n",
      "10976/10976 [==============================] - 6s 508us/step - loss: 0.0233 - acc: 0.5039 - val_loss: 0.0242 - val_acc: 0.5191\n",
      "Epoch 295/300\n",
      "10976/10976 [==============================] - 6s 504us/step - loss: 0.0232 - acc: 0.5060 - val_loss: 0.0238 - val_acc: 0.4812\n",
      "Epoch 296/300\n",
      "10976/10976 [==============================] - 6s 524us/step - loss: 0.0230 - acc: 0.5046 - val_loss: 0.0217 - val_acc: 0.5484\n",
      "Epoch 297/300\n",
      "10976/10976 [==============================] - 6s 569us/step - loss: 0.0227 - acc: 0.4754 - val_loss: 0.0229 - val_acc: 0.3974\n",
      "Epoch 298/300\n",
      "10976/10976 [==============================] - 6s 508us/step - loss: 0.0229 - acc: 0.4775 - val_loss: 0.0242 - val_acc: 0.5401\n",
      "Epoch 299/300\n",
      "10976/10976 [==============================] - 6s 531us/step - loss: 0.0231 - acc: 0.5068 - val_loss: 0.0228 - val_acc: 0.4981\n",
      "Epoch 300/300\n",
      "10976/10976 [==============================] - 6s 518us/step - loss: 0.0229 - acc: 0.5006 - val_loss: 0.0227 - val_acc: 0.5463\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 281/300\n",
      "10976/10976 [==============================] - 6s 506us/step - loss: 0.0181 - acc: 0.3379 - val_loss: 0.0205 - val_acc: 0.5067\n",
      "Epoch 282/300\n",
      "10976/10976 [==============================] - 6s 525us/step - loss: 0.0181 - acc: 0.3597 - val_loss: 0.0165 - val_acc: 0.2218\n",
      "Epoch 283/300\n",
      "10976/10976 [==============================] - 6s 505us/step - loss: 0.0181 - acc: 0.3509 - val_loss: 0.0204 - val_acc: 0.5084\n",
      "Epoch 284/300\n",
      "10976/10976 [==============================] - 6s 503us/step - loss: 0.0181 - acc: 0.3564 - val_loss: 0.0168 - val_acc: 0.2217\n",
      "Epoch 285/300\n",
      "10976/10976 [==============================] - 6s 545us/step - loss: 0.0179 - acc: 0.3461 - val_loss: 0.0207 - val_acc: 0.4461\n",
      "Epoch 286/300\n",
      "10976/10976 [==============================] - 6s 515us/step - loss: 0.0181 - acc: 0.3524 - val_loss: 0.0165 - val_acc: 0.2206\n",
      "Epoch 287/300\n",
      "10976/10976 [==============================] - 6s 543us/step - loss: 0.0180 - acc: 0.3505 - val_loss: 0.0197 - val_acc: 0.4349\n",
      "Epoch 288/300\n",
      "10976/10976 [==============================] - 6s 514us/step - loss: 0.0180 - acc: 0.3483 - val_loss: 0.0165 - val_acc: 0.2225\n",
      "Epoch 289/300\n",
      "10976/10976 [==============================] - 6s 511us/step - loss: 0.0180 - acc: 0.3481 - val_loss: 0.0203 - val_acc: 0.4389\n",
      "Epoch 290/300\n",
      "10976/10976 [==============================] - 6s 517us/step - loss: 0.0181 - acc: 0.3478 - val_loss: 0.0157 - val_acc: 0.2240\n",
      "Epoch 291/300\n",
      "10976/10976 [==============================] - 6s 531us/step - loss: 0.0179 - acc: 0.3429 - val_loss: 0.0205 - val_acc: 0.5061\n",
      "Epoch 292/300\n",
      "10976/10976 [==============================] - 6s 518us/step - loss: 0.0180 - acc: 0.3525 - val_loss: 0.0169 - val_acc: 0.2209\n",
      "Epoch 293/300\n",
      "10976/10976 [==============================] - 6s 530us/step - loss: 0.0180 - acc: 0.3422 - val_loss: 0.0198 - val_acc: 0.4474\n",
      "Epoch 294/300\n",
      "10976/10976 [==============================] - 5s 500us/step - loss: 0.0179 - acc: 0.4545 - val_loss: 0.0242 - val_acc: 0.5067\n",
      "Epoch 295/300\n",
      "10976/10976 [==============================] - 6s 505us/step - loss: 0.0184 - acc: 0.4594 - val_loss: 0.0127 - val_acc: 0.4459\n",
      "Epoch 296/300\n",
      "10976/10976 [==============================] - 6s 519us/step - loss: 0.0177 - acc: 0.4548 - val_loss: 0.0238 - val_acc: 0.2960\n",
      "Epoch 297/300\n",
      "10976/10976 [==============================] - 6s 531us/step - loss: 0.0185 - acc: 0.4736 - val_loss: 0.0132 - val_acc: 0.4414\n",
      "Epoch 298/300\n",
      "10976/10976 [==============================] - 6s 530us/step - loss: 0.0178 - acc: 0.4565 - val_loss: 0.0248 - val_acc: 0.5067\n",
      "Epoch 299/300\n",
      "10976/10976 [==============================] - 6s 509us/step - loss: 0.0179 - acc: 0.4667 - val_loss: 0.0117 - val_acc: 0.5080\n",
      "Epoch 300/300\n",
      "10976/10976 [==============================] - 6s 529us/step - loss: 0.0179 - acc: 0.4401 - val_loss: 0.0250 - val_acc: 0.5029\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 281/300\n",
      "10976/10976 [==============================] - 6s 526us/step - loss: 0.0149 - acc: 0.4208 - val_loss: 0.0177 - val_acc: 0.4191\n",
      "Epoch 282/300\n",
      "10976/10976 [==============================] - 6s 532us/step - loss: 0.0151 - acc: 0.4197 - val_loss: 0.0140 - val_acc: 0.4356\n",
      "Epoch 283/300\n",
      "10976/10976 [==============================] - 6s 507us/step - loss: 0.0152 - acc: 0.4148 - val_loss: 0.0181 - val_acc: 0.4264\n",
      "Epoch 284/300\n",
      "10976/10976 [==============================] - 6s 550us/step - loss: 0.0152 - acc: 0.4173 - val_loss: 0.0125 - val_acc: 0.3929\n",
      "Epoch 285/300\n",
      "10976/10976 [==============================] - 6s 561us/step - loss: 0.0149 - acc: 0.4141 - val_loss: 0.0173 - val_acc: 0.4286\n",
      "Epoch 286/300\n",
      "10976/10976 [==============================] - 6s 531us/step - loss: 0.0151 - acc: 0.4223 - val_loss: 0.0130 - val_acc: 0.4265\n",
      "Epoch 287/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 6s 517us/step - loss: 0.0151 - acc: 0.4190 - val_loss: 0.0157 - val_acc: 0.4196\n",
      "Epoch 288/300\n",
      "10976/10976 [==============================] - 6s 523us/step - loss: 0.0153 - acc: 0.4192 - val_loss: 0.0140 - val_acc: 0.3848\n",
      "Epoch 289/300\n",
      "10976/10976 [==============================] - 6s 519us/step - loss: 0.0154 - acc: 0.4067 - val_loss: 0.0189 - val_acc: 0.4288\n",
      "Epoch 290/300\n",
      "10976/10976 [==============================] - 6s 541us/step - loss: 0.0153 - acc: 0.4078 - val_loss: 0.0105 - val_acc: 0.4652\n",
      "Epoch 291/300\n",
      "10976/10976 [==============================] - 6s 509us/step - loss: 0.0151 - acc: 0.4128 - val_loss: 0.0176 - val_acc: 0.4131\n",
      "Epoch 292/300\n",
      "10976/10976 [==============================] - 6s 528us/step - loss: 0.0150 - acc: 0.4153 - val_loss: 0.0108 - val_acc: 0.3936\n",
      "Epoch 293/300\n",
      "10976/10976 [==============================] - 6s 526us/step - loss: 0.0150 - acc: 0.4074 - val_loss: 0.0184 - val_acc: 0.4177\n",
      "Epoch 294/300\n",
      "10976/10976 [==============================] - 6s 508us/step - loss: 0.0151 - acc: 0.4108 - val_loss: 0.0117 - val_acc: 0.3843\n",
      "Epoch 295/300\n",
      "10976/10976 [==============================] - 6s 502us/step - loss: 0.0152 - acc: 0.4101 - val_loss: 0.0178 - val_acc: 0.4078\n",
      "Epoch 296/300\n",
      "10976/10976 [==============================] - 6s 521us/step - loss: 0.0149 - acc: 0.4193 - val_loss: 0.0124 - val_acc: 0.3898\n",
      "Epoch 297/300\n",
      "10976/10976 [==============================] - 6s 508us/step - loss: 0.0148 - acc: 0.4232 - val_loss: 0.0174 - val_acc: 0.4242\n",
      "Epoch 298/300\n",
      "10976/10976 [==============================] - 6s 565us/step - loss: 0.0152 - acc: 0.4112 - val_loss: 0.0107 - val_acc: 0.4615\n",
      "Epoch 299/300\n",
      "10976/10976 [==============================] - 6s 522us/step - loss: 0.0151 - acc: 0.4048 - val_loss: 0.0176 - val_acc: 0.3739\n",
      "Epoch 300/300\n",
      "10976/10976 [==============================] - 6s 544us/step - loss: 0.0150 - acc: 0.3971 - val_loss: 0.0110 - val_acc: 0.4567\n",
      "start training round 15\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 301/320\n",
      "10976/10976 [==============================] - 5s 498us/step - loss: 0.0198 - acc: 0.4280 - val_loss: 0.0174 - val_acc: 0.3806\n",
      "Epoch 302/320\n",
      "10976/10976 [==============================] - 6s 542us/step - loss: 0.0195 - acc: 0.4261 - val_loss: 0.0204 - val_acc: 0.4789\n",
      "Epoch 303/320\n",
      "10976/10976 [==============================] - 5s 482us/step - loss: 0.0194 - acc: 0.4250 - val_loss: 0.0193 - val_acc: 0.3783\n",
      "Epoch 304/320\n",
      "10976/10976 [==============================] - 5s 490us/step - loss: 0.0199 - acc: 0.4243 - val_loss: 0.0212 - val_acc: 0.4789\n",
      "Epoch 305/320\n",
      "10976/10976 [==============================] - 6s 520us/step - loss: 0.0198 - acc: 0.4281 - val_loss: 0.0195 - val_acc: 0.3724\n",
      "Epoch 306/320\n",
      "10976/10976 [==============================] - 6s 514us/step - loss: 0.0198 - acc: 0.4257 - val_loss: 0.0205 - val_acc: 0.4814\n",
      "Epoch 307/320\n",
      "10976/10976 [==============================] - 6s 542us/step - loss: 0.0197 - acc: 0.4264 - val_loss: 0.0200 - val_acc: 0.3752\n",
      "Epoch 308/320\n",
      "10976/10976 [==============================] - 6s 522us/step - loss: 0.0198 - acc: 0.4279 - val_loss: 0.0212 - val_acc: 0.4811\n",
      "Epoch 309/320\n",
      "10976/10976 [==============================] - 6s 520us/step - loss: 0.0200 - acc: 0.4262 - val_loss: 0.0328 - val_acc: 0.4050\n",
      "Epoch 310/320\n",
      "10976/10976 [==============================] - 6s 502us/step - loss: 0.0201 - acc: 0.4663 - val_loss: 0.0178 - val_acc: 0.4845\n",
      "Epoch 311/320\n",
      "10976/10976 [==============================] - 6s 533us/step - loss: 0.0202 - acc: 0.4699 - val_loss: 0.0244 - val_acc: 0.4768\n",
      "Epoch 312/320\n",
      "10976/10976 [==============================] - 6s 501us/step - loss: 0.0202 - acc: 0.4723 - val_loss: 0.0168 - val_acc: 0.4837\n",
      "Epoch 313/320\n",
      "10976/10976 [==============================] - 6s 535us/step - loss: 0.0198 - acc: 0.4648 - val_loss: 0.0226 - val_acc: 0.4355\n",
      "Epoch 314/320\n",
      "10976/10976 [==============================] - 6s 530us/step - loss: 0.0198 - acc: 0.4656 - val_loss: 0.0172 - val_acc: 0.4841\n",
      "Epoch 315/320\n",
      "10976/10976 [==============================] - 6s 529us/step - loss: 0.0196 - acc: 0.4658 - val_loss: 0.0233 - val_acc: 0.4583\n",
      "Epoch 316/320\n",
      "10976/10976 [==============================] - 6s 518us/step - loss: 0.0199 - acc: 0.4697 - val_loss: 0.0166 - val_acc: 0.4840\n",
      "Epoch 317/320\n",
      "10976/10976 [==============================] - 6s 518us/step - loss: 0.0202 - acc: 0.4716 - val_loss: 0.0224 - val_acc: 0.4750\n",
      "Epoch 318/320\n",
      "10976/10976 [==============================] - 6s 517us/step - loss: 0.0201 - acc: 0.4690 - val_loss: 0.0171 - val_acc: 0.4806\n",
      "Epoch 319/320\n",
      "10976/10976 [==============================] - 6s 519us/step - loss: 0.0197 - acc: 0.4680 - val_loss: 0.0221 - val_acc: 0.4678\n",
      "Epoch 320/320\n",
      "10976/10976 [==============================] - 6s 515us/step - loss: 0.0198 - acc: 0.4685 - val_loss: 0.0171 - val_acc: 0.4702\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 301/320\n",
      "10976/10976 [==============================] - 6s 519us/step - loss: 0.0230 - acc: 0.4918 - val_loss: 0.0235 - val_acc: 0.3769\n",
      "Epoch 302/320\n",
      "10976/10976 [==============================] - 6s 531us/step - loss: 0.0230 - acc: 0.4983 - val_loss: 0.0231 - val_acc: 0.4525\n",
      "Epoch 303/320\n",
      "10976/10976 [==============================] - 6s 534us/step - loss: 0.0231 - acc: 0.5005 - val_loss: 0.0226 - val_acc: 0.5217\n",
      "Epoch 304/320\n",
      "10976/10976 [==============================] - 6s 513us/step - loss: 0.0235 - acc: 0.5068 - val_loss: 0.0243 - val_acc: 0.5017\n",
      "Epoch 305/320\n",
      "10976/10976 [==============================] - 6s 523us/step - loss: 0.0235 - acc: 0.5026 - val_loss: 0.0231 - val_acc: 0.5189\n",
      "Epoch 306/320\n",
      "10976/10976 [==============================] - 6s 520us/step - loss: 0.0230 - acc: 0.5043 - val_loss: 0.0225 - val_acc: 0.4741\n",
      "Epoch 307/320\n",
      "10976/10976 [==============================] - 6s 512us/step - loss: 0.0228 - acc: 0.5033 - val_loss: 0.0228 - val_acc: 0.5367\n",
      "Epoch 308/320\n",
      "10976/10976 [==============================] - 6s 523us/step - loss: 0.0232 - acc: 0.5046 - val_loss: 0.0245 - val_acc: 0.4695\n",
      "Epoch 309/320\n",
      "10976/10976 [==============================] - 6s 554us/step - loss: 0.0231 - acc: 0.4916 - val_loss: 0.0223 - val_acc: 0.5245\n",
      "Epoch 310/320\n",
      "10976/10976 [==============================] - 6s 532us/step - loss: 0.0230 - acc: 0.5034 - val_loss: 0.0249 - val_acc: 0.4764\n",
      "Epoch 311/320\n",
      "10976/10976 [==============================] - 5s 501us/step - loss: 0.0230 - acc: 0.4937 - val_loss: 0.0248 - val_acc: 0.5512\n",
      "Epoch 312/320\n",
      "10976/10976 [==============================] - 5s 499us/step - loss: 0.0227 - acc: 0.5056 - val_loss: 0.0229 - val_acc: 0.5069\n",
      "Epoch 313/320\n",
      "10976/10976 [==============================] - 5s 500us/step - loss: 0.0227 - acc: 0.5023 - val_loss: 0.0227 - val_acc: 0.5233\n",
      "Epoch 314/320\n",
      "10976/10976 [==============================] - 6s 528us/step - loss: 0.0234 - acc: 0.5032 - val_loss: 0.0223 - val_acc: 0.4595\n",
      "Epoch 315/320\n",
      "10976/10976 [==============================] - 6s 523us/step - loss: 0.0228 - acc: 0.4416 - val_loss: 0.0234 - val_acc: 0.5020\n",
      "Epoch 316/320\n",
      "10976/10976 [==============================] - 6s 510us/step - loss: 0.0229 - acc: 0.4450 - val_loss: 0.0230 - val_acc: 0.3864\n",
      "Epoch 317/320\n",
      "10976/10976 [==============================] - 6s 506us/step - loss: 0.0231 - acc: 0.4422 - val_loss: 0.0229 - val_acc: 0.5267\n",
      "Epoch 318/320\n",
      "10976/10976 [==============================] - 6s 530us/step - loss: 0.0230 - acc: 0.4466 - val_loss: 0.0239 - val_acc: 0.4867\n",
      "Epoch 319/320\n",
      "10976/10976 [==============================] - 5s 495us/step - loss: 0.0228 - acc: 0.4536 - val_loss: 0.0231 - val_acc: 0.5035\n",
      "Epoch 320/320\n",
      "10976/10976 [==============================] - 6s 511us/step - loss: 0.0228 - acc: 0.4467 - val_loss: 0.0232 - val_acc: 0.3901\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 301/320\n",
      "10976/10976 [==============================] - 6s 530us/step - loss: 0.0181 - acc: 0.4487 - val_loss: 0.0111 - val_acc: 0.5111\n",
      "Epoch 302/320\n",
      "10976/10976 [==============================] - 6s 518us/step - loss: 0.0178 - acc: 0.4280 - val_loss: 0.0241 - val_acc: 0.3036\n",
      "Epoch 303/320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 6s 513us/step - loss: 0.0181 - acc: 0.4692 - val_loss: 0.0148 - val_acc: 0.2836\n",
      "Epoch 304/320\n",
      "10976/10976 [==============================] - 6s 531us/step - loss: 0.0178 - acc: 0.4279 - val_loss: 0.0242 - val_acc: 0.2942\n",
      "Epoch 305/320\n",
      "10976/10976 [==============================] - 6s 514us/step - loss: 0.0180 - acc: 0.4385 - val_loss: 0.0126 - val_acc: 0.5081\n",
      "Epoch 306/320\n",
      "10976/10976 [==============================] - 6s 536us/step - loss: 0.0181 - acc: 0.4578 - val_loss: 0.0222 - val_acc: 0.4255\n",
      "Epoch 307/320\n",
      "10976/10976 [==============================] - 6s 543us/step - loss: 0.0179 - acc: 0.4603 - val_loss: 0.0121 - val_acc: 0.5114\n",
      "Epoch 308/320\n",
      "10976/10976 [==============================] - 6s 527us/step - loss: 0.0177 - acc: 0.4647 - val_loss: 0.0251 - val_acc: 0.4421\n",
      "Epoch 309/320\n",
      "10976/10976 [==============================] - 6s 522us/step - loss: 0.0178 - acc: 0.4359 - val_loss: 0.0136 - val_acc: 0.2914\n",
      "Epoch 310/320\n",
      "10976/10976 [==============================] - 6s 547us/step - loss: 0.0179 - acc: 0.4385 - val_loss: 0.0238 - val_acc: 0.2976\n",
      "Epoch 311/320\n",
      "10976/10976 [==============================] - 6s 542us/step - loss: 0.0180 - acc: 0.4499 - val_loss: 0.0123 - val_acc: 0.5123\n",
      "Epoch 312/320\n",
      "10976/10976 [==============================] - 6s 561us/step - loss: 0.0181 - acc: 0.4537 - val_loss: 0.0248 - val_acc: 0.4362\n",
      "Epoch 313/320\n",
      "10976/10976 [==============================] - 6s 513us/step - loss: 0.0182 - acc: 0.4560 - val_loss: 0.0114 - val_acc: 0.4449\n",
      "Epoch 314/320\n",
      "10976/10976 [==============================] - 6s 532us/step - loss: 0.0175 - acc: 0.4332 - val_loss: 0.0241 - val_acc: 0.4331\n",
      "Epoch 315/320\n",
      "10976/10976 [==============================] - 6s 536us/step - loss: 0.0181 - acc: 0.4298 - val_loss: 0.0115 - val_acc: 0.5126\n",
      "Epoch 316/320\n",
      "10976/10976 [==============================] - 6s 538us/step - loss: 0.0179 - acc: 0.4348 - val_loss: 0.0243 - val_acc: 0.2963\n",
      "Epoch 317/320\n",
      "10976/10976 [==============================] - 6s 525us/step - loss: 0.0179 - acc: 0.4225 - val_loss: 0.0114 - val_acc: 0.5097\n",
      "Epoch 318/320\n",
      "10976/10976 [==============================] - 6s 507us/step - loss: 0.0178 - acc: 0.4486 - val_loss: 0.0219 - val_acc: 0.4310\n",
      "Epoch 319/320\n",
      "10976/10976 [==============================] - 6s 537us/step - loss: 0.0180 - acc: 0.4707 - val_loss: 0.0110 - val_acc: 0.5144\n",
      "Epoch 320/320\n",
      "10976/10976 [==============================] - 6s 537us/step - loss: 0.0177 - acc: 0.4383 - val_loss: 0.0238 - val_acc: 0.4326\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 301/320\n",
      "10976/10976 [==============================] - 6s 514us/step - loss: 0.0150 - acc: 0.4187 - val_loss: 0.0177 - val_acc: 0.4181\n",
      "Epoch 302/320\n",
      "10976/10976 [==============================] - 6s 512us/step - loss: 0.0153 - acc: 0.4125 - val_loss: 0.0124 - val_acc: 0.3818\n",
      "Epoch 303/320\n",
      "10976/10976 [==============================] - 6s 549us/step - loss: 0.0149 - acc: 0.4086 - val_loss: 0.0177 - val_acc: 0.4107\n",
      "Epoch 304/320\n",
      "10976/10976 [==============================] - 6s 513us/step - loss: 0.0151 - acc: 0.4075 - val_loss: 0.0108 - val_acc: 0.4649\n",
      "Epoch 305/320\n",
      "10976/10976 [==============================] - 6s 512us/step - loss: 0.0148 - acc: 0.4050 - val_loss: 0.0176 - val_acc: 0.4189\n",
      "Epoch 306/320\n",
      "10976/10976 [==============================] - 6s 515us/step - loss: 0.0151 - acc: 0.4185 - val_loss: 0.0111 - val_acc: 0.3981\n",
      "Epoch 307/320\n",
      "10976/10976 [==============================] - 6s 524us/step - loss: 0.0151 - acc: 0.4145 - val_loss: 0.0175 - val_acc: 0.3864\n",
      "Epoch 308/320\n",
      "10976/10976 [==============================] - 6s 527us/step - loss: 0.0152 - acc: 0.4209 - val_loss: 0.0117 - val_acc: 0.4575\n",
      "Epoch 309/320\n",
      "10976/10976 [==============================] - 6s 555us/step - loss: 0.0148 - acc: 0.4329 - val_loss: 0.0180 - val_acc: 0.4391\n",
      "Epoch 310/320\n",
      "10976/10976 [==============================] - 6s 528us/step - loss: 0.0149 - acc: 0.4253 - val_loss: 0.0115 - val_acc: 0.4649\n",
      "Epoch 311/320\n",
      "10976/10976 [==============================] - 6s 556us/step - loss: 0.0148 - acc: 0.4307 - val_loss: 0.0176 - val_acc: 0.4375\n",
      "Epoch 312/320\n",
      "10976/10976 [==============================] - 6s 524us/step - loss: 0.0151 - acc: 0.4300 - val_loss: 0.0120 - val_acc: 0.4388\n",
      "Epoch 313/320\n",
      "10976/10976 [==============================] - 6s 535us/step - loss: 0.0149 - acc: 0.4380 - val_loss: 0.0178 - val_acc: 0.4483\n",
      "Epoch 314/320\n",
      "10976/10976 [==============================] - 6s 528us/step - loss: 0.0148 - acc: 0.4334 - val_loss: 0.0115 - val_acc: 0.4039\n",
      "Epoch 315/320\n",
      "10976/10976 [==============================] - 6s 526us/step - loss: 0.0146 - acc: 0.4113 - val_loss: 0.0155 - val_acc: 0.3587\n",
      "Epoch 316/320\n",
      "10976/10976 [==============================] - 6s 526us/step - loss: 0.0148 - acc: 0.3649 - val_loss: 0.0147 - val_acc: 0.3662\n",
      "Epoch 317/320\n",
      "10976/10976 [==============================] - 6s 549us/step - loss: 0.0148 - acc: 0.3672 - val_loss: 0.0152 - val_acc: 0.3545\n",
      "Epoch 318/320\n",
      "10976/10976 [==============================] - 6s 525us/step - loss: 0.0148 - acc: 0.3603 - val_loss: 0.0148 - val_acc: 0.4252\n",
      "Epoch 319/320\n",
      "10976/10976 [==============================] - 6s 539us/step - loss: 0.0148 - acc: 0.3584 - val_loss: 0.0156 - val_acc: 0.3527\n",
      "Epoch 320/320\n",
      "10976/10976 [==============================] - 6s 538us/step - loss: 0.0150 - acc: 0.3557 - val_loss: 0.0132 - val_acc: 0.3541\n",
      "start training round 16\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 321/340\n",
      "10976/10976 [==============================] - 6s 533us/step - loss: 0.0202 - acc: 0.4655 - val_loss: 0.0227 - val_acc: 0.4505\n",
      "Epoch 322/340\n",
      "10976/10976 [==============================] - 6s 538us/step - loss: 0.0200 - acc: 0.4728 - val_loss: 0.0186 - val_acc: 0.4705\n",
      "Epoch 323/340\n",
      "10976/10976 [==============================] - 6s 532us/step - loss: 0.0199 - acc: 0.4697 - val_loss: 0.0238 - val_acc: 0.4760\n",
      "Epoch 324/340\n",
      "10976/10976 [==============================] - 6s 520us/step - loss: 0.0203 - acc: 0.4700 - val_loss: 0.0171 - val_acc: 0.4748\n",
      "Epoch 325/340\n",
      "10976/10976 [==============================] - 6s 532us/step - loss: 0.0201 - acc: 0.4699 - val_loss: 0.0239 - val_acc: 0.4616\n",
      "Epoch 326/340\n",
      "10976/10976 [==============================] - 6s 527us/step - loss: 0.0202 - acc: 0.4710 - val_loss: 0.0167 - val_acc: 0.4856\n",
      "Epoch 327/340\n",
      "10976/10976 [==============================] - 6s 524us/step - loss: 0.0195 - acc: 0.4688 - val_loss: 0.0233 - val_acc: 0.4691\n",
      "Epoch 328/340\n",
      "10976/10976 [==============================] - 6s 515us/step - loss: 0.0200 - acc: 0.4706 - val_loss: 0.0158 - val_acc: 0.4857\n",
      "Epoch 329/340\n",
      "10976/10976 [==============================] - 6s 506us/step - loss: 0.0199 - acc: 0.4706 - val_loss: 0.0234 - val_acc: 0.4663\n",
      "Epoch 330/340\n",
      "10976/10976 [==============================] - 5s 490us/step - loss: 0.0201 - acc: 0.4716 - val_loss: 0.0162 - val_acc: 0.4849\n",
      "Epoch 331/340\n",
      "10976/10976 [==============================] - 6s 513us/step - loss: 0.0200 - acc: 0.4704 - val_loss: 0.0234 - val_acc: 0.4793\n",
      "Epoch 332/340\n",
      "10976/10976 [==============================] - 6s 528us/step - loss: 0.0201 - acc: 0.4714 - val_loss: 0.0162 - val_acc: 0.4868\n",
      "Epoch 333/340\n",
      "10976/10976 [==============================] - 6s 510us/step - loss: 0.0198 - acc: 0.4726 - val_loss: 0.0228 - val_acc: 0.4637\n",
      "Epoch 334/340\n",
      "10976/10976 [==============================] - 6s 517us/step - loss: 0.0195 - acc: 0.4658 - val_loss: 0.0156 - val_acc: 0.4848\n",
      "Epoch 335/340\n",
      "10976/10976 [==============================] - 6s 539us/step - loss: 0.0197 - acc: 0.4692 - val_loss: 0.0221 - val_acc: 0.4717\n",
      "Epoch 336/340\n",
      "10976/10976 [==============================] - 6s 507us/step - loss: 0.0195 - acc: 0.4713 - val_loss: 0.0177 - val_acc: 0.4832\n",
      "Epoch 337/340\n",
      "10976/10976 [==============================] - 6s 533us/step - loss: 0.0195 - acc: 0.4707 - val_loss: 0.0227 - val_acc: 0.4698\n",
      "Epoch 338/340\n",
      "10976/10976 [==============================] - 6s 519us/step - loss: 0.0197 - acc: 0.4716 - val_loss: 0.0155 - val_acc: 0.4878\n",
      "Epoch 339/340\n",
      "10976/10976 [==============================] - 5s 498us/step - loss: 0.0196 - acc: 0.4667 - val_loss: 0.0229 - val_acc: 0.4804\n",
      "Epoch 340/340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 5s 500us/step - loss: 0.0195 - acc: 0.4722 - val_loss: 0.0156 - val_acc: 0.4879\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 321/340\n",
      "10976/10976 [==============================] - 6s 510us/step - loss: 0.0227 - acc: 0.4428 - val_loss: 0.0232 - val_acc: 0.4387\n",
      "Epoch 322/340\n",
      "10976/10976 [==============================] - 6s 529us/step - loss: 0.0229 - acc: 0.4821 - val_loss: 0.0231 - val_acc: 0.5434\n",
      "Epoch 323/340\n",
      "10976/10976 [==============================] - 6s 525us/step - loss: 0.0226 - acc: 0.4683 - val_loss: 0.0231 - val_acc: 0.3841\n",
      "Epoch 324/340\n",
      "10976/10976 [==============================] - 6s 543us/step - loss: 0.0227 - acc: 0.4488 - val_loss: 0.0226 - val_acc: 0.5108\n",
      "Epoch 325/340\n",
      "10976/10976 [==============================] - 6s 511us/step - loss: 0.0227 - acc: 0.4520 - val_loss: 0.0232 - val_acc: 0.3759\n",
      "Epoch 326/340\n",
      "10976/10976 [==============================] - 6s 542us/step - loss: 0.0228 - acc: 0.4823 - val_loss: 0.0238 - val_acc: 0.4173\n",
      "Epoch 327/340\n",
      "10976/10976 [==============================] - 6s 534us/step - loss: 0.0230 - acc: 0.4819 - val_loss: 0.0221 - val_acc: 0.5489\n",
      "Epoch 328/340\n",
      "10976/10976 [==============================] - 6s 511us/step - loss: 0.0226 - acc: 0.4540 - val_loss: 0.0229 - val_acc: 0.3852\n",
      "Epoch 329/340\n",
      "10976/10976 [==============================] - 6s 528us/step - loss: 0.0228 - acc: 0.4433 - val_loss: 0.0231 - val_acc: 0.5124\n",
      "Epoch 330/340\n",
      "10976/10976 [==============================] - 6s 509us/step - loss: 0.0226 - acc: 0.4487 - val_loss: 0.0230 - val_acc: 0.4750\n",
      "Epoch 331/340\n",
      "10976/10976 [==============================] - 6s 516us/step - loss: 0.0225 - acc: 0.4729 - val_loss: 0.0227 - val_acc: 0.4928\n",
      "Epoch 332/340\n",
      "10976/10976 [==============================] - 6s 526us/step - loss: 0.0228 - acc: 0.5109 - val_loss: 0.0220 - val_acc: 0.5337\n",
      "Epoch 333/340\n",
      "10976/10976 [==============================] - 6s 535us/step - loss: 0.0232 - acc: 0.5137 - val_loss: 0.0236 - val_acc: 0.5059\n",
      "Epoch 334/340\n",
      "10976/10976 [==============================] - 6s 517us/step - loss: 0.0227 - acc: 0.5068 - val_loss: 0.0206 - val_acc: 0.5255\n",
      "Epoch 335/340\n",
      "10976/10976 [==============================] - 6s 533us/step - loss: 0.0227 - acc: 0.4584 - val_loss: 0.0223 - val_acc: 0.4304\n",
      "Epoch 336/340\n",
      "10976/10976 [==============================] - 6s 523us/step - loss: 0.0226 - acc: 0.4864 - val_loss: 0.0238 - val_acc: 0.4797\n",
      "Epoch 337/340\n",
      "10976/10976 [==============================] - 6s 546us/step - loss: 0.0228 - acc: 0.5134 - val_loss: 0.0233 - val_acc: 0.5345\n",
      "Epoch 338/340\n",
      "10976/10976 [==============================] - 6s 522us/step - loss: 0.0227 - acc: 0.5130 - val_loss: 0.0231 - val_acc: 0.4784\n",
      "Epoch 339/340\n",
      "10976/10976 [==============================] - 5s 491us/step - loss: 0.0228 - acc: 0.5094 - val_loss: 0.0233 - val_acc: 0.5385\n",
      "Epoch 340/340\n",
      "10976/10976 [==============================] - 6s 517us/step - loss: 0.0224 - acc: 0.4858 - val_loss: 0.0231 - val_acc: 0.4217\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 321/340\n",
      "10976/10976 [==============================] - 6s 518us/step - loss: 0.0178 - acc: 0.4028 - val_loss: 0.0131 - val_acc: 0.4378\n",
      "Epoch 322/340\n",
      "10976/10976 [==============================] - 6s 537us/step - loss: 0.0175 - acc: 0.4236 - val_loss: 0.0232 - val_acc: 0.4835\n",
      "Epoch 323/340\n",
      "10976/10976 [==============================] - 6s 514us/step - loss: 0.0180 - acc: 0.3980 - val_loss: 0.0128 - val_acc: 0.4519\n",
      "Epoch 324/340\n",
      "10976/10976 [==============================] - 6s 542us/step - loss: 0.0180 - acc: 0.4683 - val_loss: 0.0238 - val_acc: 0.4448\n",
      "Epoch 325/340\n",
      "10976/10976 [==============================] - 6s 534us/step - loss: 0.0180 - acc: 0.4364 - val_loss: 0.0110 - val_acc: 0.5150\n",
      "Epoch 326/340\n",
      "10976/10976 [==============================] - 6s 512us/step - loss: 0.0176 - acc: 0.4207 - val_loss: 0.0235 - val_acc: 0.4349\n",
      "Epoch 327/340\n",
      "10976/10976 [==============================] - 5s 499us/step - loss: 0.0180 - acc: 0.4561 - val_loss: 0.0124 - val_acc: 0.5079\n",
      "Epoch 328/340\n",
      "10976/10976 [==============================] - 6s 524us/step - loss: 0.0177 - acc: 0.4368 - val_loss: 0.0240 - val_acc: 0.2995\n",
      "Epoch 329/340\n",
      "10976/10976 [==============================] - 5s 500us/step - loss: 0.0180 - acc: 0.4695 - val_loss: 0.0111 - val_acc: 0.5106\n",
      "Epoch 330/340\n",
      "10976/10976 [==============================] - 6s 531us/step - loss: 0.0177 - acc: 0.4329 - val_loss: 0.0229 - val_acc: 0.4210\n",
      "Epoch 331/340\n",
      "10976/10976 [==============================] - 6s 513us/step - loss: 0.0180 - acc: 0.4236 - val_loss: 0.0111 - val_acc: 0.4545\n",
      "Epoch 332/340\n",
      "10976/10976 [==============================] - 6s 546us/step - loss: 0.0176 - acc: 0.4422 - val_loss: 0.0234 - val_acc: 0.2934\n",
      "Epoch 333/340\n",
      "10976/10976 [==============================] - 6s 517us/step - loss: 0.0180 - acc: 0.4455 - val_loss: 0.0133 - val_acc: 0.4294\n",
      "Epoch 334/340\n",
      "10976/10976 [==============================] - 6s 509us/step - loss: 0.0178 - acc: 0.4542 - val_loss: 0.0235 - val_acc: 0.5129\n",
      "Epoch 335/340\n",
      "10976/10976 [==============================] - 6s 541us/step - loss: 0.0179 - acc: 0.4298 - val_loss: 0.0119 - val_acc: 0.4493\n",
      "Epoch 336/340\n",
      "10976/10976 [==============================] - 6s 516us/step - loss: 0.0176 - acc: 0.4305 - val_loss: 0.0244 - val_acc: 0.5135\n",
      "Epoch 337/340\n",
      "10976/10976 [==============================] - 6s 530us/step - loss: 0.0181 - acc: 0.4393 - val_loss: 0.0115 - val_acc: 0.4418\n",
      "Epoch 338/340\n",
      "10976/10976 [==============================] - 6s 522us/step - loss: 0.0176 - acc: 0.4259 - val_loss: 0.0233 - val_acc: 0.2934\n",
      "Epoch 339/340\n",
      "10976/10976 [==============================] - 6s 516us/step - loss: 0.0181 - acc: 0.4230 - val_loss: 0.0126 - val_acc: 0.5138\n",
      "Epoch 340/340\n",
      "10976/10976 [==============================] - 6s 515us/step - loss: 0.0178 - acc: 0.4534 - val_loss: 0.0246 - val_acc: 0.5106\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 321/340\n",
      "10976/10976 [==============================] - 6s 557us/step - loss: 0.0148 - acc: 0.3695 - val_loss: 0.0147 - val_acc: 0.3622\n",
      "Epoch 322/340\n",
      "10976/10976 [==============================] - 6s 513us/step - loss: 0.0148 - acc: 0.3521 - val_loss: 0.0148 - val_acc: 0.3609\n",
      "Epoch 323/340\n",
      "10976/10976 [==============================] - 6s 544us/step - loss: 0.0149 - acc: 0.3603 - val_loss: 0.0150 - val_acc: 0.3573\n",
      "Epoch 324/340\n",
      "10976/10976 [==============================] - 6s 527us/step - loss: 0.0148 - acc: 0.3481 - val_loss: 0.0143 - val_acc: 0.3621\n",
      "Epoch 325/340\n",
      "10976/10976 [==============================] - 6s 524us/step - loss: 0.0148 - acc: 0.3523 - val_loss: 0.0156 - val_acc: 0.3561\n",
      "Epoch 326/340\n",
      "10976/10976 [==============================] - 6s 533us/step - loss: 0.0149 - acc: 0.3659 - val_loss: 0.0120 - val_acc: 0.3209\n",
      "Epoch 327/340\n",
      "10976/10976 [==============================] - 6s 507us/step - loss: 0.0148 - acc: 0.3636 - val_loss: 0.0159 - val_acc: 0.4221\n",
      "Epoch 328/340\n",
      "10976/10976 [==============================] - 6s 505us/step - loss: 0.0149 - acc: 0.3585 - val_loss: 0.0135 - val_acc: 0.3016\n",
      "Epoch 329/340\n",
      "10976/10976 [==============================] - 6s 510us/step - loss: 0.0150 - acc: 0.3614 - val_loss: 0.0167 - val_acc: 0.4331\n",
      "Epoch 330/340\n",
      "10976/10976 [==============================] - 6s 570us/step - loss: 0.0149 - acc: 0.3671 - val_loss: 0.0132 - val_acc: 0.3088\n",
      "Epoch 331/340\n",
      "10976/10976 [==============================] - 6s 520us/step - loss: 0.0147 - acc: 0.3595 - val_loss: 0.0163 - val_acc: 0.4188\n",
      "Epoch 332/340\n",
      "10976/10976 [==============================] - 6s 530us/step - loss: 0.0149 - acc: 0.3608 - val_loss: 0.0121 - val_acc: 0.3115\n",
      "Epoch 333/340\n",
      "10976/10976 [==============================] - 6s 538us/step - loss: 0.0148 - acc: 0.3649 - val_loss: 0.0161 - val_acc: 0.4222\n",
      "Epoch 334/340\n",
      "10976/10976 [==============================] - 6s 529us/step - loss: 0.0149 - acc: 0.3650 - val_loss: 0.0133 - val_acc: 0.3060\n",
      "Epoch 335/340\n",
      "10976/10976 [==============================] - 6s 554us/step - loss: 0.0149 - acc: 0.3595 - val_loss: 0.0164 - val_acc: 0.4373\n",
      "Epoch 336/340\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 6s 522us/step - loss: 0.0149 - acc: 0.3625 - val_loss: 0.0125 - val_acc: 0.3195\n",
      "Epoch 337/340\n",
      "10976/10976 [==============================] - 6s 524us/step - loss: 0.0149 - acc: 0.3631 - val_loss: 0.0160 - val_acc: 0.4263\n",
      "Epoch 338/340\n",
      "10976/10976 [==============================] - 6s 530us/step - loss: 0.0150 - acc: 0.3613 - val_loss: 0.0126 - val_acc: 0.3109\n",
      "Epoch 339/340\n",
      "10976/10976 [==============================] - 6s 516us/step - loss: 0.0150 - acc: 0.3627 - val_loss: 0.0164 - val_acc: 0.4311\n",
      "Epoch 340/340\n",
      "10976/10976 [==============================] - 6s 529us/step - loss: 0.0151 - acc: 0.3608 - val_loss: 0.0133 - val_acc: 0.3063\n",
      "start training round 17\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 341/360\n",
      "10976/10976 [==============================] - 6s 523us/step - loss: 0.0197 - acc: 0.4705 - val_loss: 0.0233 - val_acc: 0.4686\n",
      "Epoch 342/360\n",
      "10976/10976 [==============================] - 6s 535us/step - loss: 0.0199 - acc: 0.4675 - val_loss: 0.0157 - val_acc: 0.4735\n",
      "Epoch 343/360\n",
      "10976/10976 [==============================] - 6s 533us/step - loss: 0.0198 - acc: 0.4705 - val_loss: 0.0226 - val_acc: 0.4681\n",
      "Epoch 344/360\n",
      "10976/10976 [==============================] - 6s 532us/step - loss: 0.0196 - acc: 0.4665 - val_loss: 0.0198 - val_acc: 0.4852\n",
      "Epoch 345/360\n",
      "10976/10976 [==============================] - 6s 519us/step - loss: 0.0197 - acc: 0.4739 - val_loss: 0.0234 - val_acc: 0.4694\n",
      "Epoch 346/360\n",
      "10976/10976 [==============================] - 6s 569us/step - loss: 0.0197 - acc: 0.4725 - val_loss: 0.0165 - val_acc: 0.4869\n",
      "Epoch 347/360\n",
      "10976/10976 [==============================] - 7s 599us/step - loss: 0.0194 - acc: 0.4734 - val_loss: 0.0203 - val_acc: 0.4425\n",
      "Epoch 348/360\n",
      "10976/10976 [==============================] - 6s 558us/step - loss: 0.0199 - acc: 0.4732 - val_loss: 0.0163 - val_acc: 0.4732\n",
      "Epoch 349/360\n",
      "10976/10976 [==============================] - 6s 575us/step - loss: 0.0192 - acc: 0.4726 - val_loss: 0.0227 - val_acc: 0.4772\n",
      "Epoch 350/360\n",
      "10976/10976 [==============================] - 6s 542us/step - loss: 0.0200 - acc: 0.4742 - val_loss: 0.0169 - val_acc: 0.4778\n",
      "Epoch 351/360\n",
      "10976/10976 [==============================] - 6s 538us/step - loss: 0.0195 - acc: 0.4739 - val_loss: 0.0183 - val_acc: 0.4469\n",
      "Epoch 352/360\n",
      "10976/10976 [==============================] - 6s 588us/step - loss: 0.0196 - acc: 0.4709 - val_loss: 0.0175 - val_acc: 0.4846\n",
      "Epoch 353/360\n",
      "10976/10976 [==============================] - 6s 561us/step - loss: 0.0194 - acc: 0.4748 - val_loss: 0.0210 - val_acc: 0.4616\n",
      "Epoch 354/360\n",
      "10976/10976 [==============================] - 6s 570us/step - loss: 0.0195 - acc: 0.4702 - val_loss: 0.0161 - val_acc: 0.4902\n",
      "Epoch 355/360\n",
      "10976/10976 [==============================] - 6s 556us/step - loss: 0.0197 - acc: 0.4713 - val_loss: 0.0238 - val_acc: 0.4678\n",
      "Epoch 356/360\n",
      "10976/10976 [==============================] - 6s 530us/step - loss: 0.0194 - acc: 0.4727 - val_loss: 0.0169 - val_acc: 0.4868\n",
      "Epoch 357/360\n",
      "10976/10976 [==============================] - 6s 557us/step - loss: 0.0197 - acc: 0.4710 - val_loss: 0.0200 - val_acc: 0.4447\n",
      "Epoch 358/360\n",
      "10976/10976 [==============================] - 6s 542us/step - loss: 0.0194 - acc: 0.4657 - val_loss: 0.0168 - val_acc: 0.4716\n",
      "Epoch 359/360\n",
      "10976/10976 [==============================] - 6s 524us/step - loss: 0.0195 - acc: 0.4744 - val_loss: 0.0224 - val_acc: 0.4735\n",
      "Epoch 360/360\n",
      "10976/10976 [==============================] - 6s 569us/step - loss: 0.0196 - acc: 0.4714 - val_loss: 0.0164 - val_acc: 0.4797\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 341/360\n",
      "10976/10976 [==============================] - 6s 557us/step - loss: 0.0227 - acc: 0.4894 - val_loss: 0.0229 - val_acc: 0.5213\n",
      "Epoch 342/360\n",
      "10976/10976 [==============================] - 6s 564us/step - loss: 0.0228 - acc: 0.5117 - val_loss: 0.0240 - val_acc: 0.4805\n",
      "Epoch 343/360\n",
      "10976/10976 [==============================] - 6s 555us/step - loss: 0.0228 - acc: 0.5101 - val_loss: 0.0236 - val_acc: 0.5405\n",
      "Epoch 344/360\n",
      "10976/10976 [==============================] - 6s 549us/step - loss: 0.0226 - acc: 0.5012 - val_loss: 0.0228 - val_acc: 0.4764\n",
      "Epoch 345/360\n",
      "10976/10976 [==============================] - 6s 546us/step - loss: 0.0226 - acc: 0.4469 - val_loss: 0.0226 - val_acc: 0.4751\n",
      "Epoch 346/360\n",
      "10976/10976 [==============================] - 6s 541us/step - loss: 0.0229 - acc: 0.5020 - val_loss: 0.0243 - val_acc: 0.4960\n",
      "Epoch 347/360\n",
      "10976/10976 [==============================] - 6s 564us/step - loss: 0.0223 - acc: 0.4879 - val_loss: 0.0213 - val_acc: 0.5525\n",
      "Epoch 348/360\n",
      "10976/10976 [==============================] - 6s 557us/step - loss: 0.0225 - acc: 0.4848 - val_loss: 0.0232 - val_acc: 0.4128\n",
      "Epoch 349/360\n",
      "10976/10976 [==============================] - 6s 557us/step - loss: 0.0226 - acc: 0.5015 - val_loss: 0.0229 - val_acc: 0.5556\n",
      "Epoch 350/360\n",
      "10976/10976 [==============================] - 6s 539us/step - loss: 0.0226 - acc: 0.5138 - val_loss: 0.0229 - val_acc: 0.5138\n",
      "Epoch 351/360\n",
      "10976/10976 [==============================] - 6s 540us/step - loss: 0.0225 - acc: 0.5113 - val_loss: 0.0231 - val_acc: 0.5369\n",
      "Epoch 352/360\n",
      "10976/10976 [==============================] - 6s 558us/step - loss: 0.0226 - acc: 0.5127 - val_loss: 0.0227 - val_acc: 0.4798\n",
      "Epoch 353/360\n",
      "10976/10976 [==============================] - 6s 573us/step - loss: 0.0228 - acc: 0.5075 - val_loss: 0.0220 - val_acc: 0.5309\n",
      "Epoch 354/360\n",
      "10976/10976 [==============================] - 6s 559us/step - loss: 0.0227 - acc: 0.5113 - val_loss: 0.0235 - val_acc: 0.4808\n",
      "Epoch 355/360\n",
      "10976/10976 [==============================] - 6s 572us/step - loss: 0.0232 - acc: 0.5140 - val_loss: 0.0209 - val_acc: 0.5524\n",
      "Epoch 356/360\n",
      "10976/10976 [==============================] - 6s 561us/step - loss: 0.0227 - acc: 0.5090 - val_loss: 0.0226 - val_acc: 0.5070\n",
      "Epoch 357/360\n",
      "10976/10976 [==============================] - 6s 555us/step - loss: 0.0224 - acc: 0.5150 - val_loss: 0.0229 - val_acc: 0.5357\n",
      "Epoch 358/360\n",
      "10976/10976 [==============================] - 6s 564us/step - loss: 0.0224 - acc: 0.5053 - val_loss: 0.0223 - val_acc: 0.4527\n",
      "Epoch 359/360\n",
      "10976/10976 [==============================] - 6s 557us/step - loss: 0.0224 - acc: 0.4991 - val_loss: 0.0233 - val_acc: 0.5493\n",
      "Epoch 360/360\n",
      "10976/10976 [==============================] - 6s 560us/step - loss: 0.0229 - acc: 0.5068 - val_loss: 0.0230 - val_acc: 0.4875\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 341/360\n",
      "10976/10976 [==============================] - 6s 560us/step - loss: 0.0180 - acc: 0.4431 - val_loss: 0.0118 - val_acc: 0.5178\n",
      "Epoch 342/360\n",
      "10976/10976 [==============================] - 6s 546us/step - loss: 0.0178 - acc: 0.4309 - val_loss: 0.0174 - val_acc: 0.2586\n",
      "Epoch 343/360\n",
      "10976/10976 [==============================] - 6s 558us/step - loss: 0.0177 - acc: 0.4221 - val_loss: 0.0117 - val_acc: 0.5129\n",
      "Epoch 344/360\n",
      "10976/10976 [==============================] - 6s 577us/step - loss: 0.0178 - acc: 0.4291 - val_loss: 0.0237 - val_acc: 0.2949\n",
      "Epoch 345/360\n",
      "10976/10976 [==============================] - 6s 551us/step - loss: 0.0178 - acc: 0.4087 - val_loss: 0.0145 - val_acc: 0.2866\n",
      "Epoch 346/360\n",
      "10976/10976 [==============================] - 6s 583us/step - loss: 0.0179 - acc: 0.4225 - val_loss: 0.0237 - val_acc: 0.2944\n",
      "Epoch 347/360\n",
      "10976/10976 [==============================] - 6s 580us/step - loss: 0.0178 - acc: 0.4356 - val_loss: 0.0112 - val_acc: 0.4465\n",
      "Epoch 348/360\n",
      "10976/10976 [==============================] - 5s 497us/step - loss: 0.0175 - acc: 0.4272 - val_loss: 0.0239 - val_acc: 0.5123\n",
      "Epoch 349/360\n",
      "10976/10976 [==============================] - 5s 436us/step - loss: 0.0179 - acc: 0.4520 - val_loss: 0.0129 - val_acc: 0.5128\n",
      "Epoch 350/360\n",
      "10976/10976 [==============================] - 5s 458us/step - loss: 0.0175 - acc: 0.3967 - val_loss: 0.0237 - val_acc: 0.3029\n",
      "Epoch 351/360\n",
      "10976/10976 [==============================] - 5s 440us/step - loss: 0.0179 - acc: 0.4194 - val_loss: 0.0117 - val_acc: 0.5126\n",
      "Epoch 352/360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 5s 441us/step - loss: 0.0175 - acc: 0.4521 - val_loss: 0.0242 - val_acc: 0.3045\n",
      "Epoch 353/360\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0175 - acc: 0.4403 - val_loss: 0.0134 - val_acc: 0.5104\n",
      "Epoch 354/360\n",
      "10976/10976 [==============================] - 5s 433us/step - loss: 0.0177 - acc: 0.4200 - val_loss: 0.0245 - val_acc: 0.4436\n",
      "Epoch 355/360\n",
      "10976/10976 [==============================] - 5s 437us/step - loss: 0.0179 - acc: 0.4345 - val_loss: 0.0113 - val_acc: 0.4444\n",
      "Epoch 356/360\n",
      "10976/10976 [==============================] - 5s 433us/step - loss: 0.0176 - acc: 0.4330 - val_loss: 0.0245 - val_acc: 0.4438\n",
      "Epoch 357/360\n",
      "10976/10976 [==============================] - 5s 435us/step - loss: 0.0179 - acc: 0.4343 - val_loss: 0.0128 - val_acc: 0.4268\n",
      "Epoch 358/360\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0175 - acc: 0.4320 - val_loss: 0.0241 - val_acc: 0.4481\n",
      "Epoch 359/360\n",
      "10976/10976 [==============================] - 5s 451us/step - loss: 0.0177 - acc: 0.4320 - val_loss: 0.0128 - val_acc: 0.5177\n",
      "Epoch 360/360\n",
      "10976/10976 [==============================] - 5s 437us/step - loss: 0.0176 - acc: 0.4419 - val_loss: 0.0215 - val_acc: 0.4978\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 341/360\n",
      "10976/10976 [==============================] - 5s 436us/step - loss: 0.0148 - acc: 0.3589 - val_loss: 0.0158 - val_acc: 0.4196\n",
      "Epoch 342/360\n",
      "10976/10976 [==============================] - 5s 447us/step - loss: 0.0150 - acc: 0.3640 - val_loss: 0.0127 - val_acc: 0.3139\n",
      "Epoch 343/360\n",
      "10976/10976 [==============================] - 5s 444us/step - loss: 0.0148 - acc: 0.3631 - val_loss: 0.0157 - val_acc: 0.4036\n",
      "Epoch 344/360\n",
      "10976/10976 [==============================] - 5s 489us/step - loss: 0.0150 - acc: 0.3698 - val_loss: 0.0132 - val_acc: 0.3057\n",
      "Epoch 345/360\n",
      "10976/10976 [==============================] - 6s 567us/step - loss: 0.0148 - acc: 0.3606 - val_loss: 0.0159 - val_acc: 0.4289\n",
      "Epoch 346/360\n",
      "10976/10976 [==============================] - 7s 593us/step - loss: 0.0149 - acc: 0.3675 - val_loss: 0.0122 - val_acc: 0.3141\n",
      "Epoch 347/360\n",
      "10976/10976 [==============================] - 6s 566us/step - loss: 0.0148 - acc: 0.3622 - val_loss: 0.0161 - val_acc: 0.4263\n",
      "Epoch 348/360\n",
      "10976/10976 [==============================] - 6s 553us/step - loss: 0.0148 - acc: 0.3626 - val_loss: 0.0127 - val_acc: 0.3102\n",
      "Epoch 349/360\n",
      "10976/10976 [==============================] - 7s 598us/step - loss: 0.0148 - acc: 0.3613 - val_loss: 0.0159 - val_acc: 0.4232\n",
      "Epoch 350/360\n",
      "10976/10976 [==============================] - 6s 560us/step - loss: 0.0149 - acc: 0.3623 - val_loss: 0.0133 - val_acc: 0.3061\n",
      "Epoch 351/360\n",
      "10976/10976 [==============================] - 6s 559us/step - loss: 0.0149 - acc: 0.3580 - val_loss: 0.0162 - val_acc: 0.4217\n",
      "Epoch 352/360\n",
      "10976/10976 [==============================] - 6s 574us/step - loss: 0.0149 - acc: 0.3628 - val_loss: 0.0133 - val_acc: 0.3062\n",
      "Epoch 353/360\n",
      "10976/10976 [==============================] - 6s 578us/step - loss: 0.0147 - acc: 0.3658 - val_loss: 0.0165 - val_acc: 0.4127\n",
      "Epoch 354/360\n",
      "10976/10976 [==============================] - 6s 550us/step - loss: 0.0148 - acc: 0.3638 - val_loss: 0.0132 - val_acc: 0.3071\n",
      "Epoch 355/360\n",
      "10976/10976 [==============================] - 6s 543us/step - loss: 0.0148 - acc: 0.3585 - val_loss: 0.0157 - val_acc: 0.4094\n",
      "Epoch 356/360\n",
      "10976/10976 [==============================] - 6s 569us/step - loss: 0.0150 - acc: 0.3655 - val_loss: 0.0125 - val_acc: 0.3146\n",
      "Epoch 357/360\n",
      "10976/10976 [==============================] - 6s 546us/step - loss: 0.0147 - acc: 0.3650 - val_loss: 0.0159 - val_acc: 0.4097\n",
      "Epoch 358/360\n",
      "10976/10976 [==============================] - 6s 563us/step - loss: 0.0149 - acc: 0.3613 - val_loss: 0.0129 - val_acc: 0.3121\n",
      "Epoch 359/360\n",
      "10976/10976 [==============================] - 6s 579us/step - loss: 0.0147 - acc: 0.3639 - val_loss: 0.0161 - val_acc: 0.4203\n",
      "Epoch 360/360\n",
      "10976/10976 [==============================] - 6s 557us/step - loss: 0.0147 - acc: 0.3699 - val_loss: 0.0131 - val_acc: 0.3113\n",
      "start training round 18\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 361/380\n",
      "10976/10976 [==============================] - 6s 573us/step - loss: 0.0194 - acc: 0.4692 - val_loss: 0.0235 - val_acc: 0.4741\n",
      "Epoch 362/380\n",
      "10976/10976 [==============================] - 6s 536us/step - loss: 0.0198 - acc: 0.4714 - val_loss: 0.0160 - val_acc: 0.4879\n",
      "Epoch 363/380\n",
      "10976/10976 [==============================] - 6s 555us/step - loss: 0.0191 - acc: 0.4337 - val_loss: 0.0198 - val_acc: 0.3865\n",
      "Epoch 364/380\n",
      "10976/10976 [==============================] - 6s 549us/step - loss: 0.0191 - acc: 0.4291 - val_loss: 0.0182 - val_acc: 0.4845\n",
      "Epoch 365/380\n",
      "10976/10976 [==============================] - 6s 577us/step - loss: 0.0194 - acc: 0.4331 - val_loss: 0.0214 - val_acc: 0.3812\n",
      "Epoch 366/380\n",
      "10976/10976 [==============================] - 6s 574us/step - loss: 0.0195 - acc: 0.4343 - val_loss: 0.0162 - val_acc: 0.4855\n",
      "Epoch 367/380\n",
      "10976/10976 [==============================] - 6s 571us/step - loss: 0.0191 - acc: 0.4344 - val_loss: 0.0204 - val_acc: 0.3998\n",
      "Epoch 368/380\n",
      "10976/10976 [==============================] - 6s 552us/step - loss: 0.0193 - acc: 0.4369 - val_loss: 0.0157 - val_acc: 0.4866\n",
      "Epoch 369/380\n",
      "10976/10976 [==============================] - 6s 554us/step - loss: 0.0189 - acc: 0.4351 - val_loss: 0.0217 - val_acc: 0.3820\n",
      "Epoch 370/380\n",
      "10976/10976 [==============================] - 6s 560us/step - loss: 0.0189 - acc: 0.4345 - val_loss: 0.0177 - val_acc: 0.4842\n",
      "Epoch 371/380\n",
      "10976/10976 [==============================] - 6s 558us/step - loss: 0.0192 - acc: 0.4325 - val_loss: 0.0206 - val_acc: 0.3988\n",
      "Epoch 372/380\n",
      "10976/10976 [==============================] - 6s 552us/step - loss: 0.0195 - acc: 0.4303 - val_loss: 0.0166 - val_acc: 0.4857\n",
      "Epoch 373/380\n",
      "10976/10976 [==============================] - 6s 537us/step - loss: 0.0192 - acc: 0.4347 - val_loss: 0.0212 - val_acc: 0.3814\n",
      "Epoch 374/380\n",
      "10976/10976 [==============================] - 6s 517us/step - loss: 0.0192 - acc: 0.4352 - val_loss: 0.0172 - val_acc: 0.4858\n",
      "Epoch 375/380\n",
      "10976/10976 [==============================] - 6s 572us/step - loss: 0.0191 - acc: 0.4380 - val_loss: 0.0201 - val_acc: 0.4148\n",
      "Epoch 376/380\n",
      "10976/10976 [==============================] - 6s 546us/step - loss: 0.0195 - acc: 0.4759 - val_loss: 0.0155 - val_acc: 0.4900\n",
      "Epoch 377/380\n",
      "10976/10976 [==============================] - 6s 522us/step - loss: 0.0196 - acc: 0.4759 - val_loss: 0.0230 - val_acc: 0.4697\n",
      "Epoch 378/380\n",
      "10976/10976 [==============================] - 6s 569us/step - loss: 0.0194 - acc: 0.4752 - val_loss: 0.0173 - val_acc: 0.4924\n",
      "Epoch 379/380\n",
      "10976/10976 [==============================] - 6s 542us/step - loss: 0.0197 - acc: 0.4722 - val_loss: 0.0221 - val_acc: 0.4636\n",
      "Epoch 380/380\n",
      "10976/10976 [==============================] - 6s 538us/step - loss: 0.0190 - acc: 0.4740 - val_loss: 0.0161 - val_acc: 0.4797\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 361/380\n",
      "10976/10976 [==============================] - 6s 553us/step - loss: 0.0226 - acc: 0.4999 - val_loss: 0.0232 - val_acc: 0.5586\n",
      "Epoch 362/380\n",
      "10976/10976 [==============================] - 6s 528us/step - loss: 0.0226 - acc: 0.5144 - val_loss: 0.0235 - val_acc: 0.4922\n",
      "Epoch 363/380\n",
      "10976/10976 [==============================] - 6s 542us/step - loss: 0.0229 - acc: 0.5114 - val_loss: 0.0224 - val_acc: 0.5554\n",
      "Epoch 364/380\n",
      "10976/10976 [==============================] - 6s 541us/step - loss: 0.0224 - acc: 0.4647 - val_loss: 0.0230 - val_acc: 0.5114\n",
      "Epoch 365/380\n",
      "10976/10976 [==============================] - 6s 541us/step - loss: 0.0224 - acc: 0.4587 - val_loss: 0.0224 - val_acc: 0.3957\n",
      "Epoch 366/380\n",
      "10976/10976 [==============================] - 5s 496us/step - loss: 0.0225 - acc: 0.4503 - val_loss: 0.0217 - val_acc: 0.4841\n",
      "Epoch 367/380\n",
      "10976/10976 [==============================] - 6s 514us/step - loss: 0.0222 - acc: 0.5057 - val_loss: 0.0225 - val_acc: 0.5070\n",
      "Epoch 368/380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 6s 529us/step - loss: 0.0229 - acc: 0.5138 - val_loss: 0.0220 - val_acc: 0.5339\n",
      "Epoch 369/380\n",
      "10976/10976 [==============================] - 6s 542us/step - loss: 0.0229 - acc: 0.5151 - val_loss: 0.0241 - val_acc: 0.5127\n",
      "Epoch 370/380\n",
      "10976/10976 [==============================] - 6s 565us/step - loss: 0.0223 - acc: 0.5097 - val_loss: 0.0240 - val_acc: 0.5523\n",
      "Epoch 371/380\n",
      "10976/10976 [==============================] - 6s 552us/step - loss: 0.0223 - acc: 0.5147 - val_loss: 0.0237 - val_acc: 0.4925\n",
      "Epoch 372/380\n",
      "10976/10976 [==============================] - 6s 515us/step - loss: 0.0229 - acc: 0.5197 - val_loss: 0.0245 - val_acc: 0.5253\n",
      "Epoch 373/380\n",
      "10976/10976 [==============================] - 6s 527us/step - loss: 0.0227 - acc: 0.5164 - val_loss: 0.0221 - val_acc: 0.5028\n",
      "Epoch 374/380\n",
      "10976/10976 [==============================] - 6s 540us/step - loss: 0.0225 - acc: 0.5090 - val_loss: 0.0221 - val_acc: 0.5530\n",
      "Epoch 375/380\n",
      "10976/10976 [==============================] - 6s 542us/step - loss: 0.0223 - acc: 0.4861 - val_loss: 0.0228 - val_acc: 0.4226\n",
      "Epoch 376/380\n",
      "10976/10976 [==============================] - 6s 520us/step - loss: 0.0226 - acc: 0.4815 - val_loss: 0.0216 - val_acc: 0.5560\n",
      "Epoch 377/380\n",
      "10976/10976 [==============================] - 6s 545us/step - loss: 0.0223 - acc: 0.5032 - val_loss: 0.0208 - val_acc: 0.4529\n",
      "Epoch 378/380\n",
      "10976/10976 [==============================] - 5s 500us/step - loss: 0.0225 - acc: 0.5116 - val_loss: 0.0231 - val_acc: 0.5330\n",
      "Epoch 379/380\n",
      "10976/10976 [==============================] - 6s 524us/step - loss: 0.0225 - acc: 0.5189 - val_loss: 0.0232 - val_acc: 0.5076\n",
      "Epoch 380/380\n",
      "10976/10976 [==============================] - 6s 516us/step - loss: 0.0224 - acc: 0.4948 - val_loss: 0.0232 - val_acc: 0.5621\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 361/380\n",
      "10976/10976 [==============================] - 6s 540us/step - loss: 0.0175 - acc: 0.4226 - val_loss: 0.0120 - val_acc: 0.4424\n",
      "Epoch 362/380\n",
      "10976/10976 [==============================] - 6s 568us/step - loss: 0.0175 - acc: 0.4287 - val_loss: 0.0242 - val_acc: 0.5055\n",
      "Epoch 363/380\n",
      "10976/10976 [==============================] - 6s 514us/step - loss: 0.0178 - acc: 0.4412 - val_loss: 0.0124 - val_acc: 0.4962\n",
      "Epoch 364/380\n",
      "10976/10976 [==============================] - 5s 500us/step - loss: 0.0173 - acc: 0.4186 - val_loss: 0.0230 - val_acc: 0.4963\n",
      "Epoch 365/380\n",
      "10976/10976 [==============================] - 6s 533us/step - loss: 0.0179 - acc: 0.4339 - val_loss: 0.0122 - val_acc: 0.4973\n",
      "Epoch 366/380\n",
      "10976/10976 [==============================] - 6s 529us/step - loss: 0.0174 - acc: 0.4348 - val_loss: 0.0243 - val_acc: 0.4757\n",
      "Epoch 367/380\n",
      "10976/10976 [==============================] - 6s 507us/step - loss: 0.0179 - acc: 0.4316 - val_loss: 0.0147 - val_acc: 0.5164\n",
      "Epoch 368/380\n",
      "10976/10976 [==============================] - 6s 523us/step - loss: 0.0176 - acc: 0.4483 - val_loss: 0.0224 - val_acc: 0.4221\n",
      "Epoch 369/380\n",
      "10976/10976 [==============================] - 6s 532us/step - loss: 0.0177 - acc: 0.4096 - val_loss: 0.0114 - val_acc: 0.5168\n",
      "Epoch 370/380\n",
      "10976/10976 [==============================] - 6s 526us/step - loss: 0.0172 - acc: 0.4095 - val_loss: 0.0235 - val_acc: 0.3009\n",
      "Epoch 371/380\n",
      "10976/10976 [==============================] - 6s 540us/step - loss: 0.0176 - acc: 0.4301 - val_loss: 0.0112 - val_acc: 0.5180\n",
      "Epoch 372/380\n",
      "10976/10976 [==============================] - 6s 545us/step - loss: 0.0177 - acc: 0.4137 - val_loss: 0.0247 - val_acc: 0.4445\n",
      "Epoch 373/380\n",
      "10976/10976 [==============================] - 6s 531us/step - loss: 0.0178 - acc: 0.4327 - val_loss: 0.0129 - val_acc: 0.5136\n",
      "Epoch 374/380\n",
      "10976/10976 [==============================] - 6s 560us/step - loss: 0.0174 - acc: 0.4362 - val_loss: 0.0236 - val_acc: 0.2986\n",
      "Epoch 375/380\n",
      "10976/10976 [==============================] - 6s 536us/step - loss: 0.0179 - acc: 0.4103 - val_loss: 0.0122 - val_acc: 0.4454\n",
      "Epoch 376/380\n",
      "10976/10976 [==============================] - 6s 550us/step - loss: 0.0175 - acc: 0.4158 - val_loss: 0.0239 - val_acc: 0.4314\n",
      "Epoch 377/380\n",
      "10976/10976 [==============================] - 6s 535us/step - loss: 0.0178 - acc: 0.3819 - val_loss: 0.0110 - val_acc: 0.5185\n",
      "Epoch 378/380\n",
      "10976/10976 [==============================] - 6s 536us/step - loss: 0.0176 - acc: 0.4229 - val_loss: 0.0215 - val_acc: 0.3014\n",
      "Epoch 379/380\n",
      "10976/10976 [==============================] - 6s 529us/step - loss: 0.0178 - acc: 0.4041 - val_loss: 0.0126 - val_acc: 0.4496\n",
      "Epoch 380/380\n",
      "10976/10976 [==============================] - 6s 531us/step - loss: 0.0175 - acc: 0.4197 - val_loss: 0.0241 - val_acc: 0.3083\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 361/380\n",
      "10976/10976 [==============================] - 6s 567us/step - loss: 0.0148 - acc: 0.3611 - val_loss: 0.0162 - val_acc: 0.4297\n",
      "Epoch 362/380\n",
      "10976/10976 [==============================] - 6s 527us/step - loss: 0.0149 - acc: 0.3716 - val_loss: 0.0133 - val_acc: 0.3079\n",
      "Epoch 363/380\n",
      "10976/10976 [==============================] - 6s 521us/step - loss: 0.0147 - acc: 0.3667 - val_loss: 0.0155 - val_acc: 0.4347\n",
      "Epoch 364/380\n",
      "10976/10976 [==============================] - 6s 543us/step - loss: 0.0147 - acc: 0.3637 - val_loss: 0.0142 - val_acc: 0.3057\n",
      "Epoch 365/380\n",
      "10976/10976 [==============================] - 6s 536us/step - loss: 0.0148 - acc: 0.3646 - val_loss: 0.0162 - val_acc: 0.4287\n",
      "Epoch 366/380\n",
      "10976/10976 [==============================] - 6s 538us/step - loss: 0.0148 - acc: 0.3607 - val_loss: 0.0135 - val_acc: 0.3066\n",
      "Epoch 367/380\n",
      "10976/10976 [==============================] - 6s 568us/step - loss: 0.0147 - acc: 0.3649 - val_loss: 0.0160 - val_acc: 0.4234\n",
      "Epoch 368/380\n",
      "10976/10976 [==============================] - 6s 512us/step - loss: 0.0148 - acc: 0.3693 - val_loss: 0.0136 - val_acc: 0.3062\n",
      "Epoch 369/380\n",
      "10976/10976 [==============================] - 6s 546us/step - loss: 0.0147 - acc: 0.3610 - val_loss: 0.0162 - val_acc: 0.4218\n",
      "Epoch 370/380\n",
      "10976/10976 [==============================] - 6s 566us/step - loss: 0.0147 - acc: 0.3621 - val_loss: 0.0127 - val_acc: 0.3117\n",
      "Epoch 371/380\n",
      "10976/10976 [==============================] - 6s 546us/step - loss: 0.0146 - acc: 0.3640 - val_loss: 0.0164 - val_acc: 0.4363\n",
      "Epoch 372/380\n",
      "10976/10976 [==============================] - 6s 541us/step - loss: 0.0148 - acc: 0.3672 - val_loss: 0.0129 - val_acc: 0.3159\n",
      "Epoch 373/380\n",
      "10976/10976 [==============================] - 6s 518us/step - loss: 0.0147 - acc: 0.3663 - val_loss: 0.0153 - val_acc: 0.4138\n",
      "Epoch 374/380\n",
      "10976/10976 [==============================] - 6s 542us/step - loss: 0.0147 - acc: 0.3700 - val_loss: 0.0133 - val_acc: 0.3116\n",
      "Epoch 375/380\n",
      "10976/10976 [==============================] - 6s 542us/step - loss: 0.0147 - acc: 0.3671 - val_loss: 0.0161 - val_acc: 0.4290\n",
      "Epoch 376/380\n",
      "10976/10976 [==============================] - 6s 534us/step - loss: 0.0147 - acc: 0.3722 - val_loss: 0.0130 - val_acc: 0.3145\n",
      "Epoch 377/380\n",
      "10976/10976 [==============================] - 6s 534us/step - loss: 0.0147 - acc: 0.3714 - val_loss: 0.0162 - val_acc: 0.4287\n",
      "Epoch 378/380\n",
      "10976/10976 [==============================] - 6s 528us/step - loss: 0.0147 - acc: 0.3711 - val_loss: 0.0138 - val_acc: 0.3128\n",
      "Epoch 379/380\n",
      "10976/10976 [==============================] - 6s 550us/step - loss: 0.0147 - acc: 0.3673 - val_loss: 0.0162 - val_acc: 0.4305\n",
      "Epoch 380/380\n",
      "10976/10976 [==============================] - 6s 551us/step - loss: 0.0147 - acc: 0.3767 - val_loss: 0.0129 - val_acc: 0.3135\n",
      "start training round 19\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 381/400\n",
      "10976/10976 [==============================] - 6s 521us/step - loss: 0.0195 - acc: 0.4713 - val_loss: 0.0224 - val_acc: 0.4677\n",
      "Epoch 382/400\n",
      "10976/10976 [==============================] - 6s 530us/step - loss: 0.0189 - acc: 0.4721 - val_loss: 0.0164 - val_acc: 0.4882\n",
      "Epoch 383/400\n",
      "10976/10976 [==============================] - 6s 557us/step - loss: 0.0193 - acc: 0.4737 - val_loss: 0.0219 - val_acc: 0.4658\n",
      "Epoch 384/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 6s 546us/step - loss: 0.0195 - acc: 0.4748 - val_loss: 0.0191 - val_acc: 0.4848\n",
      "Epoch 385/400\n",
      "10976/10976 [==============================] - 6s 537us/step - loss: 0.0195 - acc: 0.4759 - val_loss: 0.0240 - val_acc: 0.4780\n",
      "Epoch 386/400\n",
      "10976/10976 [==============================] - 5s 495us/step - loss: 0.0197 - acc: 0.4746 - val_loss: 0.0171 - val_acc: 0.4814\n",
      "Epoch 387/400\n",
      "10976/10976 [==============================] - 6s 522us/step - loss: 0.0192 - acc: 0.4728 - val_loss: 0.0220 - val_acc: 0.4665\n",
      "Epoch 388/400\n",
      "10976/10976 [==============================] - 6s 528us/step - loss: 0.0191 - acc: 0.4757 - val_loss: 0.0157 - val_acc: 0.4766\n",
      "Epoch 389/400\n",
      "10976/10976 [==============================] - 6s 530us/step - loss: 0.0191 - acc: 0.4738 - val_loss: 0.0219 - val_acc: 0.4855\n",
      "Epoch 390/400\n",
      "10976/10976 [==============================] - 6s 552us/step - loss: 0.0193 - acc: 0.4683 - val_loss: 0.0160 - val_acc: 0.4904\n",
      "Epoch 391/400\n",
      "10976/10976 [==============================] - 6s 512us/step - loss: 0.0195 - acc: 0.4756 - val_loss: 0.0217 - val_acc: 0.4549\n",
      "Epoch 392/400\n",
      "10976/10976 [==============================] - 6s 532us/step - loss: 0.0193 - acc: 0.4735 - val_loss: 0.0182 - val_acc: 0.4745\n",
      "Epoch 393/400\n",
      "10976/10976 [==============================] - 6s 529us/step - loss: 0.0200 - acc: 0.4750 - val_loss: 0.0220 - val_acc: 0.4690\n",
      "Epoch 394/400\n",
      "10976/10976 [==============================] - 6s 528us/step - loss: 0.0192 - acc: 0.4754 - val_loss: 0.0165 - val_acc: 0.4781\n",
      "Epoch 395/400\n",
      "10976/10976 [==============================] - 6s 555us/step - loss: 0.0194 - acc: 0.4719 - val_loss: 0.0218 - val_acc: 0.4589\n",
      "Epoch 396/400\n",
      "10976/10976 [==============================] - 6s 536us/step - loss: 0.0194 - acc: 0.4748 - val_loss: 0.0157 - val_acc: 0.4901\n",
      "Epoch 397/400\n",
      "10976/10976 [==============================] - 6s 532us/step - loss: 0.0187 - acc: 0.4678 - val_loss: 0.0221 - val_acc: 0.4766\n",
      "Epoch 398/400\n",
      "10976/10976 [==============================] - 6s 534us/step - loss: 0.0193 - acc: 0.4729 - val_loss: 0.0144 - val_acc: 0.4916\n",
      "Epoch 399/400\n",
      "10976/10976 [==============================] - 6s 535us/step - loss: 0.0195 - acc: 0.4738 - val_loss: 0.0218 - val_acc: 0.4362\n",
      "Epoch 400/400\n",
      "10976/10976 [==============================] - 6s 551us/step - loss: 0.0198 - acc: 0.4784 - val_loss: 0.0171 - val_acc: 0.4890\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 381/400\n",
      "10976/10976 [==============================] - 6s 547us/step - loss: 0.0225 - acc: 0.5137 - val_loss: 0.0236 - val_acc: 0.5112\n",
      "Epoch 382/400\n",
      "10976/10976 [==============================] - 6s 507us/step - loss: 0.0229 - acc: 0.5194 - val_loss: 0.0218 - val_acc: 0.5581\n",
      "Epoch 383/400\n",
      "10976/10976 [==============================] - 6s 557us/step - loss: 0.0225 - acc: 0.5175 - val_loss: 0.0223 - val_acc: 0.4895\n",
      "Epoch 384/400\n",
      "10976/10976 [==============================] - 6s 550us/step - loss: 0.0226 - acc: 0.5156 - val_loss: 0.0209 - val_acc: 0.5334\n",
      "Epoch 385/400\n",
      "10976/10976 [==============================] - 6s 539us/step - loss: 0.0224 - acc: 0.5170 - val_loss: 0.0236 - val_acc: 0.5207\n",
      "Epoch 386/400\n",
      "10976/10976 [==============================] - 6s 544us/step - loss: 0.0227 - acc: 0.5177 - val_loss: 0.0239 - val_acc: 0.5487\n",
      "Epoch 387/400\n",
      "10976/10976 [==============================] - 6s 543us/step - loss: 0.0221 - acc: 0.4679 - val_loss: 0.0220 - val_acc: 0.4047\n",
      "Epoch 388/400\n",
      "10976/10976 [==============================] - 6s 521us/step - loss: 0.0224 - acc: 0.4544 - val_loss: 0.0227 - val_acc: 0.5204\n",
      "Epoch 389/400\n",
      "10976/10976 [==============================] - 6s 548us/step - loss: 0.0220 - acc: 0.4785 - val_loss: 0.0224 - val_acc: 0.5174\n",
      "Epoch 390/400\n",
      "10976/10976 [==============================] - 6s 557us/step - loss: 0.0220 - acc: 0.5141 - val_loss: 0.0221 - val_acc: 0.5568\n",
      "Epoch 391/400\n",
      "10976/10976 [==============================] - 6s 545us/step - loss: 0.0223 - acc: 0.4914 - val_loss: 0.0220 - val_acc: 0.4315\n",
      "Epoch 392/400\n",
      "10976/10976 [==============================] - 6s 554us/step - loss: 0.0224 - acc: 0.4859 - val_loss: 0.0218 - val_acc: 0.5534\n",
      "Epoch 393/400\n",
      "10976/10976 [==============================] - 6s 534us/step - loss: 0.0222 - acc: 0.4919 - val_loss: 0.0232 - val_acc: 0.4387\n",
      "Epoch 394/400\n",
      "10976/10976 [==============================] - 6s 579us/step - loss: 0.0224 - acc: 0.5006 - val_loss: 0.0225 - val_acc: 0.5393\n",
      "Epoch 395/400\n",
      "10976/10976 [==============================] - 6s 525us/step - loss: 0.0228 - acc: 0.5191 - val_loss: 0.0206 - val_acc: 0.4894\n",
      "Epoch 396/400\n",
      "10976/10976 [==============================] - 6s 556us/step - loss: 0.0224 - acc: 0.4976 - val_loss: 0.0226 - val_acc: 0.5378\n",
      "Epoch 397/400\n",
      "10976/10976 [==============================] - 6s 549us/step - loss: 0.0221 - acc: 0.5162 - val_loss: 0.0228 - val_acc: 0.5490\n",
      "Epoch 398/400\n",
      "10976/10976 [==============================] - 6s 522us/step - loss: 0.0224 - acc: 0.5203 - val_loss: 0.0209 - val_acc: 0.5582\n",
      "Epoch 399/400\n",
      "10976/10976 [==============================] - 6s 550us/step - loss: 0.0221 - acc: 0.5014 - val_loss: 0.0229 - val_acc: 0.3957\n",
      "Epoch 400/400\n",
      "10976/10976 [==============================] - 6s 541us/step - loss: 0.0223 - acc: 0.4595 - val_loss: 0.0227 - val_acc: 0.5173\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 381/400\n",
      "10976/10976 [==============================] - 6s 555us/step - loss: 0.0177 - acc: 0.4293 - val_loss: 0.0126 - val_acc: 0.2993\n",
      "Epoch 382/400\n",
      "10976/10976 [==============================] - 6s 541us/step - loss: 0.0176 - acc: 0.4044 - val_loss: 0.0234 - val_acc: 0.2977\n",
      "Epoch 383/400\n",
      "10976/10976 [==============================] - 6s 555us/step - loss: 0.0175 - acc: 0.4511 - val_loss: 0.0110 - val_acc: 0.5189\n",
      "Epoch 384/400\n",
      "10976/10976 [==============================] - 6s 546us/step - loss: 0.0174 - acc: 0.4172 - val_loss: 0.0228 - val_acc: 0.4231\n",
      "Epoch 385/400\n",
      "10976/10976 [==============================] - 6s 525us/step - loss: 0.0177 - acc: 0.3941 - val_loss: 0.0117 - val_acc: 0.4449\n",
      "Epoch 386/400\n",
      "10976/10976 [==============================] - 6s 535us/step - loss: 0.0175 - acc: 0.4028 - val_loss: 0.0226 - val_acc: 0.5075\n",
      "Epoch 387/400\n",
      "10976/10976 [==============================] - 6s 534us/step - loss: 0.0177 - acc: 0.3995 - val_loss: 0.0108 - val_acc: 0.5183\n",
      "Epoch 388/400\n",
      "10976/10976 [==============================] - 6s 552us/step - loss: 0.0173 - acc: 0.3973 - val_loss: 0.0245 - val_acc: 0.4438\n",
      "Epoch 389/400\n",
      "10976/10976 [==============================] - 6s 556us/step - loss: 0.0178 - acc: 0.4386 - val_loss: 0.0114 - val_acc: 0.5194\n",
      "Epoch 390/400\n",
      "10976/10976 [==============================] - 6s 541us/step - loss: 0.0174 - acc: 0.4198 - val_loss: 0.0239 - val_acc: 0.3038\n",
      "Epoch 391/400\n",
      "10976/10976 [==============================] - 6s 527us/step - loss: 0.0176 - acc: 0.4039 - val_loss: 0.0113 - val_acc: 0.5156\n",
      "Epoch 392/400\n",
      "10976/10976 [==============================] - 6s 555us/step - loss: 0.0176 - acc: 0.4180 - val_loss: 0.0249 - val_acc: 0.3057\n",
      "Epoch 393/400\n",
      "10976/10976 [==============================] - 6s 544us/step - loss: 0.0177 - acc: 0.3864 - val_loss: 0.0122 - val_acc: 0.4462\n",
      "Epoch 394/400\n",
      "10976/10976 [==============================] - 6s 554us/step - loss: 0.0174 - acc: 0.3631 - val_loss: 0.0247 - val_acc: 0.3024\n",
      "Epoch 395/400\n",
      "10976/10976 [==============================] - 6s 556us/step - loss: 0.0174 - acc: 0.3946 - val_loss: 0.0116 - val_acc: 0.5155\n",
      "Epoch 396/400\n",
      "10976/10976 [==============================] - 6s 558us/step - loss: 0.0175 - acc: 0.3919 - val_loss: 0.0236 - val_acc: 0.2900\n",
      "Epoch 397/400\n",
      "10976/10976 [==============================] - 6s 556us/step - loss: 0.0176 - acc: 0.3876 - val_loss: 0.0118 - val_acc: 0.3076\n",
      "Epoch 398/400\n",
      "10976/10976 [==============================] - 6s 539us/step - loss: 0.0174 - acc: 0.4103 - val_loss: 0.0243 - val_acc: 0.2949\n",
      "Epoch 399/400\n",
      "10976/10976 [==============================] - 6s 533us/step - loss: 0.0174 - acc: 0.3976 - val_loss: 0.0106 - val_acc: 0.4459\n",
      "Epoch 400/400\n",
      "10976/10976 [==============================] - 6s 541us/step - loss: 0.0173 - acc: 0.3834 - val_loss: 0.0222 - val_acc: 0.4342\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 381/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 6s 545us/step - loss: 0.0147 - acc: 0.3680 - val_loss: 0.0158 - val_acc: 0.4392\n",
      "Epoch 382/400\n",
      "10976/10976 [==============================] - 6s 546us/step - loss: 0.0148 - acc: 0.3701 - val_loss: 0.0133 - val_acc: 0.3119\n",
      "Epoch 383/400\n",
      "10976/10976 [==============================] - 6s 530us/step - loss: 0.0147 - acc: 0.3651 - val_loss: 0.0156 - val_acc: 0.4300\n",
      "Epoch 384/400\n",
      "10976/10976 [==============================] - 6s 550us/step - loss: 0.0147 - acc: 0.3709 - val_loss: 0.0131 - val_acc: 0.3116\n",
      "Epoch 385/400\n",
      "10976/10976 [==============================] - 6s 541us/step - loss: 0.0146 - acc: 0.3670 - val_loss: 0.0162 - val_acc: 0.4338\n",
      "Epoch 386/400\n",
      "10976/10976 [==============================] - 6s 531us/step - loss: 0.0148 - acc: 0.3685 - val_loss: 0.0124 - val_acc: 0.3126\n",
      "Epoch 387/400\n",
      "10976/10976 [==============================] - 6s 557us/step - loss: 0.0146 - acc: 0.3681 - val_loss: 0.0156 - val_acc: 0.4099\n",
      "Epoch 388/400\n",
      "10976/10976 [==============================] - 6s 552us/step - loss: 0.0148 - acc: 0.3713 - val_loss: 0.0136 - val_acc: 0.3114\n",
      "Epoch 389/400\n",
      "10976/10976 [==============================] - 6s 562us/step - loss: 0.0145 - acc: 0.3626 - val_loss: 0.0156 - val_acc: 0.2583\n",
      "Epoch 390/400\n",
      "10976/10976 [==============================] - 6s 557us/step - loss: 0.0149 - acc: 0.3534 - val_loss: 0.0146 - val_acc: 0.4329\n",
      "Epoch 391/400\n",
      "10976/10976 [==============================] - 6s 567us/step - loss: 0.0148 - acc: 0.3590 - val_loss: 0.0152 - val_acc: 0.2694\n",
      "Epoch 392/400\n",
      "10976/10976 [==============================] - 6s 520us/step - loss: 0.0148 - acc: 0.3461 - val_loss: 0.0143 - val_acc: 0.4218\n",
      "Epoch 393/400\n",
      "10976/10976 [==============================] - 6s 557us/step - loss: 0.0147 - acc: 0.3543 - val_loss: 0.0142 - val_acc: 0.3006\n",
      "Epoch 394/400\n",
      "10976/10976 [==============================] - 6s 555us/step - loss: 0.0148 - acc: 0.3486 - val_loss: 0.0148 - val_acc: 0.4336\n",
      "Epoch 395/400\n",
      "10976/10976 [==============================] - 6s 563us/step - loss: 0.0147 - acc: 0.3503 - val_loss: 0.0151 - val_acc: 0.2833\n",
      "Epoch 396/400\n",
      "10976/10976 [==============================] - 6s 534us/step - loss: 0.0149 - acc: 0.3517 - val_loss: 0.0150 - val_acc: 0.4336\n",
      "Epoch 397/400\n",
      "10976/10976 [==============================] - 6s 557us/step - loss: 0.0148 - acc: 0.3543 - val_loss: 0.0138 - val_acc: 0.2863\n",
      "Epoch 398/400\n",
      "10976/10976 [==============================] - 6s 558us/step - loss: 0.0148 - acc: 0.3531 - val_loss: 0.0147 - val_acc: 0.4324\n",
      "Epoch 399/400\n",
      "10976/10976 [==============================] - 6s 547us/step - loss: 0.0147 - acc: 0.3569 - val_loss: 0.0150 - val_acc: 0.2696\n",
      "Epoch 400/400\n",
      "10976/10976 [==============================] - 6s 529us/step - loss: 0.0148 - acc: 0.3510 - val_loss: 0.0142 - val_acc: 0.4329\n",
      "start training round 20\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 401/420\n",
      "10976/10976 [==============================] - 6s 531us/step - loss: 0.0193 - acc: 0.4743 - val_loss: 0.0203 - val_acc: 0.4473\n",
      "Epoch 402/420\n",
      "10976/10976 [==============================] - 6s 549us/step - loss: 0.0189 - acc: 0.4681 - val_loss: 0.0162 - val_acc: 0.4793\n",
      "Epoch 403/420\n",
      "10976/10976 [==============================] - 6s 555us/step - loss: 0.0200 - acc: 0.4779 - val_loss: 0.0224 - val_acc: 0.4698\n",
      "Epoch 404/420\n",
      "10976/10976 [==============================] - 6s 523us/step - loss: 0.0191 - acc: 0.4714 - val_loss: 0.0169 - val_acc: 0.4925\n",
      "Epoch 405/420\n",
      "10976/10976 [==============================] - 6s 546us/step - loss: 0.0191 - acc: 0.4756 - val_loss: 0.0234 - val_acc: 0.4718\n",
      "Epoch 406/420\n",
      "10976/10976 [==============================] - 6s 571us/step - loss: 0.0194 - acc: 0.4766 - val_loss: 0.0149 - val_acc: 0.4902\n",
      "Epoch 407/420\n",
      "10976/10976 [==============================] - 6s 526us/step - loss: 0.0191 - acc: 0.4779 - val_loss: 0.0226 - val_acc: 0.4730\n",
      "Epoch 408/420\n",
      "10976/10976 [==============================] - 6s 550us/step - loss: 0.0195 - acc: 0.4770 - val_loss: 0.0164 - val_acc: 0.4901\n",
      "Epoch 409/420\n",
      "10976/10976 [==============================] - 6s 516us/step - loss: 0.0193 - acc: 0.4754 - val_loss: 0.0233 - val_acc: 0.4794\n",
      "Epoch 410/420\n",
      "10976/10976 [==============================] - 6s 540us/step - loss: 0.0195 - acc: 0.4768 - val_loss: 0.0158 - val_acc: 0.4885\n",
      "Epoch 411/420\n",
      "10976/10976 [==============================] - 6s 544us/step - loss: 0.0192 - acc: 0.4785 - val_loss: 0.0211 - val_acc: 0.4883\n",
      "Epoch 412/420\n",
      "10976/10976 [==============================] - 6s 545us/step - loss: 0.0194 - acc: 0.4742 - val_loss: 0.0148 - val_acc: 0.4932\n",
      "Epoch 413/420\n",
      "10976/10976 [==============================] - 6s 518us/step - loss: 0.0192 - acc: 0.4779 - val_loss: 0.0224 - val_acc: 0.4689\n",
      "Epoch 414/420\n",
      "10976/10976 [==============================] - 6s 544us/step - loss: 0.0192 - acc: 0.4738 - val_loss: 0.0151 - val_acc: 0.4900\n",
      "Epoch 415/420\n",
      "10976/10976 [==============================] - 6s 545us/step - loss: 0.0189 - acc: 0.4769 - val_loss: 0.0225 - val_acc: 0.4731\n",
      "Epoch 416/420\n",
      "10976/10976 [==============================] - 6s 551us/step - loss: 0.0193 - acc: 0.4742 - val_loss: 0.0167 - val_acc: 0.4835\n",
      "Epoch 417/420\n",
      "10976/10976 [==============================] - 6s 558us/step - loss: 0.0197 - acc: 0.4724 - val_loss: 0.0240 - val_acc: 0.4801\n",
      "Epoch 418/420\n",
      "10976/10976 [==============================] - 6s 543us/step - loss: 0.0192 - acc: 0.4746 - val_loss: 0.0150 - val_acc: 0.4908\n",
      "Epoch 419/420\n",
      "10976/10976 [==============================] - 6s 549us/step - loss: 0.0189 - acc: 0.4754 - val_loss: 0.0214 - val_acc: 0.4775\n",
      "Epoch 420/420\n",
      "10976/10976 [==============================] - 6s 541us/step - loss: 0.0193 - acc: 0.4784 - val_loss: 0.0169 - val_acc: 0.4785\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 401/420\n",
      "10976/10976 [==============================] - 6s 528us/step - loss: 0.0224 - acc: 0.4564 - val_loss: 0.0232 - val_acc: 0.4933\n",
      "Epoch 402/420\n",
      "10976/10976 [==============================] - 6s 538us/step - loss: 0.0224 - acc: 0.4661 - val_loss: 0.0215 - val_acc: 0.4688\n",
      "Epoch 403/420\n",
      "10976/10976 [==============================] - 6s 519us/step - loss: 0.0221 - acc: 0.4784 - val_loss: 0.0213 - val_acc: 0.5153\n",
      "Epoch 404/420\n",
      "10976/10976 [==============================] - 6s 583us/step - loss: 0.0221 - acc: 0.5065 - val_loss: 0.0217 - val_acc: 0.5370\n",
      "Epoch 405/420\n",
      "10976/10976 [==============================] - 6s 536us/step - loss: 0.0226 - acc: 0.5221 - val_loss: 0.0218 - val_acc: 0.4916\n",
      "Epoch 406/420\n",
      "10976/10976 [==============================] - 6s 531us/step - loss: 0.0223 - acc: 0.4655 - val_loss: 0.0221 - val_acc: 0.5185\n",
      "Epoch 407/420\n",
      "10976/10976 [==============================] - 6s 561us/step - loss: 0.0222 - acc: 0.4557 - val_loss: 0.0229 - val_acc: 0.4018\n",
      "Epoch 408/420\n",
      "10976/10976 [==============================] - 6s 531us/step - loss: 0.0223 - acc: 0.4580 - val_loss: 0.0228 - val_acc: 0.5014\n",
      "Epoch 409/420\n",
      "10976/10976 [==============================] - 6s 522us/step - loss: 0.0220 - acc: 0.4594 - val_loss: 0.0226 - val_acc: 0.4051\n",
      "Epoch 410/420\n",
      "10976/10976 [==============================] - 6s 542us/step - loss: 0.0223 - acc: 0.5206 - val_loss: 0.0224 - val_acc: 0.5469\n",
      "Epoch 411/420\n",
      "10976/10976 [==============================] - 6s 523us/step - loss: 0.0219 - acc: 0.5164 - val_loss: 0.0213 - val_acc: 0.4372\n",
      "Epoch 412/420\n",
      "10976/10976 [==============================] - 6s 553us/step - loss: 0.0224 - acc: 0.5130 - val_loss: 0.0213 - val_acc: 0.5635\n",
      "Epoch 413/420\n",
      "10976/10976 [==============================] - 6s 548us/step - loss: 0.0219 - acc: 0.5146 - val_loss: 0.0244 - val_acc: 0.4839\n",
      "Epoch 414/420\n",
      "10976/10976 [==============================] - 6s 567us/step - loss: 0.0226 - acc: 0.5212 - val_loss: 0.0215 - val_acc: 0.5316\n",
      "Epoch 415/420\n",
      "10976/10976 [==============================] - 6s 540us/step - loss: 0.0220 - acc: 0.4881 - val_loss: 0.0220 - val_acc: 0.4270\n",
      "Epoch 416/420\n",
      "10976/10976 [==============================] - 6s 542us/step - loss: 0.0222 - acc: 0.4518 - val_loss: 0.0229 - val_acc: 0.5241\n",
      "Epoch 417/420\n",
      "10976/10976 [==============================] - 6s 551us/step - loss: 0.0221 - acc: 0.4606 - val_loss: 0.0222 - val_acc: 0.4691\n",
      "Epoch 418/420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 6s 545us/step - loss: 0.0224 - acc: 0.4695 - val_loss: 0.0224 - val_acc: 0.4528\n",
      "Epoch 419/420\n",
      "10976/10976 [==============================] - 6s 557us/step - loss: 0.0221 - acc: 0.4619 - val_loss: 0.0221 - val_acc: 0.4065\n",
      "Epoch 420/420\n",
      "10976/10976 [==============================] - 6s 537us/step - loss: 0.0225 - acc: 0.4623 - val_loss: 0.0224 - val_acc: 0.4543\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 401/420\n",
      "10976/10976 [==============================] - 6s 543us/step - loss: 0.0178 - acc: 0.4161 - val_loss: 0.0128 - val_acc: 0.3008\n",
      "Epoch 402/420\n",
      "10976/10976 [==============================] - 6s 547us/step - loss: 0.0175 - acc: 0.4034 - val_loss: 0.0210 - val_acc: 0.4448\n",
      "Epoch 403/420\n",
      "10976/10976 [==============================] - 6s 556us/step - loss: 0.0175 - acc: 0.4377 - val_loss: 0.0127 - val_acc: 0.5167\n",
      "Epoch 404/420\n",
      "10976/10976 [==============================] - 6s 540us/step - loss: 0.0173 - acc: 0.4023 - val_loss: 0.0232 - val_acc: 0.2916\n",
      "Epoch 405/420\n",
      "10976/10976 [==============================] - 6s 509us/step - loss: 0.0176 - acc: 0.4095 - val_loss: 0.0111 - val_acc: 0.4487\n",
      "Epoch 406/420\n",
      "10976/10976 [==============================] - 5s 413us/step - loss: 0.0173 - acc: 0.4154 - val_loss: 0.0243 - val_acc: 0.3057\n",
      "Epoch 407/420\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0177 - acc: 0.4172 - val_loss: 0.0140 - val_acc: 0.5186\n",
      "Epoch 408/420\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0173 - acc: 0.4454 - val_loss: 0.0243 - val_acc: 0.3017\n",
      "Epoch 409/420\n",
      "10976/10976 [==============================] - 5s 413us/step - loss: 0.0176 - acc: 0.4242 - val_loss: 0.0106 - val_acc: 0.5190\n",
      "Epoch 410/420\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0174 - acc: 0.4193 - val_loss: 0.0235 - val_acc: 0.2961\n",
      "Epoch 411/420\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0174 - acc: 0.3953 - val_loss: 0.0122 - val_acc: 0.4308\n",
      "Epoch 412/420\n",
      "10976/10976 [==============================] - 5s 412us/step - loss: 0.0175 - acc: 0.4149 - val_loss: 0.0242 - val_acc: 0.3065\n",
      "Epoch 413/420\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0176 - acc: 0.4359 - val_loss: 0.0131 - val_acc: 0.2951\n",
      "Epoch 414/420\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0175 - acc: 0.4130 - val_loss: 0.0238 - val_acc: 0.3048\n",
      "Epoch 415/420\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0178 - acc: 0.4232 - val_loss: 0.0118 - val_acc: 0.4470\n",
      "Epoch 416/420\n",
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0172 - acc: 0.4123 - val_loss: 0.0246 - val_acc: 0.3048\n",
      "Epoch 417/420\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0177 - acc: 0.4157 - val_loss: 0.0117 - val_acc: 0.5194\n",
      "Epoch 418/420\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0172 - acc: 0.4010 - val_loss: 0.0233 - val_acc: 0.2868\n",
      "Epoch 419/420\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0176 - acc: 0.4065 - val_loss: 0.0117 - val_acc: 0.2965\n",
      "Epoch 420/420\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0174 - acc: 0.4102 - val_loss: 0.0242 - val_acc: 0.4472\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 401/420\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0147 - acc: 0.3587 - val_loss: 0.0135 - val_acc: 0.3095\n",
      "Epoch 402/420\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0147 - acc: 0.3605 - val_loss: 0.0145 - val_acc: 0.4184\n",
      "Epoch 403/420\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0147 - acc: 0.3589 - val_loss: 0.0143 - val_acc: 0.2803\n",
      "Epoch 404/420\n",
      "10976/10976 [==============================] - 5s 417us/step - loss: 0.0147 - acc: 0.3522 - val_loss: 0.0150 - val_acc: 0.4283\n",
      "Epoch 405/420\n",
      "10976/10976 [==============================] - 5s 417us/step - loss: 0.0148 - acc: 0.3606 - val_loss: 0.0145 - val_acc: 0.2819\n",
      "Epoch 406/420\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0146 - acc: 0.3566 - val_loss: 0.0142 - val_acc: 0.4285\n",
      "Epoch 407/420\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0146 - acc: 0.3575 - val_loss: 0.0148 - val_acc: 0.2830\n",
      "Epoch 408/420\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0147 - acc: 0.3552 - val_loss: 0.0152 - val_acc: 0.4344\n",
      "Epoch 409/420\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0148 - acc: 0.3537 - val_loss: 0.0143 - val_acc: 0.2757\n",
      "Epoch 410/420\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0147 - acc: 0.3504 - val_loss: 0.0143 - val_acc: 0.4300\n",
      "Epoch 411/420\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0147 - acc: 0.3531 - val_loss: 0.0142 - val_acc: 0.2982\n",
      "Epoch 412/420\n",
      "10976/10976 [==============================] - 5s 414us/step - loss: 0.0148 - acc: 0.3455 - val_loss: 0.0145 - val_acc: 0.4441\n",
      "Epoch 413/420\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0147 - acc: 0.3554 - val_loss: 0.0147 - val_acc: 0.2962\n",
      "Epoch 414/420\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0146 - acc: 0.3509 - val_loss: 0.0144 - val_acc: 0.4264\n",
      "Epoch 415/420\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0146 - acc: 0.3588 - val_loss: 0.0151 - val_acc: 0.2655\n",
      "Epoch 416/420\n",
      "10976/10976 [==============================] - 5s 413us/step - loss: 0.0145 - acc: 0.3584 - val_loss: 0.0150 - val_acc: 0.4382\n",
      "Epoch 417/420\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0146 - acc: 0.3625 - val_loss: 0.0144 - val_acc: 0.2926\n",
      "Epoch 418/420\n",
      "10976/10976 [==============================] - 5s 414us/step - loss: 0.0146 - acc: 0.3549 - val_loss: 0.0146 - val_acc: 0.4450\n",
      "Epoch 419/420\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0147 - acc: 0.3633 - val_loss: 0.0146 - val_acc: 0.2963\n",
      "Epoch 420/420\n",
      "10976/10976 [==============================] - 5s 417us/step - loss: 0.0147 - acc: 0.3596 - val_loss: 0.0136 - val_acc: 0.4325\n",
      "start training round 21\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 421/440\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0189 - acc: 0.4762 - val_loss: 0.0222 - val_acc: 0.4845\n",
      "Epoch 422/440\n",
      "10976/10976 [==============================] - 5s 414us/step - loss: 0.0192 - acc: 0.4748 - val_loss: 0.0158 - val_acc: 0.4923\n",
      "Epoch 423/440\n",
      "10976/10976 [==============================] - 5s 415us/step - loss: 0.0193 - acc: 0.4805 - val_loss: 0.0230 - val_acc: 0.4909\n",
      "Epoch 424/440\n",
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0192 - acc: 0.4756 - val_loss: 0.0167 - val_acc: 0.4904\n",
      "Epoch 425/440\n",
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0190 - acc: 0.4774 - val_loss: 0.0228 - val_acc: 0.4761\n",
      "Epoch 426/440\n",
      "10976/10976 [==============================] - 5s 410us/step - loss: 0.0194 - acc: 0.4797 - val_loss: 0.0192 - val_acc: 0.4668\n",
      "Epoch 427/440\n",
      "10976/10976 [==============================] - 5s 415us/step - loss: 0.0195 - acc: 0.4740 - val_loss: 0.0227 - val_acc: 0.4746\n",
      "Epoch 428/440\n",
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0188 - acc: 0.4752 - val_loss: 0.0158 - val_acc: 0.4888\n",
      "Epoch 429/440\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0191 - acc: 0.4770 - val_loss: 0.0237 - val_acc: 0.4784\n",
      "Epoch 430/440\n",
      "10976/10976 [==============================] - 5s 415us/step - loss: 0.0196 - acc: 0.4767 - val_loss: 0.0165 - val_acc: 0.4917\n",
      "Epoch 431/440\n",
      "10976/10976 [==============================] - 5s 415us/step - loss: 0.0193 - acc: 0.4767 - val_loss: 0.0219 - val_acc: 0.4720\n",
      "Epoch 432/440\n",
      "10976/10976 [==============================] - 5s 415us/step - loss: 0.0192 - acc: 0.4802 - val_loss: 0.0147 - val_acc: 0.4922\n",
      "Epoch 433/440\n",
      "10976/10976 [==============================] - 5s 415us/step - loss: 0.0189 - acc: 0.4743 - val_loss: 0.0224 - val_acc: 0.4747\n",
      "Epoch 434/440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0194 - acc: 0.4798 - val_loss: 0.0159 - val_acc: 0.4850\n",
      "Epoch 435/440\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0189 - acc: 0.4715 - val_loss: 0.0227 - val_acc: 0.4744\n",
      "Epoch 436/440\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0195 - acc: 0.4758 - val_loss: 0.0159 - val_acc: 0.4844\n",
      "Epoch 437/440\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0186 - acc: 0.4783 - val_loss: 0.0230 - val_acc: 0.4890\n",
      "Epoch 438/440\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0189 - acc: 0.4810 - val_loss: 0.0167 - val_acc: 0.4913\n",
      "Epoch 439/440\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0189 - acc: 0.4747 - val_loss: 0.0231 - val_acc: 0.4788\n",
      "Epoch 440/440\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0192 - acc: 0.4806 - val_loss: 0.0159 - val_acc: 0.4930\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 421/440\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0222 - acc: 0.4748 - val_loss: 0.0229 - val_acc: 0.4932\n",
      "Epoch 422/440\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0228 - acc: 0.5281 - val_loss: 0.0206 - val_acc: 0.5474\n",
      "Epoch 423/440\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0221 - acc: 0.5243 - val_loss: 0.0210 - val_acc: 0.4684\n",
      "Epoch 424/440\n",
      "10976/10976 [==============================] - 5s 415us/step - loss: 0.0224 - acc: 0.4706 - val_loss: 0.0224 - val_acc: 0.4940\n",
      "Epoch 425/440\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0221 - acc: 0.4633 - val_loss: 0.0234 - val_acc: 0.5420\n",
      "Epoch 426/440\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0229 - acc: 0.5255 - val_loss: 0.0244 - val_acc: 0.5047\n",
      "Epoch 427/440\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0224 - acc: 0.5216 - val_loss: 0.0209 - val_acc: 0.5320\n",
      "Epoch 428/440\n",
      "10976/10976 [==============================] - 5s 413us/step - loss: 0.0224 - acc: 0.5273 - val_loss: 0.0222 - val_acc: 0.4950\n",
      "Epoch 429/440\n",
      "10976/10976 [==============================] - 5s 414us/step - loss: 0.0221 - acc: 0.5246 - val_loss: 0.0236 - val_acc: 0.5682\n",
      "Epoch 430/440\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0227 - acc: 0.5229 - val_loss: 0.0222 - val_acc: 0.5002\n",
      "Epoch 431/440\n",
      "10976/10976 [==============================] - 5s 415us/step - loss: 0.0222 - acc: 0.5250 - val_loss: 0.0206 - val_acc: 0.5680\n",
      "Epoch 432/440\n",
      "10976/10976 [==============================] - 5s 417us/step - loss: 0.0224 - acc: 0.5238 - val_loss: 0.0226 - val_acc: 0.5105\n",
      "Epoch 433/440\n",
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0224 - acc: 0.5079 - val_loss: 0.0224 - val_acc: 0.5649\n",
      "Epoch 434/440\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0226 - acc: 0.5228 - val_loss: 0.0227 - val_acc: 0.5072\n",
      "Epoch 435/440\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0223 - acc: 0.5225 - val_loss: 0.0214 - val_acc: 0.5616\n",
      "Epoch 436/440\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0217 - acc: 0.5209 - val_loss: 0.0222 - val_acc: 0.5052\n",
      "Epoch 437/440\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0221 - acc: 0.5196 - val_loss: 0.0231 - val_acc: 0.4204\n",
      "Epoch 438/440\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0220 - acc: 0.4716 - val_loss: 0.0219 - val_acc: 0.5164\n",
      "Epoch 439/440\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0223 - acc: 0.4620 - val_loss: 0.0223 - val_acc: 0.4753\n",
      "Epoch 440/440\n",
      "10976/10976 [==============================] - 5s 413us/step - loss: 0.0223 - acc: 0.4986 - val_loss: 0.0219 - val_acc: 0.5654\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 421/440\n",
      "10976/10976 [==============================] - 5s 449us/step - loss: 0.0180 - acc: 0.4319 - val_loss: 0.0133 - val_acc: 0.2844\n",
      "Epoch 422/440\n",
      "10976/10976 [==============================] - 5s 456us/step - loss: 0.0174 - acc: 0.3773 - val_loss: 0.0234 - val_acc: 0.2886\n",
      "Epoch 423/440\n",
      "10976/10976 [==============================] - 5s 456us/step - loss: 0.0175 - acc: 0.4329 - val_loss: 0.0116 - val_acc: 0.4503\n",
      "Epoch 424/440\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0173 - acc: 0.4305 - val_loss: 0.0224 - val_acc: 0.2918\n",
      "Epoch 425/440\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0174 - acc: 0.4080 - val_loss: 0.0120 - val_acc: 0.4309\n",
      "Epoch 426/440\n",
      "10976/10976 [==============================] - 5s 413us/step - loss: 0.0176 - acc: 0.3667 - val_loss: 0.0220 - val_acc: 0.3016\n",
      "Epoch 427/440\n",
      "10976/10976 [==============================] - 5s 415us/step - loss: 0.0176 - acc: 0.3811 - val_loss: 0.0122 - val_acc: 0.3020\n",
      "Epoch 428/440\n",
      "10976/10976 [==============================] - 5s 412us/step - loss: 0.0172 - acc: 0.3765 - val_loss: 0.0206 - val_acc: 0.4459\n",
      "Epoch 429/440\n",
      "10976/10976 [==============================] - 5s 417us/step - loss: 0.0174 - acc: 0.3822 - val_loss: 0.0140 - val_acc: 0.4391\n",
      "Epoch 430/440\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0174 - acc: 0.4070 - val_loss: 0.0247 - val_acc: 0.4462\n",
      "Epoch 431/440\n",
      "10976/10976 [==============================] - 5s 415us/step - loss: 0.0176 - acc: 0.3861 - val_loss: 0.0122 - val_acc: 0.4459\n",
      "Epoch 432/440\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0173 - acc: 0.3801 - val_loss: 0.0225 - val_acc: 0.2932\n",
      "Epoch 433/440\n",
      "10976/10976 [==============================] - 5s 411us/step - loss: 0.0176 - acc: 0.4306 - val_loss: 0.0124 - val_acc: 0.3019\n",
      "Epoch 434/440\n",
      "10976/10976 [==============================] - 5s 410us/step - loss: 0.0175 - acc: 0.3930 - val_loss: 0.0236 - val_acc: 0.3095\n",
      "Epoch 435/440\n",
      "10976/10976 [==============================] - 5s 417us/step - loss: 0.0176 - acc: 0.4260 - val_loss: 0.0127 - val_acc: 0.5202\n",
      "Epoch 436/440\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0173 - acc: 0.3652 - val_loss: 0.0243 - val_acc: 0.4434\n",
      "Epoch 437/440\n",
      "10976/10976 [==============================] - 5s 414us/step - loss: 0.0175 - acc: 0.4245 - val_loss: 0.0124 - val_acc: 0.2975\n",
      "Epoch 438/440\n",
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0174 - acc: 0.3730 - val_loss: 0.0234 - val_acc: 0.3097\n",
      "Epoch 439/440\n",
      "10976/10976 [==============================] - 5s 414us/step - loss: 0.0172 - acc: 0.3961 - val_loss: 0.0108 - val_acc: 0.5226\n",
      "Epoch 440/440\n",
      "10976/10976 [==============================] - 4s 410us/step - loss: 0.0172 - acc: 0.3774 - val_loss: 0.0243 - val_acc: 0.4417\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 421/440\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0147 - acc: 0.3570 - val_loss: 0.0138 - val_acc: 0.3048\n",
      "Epoch 422/440\n",
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0146 - acc: 0.3577 - val_loss: 0.0144 - val_acc: 0.4392\n",
      "Epoch 423/440\n",
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0146 - acc: 0.3614 - val_loss: 0.0142 - val_acc: 0.2822\n",
      "Epoch 424/440\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0146 - acc: 0.3591 - val_loss: 0.0142 - val_acc: 0.4327\n",
      "Epoch 425/440\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0146 - acc: 0.3549 - val_loss: 0.0147 - val_acc: 0.2792\n",
      "Epoch 426/440\n",
      "10976/10976 [==============================] - 5s 417us/step - loss: 0.0146 - acc: 0.3538 - val_loss: 0.0149 - val_acc: 0.4346\n",
      "Epoch 427/440\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0146 - acc: 0.3599 - val_loss: 0.0142 - val_acc: 0.3000\n",
      "Epoch 428/440\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0147 - acc: 0.3549 - val_loss: 0.0143 - val_acc: 0.4380\n",
      "Epoch 429/440\n",
      "10976/10976 [==============================] - 5s 417us/step - loss: 0.0147 - acc: 0.3599 - val_loss: 0.0144 - val_acc: 0.3030\n",
      "Epoch 430/440\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0146 - acc: 0.3510 - val_loss: 0.0143 - val_acc: 0.4368\n",
      "Epoch 431/440\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0146 - acc: 0.3619 - val_loss: 0.0145 - val_acc: 0.2954\n",
      "Epoch 432/440\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0145 - acc: 0.3584 - val_loss: 0.0142 - val_acc: 0.4317\n",
      "Epoch 433/440\n",
      "10976/10976 [==============================] - 5s 415us/step - loss: 0.0147 - acc: 0.3606 - val_loss: 0.0135 - val_acc: 0.3078\n",
      "Epoch 434/440\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0146 - acc: 0.3559 - val_loss: 0.0143 - val_acc: 0.4318\n",
      "Epoch 435/440\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0146 - acc: 0.3512 - val_loss: 0.0149 - val_acc: 0.2885\n",
      "Epoch 436/440\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0146 - acc: 0.3547 - val_loss: 0.0129 - val_acc: 0.4287\n",
      "Epoch 437/440\n",
      "10976/10976 [==============================] - 5s 415us/step - loss: 0.0146 - acc: 0.3524 - val_loss: 0.0147 - val_acc: 0.2890\n",
      "Epoch 438/440\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0145 - acc: 0.3532 - val_loss: 0.0141 - val_acc: 0.4440\n",
      "Epoch 439/440\n",
      "10976/10976 [==============================] - 5s 414us/step - loss: 0.0145 - acc: 0.3516 - val_loss: 0.0132 - val_acc: 0.2165\n",
      "Epoch 440/440\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0146 - acc: 0.3534 - val_loss: 0.0139 - val_acc: 0.4272\n",
      "start training round 22\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 441/460\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0189 - acc: 0.4793 - val_loss: 0.0216 - val_acc: 0.4687\n",
      "Epoch 442/460\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0189 - acc: 0.4634 - val_loss: 0.0184 - val_acc: 0.4880\n",
      "Epoch 443/460\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0194 - acc: 0.4365 - val_loss: 0.0204 - val_acc: 0.3862\n",
      "Epoch 444/460\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0192 - acc: 0.4371 - val_loss: 0.0165 - val_acc: 0.4880\n",
      "Epoch 445/460\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0188 - acc: 0.4346 - val_loss: 0.0199 - val_acc: 0.3892\n",
      "Epoch 446/460\n",
      "10976/10976 [==============================] - 5s 413us/step - loss: 0.0194 - acc: 0.4349 - val_loss: 0.0170 - val_acc: 0.4889\n",
      "Epoch 447/460\n",
      "10976/10976 [==============================] - 5s 414us/step - loss: 0.0189 - acc: 0.4349 - val_loss: 0.0204 - val_acc: 0.3831\n",
      "Epoch 448/460\n",
      "10976/10976 [==============================] - 5s 413us/step - loss: 0.0192 - acc: 0.4326 - val_loss: 0.0177 - val_acc: 0.4881\n",
      "Epoch 449/460\n",
      "10976/10976 [==============================] - 5s 417us/step - loss: 0.0188 - acc: 0.4384 - val_loss: 0.0200 - val_acc: 0.3865\n",
      "Epoch 450/460\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0191 - acc: 0.4344 - val_loss: 0.0186 - val_acc: 0.4885\n",
      "Epoch 451/460\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0189 - acc: 0.4384 - val_loss: 0.0207 - val_acc: 0.3901\n",
      "Epoch 452/460\n",
      "10976/10976 [==============================] - 5s 417us/step - loss: 0.0191 - acc: 0.4358 - val_loss: 0.0171 - val_acc: 0.4889\n",
      "Epoch 453/460\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0190 - acc: 0.4363 - val_loss: 0.0207 - val_acc: 0.3891\n",
      "Epoch 454/460\n",
      "10976/10976 [==============================] - 5s 417us/step - loss: 0.0194 - acc: 0.4405 - val_loss: 0.0168 - val_acc: 0.4842\n",
      "Epoch 455/460\n",
      "10976/10976 [==============================] - 5s 415us/step - loss: 0.0190 - acc: 0.4808 - val_loss: 0.0233 - val_acc: 0.4784\n",
      "Epoch 456/460\n",
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0187 - acc: 0.4766 - val_loss: 0.0161 - val_acc: 0.4923\n",
      "Epoch 457/460\n",
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0192 - acc: 0.4808 - val_loss: 0.0231 - val_acc: 0.4784\n",
      "Epoch 458/460\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0190 - acc: 0.4796 - val_loss: 0.0162 - val_acc: 0.4928\n",
      "Epoch 459/460\n",
      "10976/10976 [==============================] - 5s 415us/step - loss: 0.0189 - acc: 0.4531 - val_loss: 0.0204 - val_acc: 0.3851\n",
      "Epoch 460/460\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0192 - acc: 0.4340 - val_loss: 0.0170 - val_acc: 0.4885\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 441/460\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0222 - acc: 0.5242 - val_loss: 0.0224 - val_acc: 0.5295\n",
      "Epoch 442/460\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0221 - acc: 0.5259 - val_loss: 0.0236 - val_acc: 0.5594\n",
      "Epoch 443/460\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0218 - acc: 0.5218 - val_loss: 0.0204 - val_acc: 0.4627\n",
      "Epoch 444/460\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0221 - acc: 0.5067 - val_loss: 0.0223 - val_acc: 0.5230\n",
      "Epoch 445/460\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0223 - acc: 0.5253 - val_loss: 0.0230 - val_acc: 0.5431\n",
      "Epoch 446/460\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0222 - acc: 0.5250 - val_loss: 0.0245 - val_acc: 0.4992\n",
      "Epoch 447/460\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0218 - acc: 0.5142 - val_loss: 0.0209 - val_acc: 0.5651\n",
      "Epoch 448/460\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0223 - acc: 0.5244 - val_loss: 0.0240 - val_acc: 0.5061\n",
      "Epoch 449/460\n",
      "10976/10976 [==============================] - 5s 415us/step - loss: 0.0218 - acc: 0.4965 - val_loss: 0.0226 - val_acc: 0.5582\n",
      "Epoch 450/460\n",
      "10976/10976 [==============================] - 5s 414us/step - loss: 0.0218 - acc: 0.4934 - val_loss: 0.0215 - val_acc: 0.4277\n",
      "Epoch 451/460\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0220 - acc: 0.4827 - val_loss: 0.0210 - val_acc: 0.5607\n",
      "Epoch 452/460\n",
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0222 - acc: 0.5261 - val_loss: 0.0237 - val_acc: 0.4892\n",
      "Epoch 453/460\n",
      "10976/10976 [==============================] - 4s 409us/step - loss: 0.0223 - acc: 0.5243 - val_loss: 0.0210 - val_acc: 0.5525\n",
      "Epoch 454/460\n",
      "10976/10976 [==============================] - 5s 417us/step - loss: 0.0219 - acc: 0.5218 - val_loss: 0.0223 - val_acc: 0.4570\n",
      "Epoch 455/460\n",
      "10976/10976 [==============================] - 5s 412us/step - loss: 0.0224 - acc: 0.5064 - val_loss: 0.0216 - val_acc: 0.5639\n",
      "Epoch 456/460\n",
      "10976/10976 [==============================] - 5s 412us/step - loss: 0.0224 - acc: 0.5256 - val_loss: 0.0238 - val_acc: 0.5088\n",
      "Epoch 457/460\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0219 - acc: 0.5142 - val_loss: 0.0218 - val_acc: 0.5295\n",
      "Epoch 458/460\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0222 - acc: 0.4655 - val_loss: 0.0218 - val_acc: 0.4080\n",
      "Epoch 459/460\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0218 - acc: 0.4632 - val_loss: 0.0222 - val_acc: 0.5208\n",
      "Epoch 460/460\n",
      "10976/10976 [==============================] - 5s 417us/step - loss: 0.0220 - acc: 0.4665 - val_loss: 0.0225 - val_acc: 0.3991\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 441/460\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0173 - acc: 0.4290 - val_loss: 0.0120 - val_acc: 0.5229\n",
      "Epoch 442/460\n",
      "10976/10976 [==============================] - 5s 450us/step - loss: 0.0173 - acc: 0.4105 - val_loss: 0.0226 - val_acc: 0.5132\n",
      "Epoch 443/460\n",
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0175 - acc: 0.3993 - val_loss: 0.0132 - val_acc: 0.4432\n",
      "Epoch 444/460\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0174 - acc: 0.4075 - val_loss: 0.0250 - val_acc: 0.3029\n",
      "Epoch 445/460\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0175 - acc: 0.3917 - val_loss: 0.0109 - val_acc: 0.5215\n",
      "Epoch 446/460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 5s 415us/step - loss: 0.0173 - acc: 0.3837 - val_loss: 0.0241 - val_acc: 0.3095\n",
      "Epoch 447/460\n",
      "10976/10976 [==============================] - 5s 412us/step - loss: 0.0174 - acc: 0.3978 - val_loss: 0.0118 - val_acc: 0.3059\n",
      "Epoch 448/460\n",
      "10976/10976 [==============================] - 5s 417us/step - loss: 0.0174 - acc: 0.4444 - val_loss: 0.0238 - val_acc: 0.4356\n",
      "Epoch 449/460\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0176 - acc: 0.4037 - val_loss: 0.0114 - val_acc: 0.4508\n",
      "Epoch 450/460\n",
      "10976/10976 [==============================] - 5s 413us/step - loss: 0.0172 - acc: 0.4169 - val_loss: 0.0235 - val_acc: 0.4421\n",
      "Epoch 451/460\n",
      "10976/10976 [==============================] - 5s 417us/step - loss: 0.0176 - acc: 0.4060 - val_loss: 0.0117 - val_acc: 0.5098\n",
      "Epoch 452/460\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0172 - acc: 0.4031 - val_loss: 0.0238 - val_acc: 0.3082\n",
      "Epoch 453/460\n",
      "10976/10976 [==============================] - 5s 412us/step - loss: 0.0176 - acc: 0.4280 - val_loss: 0.0119 - val_acc: 0.4477\n",
      "Epoch 454/460\n",
      "10976/10976 [==============================] - 5s 412us/step - loss: 0.0173 - acc: 0.4250 - val_loss: 0.0239 - val_acc: 0.4494\n",
      "Epoch 455/460\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0176 - acc: 0.4402 - val_loss: 0.0119 - val_acc: 0.2952\n",
      "Epoch 456/460\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0169 - acc: 0.3850 - val_loss: 0.0204 - val_acc: 0.2677\n",
      "Epoch 457/460\n",
      "10976/10976 [==============================] - 5s 415us/step - loss: 0.0174 - acc: 0.4186 - val_loss: 0.0156 - val_acc: 0.4965\n",
      "Epoch 458/460\n",
      "10976/10976 [==============================] - 4s 408us/step - loss: 0.0174 - acc: 0.3977 - val_loss: 0.0201 - val_acc: 0.2637\n",
      "Epoch 459/460\n",
      "10976/10976 [==============================] - 5s 414us/step - loss: 0.0174 - acc: 0.4066 - val_loss: 0.0153 - val_acc: 0.4965\n",
      "Epoch 460/460\n",
      "10976/10976 [==============================] - 4s 407us/step - loss: 0.0173 - acc: 0.4069 - val_loss: 0.0203 - val_acc: 0.2644\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 441/460\n",
      "10976/10976 [==============================] - 5s 415us/step - loss: 0.0147 - acc: 0.3640 - val_loss: 0.0150 - val_acc: 0.2819\n",
      "Epoch 442/460\n",
      "10976/10976 [==============================] - 5s 413us/step - loss: 0.0147 - acc: 0.3510 - val_loss: 0.0148 - val_acc: 0.4398\n",
      "Epoch 443/460\n",
      "10976/10976 [==============================] - 5s 417us/step - loss: 0.0146 - acc: 0.3596 - val_loss: 0.0142 - val_acc: 0.3037\n",
      "Epoch 444/460\n",
      "10976/10976 [==============================] - 5s 417us/step - loss: 0.0146 - acc: 0.3614 - val_loss: 0.0147 - val_acc: 0.4367\n",
      "Epoch 445/460\n",
      "10976/10976 [==============================] - 5s 414us/step - loss: 0.0146 - acc: 0.3591 - val_loss: 0.0144 - val_acc: 0.3000\n",
      "Epoch 446/460\n",
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0146 - acc: 0.3542 - val_loss: 0.0142 - val_acc: 0.4387\n",
      "Epoch 447/460\n",
      "10976/10976 [==============================] - 5s 413us/step - loss: 0.0145 - acc: 0.3589 - val_loss: 0.0140 - val_acc: 0.3018\n",
      "Epoch 448/460\n",
      "10976/10976 [==============================] - 5s 417us/step - loss: 0.0146 - acc: 0.3605 - val_loss: 0.0146 - val_acc: 0.4322\n",
      "Epoch 449/460\n",
      "10976/10976 [==============================] - 4s 409us/step - loss: 0.0147 - acc: 0.3622 - val_loss: 0.0143 - val_acc: 0.2663\n",
      "Epoch 450/460\n",
      "10976/10976 [==============================] - 5s 415us/step - loss: 0.0145 - acc: 0.3511 - val_loss: 0.0142 - val_acc: 0.4694\n",
      "Epoch 451/460\n",
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0145 - acc: 0.3603 - val_loss: 0.0144 - val_acc: 0.2819\n",
      "Epoch 452/460\n",
      "10976/10976 [==============================] - 5s 414us/step - loss: 0.0145 - acc: 0.3571 - val_loss: 0.0142 - val_acc: 0.4305\n",
      "Epoch 453/460\n",
      "10976/10976 [==============================] - 5s 414us/step - loss: 0.0145 - acc: 0.3622 - val_loss: 0.0148 - val_acc: 0.2737\n",
      "Epoch 454/460\n",
      "10976/10976 [==============================] - 4s 407us/step - loss: 0.0145 - acc: 0.3563 - val_loss: 0.0132 - val_acc: 0.4299\n",
      "Epoch 455/460\n",
      "10976/10976 [==============================] - 5s 413us/step - loss: 0.0145 - acc: 0.3604 - val_loss: 0.0145 - val_acc: 0.2922\n",
      "Epoch 456/460\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0145 - acc: 0.3611 - val_loss: 0.0144 - val_acc: 0.4370\n",
      "Epoch 457/460\n",
      "10976/10976 [==============================] - 5s 415us/step - loss: 0.0146 - acc: 0.3581 - val_loss: 0.0147 - val_acc: 0.2921\n",
      "Epoch 458/460\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0146 - acc: 0.3627 - val_loss: 0.0146 - val_acc: 0.4459\n",
      "Epoch 459/460\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0145 - acc: 0.3656 - val_loss: 0.0141 - val_acc: 0.2897\n",
      "Epoch 460/460\n",
      "10976/10976 [==============================] - 5s 459us/step - loss: 0.0145 - acc: 0.3531 - val_loss: 0.0137 - val_acc: 0.4382\n",
      "start training round 23\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 461/480\n",
      "10976/10976 [==============================] - 5s 440us/step - loss: 0.0188 - acc: 0.4388 - val_loss: 0.0203 - val_acc: 0.3978\n",
      "Epoch 462/480\n",
      "10976/10976 [==============================] - 5s 443us/step - loss: 0.0192 - acc: 0.4315 - val_loss: 0.0173 - val_acc: 0.4889\n",
      "Epoch 463/480\n",
      "10976/10976 [==============================] - 5s 412us/step - loss: 0.0191 - acc: 0.4362 - val_loss: 0.0200 - val_acc: 0.4010\n",
      "Epoch 464/480\n",
      "10976/10976 [==============================] - 5s 414us/step - loss: 0.0191 - acc: 0.4348 - val_loss: 0.0163 - val_acc: 0.4888\n",
      "Epoch 465/480\n",
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0188 - acc: 0.4381 - val_loss: 0.0197 - val_acc: 0.3900\n",
      "Epoch 466/480\n",
      "10976/10976 [==============================] - 4s 408us/step - loss: 0.0189 - acc: 0.4346 - val_loss: 0.0174 - val_acc: 0.4884\n",
      "Epoch 467/480\n",
      "10976/10976 [==============================] - 4s 409us/step - loss: 0.0188 - acc: 0.4357 - val_loss: 0.0204 - val_acc: 0.3882\n",
      "Epoch 468/480\n",
      "10976/10976 [==============================] - 5s 411us/step - loss: 0.0191 - acc: 0.4341 - val_loss: 0.0161 - val_acc: 0.4909\n",
      "Epoch 469/480\n",
      "10976/10976 [==============================] - 4s 408us/step - loss: 0.0187 - acc: 0.4374 - val_loss: 0.0202 - val_acc: 0.3837\n",
      "Epoch 470/480\n",
      "10976/10976 [==============================] - 5s 411us/step - loss: 0.0188 - acc: 0.4350 - val_loss: 0.0173 - val_acc: 0.4879\n",
      "Epoch 471/480\n",
      "10976/10976 [==============================] - 4s 407us/step - loss: 0.0190 - acc: 0.4374 - val_loss: 0.0213 - val_acc: 0.3827\n",
      "Epoch 472/480\n",
      "10976/10976 [==============================] - 5s 412us/step - loss: 0.0191 - acc: 0.4330 - val_loss: 0.0187 - val_acc: 0.4888\n",
      "Epoch 473/480\n",
      "10976/10976 [==============================] - 4s 408us/step - loss: 0.0191 - acc: 0.4400 - val_loss: 0.0208 - val_acc: 0.3860\n",
      "Epoch 474/480\n",
      "10976/10976 [==============================] - 4s 408us/step - loss: 0.0192 - acc: 0.4351 - val_loss: 0.0171 - val_acc: 0.4892\n",
      "Epoch 475/480\n",
      "10976/10976 [==============================] - 5s 412us/step - loss: 0.0191 - acc: 0.4377 - val_loss: 0.0210 - val_acc: 0.3793\n",
      "Epoch 476/480\n",
      "10976/10976 [==============================] - 5s 414us/step - loss: 0.0189 - acc: 0.4361 - val_loss: 0.0160 - val_acc: 0.4904\n",
      "Epoch 477/480\n",
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0189 - acc: 0.4363 - val_loss: 0.0203 - val_acc: 0.3874\n",
      "Epoch 478/480\n",
      "10976/10976 [==============================] - 5s 411us/step - loss: 0.0191 - acc: 0.4382 - val_loss: 0.0173 - val_acc: 0.4886\n",
      "Epoch 479/480\n",
      "10976/10976 [==============================] - 4s 405us/step - loss: 0.0189 - acc: 0.4397 - val_loss: 0.0208 - val_acc: 0.3884\n",
      "Epoch 480/480\n",
      "10976/10976 [==============================] - 4s 405us/step - loss: 0.0192 - acc: 0.4410 - val_loss: 0.0179 - val_acc: 0.4897\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 461/480\n",
      "10976/10976 [==============================] - 5s 412us/step - loss: 0.0220 - acc: 0.4614 - val_loss: 0.0220 - val_acc: 0.4633\n",
      "Epoch 462/480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 5s 414us/step - loss: 0.0221 - acc: 0.4705 - val_loss: 0.0227 - val_acc: 0.4550\n",
      "Epoch 463/480\n",
      "10976/10976 [==============================] - 5s 415us/step - loss: 0.0220 - acc: 0.4688 - val_loss: 0.0228 - val_acc: 0.4531\n",
      "Epoch 464/480\n",
      "10976/10976 [==============================] - 4s 408us/step - loss: 0.0221 - acc: 0.4714 - val_loss: 0.0219 - val_acc: 0.5032\n",
      "Epoch 465/480\n",
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0219 - acc: 0.4717 - val_loss: 0.0220 - val_acc: 0.4642\n",
      "Epoch 466/480\n",
      "10976/10976 [==============================] - 5s 411us/step - loss: 0.0218 - acc: 0.4685 - val_loss: 0.0224 - val_acc: 0.4797\n",
      "Epoch 467/480\n",
      "10976/10976 [==============================] - 5s 441us/step - loss: 0.0217 - acc: 0.5072 - val_loss: 0.0212 - val_acc: 0.5423\n",
      "Epoch 468/480\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0223 - acc: 0.5283 - val_loss: 0.0221 - val_acc: 0.5162\n",
      "Epoch 469/480\n",
      "10976/10976 [==============================] - 5s 413us/step - loss: 0.0221 - acc: 0.5263 - val_loss: 0.0222 - val_acc: 0.5600\n",
      "Epoch 470/480\n",
      "10976/10976 [==============================] - 5s 413us/step - loss: 0.0224 - acc: 0.5235 - val_loss: 0.0219 - val_acc: 0.4841\n",
      "Epoch 471/480\n",
      "10976/10976 [==============================] - 5s 413us/step - loss: 0.0225 - acc: 0.5259 - val_loss: 0.0229 - val_acc: 0.5554\n",
      "Epoch 472/480\n",
      "10976/10976 [==============================] - 5s 412us/step - loss: 0.0223 - acc: 0.5280 - val_loss: 0.0246 - val_acc: 0.5128\n",
      "Epoch 473/480\n",
      "10976/10976 [==============================] - 5s 412us/step - loss: 0.0218 - acc: 0.5249 - val_loss: 0.0239 - val_acc: 0.5548\n",
      "Epoch 474/480\n",
      "10976/10976 [==============================] - 5s 412us/step - loss: 0.0220 - acc: 0.5273 - val_loss: 0.0234 - val_acc: 0.5193\n",
      "Epoch 475/480\n",
      "10976/10976 [==============================] - 5s 415us/step - loss: 0.0226 - acc: 0.5247 - val_loss: 0.0215 - val_acc: 0.5716\n",
      "Epoch 476/480\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0219 - acc: 0.5135 - val_loss: 0.0229 - val_acc: 0.4932\n",
      "Epoch 477/480\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0217 - acc: 0.5241 - val_loss: 0.0205 - val_acc: 0.5433\n",
      "Epoch 478/480\n",
      "10976/10976 [==============================] - 5s 414us/step - loss: 0.0221 - acc: 0.5237 - val_loss: 0.0229 - val_acc: 0.5054\n",
      "Epoch 479/480\n",
      "10976/10976 [==============================] - 5s 415us/step - loss: 0.0220 - acc: 0.5247 - val_loss: 0.0215 - val_acc: 0.5413\n",
      "Epoch 480/480\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0222 - acc: 0.5273 - val_loss: 0.0230 - val_acc: 0.5061\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 461/480\n",
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0173 - acc: 0.3986 - val_loss: 0.0152 - val_acc: 0.4980\n",
      "Epoch 462/480\n",
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0173 - acc: 0.4293 - val_loss: 0.0198 - val_acc: 0.3986\n",
      "Epoch 463/480\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0174 - acc: 0.3879 - val_loss: 0.0148 - val_acc: 0.4973\n",
      "Epoch 464/480\n",
      "10976/10976 [==============================] - 5s 414us/step - loss: 0.0171 - acc: 0.4129 - val_loss: 0.0193 - val_acc: 0.4013\n",
      "Epoch 465/480\n",
      "10976/10976 [==============================] - 5s 414us/step - loss: 0.0175 - acc: 0.4154 - val_loss: 0.0155 - val_acc: 0.4963\n",
      "Epoch 466/480\n",
      "10976/10976 [==============================] - 5s 412us/step - loss: 0.0173 - acc: 0.4022 - val_loss: 0.0191 - val_acc: 0.2697\n",
      "Epoch 467/480\n",
      "10976/10976 [==============================] - 4s 410us/step - loss: 0.0175 - acc: 0.4142 - val_loss: 0.0157 - val_acc: 0.4957\n",
      "Epoch 468/480\n",
      "10976/10976 [==============================] - 5s 413us/step - loss: 0.0173 - acc: 0.4060 - val_loss: 0.0187 - val_acc: 0.2592\n",
      "Epoch 469/480\n",
      "10976/10976 [==============================] - 5s 412us/step - loss: 0.0175 - acc: 0.4071 - val_loss: 0.0143 - val_acc: 0.4976\n",
      "Epoch 470/480\n",
      "10976/10976 [==============================] - 5s 412us/step - loss: 0.0171 - acc: 0.3922 - val_loss: 0.0205 - val_acc: 0.2643\n",
      "Epoch 471/480\n",
      "10976/10976 [==============================] - 4s 407us/step - loss: 0.0175 - acc: 0.3940 - val_loss: 0.0151 - val_acc: 0.4969\n",
      "Epoch 472/480\n",
      "10976/10976 [==============================] - 5s 438us/step - loss: 0.0174 - acc: 0.4151 - val_loss: 0.0191 - val_acc: 0.4012\n",
      "Epoch 473/480\n",
      "10976/10976 [==============================] - 4s 409us/step - loss: 0.0174 - acc: 0.4070 - val_loss: 0.0141 - val_acc: 0.4978\n",
      "Epoch 474/480\n",
      "10976/10976 [==============================] - 5s 413us/step - loss: 0.0172 - acc: 0.3985 - val_loss: 0.0200 - val_acc: 0.2647\n",
      "Epoch 475/480\n",
      "10976/10976 [==============================] - 5s 414us/step - loss: 0.0174 - acc: 0.4060 - val_loss: 0.0147 - val_acc: 0.4970\n",
      "Epoch 476/480\n",
      "10976/10976 [==============================] - 4s 408us/step - loss: 0.0172 - acc: 0.4200 - val_loss: 0.0196 - val_acc: 0.2644\n",
      "Epoch 477/480\n",
      "10976/10976 [==============================] - 4s 407us/step - loss: 0.0174 - acc: 0.4038 - val_loss: 0.0155 - val_acc: 0.4974\n",
      "Epoch 478/480\n",
      "10976/10976 [==============================] - 5s 414us/step - loss: 0.0173 - acc: 0.4140 - val_loss: 0.0194 - val_acc: 0.4007\n",
      "Epoch 479/480\n",
      "10976/10976 [==============================] - 4s 408us/step - loss: 0.0173 - acc: 0.4022 - val_loss: 0.0152 - val_acc: 0.4970\n",
      "Epoch 480/480\n",
      "10976/10976 [==============================] - 5s 410us/step - loss: 0.0173 - acc: 0.4217 - val_loss: 0.0206 - val_acc: 0.2617\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 461/480\n",
      "10976/10976 [==============================] - 5s 415us/step - loss: 0.0145 - acc: 0.3608 - val_loss: 0.0141 - val_acc: 0.3027\n",
      "Epoch 462/480\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0145 - acc: 0.3590 - val_loss: 0.0146 - val_acc: 0.4441\n",
      "Epoch 463/480\n",
      "10976/10976 [==============================] - 5s 417us/step - loss: 0.0145 - acc: 0.3655 - val_loss: 0.0146 - val_acc: 0.2994\n",
      "Epoch 464/480\n",
      "10976/10976 [==============================] - 5s 411us/step - loss: 0.0144 - acc: 0.3590 - val_loss: 0.0142 - val_acc: 0.4433\n",
      "Epoch 465/480\n",
      "10976/10976 [==============================] - 5s 413us/step - loss: 0.0145 - acc: 0.3546 - val_loss: 0.0135 - val_acc: 0.3029\n",
      "Epoch 466/480\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0145 - acc: 0.3582 - val_loss: 0.0145 - val_acc: 0.4342\n",
      "Epoch 467/480\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0144 - acc: 0.3647 - val_loss: 0.0141 - val_acc: 0.2866\n",
      "Epoch 468/480\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0145 - acc: 0.3593 - val_loss: 0.0142 - val_acc: 0.4387\n",
      "Epoch 469/480\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0144 - acc: 0.3644 - val_loss: 0.0151 - val_acc: 0.2968\n",
      "Epoch 470/480\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0145 - acc: 0.3615 - val_loss: 0.0148 - val_acc: 0.4441\n",
      "Epoch 471/480\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0144 - acc: 0.3630 - val_loss: 0.0150 - val_acc: 0.2816\n",
      "Epoch 472/480\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0146 - acc: 0.3536 - val_loss: 0.0141 - val_acc: 0.4373\n",
      "Epoch 473/480\n",
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0145 - acc: 0.3638 - val_loss: 0.0140 - val_acc: 0.2946\n",
      "Epoch 474/480\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0144 - acc: 0.3692 - val_loss: 0.0141 - val_acc: 0.4457\n",
      "Epoch 475/480\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0146 - acc: 0.3610 - val_loss: 0.0128 - val_acc: 0.2955\n",
      "Epoch 476/480\n",
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0145 - acc: 0.3596 - val_loss: 0.0137 - val_acc: 0.4305\n",
      "Epoch 477/480\n",
      "10976/10976 [==============================] - 5s 417us/step - loss: 0.0145 - acc: 0.3624 - val_loss: 0.0145 - val_acc: 0.2946\n",
      "Epoch 478/480\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0145 - acc: 0.3575 - val_loss: 0.0141 - val_acc: 0.4328\n",
      "Epoch 479/480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0143 - acc: 0.3649 - val_loss: 0.0151 - val_acc: 0.2776\n",
      "Epoch 480/480\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0144 - acc: 0.3534 - val_loss: 0.0134 - val_acc: 0.4381\n",
      "start training round 24\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 481/500\n",
      "10976/10976 [==============================] - 5s 412us/step - loss: 0.0190 - acc: 0.4384 - val_loss: 0.0207 - val_acc: 0.3965\n",
      "Epoch 482/500\n",
      "10976/10976 [==============================] - 4s 410us/step - loss: 0.0186 - acc: 0.4365 - val_loss: 0.0171 - val_acc: 0.4886\n",
      "Epoch 483/500\n",
      "10976/10976 [==============================] - 5s 410us/step - loss: 0.0188 - acc: 0.4368 - val_loss: 0.0205 - val_acc: 0.3882\n",
      "Epoch 484/500\n",
      "10976/10976 [==============================] - 4s 408us/step - loss: 0.0188 - acc: 0.4389 - val_loss: 0.0184 - val_acc: 0.4893\n",
      "Epoch 485/500\n",
      "10976/10976 [==============================] - 5s 412us/step - loss: 0.0188 - acc: 0.4360 - val_loss: 0.0206 - val_acc: 0.3834\n",
      "Epoch 486/500\n",
      "10976/10976 [==============================] - 4s 408us/step - loss: 0.0190 - acc: 0.4371 - val_loss: 0.0179 - val_acc: 0.4896\n",
      "Epoch 487/500\n",
      "10976/10976 [==============================] - 4s 406us/step - loss: 0.0190 - acc: 0.4345 - val_loss: 0.0200 - val_acc: 0.3901\n",
      "Epoch 488/500\n",
      "10976/10976 [==============================] - 5s 411us/step - loss: 0.0188 - acc: 0.4374 - val_loss: 0.0183 - val_acc: 0.4889\n",
      "Epoch 489/500\n",
      "10976/10976 [==============================] - 4s 409us/step - loss: 0.0190 - acc: 0.4391 - val_loss: 0.0201 - val_acc: 0.3948\n",
      "Epoch 490/500\n",
      "10976/10976 [==============================] - 4s 410us/step - loss: 0.0191 - acc: 0.4334 - val_loss: 0.0181 - val_acc: 0.4888\n",
      "Epoch 491/500\n",
      "10976/10976 [==============================] - 5s 414us/step - loss: 0.0187 - acc: 0.4409 - val_loss: 0.0190 - val_acc: 0.4080\n",
      "Epoch 492/500\n",
      "10976/10976 [==============================] - 5s 415us/step - loss: 0.0190 - acc: 0.4372 - val_loss: 0.0173 - val_acc: 0.4901\n",
      "Epoch 493/500\n",
      "10976/10976 [==============================] - 5s 413us/step - loss: 0.0188 - acc: 0.4389 - val_loss: 0.0207 - val_acc: 0.3978\n",
      "Epoch 494/500\n",
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0187 - acc: 0.4389 - val_loss: 0.0155 - val_acc: 0.4908\n",
      "Epoch 495/500\n",
      "10976/10976 [==============================] - 4s 409us/step - loss: 0.0188 - acc: 0.4395 - val_loss: 0.0209 - val_acc: 0.3846\n",
      "Epoch 496/500\n",
      "10976/10976 [==============================] - 5s 413us/step - loss: 0.0187 - acc: 0.4376 - val_loss: 0.0171 - val_acc: 0.4898\n",
      "Epoch 497/500\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0189 - acc: 0.4402 - val_loss: 0.0209 - val_acc: 0.3859\n",
      "Epoch 498/500\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0190 - acc: 0.4357 - val_loss: 0.0174 - val_acc: 0.4895\n",
      "Epoch 499/500\n",
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0189 - acc: 0.4396 - val_loss: 0.0209 - val_acc: 0.3881\n",
      "Epoch 500/500\n",
      "10976/10976 [==============================] - 4s 409us/step - loss: 0.0191 - acc: 0.4347 - val_loss: 0.0176 - val_acc: 0.4898\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 481/500\n",
      "10976/10976 [==============================] - 5s 413us/step - loss: 0.0217 - acc: 0.5133 - val_loss: 0.0211 - val_acc: 0.5722\n",
      "Epoch 482/500\n",
      "10976/10976 [==============================] - 5s 413us/step - loss: 0.0216 - acc: 0.4861 - val_loss: 0.0217 - val_acc: 0.5105\n",
      "Epoch 483/500\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0216 - acc: 0.4923 - val_loss: 0.0216 - val_acc: 0.5421\n",
      "Epoch 484/500\n",
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0216 - acc: 0.5271 - val_loss: 0.0197 - val_acc: 0.5155\n",
      "Epoch 485/500\n",
      "10976/10976 [==============================] - 5s 415us/step - loss: 0.0218 - acc: 0.4677 - val_loss: 0.0220 - val_acc: 0.5283\n",
      "Epoch 486/500\n",
      "10976/10976 [==============================] - 5s 414us/step - loss: 0.0219 - acc: 0.4655 - val_loss: 0.0219 - val_acc: 0.4094\n",
      "Epoch 487/500\n",
      "10976/10976 [==============================] - 5s 412us/step - loss: 0.0218 - acc: 0.4718 - val_loss: 0.0231 - val_acc: 0.4609\n",
      "Epoch 488/500\n",
      "10976/10976 [==============================] - 5s 413us/step - loss: 0.0220 - acc: 0.4732 - val_loss: 0.0221 - val_acc: 0.4841\n",
      "Epoch 489/500\n",
      "10976/10976 [==============================] - 5s 415us/step - loss: 0.0220 - acc: 0.4668 - val_loss: 0.0215 - val_acc: 0.4662\n",
      "Epoch 490/500\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0220 - acc: 0.4705 - val_loss: 0.0218 - val_acc: 0.5243\n",
      "Epoch 491/500\n",
      "10976/10976 [==============================] - 5s 411us/step - loss: 0.0220 - acc: 0.4729 - val_loss: 0.0220 - val_acc: 0.4640\n",
      "Epoch 492/500\n",
      "10976/10976 [==============================] - 5s 411us/step - loss: 0.0220 - acc: 0.4742 - val_loss: 0.0226 - val_acc: 0.3952\n",
      "Epoch 493/500\n",
      "10976/10976 [==============================] - 5s 412us/step - loss: 0.0218 - acc: 0.4650 - val_loss: 0.0222 - val_acc: 0.5360\n",
      "Epoch 494/500\n",
      "10976/10976 [==============================] - 5s 411us/step - loss: 0.0219 - acc: 0.4706 - val_loss: 0.0221 - val_acc: 0.4233\n",
      "Epoch 495/500\n",
      "10976/10976 [==============================] - 4s 408us/step - loss: 0.0218 - acc: 0.4658 - val_loss: 0.0212 - val_acc: 0.5468\n",
      "Epoch 496/500\n",
      "10976/10976 [==============================] - 4s 407us/step - loss: 0.0218 - acc: 0.4698 - val_loss: 0.0223 - val_acc: 0.4602\n",
      "Epoch 497/500\n",
      "10976/10976 [==============================] - 4s 407us/step - loss: 0.0217 - acc: 0.5319 - val_loss: 0.0214 - val_acc: 0.5743\n",
      "Epoch 498/500\n",
      "10976/10976 [==============================] - 5s 414us/step - loss: 0.0219 - acc: 0.5295 - val_loss: 0.0234 - val_acc: 0.4943\n",
      "Epoch 499/500\n",
      "10976/10976 [==============================] - 5s 417us/step - loss: 0.0220 - acc: 0.5210 - val_loss: 0.0223 - val_acc: 0.5637\n",
      "Epoch 500/500\n",
      "10976/10976 [==============================] - 5s 412us/step - loss: 0.0217 - acc: 0.5030 - val_loss: 0.0223 - val_acc: 0.4474\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 481/500\n",
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0173 - acc: 0.4135 - val_loss: 0.0164 - val_acc: 0.4970\n",
      "Epoch 482/500\n",
      "10976/10976 [==============================] - 5s 414us/step - loss: 0.0174 - acc: 0.4330 - val_loss: 0.0181 - val_acc: 0.4027\n",
      "Epoch 483/500\n",
      "10976/10976 [==============================] - 5s 415us/step - loss: 0.0174 - acc: 0.4013 - val_loss: 0.0151 - val_acc: 0.4980\n",
      "Epoch 484/500\n",
      "10976/10976 [==============================] - 5s 417us/step - loss: 0.0173 - acc: 0.3785 - val_loss: 0.0200 - val_acc: 0.2648\n",
      "Epoch 485/500\n",
      "10976/10976 [==============================] - 5s 417us/step - loss: 0.0175 - acc: 0.3869 - val_loss: 0.0151 - val_acc: 0.4972\n",
      "Epoch 486/500\n",
      "10976/10976 [==============================] - 5s 411us/step - loss: 0.0173 - acc: 0.4114 - val_loss: 0.0203 - val_acc: 0.2670\n",
      "Epoch 487/500\n",
      "10976/10976 [==============================] - 5s 410us/step - loss: 0.0175 - acc: 0.4079 - val_loss: 0.0145 - val_acc: 0.4974\n",
      "Epoch 488/500\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0173 - acc: 0.3968 - val_loss: 0.0199 - val_acc: 0.2631\n",
      "Epoch 489/500\n",
      "10976/10976 [==============================] - 5s 412us/step - loss: 0.0173 - acc: 0.3947 - val_loss: 0.0145 - val_acc: 0.4975\n",
      "Epoch 490/500\n",
      "10976/10976 [==============================] - 4s 409us/step - loss: 0.0172 - acc: 0.3933 - val_loss: 0.0201 - val_acc: 0.2666\n",
      "Epoch 491/500\n",
      "10976/10976 [==============================] - 5s 412us/step - loss: 0.0175 - acc: 0.3865 - val_loss: 0.0135 - val_acc: 0.5015\n",
      "Epoch 492/500\n",
      "10976/10976 [==============================] - 5s 413us/step - loss: 0.0173 - acc: 0.4129 - val_loss: 0.0180 - val_acc: 0.2632\n",
      "Epoch 493/500\n",
      "10976/10976 [==============================] - 5s 417us/step - loss: 0.0172 - acc: 0.4054 - val_loss: 0.0155 - val_acc: 0.4977\n",
      "Epoch 494/500\n",
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0174 - acc: 0.4055 - val_loss: 0.0185 - val_acc: 0.4037\n",
      "Epoch 495/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0174 - acc: 0.4094 - val_loss: 0.0156 - val_acc: 0.4970\n",
      "Epoch 496/500\n",
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0172 - acc: 0.4193 - val_loss: 0.0194 - val_acc: 0.2618\n",
      "Epoch 497/500\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0172 - acc: 0.4024 - val_loss: 0.0140 - val_acc: 0.4990\n",
      "Epoch 498/500\n",
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0173 - acc: 0.4093 - val_loss: 0.0199 - val_acc: 0.2641\n",
      "Epoch 499/500\n",
      "10976/10976 [==============================] - 5s 417us/step - loss: 0.0173 - acc: 0.3949 - val_loss: 0.0128 - val_acc: 0.5003\n",
      "Epoch 500/500\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0171 - acc: 0.3870 - val_loss: 0.0203 - val_acc: 0.2662\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 481/500\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0144 - acc: 0.3609 - val_loss: 0.0143 - val_acc: 0.3023\n",
      "Epoch 482/500\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0144 - acc: 0.3678 - val_loss: 0.0138 - val_acc: 0.4344\n",
      "Epoch 483/500\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0146 - acc: 0.3621 - val_loss: 0.0142 - val_acc: 0.3010\n",
      "Epoch 484/500\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0145 - acc: 0.3624 - val_loss: 0.0145 - val_acc: 0.4466\n",
      "Epoch 485/500\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0144 - acc: 0.3516 - val_loss: 0.0143 - val_acc: 0.2910\n",
      "Epoch 486/500\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0144 - acc: 0.3543 - val_loss: 0.0141 - val_acc: 0.4392\n",
      "Epoch 487/500\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0144 - acc: 0.3578 - val_loss: 0.0133 - val_acc: 0.1973\n",
      "Epoch 488/500\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0144 - acc: 0.3492 - val_loss: 0.0143 - val_acc: 0.4463\n",
      "Epoch 489/500\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0144 - acc: 0.3633 - val_loss: 0.0150 - val_acc: 0.2788\n",
      "Epoch 490/500\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0145 - acc: 0.3617 - val_loss: 0.0142 - val_acc: 0.4331\n",
      "Epoch 491/500\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0144 - acc: 0.3615 - val_loss: 0.0144 - val_acc: 0.2951\n",
      "Epoch 492/500\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0145 - acc: 0.3542 - val_loss: 0.0146 - val_acc: 0.4407\n",
      "Epoch 493/500\n",
      "10976/10976 [==============================] - 5s 414us/step - loss: 0.0144 - acc: 0.3667 - val_loss: 0.0151 - val_acc: 0.2910\n",
      "Epoch 494/500\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0145 - acc: 0.3629 - val_loss: 0.0143 - val_acc: 0.4383\n",
      "Epoch 495/500\n",
      "10976/10976 [==============================] - 5s 414us/step - loss: 0.0145 - acc: 0.3647 - val_loss: 0.0143 - val_acc: 0.2997\n",
      "Epoch 496/500\n",
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0143 - acc: 0.3740 - val_loss: 0.0129 - val_acc: 0.3795\n",
      "Epoch 497/500\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0145 - acc: 0.3779 - val_loss: 0.0159 - val_acc: 0.3590\n",
      "Epoch 498/500\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0146 - acc: 0.3805 - val_loss: 0.0139 - val_acc: 0.3764\n",
      "Epoch 499/500\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0144 - acc: 0.3797 - val_loss: 0.0162 - val_acc: 0.3933\n",
      "Epoch 500/500\n",
      "10976/10976 [==============================] - 5s 435us/step - loss: 0.0146 - acc: 0.3822 - val_loss: 0.0123 - val_acc: 0.3778\n",
      "start training round 25\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 501/520\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0189 - acc: 0.4400 - val_loss: 0.0197 - val_acc: 0.3987\n",
      "Epoch 502/520\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0187 - acc: 0.4373 - val_loss: 0.0178 - val_acc: 0.4897\n",
      "Epoch 503/520\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0188 - acc: 0.4379 - val_loss: 0.0208 - val_acc: 0.3906\n",
      "Epoch 504/520\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0190 - acc: 0.4374 - val_loss: 0.0174 - val_acc: 0.4898\n",
      "Epoch 505/520\n",
      "10976/10976 [==============================] - 5s 412us/step - loss: 0.0187 - acc: 0.4396 - val_loss: 0.0193 - val_acc: 0.3899\n",
      "Epoch 506/520\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0185 - acc: 0.4665 - val_loss: 0.0185 - val_acc: 0.4833\n",
      "Epoch 507/520\n",
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0188 - acc: 0.4819 - val_loss: 0.0225 - val_acc: 0.4889\n",
      "Epoch 508/520\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0188 - acc: 0.4828 - val_loss: 0.0180 - val_acc: 0.4873\n",
      "Epoch 509/520\n",
      "10976/10976 [==============================] - 5s 417us/step - loss: 0.0189 - acc: 0.4805 - val_loss: 0.0185 - val_acc: 0.4402\n",
      "Epoch 510/520\n",
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0188 - acc: 0.4781 - val_loss: 0.0172 - val_acc: 0.4976\n",
      "Epoch 511/520\n",
      "10976/10976 [==============================] - 5s 415us/step - loss: 0.0187 - acc: 0.4821 - val_loss: 0.0199 - val_acc: 0.4742\n",
      "Epoch 512/520\n",
      "10976/10976 [==============================] - 4s 408us/step - loss: 0.0189 - acc: 0.4871 - val_loss: 0.0164 - val_acc: 0.4867\n",
      "Epoch 513/520\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0187 - acc: 0.4817 - val_loss: 0.0227 - val_acc: 0.4819\n",
      "Epoch 514/520\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0189 - acc: 0.4804 - val_loss: 0.0161 - val_acc: 0.4948\n",
      "Epoch 515/520\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0185 - acc: 0.4773 - val_loss: 0.0221 - val_acc: 0.4741\n",
      "Epoch 516/520\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0190 - acc: 0.4804 - val_loss: 0.0169 - val_acc: 0.4970\n",
      "Epoch 517/520\n",
      "10976/10976 [==============================] - 4s 410us/step - loss: 0.0186 - acc: 0.4786 - val_loss: 0.0225 - val_acc: 0.4726\n",
      "Epoch 518/520\n",
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0189 - acc: 0.4797 - val_loss: 0.0183 - val_acc: 0.4805\n",
      "Epoch 519/520\n",
      "10976/10976 [==============================] - 5s 411us/step - loss: 0.0189 - acc: 0.4806 - val_loss: 0.0225 - val_acc: 0.4919\n",
      "Epoch 520/520\n",
      "10976/10976 [==============================] - 5s 413us/step - loss: 0.0187 - acc: 0.4778 - val_loss: 0.0147 - val_acc: 0.4960\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 501/520\n",
      "10976/10976 [==============================] - 5s 413us/step - loss: 0.0218 - acc: 0.5124 - val_loss: 0.0219 - val_acc: 0.5429\n",
      "Epoch 502/520\n",
      "10976/10976 [==============================] - 4s 409us/step - loss: 0.0217 - acc: 0.4722 - val_loss: 0.0216 - val_acc: 0.4164\n",
      "Epoch 503/520\n",
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0218 - acc: 0.4664 - val_loss: 0.0224 - val_acc: 0.5129\n",
      "Epoch 504/520\n",
      "10976/10976 [==============================] - 5s 413us/step - loss: 0.0217 - acc: 0.4712 - val_loss: 0.0220 - val_acc: 0.4127\n",
      "Epoch 505/520\n",
      "10976/10976 [==============================] - 5s 417us/step - loss: 0.0218 - acc: 0.4696 - val_loss: 0.0211 - val_acc: 0.4977\n",
      "Epoch 506/520\n",
      "10976/10976 [==============================] - 5s 414us/step - loss: 0.0212 - acc: 0.5210 - val_loss: 0.0221 - val_acc: 0.4366\n",
      "Epoch 507/520\n",
      "10976/10976 [==============================] - 5s 414us/step - loss: 0.0222 - acc: 0.4910 - val_loss: 0.0212 - val_acc: 0.5706\n",
      "Epoch 508/520\n",
      "10976/10976 [==============================] - 4s 406us/step - loss: 0.0220 - acc: 0.5281 - val_loss: 0.0223 - val_acc: 0.5200\n",
      "Epoch 509/520\n",
      "10976/10976 [==============================] - 5s 415us/step - loss: 0.0220 - acc: 0.5348 - val_loss: 0.0207 - val_acc: 0.5507\n",
      "Epoch 510/520\n",
      "10976/10976 [==============================] - 5s 414us/step - loss: 0.0217 - acc: 0.5051 - val_loss: 0.0225 - val_acc: 0.4524\n",
      "Epoch 511/520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0215 - acc: 0.4975 - val_loss: 0.0221 - val_acc: 0.5672\n",
      "Epoch 512/520\n",
      "10976/10976 [==============================] - 5s 413us/step - loss: 0.0219 - acc: 0.4708 - val_loss: 0.0213 - val_acc: 0.4226\n",
      "Epoch 513/520\n",
      "10976/10976 [==============================] - 5s 414us/step - loss: 0.0215 - acc: 0.4645 - val_loss: 0.0227 - val_acc: 0.5413\n",
      "Epoch 514/520\n",
      "10976/10976 [==============================] - 5s 412us/step - loss: 0.0219 - acc: 0.4896 - val_loss: 0.0229 - val_acc: 0.5528\n",
      "Epoch 515/520\n",
      "10976/10976 [==============================] - 5s 413us/step - loss: 0.0219 - acc: 0.5383 - val_loss: 0.0230 - val_acc: 0.5392\n",
      "Epoch 516/520\n",
      "10976/10976 [==============================] - 5s 413us/step - loss: 0.0218 - acc: 0.5177 - val_loss: 0.0215 - val_acc: 0.5668\n",
      "Epoch 517/520\n",
      "10976/10976 [==============================] - 5s 414us/step - loss: 0.0218 - acc: 0.5096 - val_loss: 0.0219 - val_acc: 0.5303\n",
      "Epoch 518/520\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0222 - acc: 0.5328 - val_loss: 0.0219 - val_acc: 0.5696\n",
      "Epoch 519/520\n",
      "10976/10976 [==============================] - 5s 417us/step - loss: 0.0218 - acc: 0.5343 - val_loss: 0.0223 - val_acc: 0.5444\n",
      "Epoch 520/520\n",
      "10976/10976 [==============================] - 5s 413us/step - loss: 0.0219 - acc: 0.5296 - val_loss: 0.0215 - val_acc: 0.4735\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 501/520\n",
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0175 - acc: 0.3902 - val_loss: 0.0148 - val_acc: 0.4978\n",
      "Epoch 502/520\n",
      "10976/10976 [==============================] - 5s 415us/step - loss: 0.0173 - acc: 0.4003 - val_loss: 0.0211 - val_acc: 0.2661\n",
      "Epoch 503/520\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0175 - acc: 0.3980 - val_loss: 0.0152 - val_acc: 0.4972\n",
      "Epoch 504/520\n",
      "10976/10976 [==============================] - 5s 415us/step - loss: 0.0174 - acc: 0.4128 - val_loss: 0.0200 - val_acc: 0.4008\n",
      "Epoch 505/520\n",
      "10976/10976 [==============================] - 5s 412us/step - loss: 0.0173 - acc: 0.4101 - val_loss: 0.0155 - val_acc: 0.4977\n",
      "Epoch 506/520\n",
      "10976/10976 [==============================] - 5s 414us/step - loss: 0.0174 - acc: 0.3971 - val_loss: 0.0202 - val_acc: 0.4057\n",
      "Epoch 507/520\n",
      "10976/10976 [==============================] - 5s 417us/step - loss: 0.0175 - acc: 0.3944 - val_loss: 0.0139 - val_acc: 0.4993\n",
      "Epoch 508/520\n",
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0173 - acc: 0.4007 - val_loss: 0.0201 - val_acc: 0.4022\n",
      "Epoch 509/520\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0173 - acc: 0.3833 - val_loss: 0.0151 - val_acc: 0.4982\n",
      "Epoch 510/520\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0172 - acc: 0.3921 - val_loss: 0.0198 - val_acc: 0.2681\n",
      "Epoch 511/520\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0175 - acc: 0.4153 - val_loss: 0.0147 - val_acc: 0.4982\n",
      "Epoch 512/520\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0172 - acc: 0.4024 - val_loss: 0.0202 - val_acc: 0.2658\n",
      "Epoch 513/520\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0173 - acc: 0.3901 - val_loss: 0.0148 - val_acc: 0.4980\n",
      "Epoch 514/520\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0173 - acc: 0.4057 - val_loss: 0.0196 - val_acc: 0.4011\n",
      "Epoch 515/520\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0173 - acc: 0.4255 - val_loss: 0.0156 - val_acc: 0.4975\n",
      "Epoch 516/520\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0173 - acc: 0.4016 - val_loss: 0.0195 - val_acc: 0.2694\n",
      "Epoch 517/520\n",
      "10976/10976 [==============================] - 5s 414us/step - loss: 0.0174 - acc: 0.4231 - val_loss: 0.0165 - val_acc: 0.4965\n",
      "Epoch 518/520\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0174 - acc: 0.4290 - val_loss: 0.0198 - val_acc: 0.4025\n",
      "Epoch 519/520\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0173 - acc: 0.3927 - val_loss: 0.0149 - val_acc: 0.4998\n",
      "Epoch 520/520\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0173 - acc: 0.3991 - val_loss: 0.0185 - val_acc: 0.2637\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 501/520\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0144 - acc: 0.3810 - val_loss: 0.0159 - val_acc: 0.3780\n",
      "Epoch 502/520\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0143 - acc: 0.3737 - val_loss: 0.0119 - val_acc: 0.3850\n",
      "Epoch 503/520\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0144 - acc: 0.3820 - val_loss: 0.0143 - val_acc: 0.3732\n",
      "Epoch 504/520\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0144 - acc: 0.3777 - val_loss: 0.0129 - val_acc: 0.3747\n",
      "Epoch 505/520\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0142 - acc: 0.3830 - val_loss: 0.0161 - val_acc: 0.3884\n",
      "Epoch 506/520\n",
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0144 - acc: 0.3828 - val_loss: 0.0134 - val_acc: 0.3767\n",
      "Epoch 507/520\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0145 - acc: 0.3868 - val_loss: 0.0155 - val_acc: 0.3825\n",
      "Epoch 508/520\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0144 - acc: 0.3808 - val_loss: 0.0126 - val_acc: 0.3793\n",
      "Epoch 509/520\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0144 - acc: 0.3798 - val_loss: 0.0156 - val_acc: 0.3731\n",
      "Epoch 510/520\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0144 - acc: 0.3794 - val_loss: 0.0130 - val_acc: 0.3772\n",
      "Epoch 511/520\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0144 - acc: 0.3843 - val_loss: 0.0154 - val_acc: 0.3751\n",
      "Epoch 512/520\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0144 - acc: 0.3719 - val_loss: 0.0130 - val_acc: 0.3756\n",
      "Epoch 513/520\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0142 - acc: 0.3713 - val_loss: 0.0154 - val_acc: 0.3763\n",
      "Epoch 514/520\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0144 - acc: 0.3776 - val_loss: 0.0128 - val_acc: 0.3793\n",
      "Epoch 515/520\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0145 - acc: 0.3815 - val_loss: 0.0162 - val_acc: 0.3835\n",
      "Epoch 516/520\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0143 - acc: 0.3798 - val_loss: 0.0128 - val_acc: 0.3777\n",
      "Epoch 517/520\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0143 - acc: 0.3850 - val_loss: 0.0161 - val_acc: 0.3842\n",
      "Epoch 518/520\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0144 - acc: 0.3717 - val_loss: 0.0129 - val_acc: 0.3776\n",
      "Epoch 519/520\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0143 - acc: 0.3698 - val_loss: 0.0159 - val_acc: 0.3690\n",
      "Epoch 520/520\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0144 - acc: 0.3722 - val_loss: 0.0126 - val_acc: 0.3768\n",
      "start training round 26\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 521/540\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0188 - acc: 0.4829 - val_loss: 0.0224 - val_acc: 0.4813\n",
      "Epoch 522/540\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0188 - acc: 0.4808 - val_loss: 0.0164 - val_acc: 0.4951\n",
      "Epoch 523/540\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0190 - acc: 0.4783 - val_loss: 0.0200 - val_acc: 0.4497\n",
      "Epoch 524/540\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0186 - acc: 0.4814 - val_loss: 0.0161 - val_acc: 0.4945\n",
      "Epoch 525/540\n",
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0189 - acc: 0.4830 - val_loss: 0.0219 - val_acc: 0.4722\n",
      "Epoch 526/540\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0193 - acc: 0.4826 - val_loss: 0.0162 - val_acc: 0.4822\n",
      "Epoch 527/540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 5s 414us/step - loss: 0.0190 - acc: 0.4817 - val_loss: 0.0208 - val_acc: 0.4777\n",
      "Epoch 528/540\n",
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0190 - acc: 0.4776 - val_loss: 0.0150 - val_acc: 0.4943\n",
      "Epoch 529/540\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0186 - acc: 0.4821 - val_loss: 0.0213 - val_acc: 0.4668\n",
      "Epoch 530/540\n",
      "10976/10976 [==============================] - 5s 412us/step - loss: 0.0190 - acc: 0.4800 - val_loss: 0.0160 - val_acc: 0.4859\n",
      "Epoch 531/540\n",
      "10976/10976 [==============================] - 5s 414us/step - loss: 0.0187 - acc: 0.4860 - val_loss: 0.0220 - val_acc: 0.4780\n",
      "Epoch 532/540\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0193 - acc: 0.4818 - val_loss: 0.0149 - val_acc: 0.4953\n",
      "Epoch 533/540\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0186 - acc: 0.4817 - val_loss: 0.0214 - val_acc: 0.4696\n",
      "Epoch 534/540\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0189 - acc: 0.4831 - val_loss: 0.0169 - val_acc: 0.4947\n",
      "Epoch 535/540\n",
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0190 - acc: 0.4826 - val_loss: 0.0229 - val_acc: 0.4864\n",
      "Epoch 536/540\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0186 - acc: 0.4804 - val_loss: 0.0142 - val_acc: 0.4967\n",
      "Epoch 537/540\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0187 - acc: 0.4795 - val_loss: 0.0220 - val_acc: 0.4732\n",
      "Epoch 538/540\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0189 - acc: 0.4788 - val_loss: 0.0157 - val_acc: 0.4969\n",
      "Epoch 539/540\n",
      "10976/10976 [==============================] - 5s 412us/step - loss: 0.0184 - acc: 0.4802 - val_loss: 0.0215 - val_acc: 0.4993\n",
      "Epoch 540/540\n",
      "10976/10976 [==============================] - 5s 414us/step - loss: 0.0188 - acc: 0.4779 - val_loss: 0.0157 - val_acc: 0.4966\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 521/540\n",
      "10976/10976 [==============================] - 5s 415us/step - loss: 0.0220 - acc: 0.5312 - val_loss: 0.0228 - val_acc: 0.5626\n",
      "Epoch 522/540\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0217 - acc: 0.5393 - val_loss: 0.0224 - val_acc: 0.5236\n",
      "Epoch 523/540\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0220 - acc: 0.5258 - val_loss: 0.0217 - val_acc: 0.5669\n",
      "Epoch 524/540\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0219 - acc: 0.5025 - val_loss: 0.0220 - val_acc: 0.4334\n",
      "Epoch 525/540\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0217 - acc: 0.4981 - val_loss: 0.0218 - val_acc: 0.5697\n",
      "Epoch 526/540\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0221 - acc: 0.5281 - val_loss: 0.0203 - val_acc: 0.4767\n",
      "Epoch 527/540\n",
      "10976/10976 [==============================] - 5s 417us/step - loss: 0.0215 - acc: 0.5008 - val_loss: 0.0224 - val_acc: 0.5669\n",
      "Epoch 528/540\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0221 - acc: 0.5249 - val_loss: 0.0240 - val_acc: 0.5040\n",
      "Epoch 529/540\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0221 - acc: 0.5322 - val_loss: 0.0209 - val_acc: 0.5667\n",
      "Epoch 530/540\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0217 - acc: 0.5301 - val_loss: 0.0232 - val_acc: 0.5216\n",
      "Epoch 531/540\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0217 - acc: 0.5314 - val_loss: 0.0216 - val_acc: 0.5264\n",
      "Epoch 532/540\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0217 - acc: 0.5298 - val_loss: 0.0203 - val_acc: 0.5621\n",
      "Epoch 533/540\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0214 - acc: 0.5318 - val_loss: 0.0214 - val_acc: 0.4709\n",
      "Epoch 534/540\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0217 - acc: 0.5303 - val_loss: 0.0203 - val_acc: 0.5762\n",
      "Epoch 535/540\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0214 - acc: 0.5081 - val_loss: 0.0212 - val_acc: 0.4685\n",
      "Epoch 536/540\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0217 - acc: 0.4988 - val_loss: 0.0226 - val_acc: 0.5689\n",
      "Epoch 537/540\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0214 - acc: 0.5047 - val_loss: 0.0219 - val_acc: 0.4597\n",
      "Epoch 538/540\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0215 - acc: 0.4969 - val_loss: 0.0220 - val_acc: 0.5372\n",
      "Epoch 539/540\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0215 - acc: 0.4771 - val_loss: 0.0217 - val_acc: 0.4996\n",
      "Epoch 540/540\n",
      "10976/10976 [==============================] - 5s 417us/step - loss: 0.0218 - acc: 0.4731 - val_loss: 0.0224 - val_acc: 0.4543\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 521/540\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0173 - acc: 0.3961 - val_loss: 0.0148 - val_acc: 0.4977\n",
      "Epoch 522/540\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0173 - acc: 0.4036 - val_loss: 0.0189 - val_acc: 0.2636\n",
      "Epoch 523/540\n",
      "10976/10976 [==============================] - 5s 417us/step - loss: 0.0173 - acc: 0.4111 - val_loss: 0.0154 - val_acc: 0.4981\n",
      "Epoch 524/540\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0171 - acc: 0.4057 - val_loss: 0.0204 - val_acc: 0.4012\n",
      "Epoch 525/540\n",
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0174 - acc: 0.4003 - val_loss: 0.0149 - val_acc: 0.4976\n",
      "Epoch 526/540\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0172 - acc: 0.3949 - val_loss: 0.0204 - val_acc: 0.2684\n",
      "Epoch 527/540\n",
      "10976/10976 [==============================] - 5s 417us/step - loss: 0.0173 - acc: 0.4074 - val_loss: 0.0154 - val_acc: 0.4980\n",
      "Epoch 528/540\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0172 - acc: 0.3965 - val_loss: 0.0200 - val_acc: 0.2617\n",
      "Epoch 529/540\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0173 - acc: 0.4136 - val_loss: 0.0131 - val_acc: 0.5247\n",
      "Epoch 530/540\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0173 - acc: 0.4654 - val_loss: 0.0235 - val_acc: 0.3018\n",
      "Epoch 531/540\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0174 - acc: 0.4310 - val_loss: 0.0126 - val_acc: 0.2933\n",
      "Epoch 532/540\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0171 - acc: 0.4120 - val_loss: 0.0240 - val_acc: 0.5219\n",
      "Epoch 533/540\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0175 - acc: 0.4323 - val_loss: 0.0103 - val_acc: 0.3091\n",
      "Epoch 534/540\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0172 - acc: 0.3960 - val_loss: 0.0241 - val_acc: 0.3084\n",
      "Epoch 535/540\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0175 - acc: 0.4270 - val_loss: 0.0099 - val_acc: 0.4537\n",
      "Epoch 536/540\n",
      "10976/10976 [==============================] - 5s 415us/step - loss: 0.0169 - acc: 0.4435 - val_loss: 0.0219 - val_acc: 0.2851\n",
      "Epoch 537/540\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0174 - acc: 0.3866 - val_loss: 0.0115 - val_acc: 0.3030\n",
      "Epoch 538/540\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0171 - acc: 0.3814 - val_loss: 0.0237 - val_acc: 0.3081\n",
      "Epoch 539/540\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0176 - acc: 0.4070 - val_loss: 0.0104 - val_acc: 0.5249\n",
      "Epoch 540/540\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0170 - acc: 0.4066 - val_loss: 0.0239 - val_acc: 0.4422\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 521/540\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0143 - acc: 0.3751 - val_loss: 0.0158 - val_acc: 0.3583\n",
      "Epoch 522/540\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0145 - acc: 0.3718 - val_loss: 0.0123 - val_acc: 0.3786\n",
      "Epoch 523/540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0143 - acc: 0.3703 - val_loss: 0.0156 - val_acc: 0.3513\n",
      "Epoch 524/540\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0143 - acc: 0.3673 - val_loss: 0.0124 - val_acc: 0.3783\n",
      "Epoch 525/540\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0143 - acc: 0.3693 - val_loss: 0.0156 - val_acc: 0.3532\n",
      "Epoch 526/540\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0143 - acc: 0.3731 - val_loss: 0.0121 - val_acc: 0.3793\n",
      "Epoch 527/540\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0143 - acc: 0.3721 - val_loss: 0.0151 - val_acc: 0.3561\n",
      "Epoch 528/540\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0143 - acc: 0.3627 - val_loss: 0.0129 - val_acc: 0.3773\n",
      "Epoch 529/540\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0143 - acc: 0.3789 - val_loss: 0.0161 - val_acc: 0.3650\n",
      "Epoch 530/540\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0143 - acc: 0.3676 - val_loss: 0.0123 - val_acc: 0.3779\n",
      "Epoch 531/540\n",
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0142 - acc: 0.3702 - val_loss: 0.0151 - val_acc: 0.4597\n",
      "Epoch 532/540\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0142 - acc: 0.3815 - val_loss: 0.0128 - val_acc: 0.3784\n",
      "Epoch 533/540\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0143 - acc: 0.3713 - val_loss: 0.0158 - val_acc: 0.3584\n",
      "Epoch 534/540\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0144 - acc: 0.3681 - val_loss: 0.0120 - val_acc: 0.3860\n",
      "Epoch 535/540\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0143 - acc: 0.3733 - val_loss: 0.0161 - val_acc: 0.3597\n",
      "Epoch 536/540\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0144 - acc: 0.3685 - val_loss: 0.0126 - val_acc: 0.3790\n",
      "Epoch 537/540\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0142 - acc: 0.3697 - val_loss: 0.0160 - val_acc: 0.3783\n",
      "Epoch 538/540\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0145 - acc: 0.3741 - val_loss: 0.0127 - val_acc: 0.3758\n",
      "Epoch 539/540\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0142 - acc: 0.3689 - val_loss: 0.0164 - val_acc: 0.3638\n",
      "Epoch 540/540\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0143 - acc: 0.3682 - val_loss: 0.0111 - val_acc: 0.3906\n",
      "start training round 27\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 541/560\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0186 - acc: 0.4787 - val_loss: 0.0196 - val_acc: 0.4747\n",
      "Epoch 542/560\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0190 - acc: 0.4819 - val_loss: 0.0160 - val_acc: 0.4955\n",
      "Epoch 543/560\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0188 - acc: 0.4810 - val_loss: 0.0222 - val_acc: 0.4777\n",
      "Epoch 544/560\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0190 - acc: 0.4803 - val_loss: 0.0165 - val_acc: 0.4973\n",
      "Epoch 545/560\n",
      "10976/10976 [==============================] - 5s 417us/step - loss: 0.0189 - acc: 0.4788 - val_loss: 0.0226 - val_acc: 0.4793\n",
      "Epoch 546/560\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0190 - acc: 0.4825 - val_loss: 0.0142 - val_acc: 0.4958\n",
      "Epoch 547/560\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0186 - acc: 0.4769 - val_loss: 0.0172 - val_acc: 0.4622\n",
      "Epoch 548/560\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0188 - acc: 0.4806 - val_loss: 0.0140 - val_acc: 0.4959\n",
      "Epoch 549/560\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0187 - acc: 0.4850 - val_loss: 0.0219 - val_acc: 0.4828\n",
      "Epoch 550/560\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0188 - acc: 0.4811 - val_loss: 0.0170 - val_acc: 0.4945\n",
      "Epoch 551/560\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0188 - acc: 0.4822 - val_loss: 0.0227 - val_acc: 0.4889\n",
      "Epoch 552/560\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0188 - acc: 0.4626 - val_loss: 0.0173 - val_acc: 0.4909\n",
      "Epoch 553/560\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0185 - acc: 0.4377 - val_loss: 0.0180 - val_acc: 0.3883\n",
      "Epoch 554/560\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0188 - acc: 0.4371 - val_loss: 0.0182 - val_acc: 0.4905\n",
      "Epoch 555/560\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0185 - acc: 0.4402 - val_loss: 0.0190 - val_acc: 0.3903\n",
      "Epoch 556/560\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0185 - acc: 0.4421 - val_loss: 0.0177 - val_acc: 0.4910\n",
      "Epoch 557/560\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0188 - acc: 0.4388 - val_loss: 0.0198 - val_acc: 0.3897\n",
      "Epoch 558/560\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0188 - acc: 0.4371 - val_loss: 0.0177 - val_acc: 0.4902\n",
      "Epoch 559/560\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0186 - acc: 0.4388 - val_loss: 0.0189 - val_acc: 0.3963\n",
      "Epoch 560/560\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0185 - acc: 0.4383 - val_loss: 0.0172 - val_acc: 0.4905\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 541/560\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0216 - acc: 0.4716 - val_loss: 0.0218 - val_acc: 0.4131\n",
      "Epoch 542/560\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0217 - acc: 0.4729 - val_loss: 0.0224 - val_acc: 0.4614\n",
      "Epoch 543/560\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0215 - acc: 0.4759 - val_loss: 0.0217 - val_acc: 0.4233\n",
      "Epoch 544/560\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0218 - acc: 0.4702 - val_loss: 0.0217 - val_acc: 0.4621\n",
      "Epoch 545/560\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0217 - acc: 0.4756 - val_loss: 0.0212 - val_acc: 0.5006\n",
      "Epoch 546/560\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0218 - acc: 0.4719 - val_loss: 0.0212 - val_acc: 0.4685\n",
      "Epoch 547/560\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0220 - acc: 0.5137 - val_loss: 0.0229 - val_acc: 0.5182\n",
      "Epoch 548/560\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0220 - acc: 0.5316 - val_loss: 0.0201 - val_acc: 0.5680\n",
      "Epoch 549/560\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0217 - acc: 0.5339 - val_loss: 0.0216 - val_acc: 0.5380\n",
      "Epoch 550/560\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0222 - acc: 0.5350 - val_loss: 0.0214 - val_acc: 0.5754\n",
      "Epoch 551/560\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0220 - acc: 0.5338 - val_loss: 0.0226 - val_acc: 0.5298\n",
      "Epoch 552/560\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0213 - acc: 0.5050 - val_loss: 0.0225 - val_acc: 0.5622\n",
      "Epoch 553/560\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0219 - acc: 0.5073 - val_loss: 0.0225 - val_acc: 0.5158\n",
      "Epoch 554/560\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0216 - acc: 0.5329 - val_loss: 0.0198 - val_acc: 0.5801\n",
      "Epoch 555/560\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0216 - acc: 0.5316 - val_loss: 0.0220 - val_acc: 0.4970\n",
      "Epoch 556/560\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0211 - acc: 0.4921 - val_loss: 0.0221 - val_acc: 0.4619\n",
      "Epoch 557/560\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0217 - acc: 0.4871 - val_loss: 0.0235 - val_acc: 0.5038\n",
      "Epoch 558/560\n",
      "10976/10976 [==============================] - 5s 417us/step - loss: 0.0212 - acc: 0.5319 - val_loss: 0.0200 - val_acc: 0.5805\n",
      "Epoch 559/560\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0210 - acc: 0.5341 - val_loss: 0.0221 - val_acc: 0.5112\n",
      "Epoch 560/560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0216 - acc: 0.5222 - val_loss: 0.0217 - val_acc: 0.5418\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 541/560\n",
      "10976/10976 [==============================] - 5s 417us/step - loss: 0.0176 - acc: 0.3872 - val_loss: 0.0105 - val_acc: 0.5263\n",
      "Epoch 542/560\n",
      "10976/10976 [==============================] - 5s 415us/step - loss: 0.0169 - acc: 0.4165 - val_loss: 0.0234 - val_acc: 0.3089\n",
      "Epoch 543/560\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0171 - acc: 0.4221 - val_loss: 0.0116 - val_acc: 0.5238\n",
      "Epoch 544/560\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0171 - acc: 0.4262 - val_loss: 0.0243 - val_acc: 0.4535\n",
      "Epoch 545/560\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0175 - acc: 0.4125 - val_loss: 0.0111 - val_acc: 0.3043\n",
      "Epoch 546/560\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0168 - acc: 0.3917 - val_loss: 0.0153 - val_acc: 0.2324\n",
      "Epoch 547/560\n",
      "10976/10976 [==============================] - 5s 415us/step - loss: 0.0173 - acc: 0.3489 - val_loss: 0.0186 - val_acc: 0.4500\n",
      "Epoch 548/560\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0171 - acc: 0.3578 - val_loss: 0.0157 - val_acc: 0.2298\n",
      "Epoch 549/560\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0173 - acc: 0.3530 - val_loss: 0.0187 - val_acc: 0.5269\n",
      "Epoch 550/560\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0172 - acc: 0.3532 - val_loss: 0.0157 - val_acc: 0.2299\n",
      "Epoch 551/560\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0171 - acc: 0.3484 - val_loss: 0.0181 - val_acc: 0.4503\n",
      "Epoch 552/560\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0173 - acc: 0.3531 - val_loss: 0.0146 - val_acc: 0.2326\n",
      "Epoch 553/560\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0172 - acc: 0.3435 - val_loss: 0.0190 - val_acc: 0.4509\n",
      "Epoch 554/560\n",
      "10976/10976 [==============================] - 5s 415us/step - loss: 0.0173 - acc: 0.3640 - val_loss: 0.0158 - val_acc: 0.2294\n",
      "Epoch 555/560\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0171 - acc: 0.3587 - val_loss: 0.0193 - val_acc: 0.4535\n",
      "Epoch 556/560\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0173 - acc: 0.3527 - val_loss: 0.0168 - val_acc: 0.2279\n",
      "Epoch 557/560\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0171 - acc: 0.3466 - val_loss: 0.0193 - val_acc: 0.5225\n",
      "Epoch 558/560\n",
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0174 - acc: 0.3622 - val_loss: 0.0156 - val_acc: 0.2299\n",
      "Epoch 559/560\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0171 - acc: 0.3528 - val_loss: 0.0193 - val_acc: 0.4525\n",
      "Epoch 560/560\n",
      "10976/10976 [==============================] - 5s 417us/step - loss: 0.0173 - acc: 0.3653 - val_loss: 0.0153 - val_acc: 0.2298\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 541/560\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0143 - acc: 0.3780 - val_loss: 0.0157 - val_acc: 0.3622\n",
      "Epoch 542/560\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0142 - acc: 0.3833 - val_loss: 0.0127 - val_acc: 0.3789\n",
      "Epoch 543/560\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0142 - acc: 0.3801 - val_loss: 0.0158 - val_acc: 0.3721\n",
      "Epoch 544/560\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0143 - acc: 0.3775 - val_loss: 0.0134 - val_acc: 0.3881\n",
      "Epoch 545/560\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0143 - acc: 0.3839 - val_loss: 0.0161 - val_acc: 0.3822\n",
      "Epoch 546/560\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0143 - acc: 0.3789 - val_loss: 0.0116 - val_acc: 0.3921\n",
      "Epoch 547/560\n",
      "10976/10976 [==============================] - 5s 434us/step - loss: 0.0142 - acc: 0.3773 - val_loss: 0.0151 - val_acc: 0.3666\n",
      "Epoch 548/560\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0143 - acc: 0.3712 - val_loss: 0.0131 - val_acc: 0.3764\n",
      "Epoch 549/560\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0143 - acc: 0.3666 - val_loss: 0.0153 - val_acc: 0.2859\n",
      "Epoch 550/560\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0145 - acc: 0.3691 - val_loss: 0.0138 - val_acc: 0.4183\n",
      "Epoch 551/560\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0144 - acc: 0.3640 - val_loss: 0.0151 - val_acc: 0.2811\n",
      "Epoch 552/560\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0143 - acc: 0.3691 - val_loss: 0.0126 - val_acc: 0.4257\n",
      "Epoch 553/560\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0143 - acc: 0.3633 - val_loss: 0.0138 - val_acc: 0.2796\n",
      "Epoch 554/560\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0143 - acc: 0.3589 - val_loss: 0.0136 - val_acc: 0.4355\n",
      "Epoch 555/560\n",
      "10976/10976 [==============================] - 5s 453us/step - loss: 0.0143 - acc: 0.3651 - val_loss: 0.0144 - val_acc: 0.3064\n",
      "Epoch 556/560\n",
      "10976/10976 [==============================] - 5s 444us/step - loss: 0.0144 - acc: 0.3590 - val_loss: 0.0138 - val_acc: 0.4638\n",
      "Epoch 557/560\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0143 - acc: 0.3624 - val_loss: 0.0143 - val_acc: 0.3022\n",
      "Epoch 558/560\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0143 - acc: 0.3621 - val_loss: 0.0138 - val_acc: 0.4364\n",
      "Epoch 559/560\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0142 - acc: 0.3699 - val_loss: 0.0144 - val_acc: 0.3103\n",
      "Epoch 560/560\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0144 - acc: 0.3720 - val_loss: 0.0137 - val_acc: 0.4762\n",
      "start training round 28\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 561/580\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0186 - acc: 0.4421 - val_loss: 0.0203 - val_acc: 0.3898\n",
      "Epoch 562/580\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0186 - acc: 0.4399 - val_loss: 0.0173 - val_acc: 0.4903\n",
      "Epoch 563/580\n",
      "10976/10976 [==============================] - 5s 439us/step - loss: 0.0186 - acc: 0.4399 - val_loss: 0.0203 - val_acc: 0.3873\n",
      "Epoch 564/580\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0186 - acc: 0.4408 - val_loss: 0.0156 - val_acc: 0.4922\n",
      "Epoch 565/580\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0186 - acc: 0.4432 - val_loss: 0.0198 - val_acc: 0.3812\n",
      "Epoch 566/580\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0186 - acc: 0.4406 - val_loss: 0.0198 - val_acc: 0.4958\n",
      "Epoch 567/580\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0190 - acc: 0.4386 - val_loss: 0.0176 - val_acc: 0.3930\n",
      "Epoch 568/580\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0188 - acc: 0.4358 - val_loss: 0.0195 - val_acc: 0.4971\n",
      "Epoch 569/580\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0188 - acc: 0.4325 - val_loss: 0.0183 - val_acc: 0.3806\n",
      "Epoch 570/580\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0185 - acc: 0.4354 - val_loss: 0.0198 - val_acc: 0.4952\n",
      "Epoch 571/580\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0188 - acc: 0.4409 - val_loss: 0.0171 - val_acc: 0.3918\n",
      "Epoch 572/580\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0185 - acc: 0.4347 - val_loss: 0.0202 - val_acc: 0.4980\n",
      "Epoch 573/580\n",
      "10976/10976 [==============================] - 5s 417us/step - loss: 0.0187 - acc: 0.4411 - val_loss: 0.0182 - val_acc: 0.3846\n",
      "Epoch 574/580\n",
      "10976/10976 [==============================] - 5s 415us/step - loss: 0.0186 - acc: 0.4345 - val_loss: 0.0200 - val_acc: 0.4960\n",
      "Epoch 575/580\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0186 - acc: 0.4370 - val_loss: 0.0168 - val_acc: 0.3927\n",
      "Epoch 576/580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0182 - acc: 0.4411 - val_loss: 0.0197 - val_acc: 0.4943\n",
      "Epoch 577/580\n",
      "10976/10976 [==============================] - 5s 417us/step - loss: 0.0186 - acc: 0.4349 - val_loss: 0.0183 - val_acc: 0.3876\n",
      "Epoch 578/580\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0186 - acc: 0.4373 - val_loss: 0.0197 - val_acc: 0.4989\n",
      "Epoch 579/580\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0182 - acc: 0.4425 - val_loss: 0.0176 - val_acc: 0.3904\n",
      "Epoch 580/580\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0187 - acc: 0.4373 - val_loss: 0.0178 - val_acc: 0.4613\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 561/580\n",
      "10976/10976 [==============================] - 5s 415us/step - loss: 0.0218 - acc: 0.4720 - val_loss: 0.0220 - val_acc: 0.4113\n",
      "Epoch 562/580\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0215 - acc: 0.4884 - val_loss: 0.0216 - val_acc: 0.5322\n",
      "Epoch 563/580\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0219 - acc: 0.5382 - val_loss: 0.0213 - val_acc: 0.5534\n",
      "Epoch 564/580\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0214 - acc: 0.5066 - val_loss: 0.0221 - val_acc: 0.4942\n",
      "Epoch 565/580\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0217 - acc: 0.4791 - val_loss: 0.0218 - val_acc: 0.4617\n",
      "Epoch 566/580\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0217 - acc: 0.4756 - val_loss: 0.0224 - val_acc: 0.5049\n",
      "Epoch 567/580\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0217 - acc: 0.4868 - val_loss: 0.0203 - val_acc: 0.5800\n",
      "Epoch 568/580\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0217 - acc: 0.5192 - val_loss: 0.0217 - val_acc: 0.5161\n",
      "Epoch 569/580\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0215 - acc: 0.4792 - val_loss: 0.0214 - val_acc: 0.5311\n",
      "Epoch 570/580\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0216 - acc: 0.4749 - val_loss: 0.0219 - val_acc: 0.4138\n",
      "Epoch 571/580\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0214 - acc: 0.4807 - val_loss: 0.0228 - val_acc: 0.4568\n",
      "Epoch 572/580\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0214 - acc: 0.4725 - val_loss: 0.0209 - val_acc: 0.4506\n",
      "Epoch 573/580\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0215 - acc: 0.4793 - val_loss: 0.0222 - val_acc: 0.4610\n",
      "Epoch 574/580\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0216 - acc: 0.4745 - val_loss: 0.0218 - val_acc: 0.4159\n",
      "Epoch 575/580\n",
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0217 - acc: 0.4680 - val_loss: 0.0218 - val_acc: 0.5511\n",
      "Epoch 576/580\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0215 - acc: 0.4732 - val_loss: 0.0212 - val_acc: 0.4333\n",
      "Epoch 577/580\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0215 - acc: 0.4787 - val_loss: 0.0221 - val_acc: 0.4587\n",
      "Epoch 578/580\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0217 - acc: 0.4831 - val_loss: 0.0221 - val_acc: 0.4952\n",
      "Epoch 579/580\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0214 - acc: 0.4793 - val_loss: 0.0217 - val_acc: 0.5263\n",
      "Epoch 580/580\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0215 - acc: 0.4720 - val_loss: 0.0212 - val_acc: 0.4261\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 561/580\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0171 - acc: 0.3531 - val_loss: 0.0178 - val_acc: 0.4517\n",
      "Epoch 562/580\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0172 - acc: 0.3570 - val_loss: 0.0163 - val_acc: 0.2292\n",
      "Epoch 563/580\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0172 - acc: 0.3580 - val_loss: 0.0195 - val_acc: 0.5281\n",
      "Epoch 564/580\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0171 - acc: 0.3724 - val_loss: 0.0162 - val_acc: 0.2285\n",
      "Epoch 565/580\n",
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0173 - acc: 0.3616 - val_loss: 0.0169 - val_acc: 0.4403\n",
      "Epoch 566/580\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0170 - acc: 0.3625 - val_loss: 0.0166 - val_acc: 0.2291\n",
      "Epoch 567/580\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0173 - acc: 0.3504 - val_loss: 0.0185 - val_acc: 0.5203\n",
      "Epoch 568/580\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0173 - acc: 0.3604 - val_loss: 0.0152 - val_acc: 0.2305\n",
      "Epoch 569/580\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0172 - acc: 0.3650 - val_loss: 0.0188 - val_acc: 0.4467\n",
      "Epoch 570/580\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0172 - acc: 0.3511 - val_loss: 0.0170 - val_acc: 0.2258\n",
      "Epoch 571/580\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0172 - acc: 0.3577 - val_loss: 0.0185 - val_acc: 0.5293\n",
      "Epoch 572/580\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0172 - acc: 0.3614 - val_loss: 0.0158 - val_acc: 0.2288\n",
      "Epoch 573/580\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0171 - acc: 0.3552 - val_loss: 0.0173 - val_acc: 0.4384\n",
      "Epoch 574/580\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0171 - acc: 0.3583 - val_loss: 0.0166 - val_acc: 0.2280\n",
      "Epoch 575/580\n",
      "10976/10976 [==============================] - 5s 415us/step - loss: 0.0172 - acc: 0.3595 - val_loss: 0.0195 - val_acc: 0.4543\n",
      "Epoch 576/580\n",
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0172 - acc: 0.3617 - val_loss: 0.0159 - val_acc: 0.2281\n",
      "Epoch 577/580\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0173 - acc: 0.3548 - val_loss: 0.0195 - val_acc: 0.5279\n",
      "Epoch 578/580\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0172 - acc: 0.3577 - val_loss: 0.0162 - val_acc: 0.2296\n",
      "Epoch 579/580\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0171 - acc: 0.3595 - val_loss: 0.0182 - val_acc: 0.4440\n",
      "Epoch 580/580\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0171 - acc: 0.3591 - val_loss: 0.0159 - val_acc: 0.2304\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 561/580\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0143 - acc: 0.3708 - val_loss: 0.0144 - val_acc: 0.3009\n",
      "Epoch 562/580\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0143 - acc: 0.3689 - val_loss: 0.0138 - val_acc: 0.4446\n",
      "Epoch 563/580\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0143 - acc: 0.3752 - val_loss: 0.0143 - val_acc: 0.3065\n",
      "Epoch 564/580\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0143 - acc: 0.3718 - val_loss: 0.0138 - val_acc: 0.4457\n",
      "Epoch 565/580\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0143 - acc: 0.3720 - val_loss: 0.0148 - val_acc: 0.3016\n",
      "Epoch 566/580\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0142 - acc: 0.3759 - val_loss: 0.0144 - val_acc: 0.4442\n",
      "Epoch 567/580\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0142 - acc: 0.3716 - val_loss: 0.0130 - val_acc: 0.3166\n",
      "Epoch 568/580\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0142 - acc: 0.3774 - val_loss: 0.0135 - val_acc: 0.4676\n",
      "Epoch 569/580\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0142 - acc: 0.3710 - val_loss: 0.0149 - val_acc: 0.3025\n",
      "Epoch 570/580\n",
      "10976/10976 [==============================] - 5s 433us/step - loss: 0.0143 - acc: 0.3626 - val_loss: 0.0143 - val_acc: 0.4422\n",
      "Epoch 571/580\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0143 - acc: 0.3713 - val_loss: 0.0148 - val_acc: 0.2929\n",
      "Epoch 572/580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0143 - acc: 0.3760 - val_loss: 0.0146 - val_acc: 0.4476\n",
      "Epoch 573/580\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0143 - acc: 0.3678 - val_loss: 0.0145 - val_acc: 0.3013\n",
      "Epoch 574/580\n",
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0141 - acc: 0.3573 - val_loss: 0.0138 - val_acc: 0.4361\n",
      "Epoch 575/580\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0142 - acc: 0.3717 - val_loss: 0.0142 - val_acc: 0.3108\n",
      "Epoch 576/580\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0142 - acc: 0.3660 - val_loss: 0.0128 - val_acc: 0.4409\n",
      "Epoch 577/580\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0142 - acc: 0.3718 - val_loss: 0.0143 - val_acc: 0.3057\n",
      "Epoch 578/580\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0141 - acc: 0.3670 - val_loss: 0.0134 - val_acc: 0.4437\n",
      "Epoch 579/580\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0142 - acc: 0.3637 - val_loss: 0.0143 - val_acc: 0.2904\n",
      "Epoch 580/580\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0143 - acc: 0.3696 - val_loss: 0.0138 - val_acc: 0.4404\n",
      "start training round 29\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 581/600\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0185 - acc: 0.4389 - val_loss: 0.0164 - val_acc: 0.4118\n",
      "Epoch 582/600\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0187 - acc: 0.4356 - val_loss: 0.0200 - val_acc: 0.4890\n",
      "Epoch 583/600\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0186 - acc: 0.4404 - val_loss: 0.0172 - val_acc: 0.3887\n",
      "Epoch 584/600\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0183 - acc: 0.4381 - val_loss: 0.0177 - val_acc: 0.4842\n",
      "Epoch 585/600\n",
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0185 - acc: 0.4407 - val_loss: 0.0187 - val_acc: 0.3846\n",
      "Epoch 586/600\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0186 - acc: 0.4399 - val_loss: 0.0192 - val_acc: 0.4943\n",
      "Epoch 587/600\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0186 - acc: 0.4373 - val_loss: 0.0176 - val_acc: 0.3949\n",
      "Epoch 588/600\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0185 - acc: 0.4329 - val_loss: 0.0199 - val_acc: 0.4919\n",
      "Epoch 589/600\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0186 - acc: 0.4395 - val_loss: 0.0187 - val_acc: 0.3880\n",
      "Epoch 590/600\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0186 - acc: 0.4338 - val_loss: 0.0180 - val_acc: 0.4636\n",
      "Epoch 591/600\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0187 - acc: 0.4414 - val_loss: 0.0174 - val_acc: 0.3904\n",
      "Epoch 592/600\n",
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0187 - acc: 0.4390 - val_loss: 0.0195 - val_acc: 0.4961\n",
      "Epoch 593/600\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0186 - acc: 0.4421 - val_loss: 0.0181 - val_acc: 0.3855\n",
      "Epoch 594/600\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0186 - acc: 0.4393 - val_loss: 0.0192 - val_acc: 0.4978\n",
      "Epoch 595/600\n",
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0185 - acc: 0.4400 - val_loss: 0.0166 - val_acc: 0.3951\n",
      "Epoch 596/600\n",
      "10976/10976 [==============================] - 5s 434us/step - loss: 0.0186 - acc: 0.4364 - val_loss: 0.0195 - val_acc: 0.4948\n",
      "Epoch 597/600\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0188 - acc: 0.4374 - val_loss: 0.0178 - val_acc: 0.3886\n",
      "Epoch 598/600\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0187 - acc: 0.4339 - val_loss: 0.0197 - val_acc: 0.4942\n",
      "Epoch 599/600\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0183 - acc: 0.4408 - val_loss: 0.0187 - val_acc: 0.3884\n",
      "Epoch 600/600\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0185 - acc: 0.4364 - val_loss: 0.0199 - val_acc: 0.4949\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 581/600\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0215 - acc: 0.4756 - val_loss: 0.0219 - val_acc: 0.5075\n",
      "Epoch 582/600\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0215 - acc: 0.4709 - val_loss: 0.0214 - val_acc: 0.4211\n",
      "Epoch 583/600\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0213 - acc: 0.4819 - val_loss: 0.0217 - val_acc: 0.5130\n",
      "Epoch 584/600\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0215 - acc: 0.4758 - val_loss: 0.0213 - val_acc: 0.4258\n",
      "Epoch 585/600\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0214 - acc: 0.4793 - val_loss: 0.0215 - val_acc: 0.5742\n",
      "Epoch 586/600\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0216 - acc: 0.5422 - val_loss: 0.0229 - val_acc: 0.4983\n",
      "Epoch 587/600\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0216 - acc: 0.5388 - val_loss: 0.0213 - val_acc: 0.5712\n",
      "Epoch 588/600\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0217 - acc: 0.5293 - val_loss: 0.0210 - val_acc: 0.5342\n",
      "Epoch 589/600\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0213 - acc: 0.4987 - val_loss: 0.0213 - val_acc: 0.5079\n",
      "Epoch 590/600\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0213 - acc: 0.4787 - val_loss: 0.0207 - val_acc: 0.4942\n",
      "Epoch 591/600\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0215 - acc: 0.4822 - val_loss: 0.0224 - val_acc: 0.5562\n",
      "Epoch 592/600\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0213 - acc: 0.5378 - val_loss: 0.0224 - val_acc: 0.5257\n",
      "Epoch 593/600\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0219 - acc: 0.5408 - val_loss: 0.0207 - val_acc: 0.5838\n",
      "Epoch 594/600\n",
      "10976/10976 [==============================] - 5s 417us/step - loss: 0.0215 - acc: 0.5391 - val_loss: 0.0218 - val_acc: 0.4665\n",
      "Epoch 595/600\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0213 - acc: 0.5074 - val_loss: 0.0202 - val_acc: 0.5748\n",
      "Epoch 596/600\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0214 - acc: 0.5065 - val_loss: 0.0209 - val_acc: 0.4448\n",
      "Epoch 597/600\n",
      "10976/10976 [==============================] - 5s 416us/step - loss: 0.0219 - acc: 0.5371 - val_loss: 0.0200 - val_acc: 0.5826\n",
      "Epoch 598/600\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0217 - acc: 0.5399 - val_loss: 0.0227 - val_acc: 0.5082\n",
      "Epoch 599/600\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0220 - acc: 0.5294 - val_loss: 0.0218 - val_acc: 0.5715\n",
      "Epoch 600/600\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0219 - acc: 0.5296 - val_loss: 0.0233 - val_acc: 0.5322\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 581/600\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0171 - acc: 0.3507 - val_loss: 0.0182 - val_acc: 0.5249\n",
      "Epoch 582/600\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0171 - acc: 0.3557 - val_loss: 0.0156 - val_acc: 0.2277\n",
      "Epoch 583/600\n",
      "10976/10976 [==============================] - 5s 417us/step - loss: 0.0172 - acc: 0.3616 - val_loss: 0.0178 - val_acc: 0.5223\n",
      "Epoch 584/600\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0171 - acc: 0.3736 - val_loss: 0.0163 - val_acc: 0.2274\n",
      "Epoch 585/600\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0170 - acc: 0.3541 - val_loss: 0.0190 - val_acc: 0.5247\n",
      "Epoch 586/600\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0172 - acc: 0.3678 - val_loss: 0.0167 - val_acc: 0.2283\n",
      "Epoch 587/600\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0172 - acc: 0.3613 - val_loss: 0.0191 - val_acc: 0.5247\n",
      "Epoch 588/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0171 - acc: 0.3700 - val_loss: 0.0180 - val_acc: 0.2244\n",
      "Epoch 589/600\n",
      "10976/10976 [==============================] - 5s 433us/step - loss: 0.0170 - acc: 0.3611 - val_loss: 0.0206 - val_acc: 0.2877\n",
      "Epoch 590/600\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0174 - acc: 0.4098 - val_loss: 0.0103 - val_acc: 0.5324\n",
      "Epoch 591/600\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0170 - acc: 0.4363 - val_loss: 0.0229 - val_acc: 0.3065\n",
      "Epoch 592/600\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0174 - acc: 0.3937 - val_loss: 0.0115 - val_acc: 0.4438\n",
      "Epoch 593/600\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0170 - acc: 0.4169 - val_loss: 0.0231 - val_acc: 0.4503\n",
      "Epoch 594/600\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0173 - acc: 0.3927 - val_loss: 0.0115 - val_acc: 0.5266\n",
      "Epoch 595/600\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0172 - acc: 0.4328 - val_loss: 0.0233 - val_acc: 0.3037\n",
      "Epoch 596/600\n",
      "10976/10976 [==============================] - 5s 439us/step - loss: 0.0171 - acc: 0.4042 - val_loss: 0.0132 - val_acc: 0.2954\n",
      "Epoch 597/600\n",
      "10976/10976 [==============================] - 5s 435us/step - loss: 0.0170 - acc: 0.3591 - val_loss: 0.0235 - val_acc: 0.3077\n",
      "Epoch 598/600\n",
      "10976/10976 [==============================] - 5s 436us/step - loss: 0.0173 - acc: 0.4133 - val_loss: 0.0103 - val_acc: 0.5307\n",
      "Epoch 599/600\n",
      "10976/10976 [==============================] - 5s 447us/step - loss: 0.0170 - acc: 0.4448 - val_loss: 0.0233 - val_acc: 0.5284\n",
      "Epoch 600/600\n",
      "10976/10976 [==============================] - 5s 467us/step - loss: 0.0175 - acc: 0.4353 - val_loss: 0.0110 - val_acc: 0.5314\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 581/600\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0143 - acc: 0.3745 - val_loss: 0.0140 - val_acc: 0.3074\n",
      "Epoch 582/600\n",
      "10976/10976 [==============================] - 5s 440us/step - loss: 0.0142 - acc: 0.3710 - val_loss: 0.0134 - val_acc: 0.4428\n",
      "Epoch 583/600\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0143 - acc: 0.3729 - val_loss: 0.0142 - val_acc: 0.3040\n",
      "Epoch 584/600\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0143 - acc: 0.3625 - val_loss: 0.0142 - val_acc: 0.4472\n",
      "Epoch 585/600\n",
      "10976/10976 [==============================] - 5s 452us/step - loss: 0.0142 - acc: 0.3701 - val_loss: 0.0150 - val_acc: 0.3033\n",
      "Epoch 586/600\n",
      "10976/10976 [==============================] - 5s 460us/step - loss: 0.0142 - acc: 0.3650 - val_loss: 0.0150 - val_acc: 0.4310\n",
      "Epoch 587/600\n",
      "10976/10976 [==============================] - 5s 448us/step - loss: 0.0142 - acc: 0.3740 - val_loss: 0.0142 - val_acc: 0.3030\n",
      "Epoch 588/600\n",
      "10976/10976 [==============================] - 5s 438us/step - loss: 0.0143 - acc: 0.3647 - val_loss: 0.0142 - val_acc: 0.4438\n",
      "Epoch 589/600\n",
      "10976/10976 [==============================] - 5s 442us/step - loss: 0.0141 - acc: 0.3682 - val_loss: 0.0140 - val_acc: 0.3046\n",
      "Epoch 590/600\n",
      "10976/10976 [==============================] - 5s 433us/step - loss: 0.0143 - acc: 0.3702 - val_loss: 0.0136 - val_acc: 0.4398\n",
      "Epoch 591/600\n",
      "10976/10976 [==============================] - 5s 434us/step - loss: 0.0142 - acc: 0.3707 - val_loss: 0.0132 - val_acc: 0.3096\n",
      "Epoch 592/600\n",
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0142 - acc: 0.3649 - val_loss: 0.0129 - val_acc: 0.4414\n",
      "Epoch 593/600\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0142 - acc: 0.3633 - val_loss: 0.0136 - val_acc: 0.3140\n",
      "Epoch 594/600\n",
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0142 - acc: 0.3603 - val_loss: 0.0140 - val_acc: 0.4760\n",
      "Epoch 595/600\n",
      "10976/10976 [==============================] - 5s 435us/step - loss: 0.0143 - acc: 0.3736 - val_loss: 0.0139 - val_acc: 0.3170\n",
      "Epoch 596/600\n",
      "10976/10976 [==============================] - 5s 434us/step - loss: 0.0142 - acc: 0.3685 - val_loss: 0.0144 - val_acc: 0.4453\n",
      "Epoch 597/600\n",
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0141 - acc: 0.3714 - val_loss: 0.0142 - val_acc: 0.3069\n",
      "Epoch 598/600\n",
      "10976/10976 [==============================] - 5s 436us/step - loss: 0.0142 - acc: 0.3558 - val_loss: 0.0134 - val_acc: 0.4418\n",
      "Epoch 599/600\n",
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0142 - acc: 0.3689 - val_loss: 0.0141 - val_acc: 0.2853\n",
      "Epoch 600/600\n",
      "10976/10976 [==============================] - 5s 437us/step - loss: 0.0141 - acc: 0.3695 - val_loss: 0.0140 - val_acc: 0.4462\n",
      "start training round 30\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 601/620\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0185 - acc: 0.4417 - val_loss: 0.0168 - val_acc: 0.3987\n",
      "Epoch 602/620\n",
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0186 - acc: 0.4408 - val_loss: 0.0175 - val_acc: 0.4951\n",
      "Epoch 603/620\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0184 - acc: 0.4371 - val_loss: 0.0171 - val_acc: 0.3920\n",
      "Epoch 604/620\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0184 - acc: 0.4383 - val_loss: 0.0198 - val_acc: 0.4982\n",
      "Epoch 605/620\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0187 - acc: 0.4386 - val_loss: 0.0173 - val_acc: 0.3966\n",
      "Epoch 606/620\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0184 - acc: 0.4416 - val_loss: 0.0200 - val_acc: 0.4991\n",
      "Epoch 607/620\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0187 - acc: 0.4430 - val_loss: 0.0175 - val_acc: 0.3884\n",
      "Epoch 608/620\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0185 - acc: 0.4401 - val_loss: 0.0178 - val_acc: 0.4976\n",
      "Epoch 609/620\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0186 - acc: 0.4371 - val_loss: 0.0171 - val_acc: 0.3938\n",
      "Epoch 610/620\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0186 - acc: 0.4358 - val_loss: 0.0187 - val_acc: 0.4976\n",
      "Epoch 611/620\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0186 - acc: 0.4396 - val_loss: 0.0177 - val_acc: 0.3875\n",
      "Epoch 612/620\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0182 - acc: 0.4362 - val_loss: 0.0196 - val_acc: 0.4982\n",
      "Epoch 613/620\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0186 - acc: 0.4389 - val_loss: 0.0168 - val_acc: 0.3899\n",
      "Epoch 614/620\n",
      "10976/10976 [==============================] - 5s 434us/step - loss: 0.0188 - acc: 0.4368 - val_loss: 0.0188 - val_acc: 0.4951\n",
      "Epoch 615/620\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0188 - acc: 0.4424 - val_loss: 0.0166 - val_acc: 0.3936\n",
      "Epoch 616/620\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0186 - acc: 0.4351 - val_loss: 0.0201 - val_acc: 0.4983\n",
      "Epoch 617/620\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0186 - acc: 0.4391 - val_loss: 0.0179 - val_acc: 0.3875\n",
      "Epoch 618/620\n",
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0187 - acc: 0.4376 - val_loss: 0.0172 - val_acc: 0.4768\n",
      "Epoch 619/620\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0185 - acc: 0.4382 - val_loss: 0.0172 - val_acc: 0.3884\n",
      "Epoch 620/620\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0185 - acc: 0.4356 - val_loss: 0.0186 - val_acc: 0.4993\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 601/620\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0212 - acc: 0.5220 - val_loss: 0.0212 - val_acc: 0.5769\n",
      "Epoch 602/620\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0214 - acc: 0.5079 - val_loss: 0.0225 - val_acc: 0.4420\n",
      "Epoch 603/620\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0215 - acc: 0.5085 - val_loss: 0.0217 - val_acc: 0.5686\n",
      "Epoch 604/620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0213 - acc: 0.5087 - val_loss: 0.0221 - val_acc: 0.4109\n",
      "Epoch 605/620\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0215 - acc: 0.4723 - val_loss: 0.0208 - val_acc: 0.5801\n",
      "Epoch 606/620\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0215 - acc: 0.4747 - val_loss: 0.0217 - val_acc: 0.4194\n",
      "Epoch 607/620\n",
      "10976/10976 [==============================] - 5s 433us/step - loss: 0.0214 - acc: 0.4769 - val_loss: 0.0219 - val_acc: 0.5432\n",
      "Epoch 608/620\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0214 - acc: 0.4773 - val_loss: 0.0218 - val_acc: 0.4930\n",
      "Epoch 609/620\n",
      "10976/10976 [==============================] - 5s 434us/step - loss: 0.0214 - acc: 0.4837 - val_loss: 0.0214 - val_acc: 0.4749\n",
      "Epoch 610/620\n",
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0215 - acc: 0.4823 - val_loss: 0.0221 - val_acc: 0.4720\n",
      "Epoch 611/620\n",
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0215 - acc: 0.4813 - val_loss: 0.0215 - val_acc: 0.4656\n",
      "Epoch 612/620\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0213 - acc: 0.4822 - val_loss: 0.0223 - val_acc: 0.5161\n",
      "Epoch 613/620\n",
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0214 - acc: 0.4828 - val_loss: 0.0221 - val_acc: 0.4657\n",
      "Epoch 614/620\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0215 - acc: 0.4833 - val_loss: 0.0218 - val_acc: 0.5568\n",
      "Epoch 615/620\n",
      "10976/10976 [==============================] - 5s 438us/step - loss: 0.0213 - acc: 0.4806 - val_loss: 0.0211 - val_acc: 0.4704\n",
      "Epoch 616/620\n",
      "10976/10976 [==============================] - 5s 456us/step - loss: 0.0214 - acc: 0.4835 - val_loss: 0.0223 - val_acc: 0.4988\n",
      "Epoch 617/620\n",
      "10976/10976 [==============================] - 5s 459us/step - loss: 0.0217 - acc: 0.5375 - val_loss: 0.0197 - val_acc: 0.5812\n",
      "Epoch 618/620\n",
      "10976/10976 [==============================] - 5s 439us/step - loss: 0.0219 - acc: 0.5456 - val_loss: 0.0237 - val_acc: 0.5000\n",
      "Epoch 619/620\n",
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0216 - acc: 0.5404 - val_loss: 0.0213 - val_acc: 0.5576\n",
      "Epoch 620/620\n",
      "10976/10976 [==============================] - 5s 437us/step - loss: 0.0212 - acc: 0.4868 - val_loss: 0.0215 - val_acc: 0.4934\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 601/620\n",
      "10976/10976 [==============================] - 5s 455us/step - loss: 0.0170 - acc: 0.4020 - val_loss: 0.0229 - val_acc: 0.3058\n",
      "Epoch 602/620\n",
      "10976/10976 [==============================] - 5s 434us/step - loss: 0.0173 - acc: 0.4347 - val_loss: 0.0110 - val_acc: 0.4579\n",
      "Epoch 603/620\n",
      "10976/10976 [==============================] - 5s 438us/step - loss: 0.0171 - acc: 0.4291 - val_loss: 0.0223 - val_acc: 0.4282\n",
      "Epoch 604/620\n",
      "10976/10976 [==============================] - 5s 446us/step - loss: 0.0171 - acc: 0.4133 - val_loss: 0.0124 - val_acc: 0.3045\n",
      "Epoch 605/620\n",
      "10976/10976 [==============================] - 5s 446us/step - loss: 0.0171 - acc: 0.3820 - val_loss: 0.0232 - val_acc: 0.3078\n",
      "Epoch 606/620\n",
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0173 - acc: 0.4019 - val_loss: 0.0102 - val_acc: 0.5339\n",
      "Epoch 607/620\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0168 - acc: 0.3957 - val_loss: 0.0239 - val_acc: 0.4477\n",
      "Epoch 608/620\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0176 - acc: 0.3919 - val_loss: 0.0102 - val_acc: 0.5328\n",
      "Epoch 609/620\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0170 - acc: 0.4021 - val_loss: 0.0235 - val_acc: 0.3104\n",
      "Epoch 610/620\n",
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0172 - acc: 0.3977 - val_loss: 0.0107 - val_acc: 0.5277\n",
      "Epoch 611/620\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0170 - acc: 0.3866 - val_loss: 0.0232 - val_acc: 0.4444\n",
      "Epoch 612/620\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0172 - acc: 0.3942 - val_loss: 0.0113 - val_acc: 0.4463\n",
      "Epoch 613/620\n",
      "10976/10976 [==============================] - 5s 434us/step - loss: 0.0170 - acc: 0.4106 - val_loss: 0.0247 - val_acc: 0.3228\n",
      "Epoch 614/620\n",
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0170 - acc: 0.4462 - val_loss: 0.0111 - val_acc: 0.5293\n",
      "Epoch 615/620\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0170 - acc: 0.4077 - val_loss: 0.0234 - val_acc: 0.3083\n",
      "Epoch 616/620\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0173 - acc: 0.3780 - val_loss: 0.0103 - val_acc: 0.4497\n",
      "Epoch 617/620\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0169 - acc: 0.3662 - val_loss: 0.0237 - val_acc: 0.3095\n",
      "Epoch 618/620\n",
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0173 - acc: 0.3771 - val_loss: 0.0107 - val_acc: 0.4567\n",
      "Epoch 619/620\n",
      "10976/10976 [==============================] - 5s 451us/step - loss: 0.0170 - acc: 0.4245 - val_loss: 0.0224 - val_acc: 0.3100\n",
      "Epoch 620/620\n",
      "10976/10976 [==============================] - 5s 444us/step - loss: 0.0173 - acc: 0.3622 - val_loss: 0.0131 - val_acc: 0.4571\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 601/620\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0142 - acc: 0.3664 - val_loss: 0.0141 - val_acc: 0.3162\n",
      "Epoch 602/620\n",
      "10976/10976 [==============================] - 5s 439us/step - loss: 0.0141 - acc: 0.3633 - val_loss: 0.0139 - val_acc: 0.4456\n",
      "Epoch 603/620\n",
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0142 - acc: 0.3703 - val_loss: 0.0138 - val_acc: 0.3082\n",
      "Epoch 604/620\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0141 - acc: 0.3710 - val_loss: 0.0139 - val_acc: 0.4410\n",
      "Epoch 605/620\n",
      "10976/10976 [==============================] - 5s 435us/step - loss: 0.0142 - acc: 0.3750 - val_loss: 0.0138 - val_acc: 0.3171\n",
      "Epoch 606/620\n",
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0142 - acc: 0.3664 - val_loss: 0.0138 - val_acc: 0.4392\n",
      "Epoch 607/620\n",
      "10976/10976 [==============================] - 5s 441us/step - loss: 0.0143 - acc: 0.3685 - val_loss: 0.0146 - val_acc: 0.3087\n",
      "Epoch 608/620\n",
      "10976/10976 [==============================] - 5s 469us/step - loss: 0.0143 - acc: 0.3685 - val_loss: 0.0138 - val_acc: 0.4359\n",
      "Epoch 609/620\n",
      "10976/10976 [==============================] - 5s 448us/step - loss: 0.0142 - acc: 0.3729 - val_loss: 0.0149 - val_acc: 0.2943\n",
      "Epoch 610/620\n",
      "10976/10976 [==============================] - 5s 442us/step - loss: 0.0143 - acc: 0.3652 - val_loss: 0.0143 - val_acc: 0.4409\n",
      "Epoch 611/620\n",
      "10976/10976 [==============================] - 5s 433us/step - loss: 0.0142 - acc: 0.3719 - val_loss: 0.0143 - val_acc: 0.3037\n",
      "Epoch 612/620\n",
      "10976/10976 [==============================] - 5s 433us/step - loss: 0.0142 - acc: 0.3705 - val_loss: 0.0145 - val_acc: 0.4447\n",
      "Epoch 613/620\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0141 - acc: 0.3681 - val_loss: 0.0141 - val_acc: 0.2844\n",
      "Epoch 614/620\n",
      "10976/10976 [==============================] - 5s 435us/step - loss: 0.0143 - acc: 0.3642 - val_loss: 0.0142 - val_acc: 0.4489\n",
      "Epoch 615/620\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0142 - acc: 0.3673 - val_loss: 0.0142 - val_acc: 0.3006\n",
      "Epoch 616/620\n",
      "10976/10976 [==============================] - 5s 433us/step - loss: 0.0142 - acc: 0.3715 - val_loss: 0.0141 - val_acc: 0.4438\n",
      "Epoch 617/620\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0141 - acc: 0.3698 - val_loss: 0.0148 - val_acc: 0.3005\n",
      "Epoch 618/620\n",
      "10976/10976 [==============================] - 5s 433us/step - loss: 0.0142 - acc: 0.3709 - val_loss: 0.0136 - val_acc: 0.4772\n",
      "Epoch 619/620\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0141 - acc: 0.3680 - val_loss: 0.0147 - val_acc: 0.3050\n",
      "Epoch 620/620\n",
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0143 - acc: 0.3746 - val_loss: 0.0154 - val_acc: 0.4386\n",
      "start training round 31\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 621/640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0184 - acc: 0.4389 - val_loss: 0.0169 - val_acc: 0.3973\n",
      "Epoch 622/640\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0186 - acc: 0.4384 - val_loss: 0.0201 - val_acc: 0.4983\n",
      "Epoch 623/640\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0185 - acc: 0.4371 - val_loss: 0.0182 - val_acc: 0.3855\n",
      "Epoch 624/640\n",
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0184 - acc: 0.4387 - val_loss: 0.0198 - val_acc: 0.4989\n",
      "Epoch 625/640\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0186 - acc: 0.4394 - val_loss: 0.0175 - val_acc: 0.3908\n",
      "Epoch 626/640\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0183 - acc: 0.4388 - val_loss: 0.0196 - val_acc: 0.4983\n",
      "Epoch 627/640\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0183 - acc: 0.4406 - val_loss: 0.0181 - val_acc: 0.3890\n",
      "Epoch 628/640\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0187 - acc: 0.4381 - val_loss: 0.0198 - val_acc: 0.4958\n",
      "Epoch 629/640\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0186 - acc: 0.4398 - val_loss: 0.0184 - val_acc: 0.3842\n",
      "Epoch 630/640\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0184 - acc: 0.4379 - val_loss: 0.0197 - val_acc: 0.4972\n",
      "Epoch 631/640\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0187 - acc: 0.4368 - val_loss: 0.0179 - val_acc: 0.3837\n",
      "Epoch 632/640\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0184 - acc: 0.4371 - val_loss: 0.0196 - val_acc: 0.4981\n",
      "Epoch 633/640\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0182 - acc: 0.4383 - val_loss: 0.0167 - val_acc: 0.3939\n",
      "Epoch 634/640\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0181 - acc: 0.4384 - val_loss: 0.0198 - val_acc: 0.4924\n",
      "Epoch 635/640\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0184 - acc: 0.4423 - val_loss: 0.0176 - val_acc: 0.3855\n",
      "Epoch 636/640\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0186 - acc: 0.4353 - val_loss: 0.0198 - val_acc: 0.4971\n",
      "Epoch 637/640\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0188 - acc: 0.4387 - val_loss: 0.0183 - val_acc: 0.3837\n",
      "Epoch 638/640\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0185 - acc: 0.4373 - val_loss: 0.0190 - val_acc: 0.4975\n",
      "Epoch 639/640\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0186 - acc: 0.4419 - val_loss: 0.0184 - val_acc: 0.3860\n",
      "Epoch 640/640\n",
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0183 - acc: 0.4393 - val_loss: 0.0195 - val_acc: 0.4979\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 621/640\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0213 - acc: 0.4904 - val_loss: 0.0215 - val_acc: 0.4399\n",
      "Epoch 622/640\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0211 - acc: 0.5130 - val_loss: 0.0208 - val_acc: 0.5861\n",
      "Epoch 623/640\n",
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0218 - acc: 0.5405 - val_loss: 0.0214 - val_acc: 0.5138\n",
      "Epoch 624/640\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0215 - acc: 0.5412 - val_loss: 0.0216 - val_acc: 0.5785\n",
      "Epoch 625/640\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0221 - acc: 0.5353 - val_loss: 0.0207 - val_acc: 0.5081\n",
      "Epoch 626/640\n",
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0217 - acc: 0.5378 - val_loss: 0.0222 - val_acc: 0.5773\n",
      "Epoch 627/640\n",
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0219 - acc: 0.5446 - val_loss: 0.0194 - val_acc: 0.5115\n",
      "Epoch 628/640\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0215 - acc: 0.4985 - val_loss: 0.0222 - val_acc: 0.5044\n",
      "Epoch 629/640\n",
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0215 - acc: 0.5236 - val_loss: 0.0222 - val_acc: 0.5667\n",
      "Epoch 630/640\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0214 - acc: 0.5126 - val_loss: 0.0199 - val_acc: 0.5248\n",
      "Epoch 631/640\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0210 - acc: 0.5413 - val_loss: 0.0205 - val_acc: 0.5527\n",
      "Epoch 632/640\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0215 - acc: 0.5410 - val_loss: 0.0235 - val_acc: 0.5440\n",
      "Epoch 633/640\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0215 - acc: 0.5428 - val_loss: 0.0199 - val_acc: 0.5755\n",
      "Epoch 634/640\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0213 - acc: 0.5175 - val_loss: 0.0214 - val_acc: 0.4674\n",
      "Epoch 635/640\n",
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0213 - acc: 0.5093 - val_loss: 0.0210 - val_acc: 0.5695\n",
      "Epoch 636/640\n",
      "10976/10976 [==============================] - 5s 434us/step - loss: 0.0214 - acc: 0.5333 - val_loss: 0.0223 - val_acc: 0.5224\n",
      "Epoch 637/640\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0212 - acc: 0.5070 - val_loss: 0.0208 - val_acc: 0.4778\n",
      "Epoch 638/640\n",
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0213 - acc: 0.4833 - val_loss: 0.0213 - val_acc: 0.4602\n",
      "Epoch 639/640\n",
      "10976/10976 [==============================] - 5s 444us/step - loss: 0.0213 - acc: 0.4845 - val_loss: 0.0214 - val_acc: 0.4767\n",
      "Epoch 640/640\n",
      "10976/10976 [==============================] - 5s 434us/step - loss: 0.0215 - acc: 0.4843 - val_loss: 0.0214 - val_acc: 0.5212\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 621/640\n",
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0170 - acc: 0.4045 - val_loss: 0.0239 - val_acc: 0.3150\n",
      "Epoch 622/640\n",
      "10976/10976 [==============================] - 5s 436us/step - loss: 0.0172 - acc: 0.3976 - val_loss: 0.0103 - val_acc: 0.4575\n",
      "Epoch 623/640\n",
      "10976/10976 [==============================] - 5s 436us/step - loss: 0.0168 - acc: 0.4155 - val_loss: 0.0241 - val_acc: 0.5289\n",
      "Epoch 624/640\n",
      "10976/10976 [==============================] - 5s 445us/step - loss: 0.0171 - acc: 0.4132 - val_loss: 0.0105 - val_acc: 0.5343\n",
      "Epoch 625/640\n",
      "10976/10976 [==============================] - 5s 436us/step - loss: 0.0168 - acc: 0.3935 - val_loss: 0.0232 - val_acc: 0.3028\n",
      "Epoch 626/640\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0172 - acc: 0.4072 - val_loss: 0.0107 - val_acc: 0.4581\n",
      "Epoch 627/640\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0169 - acc: 0.4249 - val_loss: 0.0241 - val_acc: 0.3241\n",
      "Epoch 628/640\n",
      "10976/10976 [==============================] - 5s 460us/step - loss: 0.0173 - acc: 0.3797 - val_loss: 0.0095 - val_acc: 0.4575\n",
      "Epoch 629/640\n",
      "10976/10976 [==============================] - 5s 469us/step - loss: 0.0172 - acc: 0.4105 - val_loss: 0.0227 - val_acc: 0.4396\n",
      "Epoch 630/640\n",
      "10976/10976 [==============================] - 5s 475us/step - loss: 0.0171 - acc: 0.3977 - val_loss: 0.0101 - val_acc: 0.4584\n",
      "Epoch 631/640\n",
      "10976/10976 [==============================] - 5s 446us/step - loss: 0.0168 - acc: 0.3708 - val_loss: 0.0218 - val_acc: 0.3079\n",
      "Epoch 632/640\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0171 - acc: 0.3719 - val_loss: 0.0111 - val_acc: 0.5308\n",
      "Epoch 633/640\n",
      "10976/10976 [==============================] - 5s 435us/step - loss: 0.0170 - acc: 0.3597 - val_loss: 0.0235 - val_acc: 0.3107\n",
      "Epoch 634/640\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0172 - acc: 0.4149 - val_loss: 0.0114 - val_acc: 0.5302\n",
      "Epoch 635/640\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0168 - acc: 0.3770 - val_loss: 0.0221 - val_acc: 0.3056\n",
      "Epoch 636/640\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0171 - acc: 0.3744 - val_loss: 0.0142 - val_acc: 0.5333\n",
      "Epoch 637/640\n",
      "10976/10976 [==============================] - 5s 433us/step - loss: 0.0171 - acc: 0.4190 - val_loss: 0.0234 - val_acc: 0.3102\n",
      "Epoch 638/640\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0172 - acc: 0.3832 - val_loss: 0.0113 - val_acc: 0.4553\n",
      "Epoch 639/640\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0170 - acc: 0.4154 - val_loss: 0.0238 - val_acc: 0.3234\n",
      "Epoch 640/640\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0172 - acc: 0.4006 - val_loss: 0.0103 - val_acc: 0.4512\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 621/640\n",
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0142 - acc: 0.3764 - val_loss: 0.0138 - val_acc: 0.3086\n",
      "Epoch 622/640\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0142 - acc: 0.3673 - val_loss: 0.0141 - val_acc: 0.4777\n",
      "Epoch 623/640\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0143 - acc: 0.3708 - val_loss: 0.0146 - val_acc: 0.2980\n",
      "Epoch 624/640\n",
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0142 - acc: 0.3669 - val_loss: 0.0135 - val_acc: 0.4434\n",
      "Epoch 625/640\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0141 - acc: 0.3670 - val_loss: 0.0143 - val_acc: 0.3115\n",
      "Epoch 626/640\n",
      "10976/10976 [==============================] - 5s 433us/step - loss: 0.0142 - acc: 0.3662 - val_loss: 0.0141 - val_acc: 0.4486\n",
      "Epoch 627/640\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0142 - acc: 0.3690 - val_loss: 0.0134 - val_acc: 0.3203\n",
      "Epoch 628/640\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0142 - acc: 0.3695 - val_loss: 0.0136 - val_acc: 0.4773\n",
      "Epoch 629/640\n",
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0140 - acc: 0.3726 - val_loss: 0.0146 - val_acc: 0.2950\n",
      "Epoch 630/640\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0142 - acc: 0.3614 - val_loss: 0.0132 - val_acc: 0.4558\n",
      "Epoch 631/640\n",
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0140 - acc: 0.3733 - val_loss: 0.0147 - val_acc: 0.3072\n",
      "Epoch 632/640\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0142 - acc: 0.3702 - val_loss: 0.0137 - val_acc: 0.4367\n",
      "Epoch 633/640\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0141 - acc: 0.3729 - val_loss: 0.0146 - val_acc: 0.3044\n",
      "Epoch 634/640\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0141 - acc: 0.3684 - val_loss: 0.0143 - val_acc: 0.4545\n",
      "Epoch 635/640\n",
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0142 - acc: 0.3580 - val_loss: 0.0136 - val_acc: 0.3122\n",
      "Epoch 636/640\n",
      "10976/10976 [==============================] - 5s 433us/step - loss: 0.0143 - acc: 0.3695 - val_loss: 0.0144 - val_acc: 0.4463\n",
      "Epoch 637/640\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0142 - acc: 0.3720 - val_loss: 0.0147 - val_acc: 0.3003\n",
      "Epoch 638/640\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0142 - acc: 0.3701 - val_loss: 0.0136 - val_acc: 0.4507\n",
      "Epoch 639/640\n",
      "10976/10976 [==============================] - 5s 443us/step - loss: 0.0141 - acc: 0.3737 - val_loss: 0.0142 - val_acc: 0.3086\n",
      "Epoch 640/640\n",
      "10976/10976 [==============================] - 5s 433us/step - loss: 0.0142 - acc: 0.3618 - val_loss: 0.0141 - val_acc: 0.4452\n",
      "start training round 32\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 641/660\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0186 - acc: 0.4415 - val_loss: 0.0156 - val_acc: 0.4069\n",
      "Epoch 642/660\n",
      "10976/10976 [==============================] - 5s 433us/step - loss: 0.0184 - acc: 0.4406 - val_loss: 0.0198 - val_acc: 0.4978\n",
      "Epoch 643/660\n",
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0185 - acc: 0.4395 - val_loss: 0.0194 - val_acc: 0.3785\n",
      "Epoch 644/660\n",
      "10976/10976 [==============================] - 5s 455us/step - loss: 0.0185 - acc: 0.4390 - val_loss: 0.0171 - val_acc: 0.4752\n",
      "Epoch 645/660\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0184 - acc: 0.4410 - val_loss: 0.0173 - val_acc: 0.3952\n",
      "Epoch 646/660\n",
      "10976/10976 [==============================] - 5s 434us/step - loss: 0.0188 - acc: 0.4343 - val_loss: 0.0197 - val_acc: 0.4959\n",
      "Epoch 647/660\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0185 - acc: 0.4371 - val_loss: 0.0172 - val_acc: 0.3927\n",
      "Epoch 648/660\n",
      "10976/10976 [==============================] - 5s 441us/step - loss: 0.0185 - acc: 0.4336 - val_loss: 0.0194 - val_acc: 0.4979\n",
      "Epoch 649/660\n",
      "10976/10976 [==============================] - 5s 447us/step - loss: 0.0183 - acc: 0.4397 - val_loss: 0.0164 - val_acc: 0.3961\n",
      "Epoch 650/660\n",
      "10976/10976 [==============================] - 5s 433us/step - loss: 0.0184 - acc: 0.4350 - val_loss: 0.0204 - val_acc: 0.4983\n",
      "Epoch 651/660\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0182 - acc: 0.4422 - val_loss: 0.0179 - val_acc: 0.3825\n",
      "Epoch 652/660\n",
      "10976/10976 [==============================] - 5s 434us/step - loss: 0.0185 - acc: 0.4374 - val_loss: 0.0202 - val_acc: 0.4959\n",
      "Epoch 653/660\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0185 - acc: 0.4391 - val_loss: 0.0186 - val_acc: 0.3790\n",
      "Epoch 654/660\n",
      "10976/10976 [==============================] - 5s 447us/step - loss: 0.0185 - acc: 0.4406 - val_loss: 0.0195 - val_acc: 0.4950\n",
      "Epoch 655/660\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0184 - acc: 0.4407 - val_loss: 0.0175 - val_acc: 0.3920\n",
      "Epoch 656/660\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0186 - acc: 0.4379 - val_loss: 0.0197 - val_acc: 0.4923\n",
      "Epoch 657/660\n",
      "10976/10976 [==============================] - 5s 434us/step - loss: 0.0184 - acc: 0.4398 - val_loss: 0.0171 - val_acc: 0.3947\n",
      "Epoch 658/660\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0184 - acc: 0.4388 - val_loss: 0.0197 - val_acc: 0.4947\n",
      "Epoch 659/660\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0185 - acc: 0.4391 - val_loss: 0.0171 - val_acc: 0.3927\n",
      "Epoch 660/660\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0184 - acc: 0.4365 - val_loss: 0.0184 - val_acc: 0.4913\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 641/660\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0215 - acc: 0.5380 - val_loss: 0.0209 - val_acc: 0.5886\n",
      "Epoch 642/660\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0216 - acc: 0.5445 - val_loss: 0.0222 - val_acc: 0.5405\n",
      "Epoch 643/660\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0214 - acc: 0.5163 - val_loss: 0.0208 - val_acc: 0.5691\n",
      "Epoch 644/660\n",
      "10976/10976 [==============================] - 5s 434us/step - loss: 0.0213 - acc: 0.5228 - val_loss: 0.0206 - val_acc: 0.5785\n",
      "Epoch 645/660\n",
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0221 - acc: 0.5398 - val_loss: 0.0220 - val_acc: 0.5613\n",
      "Epoch 646/660\n",
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0216 - acc: 0.5472 - val_loss: 0.0205 - val_acc: 0.5870\n",
      "Epoch 647/660\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0215 - acc: 0.5464 - val_loss: 0.0224 - val_acc: 0.5544\n",
      "Epoch 648/660\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0218 - acc: 0.5436 - val_loss: 0.0211 - val_acc: 0.5805\n",
      "Epoch 649/660\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0215 - acc: 0.5423 - val_loss: 0.0229 - val_acc: 0.5188\n",
      "Epoch 650/660\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0215 - acc: 0.5396 - val_loss: 0.0194 - val_acc: 0.5805\n",
      "Epoch 651/660\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0215 - acc: 0.5418 - val_loss: 0.0234 - val_acc: 0.5179\n",
      "Epoch 652/660\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0208 - acc: 0.5033 - val_loss: 0.0212 - val_acc: 0.4696\n",
      "Epoch 653/660\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0215 - acc: 0.4850 - val_loss: 0.0219 - val_acc: 0.5108\n",
      "Epoch 654/660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0216 - acc: 0.4819 - val_loss: 0.0213 - val_acc: 0.4729\n",
      "Epoch 655/660\n",
      "10976/10976 [==============================] - 5s 434us/step - loss: 0.0214 - acc: 0.4875 - val_loss: 0.0216 - val_acc: 0.5109\n",
      "Epoch 656/660\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0213 - acc: 0.4887 - val_loss: 0.0218 - val_acc: 0.4782\n",
      "Epoch 657/660\n",
      "10976/10976 [==============================] - 5s 433us/step - loss: 0.0214 - acc: 0.4811 - val_loss: 0.0225 - val_acc: 0.5169\n",
      "Epoch 658/660\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0213 - acc: 0.4769 - val_loss: 0.0207 - val_acc: 0.4722\n",
      "Epoch 659/660\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0213 - acc: 0.4805 - val_loss: 0.0221 - val_acc: 0.4167\n",
      "Epoch 660/660\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0212 - acc: 0.4790 - val_loss: 0.0209 - val_acc: 0.5301\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 641/660\n",
      "10976/10976 [==============================] - 5s 435us/step - loss: 0.0167 - acc: 0.4429 - val_loss: 0.0215 - val_acc: 0.2981\n",
      "Epoch 642/660\n",
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0172 - acc: 0.3891 - val_loss: 0.0141 - val_acc: 0.5306\n",
      "Epoch 643/660\n",
      "10976/10976 [==============================] - 5s 437us/step - loss: 0.0171 - acc: 0.4579 - val_loss: 0.0237 - val_acc: 0.3118\n",
      "Epoch 644/660\n",
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0172 - acc: 0.3796 - val_loss: 0.0102 - val_acc: 0.5311\n",
      "Epoch 645/660\n",
      "10976/10976 [==============================] - 5s 438us/step - loss: 0.0170 - acc: 0.4289 - val_loss: 0.0235 - val_acc: 0.3098\n",
      "Epoch 646/660\n",
      "10976/10976 [==============================] - 5s 443us/step - loss: 0.0170 - acc: 0.3853 - val_loss: 0.0104 - val_acc: 0.5307\n",
      "Epoch 647/660\n",
      "10976/10976 [==============================] - 5s 438us/step - loss: 0.0169 - acc: 0.4747 - val_loss: 0.0225 - val_acc: 0.5205\n",
      "Epoch 648/660\n",
      "10976/10976 [==============================] - 5s 450us/step - loss: 0.0172 - acc: 0.4077 - val_loss: 0.0108 - val_acc: 0.4612\n",
      "Epoch 649/660\n",
      "10976/10976 [==============================] - 5s 434us/step - loss: 0.0171 - acc: 0.3846 - val_loss: 0.0220 - val_acc: 0.2935\n",
      "Epoch 650/660\n",
      "10976/10976 [==============================] - 5s 436us/step - loss: 0.0169 - acc: 0.3852 - val_loss: 0.0114 - val_acc: 0.3159\n",
      "Epoch 651/660\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0168 - acc: 0.3750 - val_loss: 0.0245 - val_acc: 0.3157\n",
      "Epoch 652/660\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0171 - acc: 0.3859 - val_loss: 0.0107 - val_acc: 0.3204\n",
      "Epoch 653/660\n",
      "10976/10976 [==============================] - 5s 435us/step - loss: 0.0170 - acc: 0.3638 - val_loss: 0.0243 - val_acc: 0.3218\n",
      "Epoch 654/660\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0169 - acc: 0.4078 - val_loss: 0.0114 - val_acc: 0.5293\n",
      "Epoch 655/660\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0170 - acc: 0.4257 - val_loss: 0.0224 - val_acc: 0.3132\n",
      "Epoch 656/660\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0171 - acc: 0.3871 - val_loss: 0.0104 - val_acc: 0.3110\n",
      "Epoch 657/660\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0170 - acc: 0.3935 - val_loss: 0.0226 - val_acc: 0.3084\n",
      "Epoch 658/660\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0171 - acc: 0.3955 - val_loss: 0.0107 - val_acc: 0.4586\n",
      "Epoch 659/660\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0168 - acc: 0.3676 - val_loss: 0.0245 - val_acc: 0.3144\n",
      "Epoch 660/660\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0173 - acc: 0.4136 - val_loss: 0.0111 - val_acc: 0.4575\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 641/660\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0142 - acc: 0.3694 - val_loss: 0.0148 - val_acc: 0.2939\n",
      "Epoch 642/660\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0140 - acc: 0.3657 - val_loss: 0.0128 - val_acc: 0.4290\n",
      "Epoch 643/660\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0140 - acc: 0.3657 - val_loss: 0.0146 - val_acc: 0.2976\n",
      "Epoch 644/660\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0142 - acc: 0.3599 - val_loss: 0.0124 - val_acc: 0.4163\n",
      "Epoch 645/660\n",
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0141 - acc: 0.3704 - val_loss: 0.0142 - val_acc: 0.3061\n",
      "Epoch 646/660\n",
      "10976/10976 [==============================] - 5s 441us/step - loss: 0.0141 - acc: 0.3652 - val_loss: 0.0133 - val_acc: 0.4390\n",
      "Epoch 647/660\n",
      "10976/10976 [==============================] - 5s 439us/step - loss: 0.0140 - acc: 0.3670 - val_loss: 0.0141 - val_acc: 0.3096\n",
      "Epoch 648/660\n",
      "10976/10976 [==============================] - 5s 446us/step - loss: 0.0142 - acc: 0.3689 - val_loss: 0.0137 - val_acc: 0.4372\n",
      "Epoch 649/660\n",
      "10976/10976 [==============================] - 5s 435us/step - loss: 0.0140 - acc: 0.3750 - val_loss: 0.0146 - val_acc: 0.3025\n",
      "Epoch 650/660\n",
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0141 - acc: 0.3602 - val_loss: 0.0139 - val_acc: 0.4483\n",
      "Epoch 651/660\n",
      "10976/10976 [==============================] - 5s 472us/step - loss: 0.0141 - acc: 0.3701 - val_loss: 0.0141 - val_acc: 0.3029\n",
      "Epoch 652/660\n",
      "10976/10976 [==============================] - 5s 437us/step - loss: 0.0140 - acc: 0.3725 - val_loss: 0.0126 - val_acc: 0.4252\n",
      "Epoch 653/660\n",
      "10976/10976 [==============================] - 5s 436us/step - loss: 0.0141 - acc: 0.3656 - val_loss: 0.0137 - val_acc: 0.2759\n",
      "Epoch 654/660\n",
      "10976/10976 [==============================] - 5s 438us/step - loss: 0.0141 - acc: 0.3700 - val_loss: 0.0134 - val_acc: 0.4337\n",
      "Epoch 655/660\n",
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0140 - acc: 0.3674 - val_loss: 0.0136 - val_acc: 0.2957\n",
      "Epoch 656/660\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0141 - acc: 0.3716 - val_loss: 0.0139 - val_acc: 0.4434\n",
      "Epoch 657/660\n",
      "10976/10976 [==============================] - 5s 435us/step - loss: 0.0140 - acc: 0.3719 - val_loss: 0.0144 - val_acc: 0.2999\n",
      "Epoch 658/660\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0142 - acc: 0.3728 - val_loss: 0.0134 - val_acc: 0.4429\n",
      "Epoch 659/660\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0140 - acc: 0.3699 - val_loss: 0.0134 - val_acc: 0.3131\n",
      "Epoch 660/660\n",
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0141 - acc: 0.3720 - val_loss: 0.0142 - val_acc: 0.4442\n",
      "start training round 33\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 661/680\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0183 - acc: 0.4450 - val_loss: 0.0177 - val_acc: 0.3854\n",
      "Epoch 662/680\n",
      "10976/10976 [==============================] - 5s 435us/step - loss: 0.0183 - acc: 0.4388 - val_loss: 0.0197 - val_acc: 0.4975\n",
      "Epoch 663/680\n",
      "10976/10976 [==============================] - 5s 437us/step - loss: 0.0185 - acc: 0.4421 - val_loss: 0.0182 - val_acc: 0.3809\n",
      "Epoch 664/680\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0185 - acc: 0.4377 - val_loss: 0.0170 - val_acc: 0.4658\n",
      "Epoch 665/680\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0185 - acc: 0.4403 - val_loss: 0.0177 - val_acc: 0.3877\n",
      "Epoch 666/680\n",
      "10976/10976 [==============================] - 5s 433us/step - loss: 0.0185 - acc: 0.4391 - val_loss: 0.0186 - val_acc: 0.4878\n",
      "Epoch 667/680\n",
      "10976/10976 [==============================] - 5s 441us/step - loss: 0.0184 - acc: 0.4403 - val_loss: 0.0176 - val_acc: 0.3897\n",
      "Epoch 668/680\n",
      "10976/10976 [==============================] - 5s 443us/step - loss: 0.0185 - acc: 0.4427 - val_loss: 0.0197 - val_acc: 0.4967\n",
      "Epoch 669/680\n",
      "10976/10976 [==============================] - 5s 448us/step - loss: 0.0185 - acc: 0.4427 - val_loss: 0.0182 - val_acc: 0.3839\n",
      "Epoch 670/680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 5s 433us/step - loss: 0.0184 - acc: 0.4351 - val_loss: 0.0192 - val_acc: 0.4970\n",
      "Epoch 671/680\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0185 - acc: 0.4391 - val_loss: 0.0176 - val_acc: 0.3904\n",
      "Epoch 672/680\n",
      "10976/10976 [==============================] - 5s 445us/step - loss: 0.0183 - acc: 0.4394 - val_loss: 0.0195 - val_acc: 0.4982\n",
      "Epoch 673/680\n",
      "10976/10976 [==============================] - 5s 433us/step - loss: 0.0185 - acc: 0.4396 - val_loss: 0.0186 - val_acc: 0.3816\n",
      "Epoch 674/680\n",
      "10976/10976 [==============================] - 5s 436us/step - loss: 0.0184 - acc: 0.4402 - val_loss: 0.0170 - val_acc: 0.4783\n",
      "Epoch 675/680\n",
      "10976/10976 [==============================] - 5s 438us/step - loss: 0.0184 - acc: 0.4409 - val_loss: 0.0175 - val_acc: 0.3922\n",
      "Epoch 676/680\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0185 - acc: 0.4385 - val_loss: 0.0178 - val_acc: 0.4695\n",
      "Epoch 677/680\n",
      "10976/10976 [==============================] - 5s 442us/step - loss: 0.0184 - acc: 0.4421 - val_loss: 0.0172 - val_acc: 0.3891\n",
      "Epoch 678/680\n",
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0183 - acc: 0.4395 - val_loss: 0.0193 - val_acc: 0.5027\n",
      "Epoch 679/680\n",
      "10976/10976 [==============================] - 5s 444us/step - loss: 0.0187 - acc: 0.4396 - val_loss: 0.0170 - val_acc: 0.3872\n",
      "Epoch 680/680\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0182 - acc: 0.4366 - val_loss: 0.0171 - val_acc: 0.4755\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 661/680\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0214 - acc: 0.4930 - val_loss: 0.0216 - val_acc: 0.5501\n",
      "Epoch 662/680\n",
      "10976/10976 [==============================] - 5s 433us/step - loss: 0.0212 - acc: 0.5271 - val_loss: 0.0222 - val_acc: 0.4695\n",
      "Epoch 663/680\n",
      "10976/10976 [==============================] - 5s 451us/step - loss: 0.0213 - acc: 0.5101 - val_loss: 0.0228 - val_acc: 0.5644\n",
      "Epoch 664/680\n",
      "10976/10976 [==============================] - 5s 433us/step - loss: 0.0215 - acc: 0.5282 - val_loss: 0.0217 - val_acc: 0.5166\n",
      "Epoch 665/680\n",
      "10976/10976 [==============================] - 5s 443us/step - loss: 0.0217 - acc: 0.5475 - val_loss: 0.0204 - val_acc: 0.5888\n",
      "Epoch 666/680\n",
      "10976/10976 [==============================] - 5s 434us/step - loss: 0.0217 - acc: 0.5473 - val_loss: 0.0222 - val_acc: 0.5090\n",
      "Epoch 667/680\n",
      "10976/10976 [==============================] - 5s 438us/step - loss: 0.0217 - acc: 0.5459 - val_loss: 0.0202 - val_acc: 0.5590\n",
      "Epoch 668/680\n",
      "10976/10976 [==============================] - 5s 435us/step - loss: 0.0212 - acc: 0.5409 - val_loss: 0.0217 - val_acc: 0.5491\n",
      "Epoch 669/680\n",
      "10976/10976 [==============================] - 5s 434us/step - loss: 0.0214 - acc: 0.5372 - val_loss: 0.0214 - val_acc: 0.5519\n",
      "Epoch 670/680\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0212 - acc: 0.4801 - val_loss: 0.0222 - val_acc: 0.4145\n",
      "Epoch 671/680\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0214 - acc: 0.5255 - val_loss: 0.0202 - val_acc: 0.5158\n",
      "Epoch 672/680\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0212 - acc: 0.5442 - val_loss: 0.0232 - val_acc: 0.5880\n",
      "Epoch 673/680\n",
      "10976/10976 [==============================] - 5s 433us/step - loss: 0.0215 - acc: 0.5461 - val_loss: 0.0205 - val_acc: 0.5245\n",
      "Epoch 674/680\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0216 - acc: 0.5444 - val_loss: 0.0223 - val_acc: 0.5834\n",
      "Epoch 675/680\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0218 - acc: 0.5465 - val_loss: 0.0229 - val_acc: 0.5371\n",
      "Epoch 676/680\n",
      "10976/10976 [==============================] - 5s 436us/step - loss: 0.0216 - acc: 0.5449 - val_loss: 0.0198 - val_acc: 0.5891\n",
      "Epoch 677/680\n",
      "10976/10976 [==============================] - 5s 461us/step - loss: 0.0215 - acc: 0.5483 - val_loss: 0.0227 - val_acc: 0.5323\n",
      "Epoch 678/680\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0216 - acc: 0.5438 - val_loss: 0.0208 - val_acc: 0.5545\n",
      "Epoch 679/680\n",
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0212 - acc: 0.5459 - val_loss: 0.0205 - val_acc: 0.4808\n",
      "Epoch 680/680\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0212 - acc: 0.5375 - val_loss: 0.0204 - val_acc: 0.5748\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 661/680\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0169 - acc: 0.4303 - val_loss: 0.0229 - val_acc: 0.4614\n",
      "Epoch 662/680\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0171 - acc: 0.4527 - val_loss: 0.0123 - val_acc: 0.3081\n",
      "Epoch 663/680\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0170 - acc: 0.3952 - val_loss: 0.0229 - val_acc: 0.3098\n",
      "Epoch 664/680\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0171 - acc: 0.4214 - val_loss: 0.0114 - val_acc: 0.5350\n",
      "Epoch 665/680\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0170 - acc: 0.4076 - val_loss: 0.0243 - val_acc: 0.3148\n",
      "Epoch 666/680\n",
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0172 - acc: 0.4063 - val_loss: 0.0113 - val_acc: 0.3205\n",
      "Epoch 667/680\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0169 - acc: 0.3875 - val_loss: 0.0230 - val_acc: 0.5317\n",
      "Epoch 668/680\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0172 - acc: 0.4396 - val_loss: 0.0113 - val_acc: 0.3096\n",
      "Epoch 669/680\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0169 - acc: 0.4148 - val_loss: 0.0236 - val_acc: 0.3188\n",
      "Epoch 670/680\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0173 - acc: 0.4271 - val_loss: 0.0111 - val_acc: 0.4596\n",
      "Epoch 671/680\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0169 - acc: 0.4054 - val_loss: 0.0232 - val_acc: 0.3139\n",
      "Epoch 672/680\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0171 - acc: 0.3841 - val_loss: 0.0112 - val_acc: 0.4521\n",
      "Epoch 673/680\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0170 - acc: 0.4426 - val_loss: 0.0222 - val_acc: 0.3102\n",
      "Epoch 674/680\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0171 - acc: 0.3670 - val_loss: 0.0100 - val_acc: 0.5321\n",
      "Epoch 675/680\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0169 - acc: 0.3873 - val_loss: 0.0238 - val_acc: 0.5343\n",
      "Epoch 676/680\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0171 - acc: 0.4051 - val_loss: 0.0110 - val_acc: 0.3152\n",
      "Epoch 677/680\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0169 - acc: 0.3916 - val_loss: 0.0226 - val_acc: 0.3213\n",
      "Epoch 678/680\n",
      "10976/10976 [==============================] - -2s -205us/step - loss: 0.0173 - acc: 0.3798 - val_loss: 0.0112 - val_acc: 0.2994\n",
      "Epoch 679/680\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0169 - acc: 0.4105 - val_loss: 0.0225 - val_acc: 0.3224\n",
      "Epoch 680/680\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0169 - acc: 0.3933 - val_loss: 0.0102 - val_acc: 0.5309\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 661/680\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0141 - acc: 0.3743 - val_loss: 0.0128 - val_acc: 0.3240\n",
      "Epoch 662/680\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0142 - acc: 0.3694 - val_loss: 0.0138 - val_acc: 0.4422\n",
      "Epoch 663/680\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0141 - acc: 0.3745 - val_loss: 0.0132 - val_acc: 0.3131\n",
      "Epoch 664/680\n",
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0141 - acc: 0.3698 - val_loss: 0.0138 - val_acc: 0.4414\n",
      "Epoch 665/680\n",
      "10976/10976 [==============================] - 5s 448us/step - loss: 0.0140 - acc: 0.3709 - val_loss: 0.0153 - val_acc: 0.3001\n",
      "Epoch 666/680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 5s 443us/step - loss: 0.0141 - acc: 0.3676 - val_loss: 0.0146 - val_acc: 0.4477\n",
      "Epoch 667/680\n",
      "10976/10976 [==============================] - 5s 452us/step - loss: 0.0140 - acc: 0.3720 - val_loss: 0.0140 - val_acc: 0.3153\n",
      "Epoch 668/680\n",
      "10976/10976 [==============================] - 5s 439us/step - loss: 0.0139 - acc: 0.3656 - val_loss: 0.0130 - val_acc: 0.4395\n",
      "Epoch 669/680\n",
      "10976/10976 [==============================] - 5s 434us/step - loss: 0.0141 - acc: 0.3715 - val_loss: 0.0143 - val_acc: 0.2994\n",
      "Epoch 670/680\n",
      "10976/10976 [==============================] - 5s 438us/step - loss: 0.0140 - acc: 0.3717 - val_loss: 0.0140 - val_acc: 0.4486\n",
      "Epoch 671/680\n",
      "10976/10976 [==============================] - 5s 441us/step - loss: 0.0141 - acc: 0.3746 - val_loss: 0.0139 - val_acc: 0.3107\n",
      "Epoch 672/680\n",
      "10976/10976 [==============================] - 5s 433us/step - loss: 0.0141 - acc: 0.3771 - val_loss: 0.0139 - val_acc: 0.4399\n",
      "Epoch 673/680\n",
      "10976/10976 [==============================] - 5s 435us/step - loss: 0.0141 - acc: 0.3725 - val_loss: 0.0141 - val_acc: 0.3055\n",
      "Epoch 674/680\n",
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0141 - acc: 0.3643 - val_loss: 0.0132 - val_acc: 0.4403\n",
      "Epoch 675/680\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0141 - acc: 0.3696 - val_loss: 0.0145 - val_acc: 0.3057\n",
      "Epoch 676/680\n",
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0140 - acc: 0.3693 - val_loss: 0.0136 - val_acc: 0.4496\n",
      "Epoch 677/680\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0140 - acc: 0.3753 - val_loss: 0.0146 - val_acc: 0.3044\n",
      "Epoch 678/680\n",
      "10976/10976 [==============================] - 5s 434us/step - loss: 0.0141 - acc: 0.3710 - val_loss: 0.0138 - val_acc: 0.4467\n",
      "Epoch 679/680\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0140 - acc: 0.3766 - val_loss: 0.0143 - val_acc: 0.3094\n",
      "Epoch 680/680\n",
      "10976/10976 [==============================] - 5s 434us/step - loss: 0.0140 - acc: 0.3727 - val_loss: 0.0138 - val_acc: 0.4458\n",
      "start training round 34\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 681/700\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0187 - acc: 0.4361 - val_loss: 0.0173 - val_acc: 0.3884\n",
      "Epoch 682/700\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0184 - acc: 0.4369 - val_loss: 0.0195 - val_acc: 0.4922\n",
      "Epoch 683/700\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0184 - acc: 0.4420 - val_loss: 0.0185 - val_acc: 0.3839\n",
      "Epoch 684/700\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0184 - acc: 0.4390 - val_loss: 0.0198 - val_acc: 0.4942\n",
      "Epoch 685/700\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0183 - acc: 0.4419 - val_loss: 0.0179 - val_acc: 0.3932\n",
      "Epoch 686/700\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0185 - acc: 0.4421 - val_loss: 0.0185 - val_acc: 0.4960\n",
      "Epoch 687/700\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0184 - acc: 0.4408 - val_loss: 0.0176 - val_acc: 0.3864\n",
      "Epoch 688/700\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0184 - acc: 0.4370 - val_loss: 0.0200 - val_acc: 0.4989\n",
      "Epoch 689/700\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0185 - acc: 0.4400 - val_loss: 0.0193 - val_acc: 0.3804\n",
      "Epoch 690/700\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0183 - acc: 0.4407 - val_loss: 0.0200 - val_acc: 0.5001\n",
      "Epoch 691/700\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0185 - acc: 0.4423 - val_loss: 0.0176 - val_acc: 0.3897\n",
      "Epoch 692/700\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0184 - acc: 0.4428 - val_loss: 0.0193 - val_acc: 0.4874\n",
      "Epoch 693/700\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0183 - acc: 0.4396 - val_loss: 0.0177 - val_acc: 0.3888\n",
      "Epoch 694/700\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0183 - acc: 0.4385 - val_loss: 0.0195 - val_acc: 0.4971\n",
      "Epoch 695/700\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0183 - acc: 0.4437 - val_loss: 0.0167 - val_acc: 0.3946\n",
      "Epoch 696/700\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0183 - acc: 0.4375 - val_loss: 0.0185 - val_acc: 0.4787\n",
      "Epoch 697/700\n",
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0182 - acc: 0.4401 - val_loss: 0.0178 - val_acc: 0.3881\n",
      "Epoch 698/700\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0183 - acc: 0.4370 - val_loss: 0.0190 - val_acc: 0.4931\n",
      "Epoch 699/700\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0186 - acc: 0.4405 - val_loss: 0.0173 - val_acc: 0.3907\n",
      "Epoch 700/700\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0184 - acc: 0.4367 - val_loss: 0.0181 - val_acc: 0.4818\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 681/700\n",
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0209 - acc: 0.5154 - val_loss: 0.0220 - val_acc: 0.4635\n",
      "Epoch 682/700\n",
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0212 - acc: 0.4875 - val_loss: 0.0217 - val_acc: 0.4190\n",
      "Epoch 683/700\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0213 - acc: 0.4784 - val_loss: 0.0219 - val_acc: 0.5442\n",
      "Epoch 684/700\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0213 - acc: 0.4807 - val_loss: 0.0219 - val_acc: 0.4194\n",
      "Epoch 685/700\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0213 - acc: 0.4788 - val_loss: 0.0216 - val_acc: 0.5278\n",
      "Epoch 686/700\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0212 - acc: 0.4834 - val_loss: 0.0217 - val_acc: 0.4281\n",
      "Epoch 687/700\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0213 - acc: 0.4802 - val_loss: 0.0212 - val_acc: 0.5239\n",
      "Epoch 688/700\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0211 - acc: 0.4809 - val_loss: 0.0211 - val_acc: 0.5107\n",
      "Epoch 689/700\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0213 - acc: 0.4907 - val_loss: 0.0203 - val_acc: 0.4865\n",
      "Epoch 690/700\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0211 - acc: 0.4884 - val_loss: 0.0215 - val_acc: 0.4773\n",
      "Epoch 691/700\n",
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0210 - acc: 0.4820 - val_loss: 0.0216 - val_acc: 0.4789\n",
      "Epoch 692/700\n",
      "10976/10976 [==============================] - 5s 433us/step - loss: 0.0213 - acc: 0.4855 - val_loss: 0.0220 - val_acc: 0.4976\n",
      "Epoch 693/700\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0214 - acc: 0.4858 - val_loss: 0.0216 - val_acc: 0.4697\n",
      "Epoch 694/700\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0210 - acc: 0.4843 - val_loss: 0.0216 - val_acc: 0.4941\n",
      "Epoch 695/700\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0211 - acc: 0.4814 - val_loss: 0.0219 - val_acc: 0.4671\n",
      "Epoch 696/700\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0212 - acc: 0.4810 - val_loss: 0.0212 - val_acc: 0.4784\n",
      "Epoch 697/700\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0211 - acc: 0.4825 - val_loss: 0.0205 - val_acc: 0.4797\n",
      "Epoch 698/700\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0211 - acc: 0.4864 - val_loss: 0.0214 - val_acc: 0.4995\n",
      "Epoch 699/700\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0211 - acc: 0.4817 - val_loss: 0.0217 - val_acc: 0.5433\n",
      "Epoch 700/700\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0213 - acc: 0.4815 - val_loss: 0.0204 - val_acc: 0.4755\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 681/700\n",
      "10976/10976 [==============================] - 5s 433us/step - loss: 0.0170 - acc: 0.4241 - val_loss: 0.0226 - val_acc: 0.3098\n",
      "Epoch 682/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0172 - acc: 0.4158 - val_loss: 0.0092 - val_acc: 0.5376\n",
      "Epoch 683/700\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0170 - acc: 0.3715 - val_loss: 0.0237 - val_acc: 0.3226\n",
      "Epoch 684/700\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0172 - acc: 0.3846 - val_loss: 0.0102 - val_acc: 0.5344\n",
      "Epoch 685/700\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0170 - acc: 0.4110 - val_loss: 0.0228 - val_acc: 0.3190\n",
      "Epoch 686/700\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0171 - acc: 0.3725 - val_loss: 0.0107 - val_acc: 0.5319\n",
      "Epoch 687/700\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0170 - acc: 0.4103 - val_loss: 0.0237 - val_acc: 0.3234\n",
      "Epoch 688/700\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0170 - acc: 0.3993 - val_loss: 0.0095 - val_acc: 0.4624\n",
      "Epoch 689/700\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0168 - acc: 0.3910 - val_loss: 0.0235 - val_acc: 0.4577\n",
      "Epoch 690/700\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0171 - acc: 0.4196 - val_loss: 0.0110 - val_acc: 0.3252\n",
      "Epoch 691/700\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0169 - acc: 0.3925 - val_loss: 0.0241 - val_acc: 0.5335\n",
      "Epoch 692/700\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0172 - acc: 0.4049 - val_loss: 0.0104 - val_acc: 0.4390\n",
      "Epoch 693/700\n",
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0169 - acc: 0.4023 - val_loss: 0.0233 - val_acc: 0.3069\n",
      "Epoch 694/700\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0171 - acc: 0.3958 - val_loss: 0.0098 - val_acc: 0.4592\n",
      "Epoch 695/700\n",
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0168 - acc: 0.3661 - val_loss: 0.0232 - val_acc: 0.4466\n",
      "Epoch 696/700\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0169 - acc: 0.4275 - val_loss: 0.0122 - val_acc: 0.3106\n",
      "Epoch 697/700\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0170 - acc: 0.3815 - val_loss: 0.0229 - val_acc: 0.3050\n",
      "Epoch 698/700\n",
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0171 - acc: 0.3777 - val_loss: 0.0105 - val_acc: 0.3129\n",
      "Epoch 699/700\n",
      "10976/10976 [==============================] - 5s 438us/step - loss: 0.0168 - acc: 0.3944 - val_loss: 0.0233 - val_acc: 0.3082\n",
      "Epoch 700/700\n",
      "10976/10976 [==============================] - 5s 436us/step - loss: 0.0172 - acc: 0.4115 - val_loss: 0.0102 - val_acc: 0.4598\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 681/700\n",
      "10976/10976 [==============================] - 5s 446us/step - loss: 0.0140 - acc: 0.3687 - val_loss: 0.0143 - val_acc: 0.3095\n",
      "Epoch 682/700\n",
      "10976/10976 [==============================] - 5s 440us/step - loss: 0.0142 - acc: 0.3686 - val_loss: 0.0119 - val_acc: 0.4587\n",
      "Epoch 683/700\n",
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0140 - acc: 0.3610 - val_loss: 0.0144 - val_acc: 0.3103\n",
      "Epoch 684/700\n",
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0141 - acc: 0.3769 - val_loss: 0.0138 - val_acc: 0.4493\n",
      "Epoch 685/700\n",
      "10976/10976 [==============================] - 5s 436us/step - loss: 0.0140 - acc: 0.3702 - val_loss: 0.0147 - val_acc: 0.2944\n",
      "Epoch 686/700\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0140 - acc: 0.3671 - val_loss: 0.0139 - val_acc: 0.4374\n",
      "Epoch 687/700\n",
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0141 - acc: 0.3659 - val_loss: 0.0144 - val_acc: 0.3011\n",
      "Epoch 688/700\n",
      "10976/10976 [==============================] - 5s 438us/step - loss: 0.0142 - acc: 0.3680 - val_loss: 0.0149 - val_acc: 0.4477\n",
      "Epoch 689/700\n",
      "10976/10976 [==============================] - 5s 437us/step - loss: 0.0141 - acc: 0.3772 - val_loss: 0.0145 - val_acc: 0.3009\n",
      "Epoch 690/700\n",
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0139 - acc: 0.3719 - val_loss: 0.0135 - val_acc: 0.4454\n",
      "Epoch 691/700\n",
      "10976/10976 [==============================] - 5s 434us/step - loss: 0.0138 - acc: 0.3689 - val_loss: 0.0136 - val_acc: 0.3149\n",
      "Epoch 692/700\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0140 - acc: 0.3676 - val_loss: 0.0144 - val_acc: 0.4437\n",
      "Epoch 693/700\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0140 - acc: 0.3699 - val_loss: 0.0139 - val_acc: 0.3134\n",
      "Epoch 694/700\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0141 - acc: 0.3670 - val_loss: 0.0142 - val_acc: 0.4458\n",
      "Epoch 695/700\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0141 - acc: 0.3764 - val_loss: 0.0142 - val_acc: 0.3075\n",
      "Epoch 696/700\n",
      "10976/10976 [==============================] - 5s 435us/step - loss: 0.0140 - acc: 0.3749 - val_loss: 0.0134 - val_acc: 0.4406\n",
      "Epoch 697/700\n",
      "10976/10976 [==============================] - 5s 434us/step - loss: 0.0140 - acc: 0.3769 - val_loss: 0.0149 - val_acc: 0.2857\n",
      "Epoch 698/700\n",
      "10976/10976 [==============================] - 5s 433us/step - loss: 0.0139 - acc: 0.3772 - val_loss: 0.0143 - val_acc: 0.4415\n",
      "Epoch 699/700\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0140 - acc: 0.3724 - val_loss: 0.0145 - val_acc: 0.2977\n",
      "Epoch 700/700\n",
      "10976/10976 [==============================] - 5s 436us/step - loss: 0.0142 - acc: 0.3727 - val_loss: 0.0140 - val_acc: 0.4372\n",
      "start training round 35\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 701/720\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0183 - acc: 0.4410 - val_loss: 0.0178 - val_acc: 0.3908\n",
      "Epoch 702/720\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0184 - acc: 0.4418 - val_loss: 0.0194 - val_acc: 0.5002\n",
      "Epoch 703/720\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0182 - acc: 0.4436 - val_loss: 0.0174 - val_acc: 0.3902\n",
      "Epoch 704/720\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0185 - acc: 0.4420 - val_loss: 0.0190 - val_acc: 0.4897\n",
      "Epoch 705/720\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0185 - acc: 0.4414 - val_loss: 0.0178 - val_acc: 0.3923\n",
      "Epoch 706/720\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0184 - acc: 0.4375 - val_loss: 0.0197 - val_acc: 0.4994\n",
      "Epoch 707/720\n",
      "10976/10976 [==============================] - 5s 433us/step - loss: 0.0184 - acc: 0.4410 - val_loss: 0.0167 - val_acc: 0.3969\n",
      "Epoch 708/720\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0183 - acc: 0.4371 - val_loss: 0.0175 - val_acc: 0.4940\n",
      "Epoch 709/720\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0183 - acc: 0.4446 - val_loss: 0.0177 - val_acc: 0.3874\n",
      "Epoch 710/720\n",
      "10976/10976 [==============================] - 5s 433us/step - loss: 0.0181 - acc: 0.4397 - val_loss: 0.0179 - val_acc: 0.4926\n",
      "Epoch 711/720\n",
      "10976/10976 [==============================] - 5s 434us/step - loss: 0.0185 - acc: 0.4422 - val_loss: 0.0181 - val_acc: 0.3838\n",
      "Epoch 712/720\n",
      "10976/10976 [==============================] - 5s 435us/step - loss: 0.0185 - acc: 0.4390 - val_loss: 0.0193 - val_acc: 0.5010\n",
      "Epoch 713/720\n",
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0183 - acc: 0.4421 - val_loss: 0.0176 - val_acc: 0.3898\n",
      "Epoch 714/720\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0182 - acc: 0.4394 - val_loss: 0.0191 - val_acc: 0.4876\n",
      "Epoch 715/720\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0186 - acc: 0.4405 - val_loss: 0.0179 - val_acc: 0.3851\n",
      "Epoch 716/720\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0183 - acc: 0.4368 - val_loss: 0.0192 - val_acc: 0.4913\n",
      "Epoch 717/720\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0182 - acc: 0.4418 - val_loss: 0.0172 - val_acc: 0.3919\n",
      "Epoch 718/720\n",
      "10976/10976 [==============================] - 5s 475us/step - loss: 0.0181 - acc: 0.4436 - val_loss: 0.0197 - val_acc: 0.4939\n",
      "Epoch 719/720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0183 - acc: 0.4419 - val_loss: 0.0173 - val_acc: 0.3904\n",
      "Epoch 720/720\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0184 - acc: 0.4402 - val_loss: 0.0200 - val_acc: 0.5001\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 701/720\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0212 - acc: 0.4792 - val_loss: 0.0214 - val_acc: 0.5511\n",
      "Epoch 702/720\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0211 - acc: 0.4824 - val_loss: 0.0209 - val_acc: 0.4347\n",
      "Epoch 703/720\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0213 - acc: 0.4802 - val_loss: 0.0214 - val_acc: 0.5454\n",
      "Epoch 704/720\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0210 - acc: 0.4844 - val_loss: 0.0213 - val_acc: 0.4286\n",
      "Epoch 705/720\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0213 - acc: 0.4803 - val_loss: 0.0208 - val_acc: 0.5183\n",
      "Epoch 706/720\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0213 - acc: 0.4809 - val_loss: 0.0211 - val_acc: 0.4385\n",
      "Epoch 707/720\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0211 - acc: 0.5130 - val_loss: 0.0217 - val_acc: 0.5754\n",
      "Epoch 708/720\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0212 - acc: 0.5122 - val_loss: 0.0208 - val_acc: 0.4408\n",
      "Epoch 709/720\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0210 - acc: 0.5055 - val_loss: 0.0219 - val_acc: 0.5436\n",
      "Epoch 710/720\n",
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0210 - acc: 0.4825 - val_loss: 0.0213 - val_acc: 0.4275\n",
      "Epoch 711/720\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0211 - acc: 0.4833 - val_loss: 0.0204 - val_acc: 0.4804\n",
      "Epoch 712/720\n",
      "10976/10976 [==============================] - 5s 437us/step - loss: 0.0210 - acc: 0.5163 - val_loss: 0.0234 - val_acc: 0.5355\n",
      "Epoch 713/720\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0218 - acc: 0.5497 - val_loss: 0.0190 - val_acc: 0.5912\n",
      "Epoch 714/720\n",
      "10976/10976 [==============================] - 5s 474us/step - loss: 0.0214 - acc: 0.5558 - val_loss: 0.0223 - val_acc: 0.5384\n",
      "Epoch 715/720\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0216 - acc: 0.5384 - val_loss: 0.0203 - val_acc: 0.5754\n",
      "Epoch 716/720\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0211 - acc: 0.5118 - val_loss: 0.0216 - val_acc: 0.4481\n",
      "Epoch 717/720\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0213 - acc: 0.5121 - val_loss: 0.0217 - val_acc: 0.5747\n",
      "Epoch 718/720\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0213 - acc: 0.5373 - val_loss: 0.0204 - val_acc: 0.5163\n",
      "Epoch 719/720\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0213 - acc: 0.5469 - val_loss: 0.0204 - val_acc: 0.5915\n",
      "Epoch 720/720\n",
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0216 - acc: 0.5467 - val_loss: 0.0232 - val_acc: 0.5232\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 701/720\n",
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0168 - acc: 0.4094 - val_loss: 0.0230 - val_acc: 0.3177\n",
      "Epoch 702/720\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0172 - acc: 0.4482 - val_loss: 0.0105 - val_acc: 0.5362\n",
      "Epoch 703/720\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0169 - acc: 0.4487 - val_loss: 0.0239 - val_acc: 0.3179\n",
      "Epoch 704/720\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0170 - acc: 0.4272 - val_loss: 0.0114 - val_acc: 0.5353\n",
      "Epoch 705/720\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0168 - acc: 0.4460 - val_loss: 0.0237 - val_acc: 0.3126\n",
      "Epoch 706/720\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0170 - acc: 0.4053 - val_loss: 0.0095 - val_acc: 0.4616\n",
      "Epoch 707/720\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0167 - acc: 0.3769 - val_loss: 0.0243 - val_acc: 0.4552\n",
      "Epoch 708/720\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0169 - acc: 0.4327 - val_loss: 0.0101 - val_acc: 0.5352\n",
      "Epoch 709/720\n",
      "10976/10976 [==============================] - 5s 464us/step - loss: 0.0168 - acc: 0.4494 - val_loss: 0.0230 - val_acc: 0.4525\n",
      "Epoch 710/720\n",
      "10976/10976 [==============================] - 5s 436us/step - loss: 0.0170 - acc: 0.4194 - val_loss: 0.0101 - val_acc: 0.4573\n",
      "Epoch 711/720\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0167 - acc: 0.3917 - val_loss: 0.0239 - val_acc: 0.3206\n",
      "Epoch 712/720\n",
      "10976/10976 [==============================] - 5s 470us/step - loss: 0.0170 - acc: 0.4359 - val_loss: 0.0109 - val_acc: 0.5348\n",
      "Epoch 713/720\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0170 - acc: 0.4167 - val_loss: 0.0230 - val_acc: 0.3047\n",
      "Epoch 714/720\n",
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0171 - acc: 0.4052 - val_loss: 0.0097 - val_acc: 0.4609\n",
      "Epoch 715/720\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0168 - acc: 0.3969 - val_loss: 0.0220 - val_acc: 0.2993\n",
      "Epoch 716/720\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0172 - acc: 0.3955 - val_loss: 0.0104 - val_acc: 0.4481\n",
      "Epoch 717/720\n",
      "10976/10976 [==============================] - 5s 437us/step - loss: 0.0169 - acc: 0.3809 - val_loss: 0.0219 - val_acc: 0.3085\n",
      "Epoch 718/720\n",
      "10976/10976 [==============================] - 5s 436us/step - loss: 0.0173 - acc: 0.4156 - val_loss: 0.0109 - val_acc: 0.5345\n",
      "Epoch 719/720\n",
      "10976/10976 [==============================] - 5s 461us/step - loss: 0.0169 - acc: 0.3979 - val_loss: 0.0233 - val_acc: 0.3155\n",
      "Epoch 720/720\n",
      "10976/10976 [==============================] - 5s 434us/step - loss: 0.0172 - acc: 0.3905 - val_loss: 0.0096 - val_acc: 0.4641\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 701/720\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0140 - acc: 0.3769 - val_loss: 0.0142 - val_acc: 0.3157\n",
      "Epoch 702/720\n",
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0140 - acc: 0.3721 - val_loss: 0.0127 - val_acc: 0.4382\n",
      "Epoch 703/720\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0140 - acc: 0.3742 - val_loss: 0.0137 - val_acc: 0.3120\n",
      "Epoch 704/720\n",
      "10976/10976 [==============================] - 5s 474us/step - loss: 0.0140 - acc: 0.3767 - val_loss: 0.0143 - val_acc: 0.4471\n",
      "Epoch 705/720\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0140 - acc: 0.3811 - val_loss: 0.0138 - val_acc: 0.3162\n",
      "Epoch 706/720\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0140 - acc: 0.3713 - val_loss: 0.0128 - val_acc: 0.4367\n",
      "Epoch 707/720\n",
      "10976/10976 [==============================] - 5s 433us/step - loss: 0.0140 - acc: 0.3728 - val_loss: 0.0144 - val_acc: 0.2960\n",
      "Epoch 708/720\n",
      "10976/10976 [==============================] - 5s 435us/step - loss: 0.0141 - acc: 0.3668 - val_loss: 0.0122 - val_acc: 0.4428\n",
      "Epoch 709/720\n",
      "10976/10976 [==============================] - 5s 481us/step - loss: 0.0140 - acc: 0.3654 - val_loss: 0.0137 - val_acc: 0.3206\n",
      "Epoch 710/720\n",
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0140 - acc: 0.3677 - val_loss: 0.0137 - val_acc: 0.4506\n",
      "Epoch 711/720\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0141 - acc: 0.3657 - val_loss: 0.0151 - val_acc: 0.3065\n",
      "Epoch 712/720\n",
      "10976/10976 [==============================] - 5s 473us/step - loss: 0.0140 - acc: 0.3661 - val_loss: 0.0133 - val_acc: 0.4331\n",
      "Epoch 713/720\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0140 - acc: 0.3775 - val_loss: 0.0145 - val_acc: 0.3000\n",
      "Epoch 714/720\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0141 - acc: 0.3718 - val_loss: 0.0141 - val_acc: 0.4417\n",
      "Epoch 715/720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0140 - acc: 0.3803 - val_loss: 0.0138 - val_acc: 0.3033\n",
      "Epoch 716/720\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0139 - acc: 0.3728 - val_loss: 0.0132 - val_acc: 0.4427\n",
      "Epoch 717/720\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0140 - acc: 0.3740 - val_loss: 0.0140 - val_acc: 0.3128\n",
      "Epoch 718/720\n",
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0141 - acc: 0.3720 - val_loss: 0.0142 - val_acc: 0.4490\n",
      "Epoch 719/720\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0139 - acc: 0.3714 - val_loss: 0.0149 - val_acc: 0.3073\n",
      "Epoch 720/720\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0142 - acc: 0.3744 - val_loss: 0.0145 - val_acc: 0.4468\n",
      "start training round 36\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 721/740\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0183 - acc: 0.4417 - val_loss: 0.0191 - val_acc: 0.3829\n",
      "Epoch 722/740\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0183 - acc: 0.4415 - val_loss: 0.0195 - val_acc: 0.4991\n",
      "Epoch 723/740\n",
      "10976/10976 [==============================] - 5s 433us/step - loss: 0.0185 - acc: 0.4412 - val_loss: 0.0181 - val_acc: 0.3881\n",
      "Epoch 724/740\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0184 - acc: 0.4407 - val_loss: 0.0191 - val_acc: 0.4950\n",
      "Epoch 725/740\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0183 - acc: 0.4415 - val_loss: 0.0168 - val_acc: 0.3929\n",
      "Epoch 726/740\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0181 - acc: 0.4379 - val_loss: 0.0196 - val_acc: 0.4984\n",
      "Epoch 727/740\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0184 - acc: 0.4404 - val_loss: 0.0186 - val_acc: 0.3815\n",
      "Epoch 728/740\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0183 - acc: 0.4404 - val_loss: 0.0201 - val_acc: 0.4997\n",
      "Epoch 729/740\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0183 - acc: 0.4398 - val_loss: 0.0173 - val_acc: 0.3914\n",
      "Epoch 730/740\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0183 - acc: 0.4425 - val_loss: 0.0205 - val_acc: 0.5016\n",
      "Epoch 731/740\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0182 - acc: 0.4429 - val_loss: 0.0183 - val_acc: 0.3889\n",
      "Epoch 732/740\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0184 - acc: 0.4412 - val_loss: 0.0198 - val_acc: 0.4978\n",
      "Epoch 733/740\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0184 - acc: 0.4382 - val_loss: 0.0176 - val_acc: 0.3878\n",
      "Epoch 734/740\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0182 - acc: 0.4367 - val_loss: 0.0195 - val_acc: 0.4979\n",
      "Epoch 735/740\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0184 - acc: 0.4425 - val_loss: 0.0162 - val_acc: 0.3965\n",
      "Epoch 736/740\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0180 - acc: 0.4398 - val_loss: 0.0143 - val_acc: 0.4925\n",
      "Epoch 737/740\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0181 - acc: 0.4452 - val_loss: 0.0165 - val_acc: 0.3995\n",
      "Epoch 738/740\n",
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0183 - acc: 0.4417 - val_loss: 0.0196 - val_acc: 0.4986\n",
      "Epoch 739/740\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0181 - acc: 0.4448 - val_loss: 0.0176 - val_acc: 0.3903\n",
      "Epoch 740/740\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0184 - acc: 0.4395 - val_loss: 0.0188 - val_acc: 0.4975\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 721/740\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0213 - acc: 0.5494 - val_loss: 0.0221 - val_acc: 0.5810\n",
      "Epoch 722/740\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0215 - acc: 0.5485 - val_loss: 0.0216 - val_acc: 0.5385\n",
      "Epoch 723/740\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0212 - acc: 0.5360 - val_loss: 0.0201 - val_acc: 0.5775\n",
      "Epoch 724/740\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0208 - acc: 0.4877 - val_loss: 0.0214 - val_acc: 0.4250\n",
      "Epoch 725/740\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0211 - acc: 0.4819 - val_loss: 0.0208 - val_acc: 0.4786\n",
      "Epoch 726/740\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0211 - acc: 0.4854 - val_loss: 0.0213 - val_acc: 0.5105\n",
      "Epoch 727/740\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0212 - acc: 0.4878 - val_loss: 0.0207 - val_acc: 0.4801\n",
      "Epoch 728/740\n",
      "10976/10976 [==============================] - 5s 472us/step - loss: 0.0211 - acc: 0.5116 - val_loss: 0.0214 - val_acc: 0.4562\n",
      "Epoch 729/740\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0209 - acc: 0.5100 - val_loss: 0.0211 - val_acc: 0.5710\n",
      "Epoch 730/740\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0209 - acc: 0.5128 - val_loss: 0.0219 - val_acc: 0.4628\n",
      "Epoch 731/740\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0210 - acc: 0.5118 - val_loss: 0.0214 - val_acc: 0.5712\n",
      "Epoch 732/740\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0211 - acc: 0.5125 - val_loss: 0.0214 - val_acc: 0.4495\n",
      "Epoch 733/740\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0211 - acc: 0.5096 - val_loss: 0.0218 - val_acc: 0.5685\n",
      "Epoch 734/740\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0212 - acc: 0.5108 - val_loss: 0.0218 - val_acc: 0.4595\n",
      "Epoch 735/740\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0213 - acc: 0.5363 - val_loss: 0.0211 - val_acc: 0.5754\n",
      "Epoch 736/740\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0215 - acc: 0.5485 - val_loss: 0.0214 - val_acc: 0.5473\n",
      "Epoch 737/740\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0211 - acc: 0.5403 - val_loss: 0.0213 - val_acc: 0.5808\n",
      "Epoch 738/740\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0215 - acc: 0.5481 - val_loss: 0.0216 - val_acc: 0.5436\n",
      "Epoch 739/740\n",
      "10976/10976 [==============================] - 5s 470us/step - loss: 0.0214 - acc: 0.5442 - val_loss: 0.0216 - val_acc: 0.5846\n",
      "Epoch 740/740\n",
      "10976/10976 [==============================] - 6s 503us/step - loss: 0.0215 - acc: 0.5282 - val_loss: 0.0214 - val_acc: 0.5229\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 721/740\n",
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0167 - acc: 0.4412 - val_loss: 0.0237 - val_acc: 0.4609\n",
      "Epoch 722/740\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0172 - acc: 0.3997 - val_loss: 0.0123 - val_acc: 0.5200\n",
      "Epoch 723/740\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0168 - acc: 0.4080 - val_loss: 0.0219 - val_acc: 0.5375\n",
      "Epoch 724/740\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0171 - acc: 0.4330 - val_loss: 0.0106 - val_acc: 0.3156\n",
      "Epoch 725/740\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0164 - acc: 0.4480 - val_loss: 0.0124 - val_acc: 0.3187\n",
      "Epoch 726/740\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0168 - acc: 0.4261 - val_loss: 0.0218 - val_acc: 0.5288\n",
      "Epoch 727/740\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0171 - acc: 0.4175 - val_loss: 0.0117 - val_acc: 0.3070\n",
      "Epoch 728/740\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0168 - acc: 0.4199 - val_loss: 0.0229 - val_acc: 0.3078\n",
      "Epoch 729/740\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0170 - acc: 0.4009 - val_loss: 0.0113 - val_acc: 0.3173\n",
      "Epoch 730/740\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0169 - acc: 0.3855 - val_loss: 0.0232 - val_acc: 0.3139\n",
      "Epoch 731/740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0170 - acc: 0.3923 - val_loss: 0.0127 - val_acc: 0.3063\n",
      "Epoch 732/740\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0169 - acc: 0.3785 - val_loss: 0.0233 - val_acc: 0.4594\n",
      "Epoch 733/740\n",
      "10976/10976 [==============================] - 5s 434us/step - loss: 0.0171 - acc: 0.4037 - val_loss: 0.0099 - val_acc: 0.5335\n",
      "Epoch 734/740\n",
      "10976/10976 [==============================] - 5s 433us/step - loss: 0.0168 - acc: 0.4074 - val_loss: 0.0233 - val_acc: 0.4655\n",
      "Epoch 735/740\n",
      "10976/10976 [==============================] - 5s 436us/step - loss: 0.0172 - acc: 0.4107 - val_loss: 0.0091 - val_acc: 0.4623\n",
      "Epoch 736/740\n",
      "10976/10976 [==============================] - 5s 439us/step - loss: 0.0167 - acc: 0.3905 - val_loss: 0.0233 - val_acc: 0.3175\n",
      "Epoch 737/740\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0170 - acc: 0.3682 - val_loss: 0.0105 - val_acc: 0.5289\n",
      "Epoch 738/740\n",
      "10976/10976 [==============================] - 5s 440us/step - loss: 0.0169 - acc: 0.4006 - val_loss: 0.0230 - val_acc: 0.3194\n",
      "Epoch 739/740\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0171 - acc: 0.4082 - val_loss: 0.0103 - val_acc: 0.5361\n",
      "Epoch 740/740\n",
      "10976/10976 [==============================] - 5s 433us/step - loss: 0.0169 - acc: 0.4267 - val_loss: 0.0242 - val_acc: 0.5310\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 721/740\n",
      "10976/10976 [==============================] - 5s 436us/step - loss: 0.0139 - acc: 0.3646 - val_loss: 0.0153 - val_acc: 0.2991\n",
      "Epoch 722/740\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0140 - acc: 0.3674 - val_loss: 0.0146 - val_acc: 0.4497\n",
      "Epoch 723/740\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0141 - acc: 0.3796 - val_loss: 0.0139 - val_acc: 0.3084\n",
      "Epoch 724/740\n",
      "10976/10976 [==============================] - 5s 456us/step - loss: 0.0139 - acc: 0.3769 - val_loss: 0.0144 - val_acc: 0.4388\n",
      "Epoch 725/740\n",
      "10976/10976 [==============================] - 5s 433us/step - loss: 0.0141 - acc: 0.3750 - val_loss: 0.0137 - val_acc: 0.3015\n",
      "Epoch 726/740\n",
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0140 - acc: 0.3727 - val_loss: 0.0137 - val_acc: 0.4450\n",
      "Epoch 727/740\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0139 - acc: 0.3752 - val_loss: 0.0139 - val_acc: 0.2818\n",
      "Epoch 728/740\n",
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0141 - acc: 0.3714 - val_loss: 0.0137 - val_acc: 0.4516\n",
      "Epoch 729/740\n",
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0140 - acc: 0.3703 - val_loss: 0.0142 - val_acc: 0.3099\n",
      "Epoch 730/740\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0141 - acc: 0.3735 - val_loss: 0.0141 - val_acc: 0.4387\n",
      "Epoch 731/740\n",
      "10976/10976 [==============================] - 5s 435us/step - loss: 0.0140 - acc: 0.3730 - val_loss: 0.0139 - val_acc: 0.3110\n",
      "Epoch 732/740\n",
      "10976/10976 [==============================] - 5s 435us/step - loss: 0.0141 - acc: 0.3799 - val_loss: 0.0153 - val_acc: 0.4378\n",
      "Epoch 733/740\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0139 - acc: 0.3773 - val_loss: 0.0138 - val_acc: 0.3080\n",
      "Epoch 734/740\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0140 - acc: 0.3782 - val_loss: 0.0140 - val_acc: 0.4495\n",
      "Epoch 735/740\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0139 - acc: 0.3773 - val_loss: 0.0140 - val_acc: 0.3071\n",
      "Epoch 736/740\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0140 - acc: 0.3673 - val_loss: 0.0148 - val_acc: 0.4475\n",
      "Epoch 737/740\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0140 - acc: 0.3771 - val_loss: 0.0134 - val_acc: 0.3187\n",
      "Epoch 738/740\n",
      "10976/10976 [==============================] - 5s 433us/step - loss: 0.0140 - acc: 0.3789 - val_loss: 0.0145 - val_acc: 0.4548\n",
      "Epoch 739/740\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0140 - acc: 0.3743 - val_loss: 0.0147 - val_acc: 0.3059\n",
      "Epoch 740/740\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0140 - acc: 0.3690 - val_loss: 0.0134 - val_acc: 0.4389\n",
      "start training round 37\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 741/760\n",
      "10976/10976 [==============================] - 5s 438us/step - loss: 0.0183 - acc: 0.4402 - val_loss: 0.0181 - val_acc: 0.3891\n",
      "Epoch 742/760\n",
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0184 - acc: 0.4389 - val_loss: 0.0196 - val_acc: 0.4981\n",
      "Epoch 743/760\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0186 - acc: 0.4375 - val_loss: 0.0188 - val_acc: 0.3812\n",
      "Epoch 744/760\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0182 - acc: 0.4388 - val_loss: 0.0199 - val_acc: 0.5014\n",
      "Epoch 745/760\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0181 - acc: 0.4463 - val_loss: 0.0179 - val_acc: 0.3830\n",
      "Epoch 746/760\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0184 - acc: 0.4372 - val_loss: 0.0186 - val_acc: 0.4970\n",
      "Epoch 747/760\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0184 - acc: 0.4399 - val_loss: 0.0167 - val_acc: 0.3954\n",
      "Epoch 748/760\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0183 - acc: 0.4403 - val_loss: 0.0194 - val_acc: 0.4971\n",
      "Epoch 749/760\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0183 - acc: 0.4437 - val_loss: 0.0176 - val_acc: 0.3895\n",
      "Epoch 750/760\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0183 - acc: 0.4409 - val_loss: 0.0189 - val_acc: 0.4763\n",
      "Epoch 751/760\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0184 - acc: 0.4372 - val_loss: 0.0162 - val_acc: 0.4007\n",
      "Epoch 752/760\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0183 - acc: 0.4367 - val_loss: 0.0181 - val_acc: 0.4789\n",
      "Epoch 753/760\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0183 - acc: 0.4401 - val_loss: 0.0175 - val_acc: 0.3858\n",
      "Epoch 754/760\n",
      "10976/10976 [==============================] - 5s 433us/step - loss: 0.0184 - acc: 0.4366 - val_loss: 0.0195 - val_acc: 0.4991\n",
      "Epoch 755/760\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0184 - acc: 0.4411 - val_loss: 0.0169 - val_acc: 0.3984\n",
      "Epoch 756/760\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0183 - acc: 0.4401 - val_loss: 0.0159 - val_acc: 0.4954\n",
      "Epoch 757/760\n",
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0183 - acc: 0.4453 - val_loss: 0.0176 - val_acc: 0.3915\n",
      "Epoch 758/760\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0181 - acc: 0.4401 - val_loss: 0.0185 - val_acc: 0.4952\n",
      "Epoch 759/760\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0184 - acc: 0.4402 - val_loss: 0.0181 - val_acc: 0.3852\n",
      "Epoch 760/760\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0183 - acc: 0.4372 - val_loss: 0.0196 - val_acc: 0.4984\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 741/760\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0213 - acc: 0.5484 - val_loss: 0.0203 - val_acc: 0.5897\n",
      "Epoch 742/760\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0209 - acc: 0.5480 - val_loss: 0.0231 - val_acc: 0.5432\n",
      "Epoch 743/760\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0208 - acc: 0.5366 - val_loss: 0.0207 - val_acc: 0.5741\n",
      "Epoch 744/760\n",
      "10976/10976 [==============================] - 5s 434us/step - loss: 0.0213 - acc: 0.5407 - val_loss: 0.0230 - val_acc: 0.5166\n",
      "Epoch 745/760\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0216 - acc: 0.5474 - val_loss: 0.0208 - val_acc: 0.5857\n",
      "Epoch 746/760\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0213 - acc: 0.5522 - val_loss: 0.0190 - val_acc: 0.5162\n",
      "Epoch 747/760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 5s 432us/step - loss: 0.0207 - acc: 0.5389 - val_loss: 0.0211 - val_acc: 0.5714\n",
      "Epoch 748/760\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0209 - acc: 0.5154 - val_loss: 0.0214 - val_acc: 0.4743\n",
      "Epoch 749/760\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0214 - acc: 0.5353 - val_loss: 0.0214 - val_acc: 0.5546\n",
      "Epoch 750/760\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0210 - acc: 0.5480 - val_loss: 0.0217 - val_acc: 0.5512\n",
      "Epoch 751/760\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0212 - acc: 0.5384 - val_loss: 0.0216 - val_acc: 0.5434\n",
      "Epoch 752/760\n",
      "10976/10976 [==============================] - 5s 439us/step - loss: 0.0209 - acc: 0.4918 - val_loss: 0.0209 - val_acc: 0.4390\n",
      "Epoch 753/760\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0211 - acc: 0.4824 - val_loss: 0.0210 - val_acc: 0.5483\n",
      "Epoch 754/760\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0211 - acc: 0.4843 - val_loss: 0.0211 - val_acc: 0.4274\n",
      "Epoch 755/760\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0209 - acc: 0.4861 - val_loss: 0.0213 - val_acc: 0.5428\n",
      "Epoch 756/760\n",
      "10976/10976 [==============================] - 5s 434us/step - loss: 0.0208 - acc: 0.4863 - val_loss: 0.0215 - val_acc: 0.5030\n",
      "Epoch 757/760\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0212 - acc: 0.4854 - val_loss: 0.0211 - val_acc: 0.4815\n",
      "Epoch 758/760\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0211 - acc: 0.4854 - val_loss: 0.0215 - val_acc: 0.4259\n",
      "Epoch 759/760\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0212 - acc: 0.4825 - val_loss: 0.0210 - val_acc: 0.5424\n",
      "Epoch 760/760\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0209 - acc: 0.4855 - val_loss: 0.0214 - val_acc: 0.4273\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 741/760\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0170 - acc: 0.4476 - val_loss: 0.0091 - val_acc: 0.5352\n",
      "Epoch 742/760\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0167 - acc: 0.4123 - val_loss: 0.0228 - val_acc: 0.3130\n",
      "Epoch 743/760\n",
      "10976/10976 [==============================] - 5s 433us/step - loss: 0.0172 - acc: 0.4262 - val_loss: 0.0097 - val_acc: 0.3236\n",
      "Epoch 744/760\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0167 - acc: 0.3806 - val_loss: 0.0227 - val_acc: 0.4506\n",
      "Epoch 745/760\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0170 - acc: 0.3731 - val_loss: 0.0101 - val_acc: 0.5378\n",
      "Epoch 746/760\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0169 - acc: 0.4175 - val_loss: 0.0236 - val_acc: 0.5337\n",
      "Epoch 747/760\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0171 - acc: 0.4029 - val_loss: 0.0095 - val_acc: 0.5378\n",
      "Epoch 748/760\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0168 - acc: 0.3875 - val_loss: 0.0245 - val_acc: 0.3186\n",
      "Epoch 749/760\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0170 - acc: 0.4287 - val_loss: 0.0099 - val_acc: 0.4658\n",
      "Epoch 750/760\n",
      "10976/10976 [==============================] - 5s 433us/step - loss: 0.0167 - acc: 0.4167 - val_loss: 0.0184 - val_acc: 0.2825\n",
      "Epoch 751/760\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0172 - acc: 0.3691 - val_loss: 0.0107 - val_acc: 0.5342\n",
      "Epoch 752/760\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0169 - acc: 0.4285 - val_loss: 0.0191 - val_acc: 0.5267\n",
      "Epoch 753/760\n",
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0171 - acc: 0.4445 - val_loss: 0.0102 - val_acc: 0.5335\n",
      "Epoch 754/760\n",
      "10976/10976 [==============================] - 5s 435us/step - loss: 0.0167 - acc: 0.4217 - val_loss: 0.0238 - val_acc: 0.5319\n",
      "Epoch 755/760\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0171 - acc: 0.4033 - val_loss: 0.0121 - val_acc: 0.3183\n",
      "Epoch 756/760\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0168 - acc: 0.4049 - val_loss: 0.0238 - val_acc: 0.4473\n",
      "Epoch 757/760\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0168 - acc: 0.4265 - val_loss: 0.0105 - val_acc: 0.4628\n",
      "Epoch 758/760\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0167 - acc: 0.4152 - val_loss: 0.0240 - val_acc: 0.3183\n",
      "Epoch 759/760\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0169 - acc: 0.4306 - val_loss: 0.0100 - val_acc: 0.3186\n",
      "Epoch 760/760\n",
      "10976/10976 [==============================] - 5s 437us/step - loss: 0.0166 - acc: 0.4438 - val_loss: 0.0232 - val_acc: 0.3185\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 741/760\n",
      "10976/10976 [==============================] - 5s 442us/step - loss: 0.0138 - acc: 0.3769 - val_loss: 0.0143 - val_acc: 0.3012\n",
      "Epoch 742/760\n",
      "10976/10976 [==============================] - 5s 468us/step - loss: 0.0139 - acc: 0.3734 - val_loss: 0.0134 - val_acc: 0.4393\n",
      "Epoch 743/760\n",
      "10976/10976 [==============================] - 5s 452us/step - loss: 0.0141 - acc: 0.3730 - val_loss: 0.0134 - val_acc: 0.3110\n",
      "Epoch 744/760\n",
      "10976/10976 [==============================] - 5s 440us/step - loss: 0.0140 - acc: 0.3711 - val_loss: 0.0132 - val_acc: 0.4397\n",
      "Epoch 745/760\n",
      "10976/10976 [==============================] - 5s 477us/step - loss: 0.0139 - acc: 0.3774 - val_loss: 0.0138 - val_acc: 0.2950\n",
      "Epoch 746/760\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0140 - acc: 0.3682 - val_loss: 0.0138 - val_acc: 0.4512\n",
      "Epoch 747/760\n",
      "10976/10976 [==============================] - 5s 437us/step - loss: 0.0139 - acc: 0.3759 - val_loss: 0.0137 - val_acc: 0.3069\n",
      "Epoch 748/760\n",
      "10976/10976 [==============================] - 5s 434us/step - loss: 0.0139 - acc: 0.3717 - val_loss: 0.0149 - val_acc: 0.4474\n",
      "Epoch 749/760\n",
      "10976/10976 [==============================] - 5s 434us/step - loss: 0.0139 - acc: 0.3706 - val_loss: 0.0138 - val_acc: 0.3101\n",
      "Epoch 750/760\n",
      "10976/10976 [==============================] - 5s 436us/step - loss: 0.0140 - acc: 0.3709 - val_loss: 0.0131 - val_acc: 0.4499\n",
      "Epoch 751/760\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0140 - acc: 0.3772 - val_loss: 0.0141 - val_acc: 0.3071\n",
      "Epoch 752/760\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0140 - acc: 0.3714 - val_loss: 0.0142 - val_acc: 0.4481\n",
      "Epoch 753/760\n",
      "10976/10976 [==============================] - 5s 444us/step - loss: 0.0139 - acc: 0.3734 - val_loss: 0.0143 - val_acc: 0.3063\n",
      "Epoch 754/760\n",
      "10976/10976 [==============================] - 5s 447us/step - loss: 0.0140 - acc: 0.3755 - val_loss: 0.0137 - val_acc: 0.4497\n",
      "Epoch 755/760\n",
      "10976/10976 [==============================] - 5s 434us/step - loss: 0.0139 - acc: 0.3704 - val_loss: 0.0136 - val_acc: 0.3168\n",
      "Epoch 756/760\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0138 - acc: 0.3785 - val_loss: 0.0138 - val_acc: 0.4474\n",
      "Epoch 757/760\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0138 - acc: 0.3777 - val_loss: 0.0141 - val_acc: 0.3099\n",
      "Epoch 758/760\n",
      "10976/10976 [==============================] - 5s 431us/step - loss: 0.0139 - acc: 0.3732 - val_loss: 0.0135 - val_acc: 0.4513\n",
      "Epoch 759/760\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0139 - acc: 0.3759 - val_loss: 0.0141 - val_acc: 0.3067\n",
      "Epoch 760/760\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0138 - acc: 0.3680 - val_loss: 0.0135 - val_acc: 0.4533\n",
      "start training round 38\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 761/780\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0185 - acc: 0.4410 - val_loss: 0.0175 - val_acc: 0.3867\n",
      "Epoch 762/780\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0182 - acc: 0.4395 - val_loss: 0.0198 - val_acc: 0.5002\n",
      "Epoch 763/780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0182 - acc: 0.4438 - val_loss: 0.0166 - val_acc: 0.3945\n",
      "Epoch 764/780\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0181 - acc: 0.4415 - val_loss: 0.0184 - val_acc: 0.4842\n",
      "Epoch 765/780\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0184 - acc: 0.4374 - val_loss: 0.0166 - val_acc: 0.3967\n",
      "Epoch 766/780\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0184 - acc: 0.4389 - val_loss: 0.0196 - val_acc: 0.5016\n",
      "Epoch 767/780\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0182 - acc: 0.4382 - val_loss: 0.0171 - val_acc: 0.3893\n",
      "Epoch 768/780\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0183 - acc: 0.4387 - val_loss: 0.0187 - val_acc: 0.4998\n",
      "Epoch 769/780\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0184 - acc: 0.4424 - val_loss: 0.0175 - val_acc: 0.3878\n",
      "Epoch 770/780\n",
      "10976/10976 [==============================] - 5s 419us/step - loss: 0.0182 - acc: 0.4378 - val_loss: 0.0188 - val_acc: 0.4894\n",
      "Epoch 771/780\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0182 - acc: 0.4393 - val_loss: 0.0156 - val_acc: 0.4090\n",
      "Epoch 772/780\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0181 - acc: 0.4380 - val_loss: 0.0196 - val_acc: 0.5000\n",
      "Epoch 773/780\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0184 - acc: 0.4437 - val_loss: 0.0168 - val_acc: 0.3928\n",
      "Epoch 774/780\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0181 - acc: 0.4360 - val_loss: 0.0186 - val_acc: 0.4967\n",
      "Epoch 775/780\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0184 - acc: 0.4388 - val_loss: 0.0162 - val_acc: 0.3968\n",
      "Epoch 776/780\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0182 - acc: 0.4395 - val_loss: 0.0199 - val_acc: 0.4995\n",
      "Epoch 777/780\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0182 - acc: 0.4455 - val_loss: 0.0177 - val_acc: 0.3837\n",
      "Epoch 778/780\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0183 - acc: 0.4418 - val_loss: 0.0200 - val_acc: 0.4987\n",
      "Epoch 779/780\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0183 - acc: 0.4428 - val_loss: 0.0169 - val_acc: 0.3924\n",
      "Epoch 780/780\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0181 - acc: 0.4409 - val_loss: 0.0200 - val_acc: 0.4999\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 761/780\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0210 - acc: 0.4850 - val_loss: 0.0199 - val_acc: 0.5531\n",
      "Epoch 762/780\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0212 - acc: 0.5556 - val_loss: 0.0210 - val_acc: 0.5231\n",
      "Epoch 763/780\n",
      "10976/10976 [==============================] - 5s 436us/step - loss: 0.0211 - acc: 0.5505 - val_loss: 0.0204 - val_acc: 0.5846\n",
      "Epoch 764/780\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0210 - acc: 0.5304 - val_loss: 0.0220 - val_acc: 0.4531\n",
      "Epoch 765/780\n",
      "10976/10976 [==============================] - 5s 445us/step - loss: 0.0214 - acc: 0.5457 - val_loss: 0.0216 - val_acc: 0.5938\n",
      "Epoch 766/780\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0210 - acc: 0.5525 - val_loss: 0.0232 - val_acc: 0.5165\n",
      "Epoch 767/780\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0217 - acc: 0.5464 - val_loss: 0.0213 - val_acc: 0.5770\n",
      "Epoch 768/780\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0213 - acc: 0.5513 - val_loss: 0.0208 - val_acc: 0.4951\n",
      "Epoch 769/780\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0213 - acc: 0.5442 - val_loss: 0.0209 - val_acc: 0.5847\n",
      "Epoch 770/780\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0209 - acc: 0.5498 - val_loss: 0.0215 - val_acc: 0.5210\n",
      "Epoch 771/780\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0212 - acc: 0.5484 - val_loss: 0.0201 - val_acc: 0.5927\n",
      "Epoch 772/780\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0212 - acc: 0.5529 - val_loss: 0.0219 - val_acc: 0.5511\n",
      "Epoch 773/780\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0208 - acc: 0.5520 - val_loss: 0.0205 - val_acc: 0.5928\n",
      "Epoch 774/780\n",
      "10976/10976 [==============================] - 5s 420us/step - loss: 0.0217 - acc: 0.5482 - val_loss: 0.0222 - val_acc: 0.5196\n",
      "Epoch 775/780\n",
      "10976/10976 [==============================] - 5s 438us/step - loss: 0.0212 - acc: 0.5486 - val_loss: 0.0198 - val_acc: 0.5642\n",
      "Epoch 776/780\n",
      "10976/10976 [==============================] - 5s 438us/step - loss: 0.0215 - acc: 0.5490 - val_loss: 0.0230 - val_acc: 0.5434\n",
      "Epoch 777/780\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0209 - acc: 0.5479 - val_loss: 0.0202 - val_acc: 0.5888\n",
      "Epoch 778/780\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0213 - acc: 0.5475 - val_loss: 0.0220 - val_acc: 0.5315\n",
      "Epoch 779/780\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0208 - acc: 0.5487 - val_loss: 0.0234 - val_acc: 0.5811\n",
      "Epoch 780/780\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0214 - acc: 0.5495 - val_loss: 0.0223 - val_acc: 0.5369\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 761/780\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0171 - acc: 0.4416 - val_loss: 0.0104 - val_acc: 0.5397\n",
      "Epoch 762/780\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0170 - acc: 0.4353 - val_loss: 0.0239 - val_acc: 0.4613\n",
      "Epoch 763/780\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0170 - acc: 0.3565 - val_loss: 0.0096 - val_acc: 0.3197\n",
      "Epoch 764/780\n",
      "10976/10976 [==============================] - 5s 442us/step - loss: 0.0168 - acc: 0.3826 - val_loss: 0.0239 - val_acc: 0.4550\n",
      "Epoch 765/780\n",
      "10976/10976 [==============================] - 5s 434us/step - loss: 0.0173 - acc: 0.4100 - val_loss: 0.0107 - val_acc: 0.4542\n",
      "Epoch 766/780\n",
      "10976/10976 [==============================] - 5s 452us/step - loss: 0.0167 - acc: 0.4276 - val_loss: 0.0232 - val_acc: 0.3139\n",
      "Epoch 767/780\n",
      "10976/10976 [==============================] - 5s 450us/step - loss: 0.0171 - acc: 0.4304 - val_loss: 0.0104 - val_acc: 0.5304\n",
      "Epoch 768/780\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0167 - acc: 0.4280 - val_loss: 0.0236 - val_acc: 0.3168\n",
      "Epoch 769/780\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0168 - acc: 0.4348 - val_loss: 0.0097 - val_acc: 0.5364\n",
      "Epoch 770/780\n",
      "10976/10976 [==============================] - 5s 421us/step - loss: 0.0166 - acc: 0.4304 - val_loss: 0.0231 - val_acc: 0.4559\n",
      "Epoch 771/780\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0171 - acc: 0.4542 - val_loss: 0.0108 - val_acc: 0.5284\n",
      "Epoch 772/780\n",
      "10976/10976 [==============================] - 5s 418us/step - loss: 0.0171 - acc: 0.4371 - val_loss: 0.0228 - val_acc: 0.3170\n",
      "Epoch 773/780\n",
      "10976/10976 [==============================] - 5s 423us/step - loss: 0.0168 - acc: 0.3799 - val_loss: 0.0123 - val_acc: 0.3053\n",
      "Epoch 774/780\n",
      "10976/10976 [==============================] - 5s 422us/step - loss: 0.0172 - acc: 0.3870 - val_loss: 0.0221 - val_acc: 0.4598\n",
      "Epoch 775/780\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0169 - acc: 0.4157 - val_loss: 0.0114 - val_acc: 0.4566\n",
      "Epoch 776/780\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0167 - acc: 0.4226 - val_loss: 0.0234 - val_acc: 0.3163\n",
      "Epoch 777/780\n",
      "10976/10976 [==============================] - 5s 425us/step - loss: 0.0169 - acc: 0.3773 - val_loss: 0.0110 - val_acc: 0.5262\n",
      "Epoch 778/780\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0170 - acc: 0.3747 - val_loss: 0.0233 - val_acc: 0.4552\n",
      "Epoch 779/780\n",
      "10976/10976 [==============================] - 5s 433us/step - loss: 0.0170 - acc: 0.3918 - val_loss: 0.0116 - val_acc: 0.3168\n",
      "Epoch 780/780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0168 - acc: 0.4506 - val_loss: 0.0227 - val_acc: 0.5220\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 761/780\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0140 - acc: 0.3714 - val_loss: 0.0141 - val_acc: 0.3105\n",
      "Epoch 762/780\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0139 - acc: 0.3724 - val_loss: 0.0111 - val_acc: 0.4150\n",
      "Epoch 763/780\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0139 - acc: 0.3750 - val_loss: 0.0129 - val_acc: 0.3214\n",
      "Epoch 764/780\n",
      "10976/10976 [==============================] - 5s 429us/step - loss: 0.0139 - acc: 0.3721 - val_loss: 0.0143 - val_acc: 0.4577\n",
      "Epoch 765/780\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0140 - acc: 0.3757 - val_loss: 0.0141 - val_acc: 0.3062\n",
      "Epoch 766/780\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0139 - acc: 0.3726 - val_loss: 0.0139 - val_acc: 0.4426\n",
      "Epoch 767/780\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0138 - acc: 0.3665 - val_loss: 0.0141 - val_acc: 0.3025\n",
      "Epoch 768/780\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0139 - acc: 0.3718 - val_loss: 0.0135 - val_acc: 0.4516\n",
      "Epoch 769/780\n",
      "10976/10976 [==============================] - 5s 428us/step - loss: 0.0140 - acc: 0.3680 - val_loss: 0.0145 - val_acc: 0.3010\n",
      "Epoch 770/780\n",
      "10976/10976 [==============================] - 5s 430us/step - loss: 0.0140 - acc: 0.3728 - val_loss: 0.0139 - val_acc: 0.4385\n",
      "Epoch 771/780\n",
      "10976/10976 [==============================] - 5s 434us/step - loss: 0.0138 - acc: 0.3690 - val_loss: 0.0145 - val_acc: 0.2994\n",
      "Epoch 772/780\n",
      "10976/10976 [==============================] - 5s 427us/step - loss: 0.0140 - acc: 0.3675 - val_loss: 0.0137 - val_acc: 0.4457\n",
      "Epoch 773/780\n",
      "10976/10976 [==============================] - 5s 426us/step - loss: 0.0138 - acc: 0.3761 - val_loss: 0.0146 - val_acc: 0.3078\n",
      "Epoch 774/780\n",
      "10976/10976 [==============================] - 5s 435us/step - loss: 0.0139 - acc: 0.3775 - val_loss: 0.0139 - val_acc: 0.4516\n",
      "Epoch 775/780\n",
      "10976/10976 [==============================] - 5s 424us/step - loss: 0.0140 - acc: 0.3758 - val_loss: 0.0136 - val_acc: 0.3088\n",
      "Epoch 776/780\n",
      "10976/10976 [==============================] - 5s 442us/step - loss: 0.0139 - acc: 0.3765 - val_loss: 0.0131 - val_acc: 0.4861\n",
      "Epoch 777/780\n",
      "10976/10976 [==============================] - 5s 414us/step - loss: 0.0140 - acc: 0.3793 - val_loss: 0.0141 - val_acc: 0.3090\n",
      "Epoch 778/780\n",
      "10976/10976 [==============================] - 4s 356us/step - loss: 0.0140 - acc: 0.3736 - val_loss: 0.0138 - val_acc: 0.4526\n",
      "Epoch 779/780\n",
      "10976/10976 [==============================] - 4s 355us/step - loss: 0.0138 - acc: 0.3774 - val_loss: 0.0139 - val_acc: 0.3078\n",
      "Epoch 780/780\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0139 - acc: 0.3711 - val_loss: 0.0139 - val_acc: 0.4460\n",
      "start training round 39\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 781/800\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0185 - acc: 0.4412 - val_loss: 0.0189 - val_acc: 0.3797\n",
      "Epoch 782/800\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0184 - acc: 0.4414 - val_loss: 0.0195 - val_acc: 0.4980\n",
      "Epoch 783/800\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0181 - acc: 0.4447 - val_loss: 0.0168 - val_acc: 0.3928\n",
      "Epoch 784/800\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0183 - acc: 0.4404 - val_loss: 0.0170 - val_acc: 0.4939\n",
      "Epoch 785/800\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0183 - acc: 0.4384 - val_loss: 0.0177 - val_acc: 0.3887\n",
      "Epoch 786/800\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0182 - acc: 0.4407 - val_loss: 0.0201 - val_acc: 0.4946\n",
      "Epoch 787/800\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0183 - acc: 0.4456 - val_loss: 0.0183 - val_acc: 0.3824\n",
      "Epoch 788/800\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0180 - acc: 0.4403 - val_loss: 0.0201 - val_acc: 0.4952\n",
      "Epoch 789/800\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0185 - acc: 0.4435 - val_loss: 0.0172 - val_acc: 0.3929\n",
      "Epoch 790/800\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0183 - acc: 0.4434 - val_loss: 0.0198 - val_acc: 0.4937\n",
      "Epoch 791/800\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0184 - acc: 0.4405 - val_loss: 0.0175 - val_acc: 0.3864\n",
      "Epoch 792/800\n",
      "10976/10976 [==============================] - 4s 356us/step - loss: 0.0183 - acc: 0.4372 - val_loss: 0.0202 - val_acc: 0.4985\n",
      "Epoch 793/800\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0180 - acc: 0.4393 - val_loss: 0.0172 - val_acc: 0.3897\n",
      "Epoch 794/800\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0183 - acc: 0.4392 - val_loss: 0.0170 - val_acc: 0.4769\n",
      "Epoch 795/800\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0182 - acc: 0.4392 - val_loss: 0.0165 - val_acc: 0.4011\n",
      "Epoch 796/800\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0182 - acc: 0.4377 - val_loss: 0.0197 - val_acc: 0.4926\n",
      "Epoch 797/800\n",
      "10976/10976 [==============================] - 4s 343us/step - loss: 0.0182 - acc: 0.4417 - val_loss: 0.0179 - val_acc: 0.3821\n",
      "Epoch 798/800\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0182 - acc: 0.4426 - val_loss: 0.0166 - val_acc: 0.4858\n",
      "Epoch 799/800\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0181 - acc: 0.4429 - val_loss: 0.0163 - val_acc: 0.3982\n",
      "Epoch 800/800\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0181 - acc: 0.4419 - val_loss: 0.0187 - val_acc: 0.4945\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 781/800\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0211 - acc: 0.5483 - val_loss: 0.0207 - val_acc: 0.5916\n",
      "Epoch 782/800\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0209 - acc: 0.5384 - val_loss: 0.0210 - val_acc: 0.5311\n",
      "Epoch 783/800\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0210 - acc: 0.4848 - val_loss: 0.0219 - val_acc: 0.4206\n",
      "Epoch 784/800\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0210 - acc: 0.4901 - val_loss: 0.0206 - val_acc: 0.4855\n",
      "Epoch 785/800\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0212 - acc: 0.4872 - val_loss: 0.0213 - val_acc: 0.4907\n",
      "Epoch 786/800\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0212 - acc: 0.4895 - val_loss: 0.0208 - val_acc: 0.5410\n",
      "Epoch 787/800\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0210 - acc: 0.4855 - val_loss: 0.0221 - val_acc: 0.4177\n",
      "Epoch 788/800\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0209 - acc: 0.4844 - val_loss: 0.0216 - val_acc: 0.5553\n",
      "Epoch 789/800\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0211 - acc: 0.4835 - val_loss: 0.0208 - val_acc: 0.4394\n",
      "Epoch 790/800\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0210 - acc: 0.4854 - val_loss: 0.0195 - val_acc: 0.5810\n",
      "Epoch 791/800\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0208 - acc: 0.4903 - val_loss: 0.0209 - val_acc: 0.4346\n",
      "Epoch 792/800\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0209 - acc: 0.4857 - val_loss: 0.0210 - val_acc: 0.5679\n",
      "Epoch 793/800\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0209 - acc: 0.4868 - val_loss: 0.0217 - val_acc: 0.4307\n",
      "Epoch 794/800\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0209 - acc: 0.4860 - val_loss: 0.0208 - val_acc: 0.4774\n",
      "Epoch 795/800\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0208 - acc: 0.4898 - val_loss: 0.0211 - val_acc: 0.5192\n",
      "Epoch 796/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0214 - acc: 0.5515 - val_loss: 0.0204 - val_acc: 0.5945\n",
      "Epoch 797/800\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0209 - acc: 0.5509 - val_loss: 0.0212 - val_acc: 0.5194\n",
      "Epoch 798/800\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0209 - acc: 0.5528 - val_loss: 0.0217 - val_acc: 0.5729\n",
      "Epoch 799/800\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0212 - acc: 0.5530 - val_loss: 0.0221 - val_acc: 0.5102\n",
      "Epoch 800/800\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0213 - acc: 0.5501 - val_loss: 0.0203 - val_acc: 0.5570\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 781/800\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0168 - acc: 0.4154 - val_loss: 0.0125 - val_acc: 0.5368\n",
      "Epoch 782/800\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0167 - acc: 0.4422 - val_loss: 0.0225 - val_acc: 0.3167\n",
      "Epoch 783/800\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0171 - acc: 0.4290 - val_loss: 0.0105 - val_acc: 0.4565\n",
      "Epoch 784/800\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0169 - acc: 0.3927 - val_loss: 0.0231 - val_acc: 0.3162\n",
      "Epoch 785/800\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0172 - acc: 0.3870 - val_loss: 0.0109 - val_acc: 0.5298\n",
      "Epoch 786/800\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0167 - acc: 0.4170 - val_loss: 0.0233 - val_acc: 0.3224\n",
      "Epoch 787/800\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0172 - acc: 0.4125 - val_loss: 0.0101 - val_acc: 0.5314\n",
      "Epoch 788/800\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0166 - acc: 0.3533 - val_loss: 0.0224 - val_acc: 0.5415\n",
      "Epoch 789/800\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0170 - acc: 0.4335 - val_loss: 0.0101 - val_acc: 0.4641\n",
      "Epoch 790/800\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0168 - acc: 0.3926 - val_loss: 0.0224 - val_acc: 0.3128\n",
      "Epoch 791/800\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0170 - acc: 0.4507 - val_loss: 0.0093 - val_acc: 0.5405\n",
      "Epoch 792/800\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0167 - acc: 0.3867 - val_loss: 0.0233 - val_acc: 0.3181\n",
      "Epoch 793/800\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0171 - acc: 0.3754 - val_loss: 0.0108 - val_acc: 0.3282\n",
      "Epoch 794/800\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0168 - acc: 0.4361 - val_loss: 0.0236 - val_acc: 0.3283\n",
      "Epoch 795/800\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0171 - acc: 0.4494 - val_loss: 0.0120 - val_acc: 0.4428\n",
      "Epoch 796/800\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0167 - acc: 0.4026 - val_loss: 0.0218 - val_acc: 0.4616\n",
      "Epoch 797/800\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0169 - acc: 0.4511 - val_loss: 0.0108 - val_acc: 0.5285\n",
      "Epoch 798/800\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0169 - acc: 0.4379 - val_loss: 0.0235 - val_acc: 0.5353\n",
      "Epoch 799/800\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0169 - acc: 0.4264 - val_loss: 0.0119 - val_acc: 0.5287\n",
      "Epoch 800/800\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0168 - acc: 0.4099 - val_loss: 0.0239 - val_acc: 0.3205\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 781/800\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0140 - acc: 0.3786 - val_loss: 0.0141 - val_acc: 0.3019\n",
      "Epoch 782/800\n",
      "10976/10976 [==============================] - 4s 356us/step - loss: 0.0140 - acc: 0.3770 - val_loss: 0.0139 - val_acc: 0.4449\n",
      "Epoch 783/800\n",
      "10976/10976 [==============================] - 4s 355us/step - loss: 0.0139 - acc: 0.3781 - val_loss: 0.0145 - val_acc: 0.3072\n",
      "Epoch 784/800\n",
      "10976/10976 [==============================] - 4s 356us/step - loss: 0.0139 - acc: 0.3696 - val_loss: 0.0148 - val_acc: 0.4435\n",
      "Epoch 785/800\n",
      "10976/10976 [==============================] - 4s 355us/step - loss: 0.0140 - acc: 0.3773 - val_loss: 0.0133 - val_acc: 0.2998\n",
      "Epoch 786/800\n",
      "10976/10976 [==============================] - 4s 355us/step - loss: 0.0140 - acc: 0.3761 - val_loss: 0.0139 - val_acc: 0.4482\n",
      "Epoch 787/800\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0138 - acc: 0.3751 - val_loss: 0.0139 - val_acc: 0.3124\n",
      "Epoch 788/800\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0140 - acc: 0.3723 - val_loss: 0.0137 - val_acc: 0.4411\n",
      "Epoch 789/800\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0139 - acc: 0.3770 - val_loss: 0.0134 - val_acc: 0.3164\n",
      "Epoch 790/800\n",
      "10976/10976 [==============================] - 4s 356us/step - loss: 0.0139 - acc: 0.3740 - val_loss: 0.0134 - val_acc: 0.4501\n",
      "Epoch 791/800\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0139 - acc: 0.3732 - val_loss: 0.0143 - val_acc: 0.3168\n",
      "Epoch 792/800\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0140 - acc: 0.3669 - val_loss: 0.0137 - val_acc: 0.4404\n",
      "Epoch 793/800\n",
      "10976/10976 [==============================] - 4s 355us/step - loss: 0.0139 - acc: 0.3708 - val_loss: 0.0138 - val_acc: 0.3119\n",
      "Epoch 794/800\n",
      "10976/10976 [==============================] - 4s 356us/step - loss: 0.0139 - acc: 0.3752 - val_loss: 0.0135 - val_acc: 0.4476\n",
      "Epoch 795/800\n",
      "10976/10976 [==============================] - 4s 356us/step - loss: 0.0139 - acc: 0.3729 - val_loss: 0.0141 - val_acc: 0.3132\n",
      "Epoch 796/800\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0139 - acc: 0.3784 - val_loss: 0.0136 - val_acc: 0.4389\n",
      "Epoch 797/800\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0139 - acc: 0.3782 - val_loss: 0.0137 - val_acc: 0.3088\n",
      "Epoch 798/800\n",
      "10976/10976 [==============================] - 4s 355us/step - loss: 0.0138 - acc: 0.3713 - val_loss: 0.0139 - val_acc: 0.4529\n",
      "Epoch 799/800\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0138 - acc: 0.3678 - val_loss: 0.0142 - val_acc: 0.3109\n",
      "Epoch 800/800\n",
      "10976/10976 [==============================] - 4s 356us/step - loss: 0.0138 - acc: 0.3724 - val_loss: 0.0141 - val_acc: 0.4500\n",
      "start training round 40\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 801/820\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0182 - acc: 0.4413 - val_loss: 0.0176 - val_acc: 0.3925\n",
      "Epoch 802/820\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0183 - acc: 0.4400 - val_loss: 0.0175 - val_acc: 0.4773\n",
      "Epoch 803/820\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0183 - acc: 0.4398 - val_loss: 0.0175 - val_acc: 0.3887\n",
      "Epoch 804/820\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0183 - acc: 0.4371 - val_loss: 0.0192 - val_acc: 0.4981\n",
      "Epoch 805/820\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0181 - acc: 0.4412 - val_loss: 0.0184 - val_acc: 0.3881\n",
      "Epoch 806/820\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0182 - acc: 0.4392 - val_loss: 0.0196 - val_acc: 0.4998\n",
      "Epoch 807/820\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0183 - acc: 0.4398 - val_loss: 0.0179 - val_acc: 0.3865\n",
      "Epoch 808/820\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0181 - acc: 0.4354 - val_loss: 0.0178 - val_acc: 0.4940\n",
      "Epoch 809/820\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0181 - acc: 0.4458 - val_loss: 0.0181 - val_acc: 0.3835\n",
      "Epoch 810/820\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0182 - acc: 0.4425 - val_loss: 0.0195 - val_acc: 0.5010\n",
      "Epoch 811/820\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0183 - acc: 0.4445 - val_loss: 0.0168 - val_acc: 0.3949\n",
      "Epoch 812/820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0180 - acc: 0.4408 - val_loss: 0.0182 - val_acc: 0.4877\n",
      "Epoch 813/820\n",
      "10976/10976 [==============================] - 4s 357us/step - loss: 0.0180 - acc: 0.4429 - val_loss: 0.0187 - val_acc: 0.3828\n",
      "Epoch 814/820\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0182 - acc: 0.4376 - val_loss: 0.0180 - val_acc: 0.4935\n",
      "Epoch 815/820\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0180 - acc: 0.4399 - val_loss: 0.0169 - val_acc: 0.3919\n",
      "Epoch 816/820\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0181 - acc: 0.4416 - val_loss: 0.0177 - val_acc: 0.4911\n",
      "Epoch 817/820\n",
      "10976/10976 [==============================] - 4s 358us/step - loss: 0.0181 - acc: 0.4417 - val_loss: 0.0165 - val_acc: 0.3952\n",
      "Epoch 818/820\n",
      "10976/10976 [==============================] - 4s 363us/step - loss: 0.0179 - acc: 0.4407 - val_loss: 0.0190 - val_acc: 0.5010\n",
      "Epoch 819/820\n",
      "10976/10976 [==============================] - 4s 398us/step - loss: 0.0182 - acc: 0.4449 - val_loss: 0.0172 - val_acc: 0.3963\n",
      "Epoch 820/820\n",
      "10976/10976 [==============================] - 4s 380us/step - loss: 0.0180 - acc: 0.4424 - val_loss: 0.0186 - val_acc: 0.4947\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 801/820\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0211 - acc: 0.5489 - val_loss: 0.0207 - val_acc: 0.5011\n",
      "Epoch 802/820\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0208 - acc: 0.5217 - val_loss: 0.0210 - val_acc: 0.5773\n",
      "Epoch 803/820\n",
      "10976/10976 [==============================] - 4s 392us/step - loss: 0.0209 - acc: 0.5396 - val_loss: 0.0222 - val_acc: 0.5260\n",
      "Epoch 804/820\n",
      "10976/10976 [==============================] - 4s 359us/step - loss: 0.0208 - acc: 0.5491 - val_loss: 0.0204 - val_acc: 0.5867\n",
      "Epoch 805/820\n",
      "10976/10976 [==============================] - 4s 357us/step - loss: 0.0208 - acc: 0.4964 - val_loss: 0.0213 - val_acc: 0.5011\n",
      "Epoch 806/820\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0209 - acc: 0.4896 - val_loss: 0.0218 - val_acc: 0.4771\n",
      "Epoch 807/820\n",
      "10976/10976 [==============================] - 4s 356us/step - loss: 0.0213 - acc: 0.4864 - val_loss: 0.0215 - val_acc: 0.5186\n",
      "Epoch 808/820\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0207 - acc: 0.4908 - val_loss: 0.0208 - val_acc: 0.4940\n",
      "Epoch 809/820\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0210 - acc: 0.4913 - val_loss: 0.0217 - val_acc: 0.5083\n",
      "Epoch 810/820\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0208 - acc: 0.4897 - val_loss: 0.0211 - val_acc: 0.4872\n",
      "Epoch 811/820\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0210 - acc: 0.4900 - val_loss: 0.0216 - val_acc: 0.5271\n",
      "Epoch 812/820\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0211 - acc: 0.4880 - val_loss: 0.0211 - val_acc: 0.4811\n",
      "Epoch 813/820\n",
      "10976/10976 [==============================] - 4s 355us/step - loss: 0.0209 - acc: 0.4882 - val_loss: 0.0208 - val_acc: 0.4412\n",
      "Epoch 814/820\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0211 - acc: 0.4837 - val_loss: 0.0216 - val_acc: 0.5396\n",
      "Epoch 815/820\n",
      "10976/10976 [==============================] - 4s 356us/step - loss: 0.0210 - acc: 0.4879 - val_loss: 0.0202 - val_acc: 0.4534\n",
      "Epoch 816/820\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0208 - acc: 0.4872 - val_loss: 0.0211 - val_acc: 0.4792\n",
      "Epoch 817/820\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0211 - acc: 0.4862 - val_loss: 0.0212 - val_acc: 0.5027\n",
      "Epoch 818/820\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0211 - acc: 0.4943 - val_loss: 0.0210 - val_acc: 0.4847\n",
      "Epoch 819/820\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0207 - acc: 0.4824 - val_loss: 0.0219 - val_acc: 0.4958\n",
      "Epoch 820/820\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0209 - acc: 0.4925 - val_loss: 0.0228 - val_acc: 0.4742\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 801/820\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0171 - acc: 0.4112 - val_loss: 0.0110 - val_acc: 0.3097\n",
      "Epoch 802/820\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0167 - acc: 0.3864 - val_loss: 0.0238 - val_acc: 0.5317\n",
      "Epoch 803/820\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0170 - acc: 0.4129 - val_loss: 0.0099 - val_acc: 0.5377\n",
      "Epoch 804/820\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0166 - acc: 0.4328 - val_loss: 0.0238 - val_acc: 0.3210\n",
      "Epoch 805/820\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0171 - acc: 0.3655 - val_loss: 0.0108 - val_acc: 0.3229\n",
      "Epoch 806/820\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0170 - acc: 0.4142 - val_loss: 0.0231 - val_acc: 0.3211\n",
      "Epoch 807/820\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0172 - acc: 0.3804 - val_loss: 0.0106 - val_acc: 0.5305\n",
      "Epoch 808/820\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0170 - acc: 0.4182 - val_loss: 0.0222 - val_acc: 0.5361\n",
      "Epoch 809/820\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0170 - acc: 0.4444 - val_loss: 0.0100 - val_acc: 0.5392\n",
      "Epoch 810/820\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0166 - acc: 0.4435 - val_loss: 0.0227 - val_acc: 0.4480\n",
      "Epoch 811/820\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0171 - acc: 0.3904 - val_loss: 0.0103 - val_acc: 0.4569\n",
      "Epoch 812/820\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0167 - acc: 0.4474 - val_loss: 0.0220 - val_acc: 0.3189\n",
      "Epoch 813/820\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0170 - acc: 0.4454 - val_loss: 0.0097 - val_acc: 0.4595\n",
      "Epoch 814/820\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0167 - acc: 0.4078 - val_loss: 0.0230 - val_acc: 0.5356\n",
      "Epoch 815/820\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0170 - acc: 0.4067 - val_loss: 0.0106 - val_acc: 0.3137\n",
      "Epoch 816/820\n",
      "10976/10976 [==============================] - 4s 358us/step - loss: 0.0169 - acc: 0.4067 - val_loss: 0.0222 - val_acc: 0.3027\n",
      "Epoch 817/820\n",
      "10976/10976 [==============================] - 4s 357us/step - loss: 0.0169 - acc: 0.3660 - val_loss: 0.0105 - val_acc: 0.5367\n",
      "Epoch 818/820\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0166 - acc: 0.4398 - val_loss: 0.0223 - val_acc: 0.3224\n",
      "Epoch 819/820\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0170 - acc: 0.3980 - val_loss: 0.0098 - val_acc: 0.3350\n",
      "Epoch 820/820\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0167 - acc: 0.4146 - val_loss: 0.0212 - val_acc: 0.4295\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 801/820\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0138 - acc: 0.3739 - val_loss: 0.0143 - val_acc: 0.3060\n",
      "Epoch 802/820\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0138 - acc: 0.3780 - val_loss: 0.0131 - val_acc: 0.4428\n",
      "Epoch 803/820\n",
      "10976/10976 [==============================] - 4s 355us/step - loss: 0.0138 - acc: 0.3800 - val_loss: 0.0142 - val_acc: 0.3101\n",
      "Epoch 804/820\n",
      "10976/10976 [==============================] - 4s 355us/step - loss: 0.0139 - acc: 0.3763 - val_loss: 0.0140 - val_acc: 0.4460\n",
      "Epoch 805/820\n",
      "10976/10976 [==============================] - 4s 361us/step - loss: 0.0139 - acc: 0.3824 - val_loss: 0.0136 - val_acc: 0.3168\n",
      "Epoch 806/820\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0139 - acc: 0.3728 - val_loss: 0.0134 - val_acc: 0.4461\n",
      "Epoch 807/820\n",
      "10976/10976 [==============================] - 4s 355us/step - loss: 0.0138 - acc: 0.3782 - val_loss: 0.0140 - val_acc: 0.3106\n",
      "Epoch 808/820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 4s 360us/step - loss: 0.0138 - acc: 0.3784 - val_loss: 0.0143 - val_acc: 0.4487\n",
      "Epoch 809/820\n",
      "10976/10976 [==============================] - 4s 357us/step - loss: 0.0139 - acc: 0.3782 - val_loss: 0.0137 - val_acc: 0.3169\n",
      "Epoch 810/820\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0139 - acc: 0.3758 - val_loss: 0.0135 - val_acc: 0.4491\n",
      "Epoch 811/820\n",
      "10976/10976 [==============================] - 4s 358us/step - loss: 0.0139 - acc: 0.3747 - val_loss: 0.0144 - val_acc: 0.3053\n",
      "Epoch 812/820\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0138 - acc: 0.3724 - val_loss: 0.0134 - val_acc: 0.4422\n",
      "Epoch 813/820\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0138 - acc: 0.3731 - val_loss: 0.0140 - val_acc: 0.2923\n",
      "Epoch 814/820\n",
      "10976/10976 [==============================] - 4s 359us/step - loss: 0.0140 - acc: 0.3615 - val_loss: 0.0139 - val_acc: 0.4424\n",
      "Epoch 815/820\n",
      "10976/10976 [==============================] - 4s 356us/step - loss: 0.0139 - acc: 0.3693 - val_loss: 0.0145 - val_acc: 0.3020\n",
      "Epoch 816/820\n",
      "10976/10976 [==============================] - 4s 359us/step - loss: 0.0139 - acc: 0.3701 - val_loss: 0.0138 - val_acc: 0.4521\n",
      "Epoch 817/820\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0138 - acc: 0.3793 - val_loss: 0.0138 - val_acc: 0.3159\n",
      "Epoch 818/820\n",
      "10976/10976 [==============================] - 4s 360us/step - loss: 0.0138 - acc: 0.3733 - val_loss: 0.0138 - val_acc: 0.4443\n",
      "Epoch 819/820\n",
      "10976/10976 [==============================] - 4s 358us/step - loss: 0.0138 - acc: 0.3799 - val_loss: 0.0143 - val_acc: 0.3040\n",
      "Epoch 820/820\n",
      "10976/10976 [==============================] - 4s 358us/step - loss: 0.0139 - acc: 0.3719 - val_loss: 0.0133 - val_acc: 0.4421\n",
      "start training round 41\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 821/840\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0183 - acc: 0.4458 - val_loss: 0.0175 - val_acc: 0.3971\n",
      "Epoch 822/840\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0181 - acc: 0.4414 - val_loss: 0.0204 - val_acc: 0.4996\n",
      "Epoch 823/840\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0181 - acc: 0.4438 - val_loss: 0.0171 - val_acc: 0.3953\n",
      "Epoch 824/840\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0182 - acc: 0.4414 - val_loss: 0.0190 - val_acc: 0.5017\n",
      "Epoch 825/840\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0182 - acc: 0.4427 - val_loss: 0.0168 - val_acc: 0.3930\n",
      "Epoch 826/840\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0181 - acc: 0.4428 - val_loss: 0.0197 - val_acc: 0.5011\n",
      "Epoch 827/840\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0182 - acc: 0.4417 - val_loss: 0.0179 - val_acc: 0.3884\n",
      "Epoch 828/840\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0183 - acc: 0.4403 - val_loss: 0.0192 - val_acc: 0.4938\n",
      "Epoch 829/840\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0181 - acc: 0.4406 - val_loss: 0.0160 - val_acc: 0.4021\n",
      "Epoch 830/840\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0181 - acc: 0.4403 - val_loss: 0.0195 - val_acc: 0.5020\n",
      "Epoch 831/840\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0182 - acc: 0.4419 - val_loss: 0.0168 - val_acc: 0.3915\n",
      "Epoch 832/840\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0182 - acc: 0.4417 - val_loss: 0.0168 - val_acc: 0.4830\n",
      "Epoch 833/840\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0182 - acc: 0.4445 - val_loss: 0.0170 - val_acc: 0.3923\n",
      "Epoch 834/840\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0181 - acc: 0.4384 - val_loss: 0.0199 - val_acc: 0.4995\n",
      "Epoch 835/840\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0181 - acc: 0.4467 - val_loss: 0.0176 - val_acc: 0.3928\n",
      "Epoch 836/840\n",
      "10976/10976 [==============================] - 4s 355us/step - loss: 0.0182 - acc: 0.4416 - val_loss: 0.0187 - val_acc: 0.4939\n",
      "Epoch 837/840\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0181 - acc: 0.4416 - val_loss: 0.0164 - val_acc: 0.3970\n",
      "Epoch 838/840\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0183 - acc: 0.4371 - val_loss: 0.0196 - val_acc: 0.5008\n",
      "Epoch 839/840\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0181 - acc: 0.4423 - val_loss: 0.0170 - val_acc: 0.3904\n",
      "Epoch 840/840\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0180 - acc: 0.4414 - val_loss: 0.0184 - val_acc: 0.4990\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 821/840\n",
      "10976/10976 [==============================] - 4s 359us/step - loss: 0.0211 - acc: 0.4899 - val_loss: 0.0211 - val_acc: 0.5338\n",
      "Epoch 822/840\n",
      "10976/10976 [==============================] - 4s 399us/step - loss: 0.0209 - acc: 0.4915 - val_loss: 0.0210 - val_acc: 0.4818\n",
      "Epoch 823/840\n",
      "10976/10976 [==============================] - 4s 391us/step - loss: 0.0208 - acc: 0.4913 - val_loss: 0.0220 - val_acc: 0.4242\n",
      "Epoch 824/840\n",
      "10976/10976 [==============================] - 4s 407us/step - loss: 0.0206 - acc: 0.4920 - val_loss: 0.0210 - val_acc: 0.4868\n",
      "Epoch 825/840\n",
      "10976/10976 [==============================] - 4s 358us/step - loss: 0.0208 - acc: 0.4925 - val_loss: 0.0221 - val_acc: 0.4899\n",
      "Epoch 826/840\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0210 - acc: 0.4880 - val_loss: 0.0208 - val_acc: 0.4830\n",
      "Epoch 827/840\n",
      "10976/10976 [==============================] - 4s 356us/step - loss: 0.0209 - acc: 0.4878 - val_loss: 0.0208 - val_acc: 0.4399\n",
      "Epoch 828/840\n",
      "10976/10976 [==============================] - 4s 356us/step - loss: 0.0208 - acc: 0.4863 - val_loss: 0.0209 - val_acc: 0.5512\n",
      "Epoch 829/840\n",
      "10976/10976 [==============================] - 4s 356us/step - loss: 0.0208 - acc: 0.4920 - val_loss: 0.0212 - val_acc: 0.4360\n",
      "Epoch 830/840\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0207 - acc: 0.4907 - val_loss: 0.0207 - val_acc: 0.4959\n",
      "Epoch 831/840\n",
      "10976/10976 [==============================] - 4s 355us/step - loss: 0.0207 - acc: 0.4978 - val_loss: 0.0210 - val_acc: 0.4416\n",
      "Epoch 832/840\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0208 - acc: 0.4889 - val_loss: 0.0209 - val_acc: 0.5450\n",
      "Epoch 833/840\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0209 - acc: 0.4893 - val_loss: 0.0202 - val_acc: 0.4554\n",
      "Epoch 834/840\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0209 - acc: 0.4861 - val_loss: 0.0211 - val_acc: 0.5414\n",
      "Epoch 835/840\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0210 - acc: 0.4876 - val_loss: 0.0211 - val_acc: 0.4451\n",
      "Epoch 836/840\n",
      "10976/10976 [==============================] - 4s 356us/step - loss: 0.0208 - acc: 0.4884 - val_loss: 0.0208 - val_acc: 0.4880\n",
      "Epoch 837/840\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0208 - acc: 0.4865 - val_loss: 0.0215 - val_acc: 0.5070\n",
      "Epoch 838/840\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0210 - acc: 0.4907 - val_loss: 0.0206 - val_acc: 0.4902\n",
      "Epoch 839/840\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0207 - acc: 0.4916 - val_loss: 0.0210 - val_acc: 0.4324\n",
      "Epoch 840/840\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0209 - acc: 0.4882 - val_loss: 0.0215 - val_acc: 0.5564\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 821/840\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0169 - acc: 0.3821 - val_loss: 0.0109 - val_acc: 0.3255\n",
      "Epoch 822/840\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0168 - acc: 0.4329 - val_loss: 0.0227 - val_acc: 0.3345\n",
      "Epoch 823/840\n",
      "10976/10976 [==============================] - 4s 370us/step - loss: 0.0170 - acc: 0.4386 - val_loss: 0.0098 - val_acc: 0.4425\n",
      "Epoch 824/840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 4s 355us/step - loss: 0.0170 - acc: 0.4688 - val_loss: 0.0234 - val_acc: 0.3408\n",
      "Epoch 825/840\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0169 - acc: 0.4340 - val_loss: 0.0117 - val_acc: 0.4530\n",
      "Epoch 826/840\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0168 - acc: 0.4750 - val_loss: 0.0234 - val_acc: 0.4504\n",
      "Epoch 827/840\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0172 - acc: 0.4619 - val_loss: 0.0102 - val_acc: 0.5376\n",
      "Epoch 828/840\n",
      "10976/10976 [==============================] - 4s 355us/step - loss: 0.0168 - acc: 0.4328 - val_loss: 0.0225 - val_acc: 0.5336\n",
      "Epoch 829/840\n",
      "10976/10976 [==============================] - 4s 357us/step - loss: 0.0170 - acc: 0.4389 - val_loss: 0.0103 - val_acc: 0.5279\n",
      "Epoch 830/840\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0165 - acc: 0.4245 - val_loss: 0.0204 - val_acc: 0.3011\n",
      "Epoch 831/840\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0171 - acc: 0.4379 - val_loss: 0.0096 - val_acc: 0.4581\n",
      "Epoch 832/840\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0165 - acc: 0.4289 - val_loss: 0.0221 - val_acc: 0.3067\n",
      "Epoch 833/840\n",
      "10976/10976 [==============================] - 4s 355us/step - loss: 0.0167 - acc: 0.3875 - val_loss: 0.0096 - val_acc: 0.5296\n",
      "Epoch 834/840\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0167 - acc: 0.4298 - val_loss: 0.0229 - val_acc: 0.3320\n",
      "Epoch 835/840\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0170 - acc: 0.4828 - val_loss: 0.0096 - val_acc: 0.4639\n",
      "Epoch 836/840\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0167 - acc: 0.4290 - val_loss: 0.0226 - val_acc: 0.3207\n",
      "Epoch 837/840\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0171 - acc: 0.4510 - val_loss: 0.0097 - val_acc: 0.5364\n",
      "Epoch 838/840\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0171 - acc: 0.4398 - val_loss: 0.0233 - val_acc: 0.3232\n",
      "Epoch 839/840\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0171 - acc: 0.4176 - val_loss: 0.0118 - val_acc: 0.5289\n",
      "Epoch 840/840\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0169 - acc: 0.4114 - val_loss: 0.0234 - val_acc: 0.3245\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 821/840\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0139 - acc: 0.3768 - val_loss: 0.0136 - val_acc: 0.3146\n",
      "Epoch 822/840\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0137 - acc: 0.3733 - val_loss: 0.0127 - val_acc: 0.4303\n",
      "Epoch 823/840\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0138 - acc: 0.3735 - val_loss: 0.0140 - val_acc: 0.3038\n",
      "Epoch 824/840\n",
      "10976/10976 [==============================] - 4s 356us/step - loss: 0.0140 - acc: 0.3734 - val_loss: 0.0140 - val_acc: 0.4512\n",
      "Epoch 825/840\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0139 - acc: 0.3763 - val_loss: 0.0137 - val_acc: 0.3134\n",
      "Epoch 826/840\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0139 - acc: 0.3689 - val_loss: 0.0137 - val_acc: 0.4453\n",
      "Epoch 827/840\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0138 - acc: 0.3772 - val_loss: 0.0140 - val_acc: 0.2786\n",
      "Epoch 828/840\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0140 - acc: 0.3737 - val_loss: 0.0150 - val_acc: 0.4434\n",
      "Epoch 829/840\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0137 - acc: 0.3823 - val_loss: 0.0140 - val_acc: 0.3149\n",
      "Epoch 830/840\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0139 - acc: 0.3744 - val_loss: 0.0142 - val_acc: 0.4527\n",
      "Epoch 831/840\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0139 - acc: 0.3786 - val_loss: 0.0141 - val_acc: 0.2994\n",
      "Epoch 832/840\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0139 - acc: 0.3705 - val_loss: 0.0138 - val_acc: 0.4480\n",
      "Epoch 833/840\n",
      "10976/10976 [==============================] - 4s 361us/step - loss: 0.0140 - acc: 0.3763 - val_loss: 0.0145 - val_acc: 0.3102\n",
      "Epoch 834/840\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0139 - acc: 0.3692 - val_loss: 0.0134 - val_acc: 0.4501\n",
      "Epoch 835/840\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0137 - acc: 0.3779 - val_loss: 0.0140 - val_acc: 0.3110\n",
      "Epoch 836/840\n",
      "10976/10976 [==============================] - 4s 356us/step - loss: 0.0138 - acc: 0.3716 - val_loss: 0.0129 - val_acc: 0.4353\n",
      "Epoch 837/840\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0139 - acc: 0.3802 - val_loss: 0.0137 - val_acc: 0.3188\n",
      "Epoch 838/840\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0140 - acc: 0.3730 - val_loss: 0.0137 - val_acc: 0.4494\n",
      "Epoch 839/840\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0138 - acc: 0.3762 - val_loss: 0.0138 - val_acc: 0.3177\n",
      "Epoch 840/840\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0138 - acc: 0.3771 - val_loss: 0.0137 - val_acc: 0.4426\n",
      "start training round 42\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 841/860\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0182 - acc: 0.4444 - val_loss: 0.0175 - val_acc: 0.3867\n",
      "Epoch 842/860\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0178 - acc: 0.4408 - val_loss: 0.0189 - val_acc: 0.4974\n",
      "Epoch 843/860\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0183 - acc: 0.4413 - val_loss: 0.0171 - val_acc: 0.3910\n",
      "Epoch 844/860\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0181 - acc: 0.4403 - val_loss: 0.0198 - val_acc: 0.5014\n",
      "Epoch 845/860\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0183 - acc: 0.4461 - val_loss: 0.0175 - val_acc: 0.3912\n",
      "Epoch 846/860\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0181 - acc: 0.4431 - val_loss: 0.0177 - val_acc: 0.4817\n",
      "Epoch 847/860\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0180 - acc: 0.4443 - val_loss: 0.0180 - val_acc: 0.3884\n",
      "Epoch 848/860\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0182 - acc: 0.4415 - val_loss: 0.0179 - val_acc: 0.4945\n",
      "Epoch 849/860\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0181 - acc: 0.4418 - val_loss: 0.0169 - val_acc: 0.3936\n",
      "Epoch 850/860\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0179 - acc: 0.4455 - val_loss: 0.0177 - val_acc: 0.5038\n",
      "Epoch 851/860\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0180 - acc: 0.4441 - val_loss: 0.0171 - val_acc: 0.3926\n",
      "Epoch 852/860\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0182 - acc: 0.4389 - val_loss: 0.0192 - val_acc: 0.4987\n",
      "Epoch 853/860\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0182 - acc: 0.4444 - val_loss: 0.0178 - val_acc: 0.3870\n",
      "Epoch 854/860\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0182 - acc: 0.4361 - val_loss: 0.0181 - val_acc: 0.4945\n",
      "Epoch 855/860\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0182 - acc: 0.4439 - val_loss: 0.0171 - val_acc: 0.3910\n",
      "Epoch 856/860\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0183 - acc: 0.4430 - val_loss: 0.0193 - val_acc: 0.5029\n",
      "Epoch 857/860\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0181 - acc: 0.4419 - val_loss: 0.0183 - val_acc: 0.3835\n",
      "Epoch 858/860\n",
      "10976/10976 [==============================] - 4s 355us/step - loss: 0.0180 - acc: 0.4419 - val_loss: 0.0200 - val_acc: 0.5029\n",
      "Epoch 859/860\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0180 - acc: 0.4447 - val_loss: 0.0160 - val_acc: 0.4000\n",
      "Epoch 860/860\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0180 - acc: 0.4373 - val_loss: 0.0192 - val_acc: 0.5014\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 841/860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0209 - acc: 0.4909 - val_loss: 0.0215 - val_acc: 0.5451\n",
      "Epoch 842/860\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0207 - acc: 0.4876 - val_loss: 0.0209 - val_acc: 0.5425\n",
      "Epoch 843/860\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0210 - acc: 0.4881 - val_loss: 0.0210 - val_acc: 0.4374\n",
      "Epoch 844/860\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0207 - acc: 0.4869 - val_loss: 0.0207 - val_acc: 0.4908\n",
      "Epoch 845/860\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0209 - acc: 0.4965 - val_loss: 0.0217 - val_acc: 0.5384\n",
      "Epoch 846/860\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0209 - acc: 0.4908 - val_loss: 0.0213 - val_acc: 0.5355\n",
      "Epoch 847/860\n",
      "10976/10976 [==============================] - 4s 358us/step - loss: 0.0208 - acc: 0.4889 - val_loss: 0.0204 - val_acc: 0.4524\n",
      "Epoch 848/860\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0208 - acc: 0.4836 - val_loss: 0.0222 - val_acc: 0.5546\n",
      "Epoch 849/860\n",
      "10976/10976 [==============================] - 4s 356us/step - loss: 0.0209 - acc: 0.4892 - val_loss: 0.0205 - val_acc: 0.4434\n",
      "Epoch 850/860\n",
      "10976/10976 [==============================] - 4s 394us/step - loss: 0.0209 - acc: 0.4818 - val_loss: 0.0211 - val_acc: 0.5522\n",
      "Epoch 851/860\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0210 - acc: 0.4895 - val_loss: 0.0211 - val_acc: 0.4319\n",
      "Epoch 852/860\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0210 - acc: 0.5344 - val_loss: 0.0207 - val_acc: 0.5758\n",
      "Epoch 853/860\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0214 - acc: 0.5613 - val_loss: 0.0210 - val_acc: 0.5305\n",
      "Epoch 854/860\n",
      "10976/10976 [==============================] - 4s 365us/step - loss: 0.0212 - acc: 0.5570 - val_loss: 0.0229 - val_acc: 0.5821\n",
      "Epoch 855/860\n",
      "10976/10976 [==============================] - 4s 355us/step - loss: 0.0207 - acc: 0.5609 - val_loss: 0.0215 - val_acc: 0.5503\n",
      "Epoch 856/860\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0209 - acc: 0.5572 - val_loss: 0.0210 - val_acc: 0.5936\n",
      "Epoch 857/860\n",
      "10976/10976 [==============================] - 4s 356us/step - loss: 0.0209 - acc: 0.5557 - val_loss: 0.0217 - val_acc: 0.5392\n",
      "Epoch 858/860\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0213 - acc: 0.5515 - val_loss: 0.0212 - val_acc: 0.5666\n",
      "Epoch 859/860\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0209 - acc: 0.5459 - val_loss: 0.0228 - val_acc: 0.5316\n",
      "Epoch 860/860\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0216 - acc: 0.5535 - val_loss: 0.0214 - val_acc: 0.5721\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 841/860\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0166 - acc: 0.4573 - val_loss: 0.0108 - val_acc: 0.5386\n",
      "Epoch 842/860\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0166 - acc: 0.4454 - val_loss: 0.0227 - val_acc: 0.3168\n",
      "Epoch 843/860\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0171 - acc: 0.4054 - val_loss: 0.0095 - val_acc: 0.5386\n",
      "Epoch 844/860\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0166 - acc: 0.4263 - val_loss: 0.0228 - val_acc: 0.5339\n",
      "Epoch 845/860\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0171 - acc: 0.3846 - val_loss: 0.0096 - val_acc: 0.5368\n",
      "Epoch 846/860\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0167 - acc: 0.3900 - val_loss: 0.0228 - val_acc: 0.5333\n",
      "Epoch 847/860\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0170 - acc: 0.4256 - val_loss: 0.0096 - val_acc: 0.5380\n",
      "Epoch 848/860\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0170 - acc: 0.4459 - val_loss: 0.0233 - val_acc: 0.3184\n",
      "Epoch 849/860\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0168 - acc: 0.4021 - val_loss: 0.0118 - val_acc: 0.5338\n",
      "Epoch 850/860\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0168 - acc: 0.3857 - val_loss: 0.0236 - val_acc: 0.3189\n",
      "Epoch 851/860\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0170 - acc: 0.3684 - val_loss: 0.0095 - val_acc: 0.4563\n",
      "Epoch 852/860\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0168 - acc: 0.3956 - val_loss: 0.0235 - val_acc: 0.3264\n",
      "Epoch 853/860\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0171 - acc: 0.4567 - val_loss: 0.0111 - val_acc: 0.5376\n",
      "Epoch 854/860\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0167 - acc: 0.4203 - val_loss: 0.0226 - val_acc: 0.3211\n",
      "Epoch 855/860\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0170 - acc: 0.4038 - val_loss: 0.0106 - val_acc: 0.4657\n",
      "Epoch 856/860\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0166 - acc: 0.3790 - val_loss: 0.0242 - val_acc: 0.4592\n",
      "Epoch 857/860\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0171 - acc: 0.4401 - val_loss: 0.0109 - val_acc: 0.5371\n",
      "Epoch 858/860\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0169 - acc: 0.4395 - val_loss: 0.0231 - val_acc: 0.3259\n",
      "Epoch 859/860\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0169 - acc: 0.4308 - val_loss: 0.0095 - val_acc: 0.4673\n",
      "Epoch 860/860\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0166 - acc: 0.4185 - val_loss: 0.0226 - val_acc: 0.4529\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 841/860\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0139 - acc: 0.3795 - val_loss: 0.0131 - val_acc: 0.3190\n",
      "Epoch 842/860\n",
      "10976/10976 [==============================] - 4s 356us/step - loss: 0.0138 - acc: 0.3739 - val_loss: 0.0140 - val_acc: 0.4426\n",
      "Epoch 843/860\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0138 - acc: 0.3821 - val_loss: 0.0142 - val_acc: 0.3053\n",
      "Epoch 844/860\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0139 - acc: 0.3746 - val_loss: 0.0138 - val_acc: 0.4439\n",
      "Epoch 845/860\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0137 - acc: 0.3722 - val_loss: 0.0139 - val_acc: 0.3164\n",
      "Epoch 846/860\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0138 - acc: 0.3763 - val_loss: 0.0141 - val_acc: 0.4404\n",
      "Epoch 847/860\n",
      "10976/10976 [==============================] - 4s 355us/step - loss: 0.0139 - acc: 0.3748 - val_loss: 0.0138 - val_acc: 0.3117\n",
      "Epoch 848/860\n",
      "10976/10976 [==============================] - 4s 355us/step - loss: 0.0138 - acc: 0.3807 - val_loss: 0.0138 - val_acc: 0.4447\n",
      "Epoch 849/860\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0138 - acc: 0.3847 - val_loss: 0.0140 - val_acc: 0.3149\n",
      "Epoch 850/860\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0139 - acc: 0.3764 - val_loss: 0.0136 - val_acc: 0.4506\n",
      "Epoch 851/860\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0138 - acc: 0.3795 - val_loss: 0.0136 - val_acc: 0.3144\n",
      "Epoch 852/860\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0139 - acc: 0.3751 - val_loss: 0.0133 - val_acc: 0.4433\n",
      "Epoch 853/860\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0139 - acc: 0.3741 - val_loss: 0.0140 - val_acc: 0.3070\n",
      "Epoch 854/860\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0138 - acc: 0.3736 - val_loss: 0.0140 - val_acc: 0.4508\n",
      "Epoch 855/860\n",
      "10976/10976 [==============================] - 4s 362us/step - loss: 0.0137 - acc: 0.3827 - val_loss: 0.0135 - val_acc: 0.3147\n",
      "Epoch 856/860\n",
      "10976/10976 [==============================] - 4s 356us/step - loss: 0.0137 - acc: 0.3731 - val_loss: 0.0134 - val_acc: 0.4438\n",
      "Epoch 857/860\n",
      "10976/10976 [==============================] - 4s 358us/step - loss: 0.0139 - acc: 0.3695 - val_loss: 0.0131 - val_acc: 0.2753\n",
      "Epoch 858/860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 4s 357us/step - loss: 0.0139 - acc: 0.3674 - val_loss: 0.0140 - val_acc: 0.4542\n",
      "Epoch 859/860\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0139 - acc: 0.3819 - val_loss: 0.0142 - val_acc: 0.3060\n",
      "Epoch 860/860\n",
      "10976/10976 [==============================] - 4s 358us/step - loss: 0.0140 - acc: 0.3705 - val_loss: 0.0138 - val_acc: 0.4458\n",
      "start training round 43\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 861/880\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0182 - acc: 0.4434 - val_loss: 0.0178 - val_acc: 0.3852\n",
      "Epoch 862/880\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0181 - acc: 0.4366 - val_loss: 0.0190 - val_acc: 0.5027\n",
      "Epoch 863/880\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0180 - acc: 0.4453 - val_loss: 0.0169 - val_acc: 0.3919\n",
      "Epoch 864/880\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0179 - acc: 0.4435 - val_loss: 0.0188 - val_acc: 0.4955\n",
      "Epoch 865/880\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0181 - acc: 0.4438 - val_loss: 0.0182 - val_acc: 0.3843\n",
      "Epoch 866/880\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0181 - acc: 0.4444 - val_loss: 0.0190 - val_acc: 0.5014\n",
      "Epoch 867/880\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0182 - acc: 0.4435 - val_loss: 0.0177 - val_acc: 0.3899\n",
      "Epoch 868/880\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0180 - acc: 0.4402 - val_loss: 0.0195 - val_acc: 0.4997\n",
      "Epoch 869/880\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0181 - acc: 0.4433 - val_loss: 0.0168 - val_acc: 0.3930\n",
      "Epoch 870/880\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0180 - acc: 0.4453 - val_loss: 0.0195 - val_acc: 0.4979\n",
      "Epoch 871/880\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0181 - acc: 0.4462 - val_loss: 0.0177 - val_acc: 0.3880\n",
      "Epoch 872/880\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0180 - acc: 0.4387 - val_loss: 0.0192 - val_acc: 0.5018\n",
      "Epoch 873/880\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0183 - acc: 0.4409 - val_loss: 0.0167 - val_acc: 0.3919\n",
      "Epoch 874/880\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0180 - acc: 0.4419 - val_loss: 0.0177 - val_acc: 0.4888\n",
      "Epoch 875/880\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0181 - acc: 0.4444 - val_loss: 0.0174 - val_acc: 0.3892\n",
      "Epoch 876/880\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0180 - acc: 0.4435 - val_loss: 0.0190 - val_acc: 0.4989\n",
      "Epoch 877/880\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0180 - acc: 0.4419 - val_loss: 0.0174 - val_acc: 0.3878\n",
      "Epoch 878/880\n",
      "10976/10976 [==============================] - 4s 344us/step - loss: 0.0180 - acc: 0.4411 - val_loss: 0.0188 - val_acc: 0.4991\n",
      "Epoch 879/880\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0180 - acc: 0.4435 - val_loss: 0.0173 - val_acc: 0.3928\n",
      "Epoch 880/880\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0181 - acc: 0.4420 - val_loss: 0.0181 - val_acc: 0.4948\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 861/880\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0212 - acc: 0.5585 - val_loss: 0.0224 - val_acc: 0.5361\n",
      "Epoch 862/880\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0210 - acc: 0.5541 - val_loss: 0.0208 - val_acc: 0.5716\n",
      "Epoch 863/880\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0208 - acc: 0.5561 - val_loss: 0.0221 - val_acc: 0.5549\n",
      "Epoch 864/880\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0210 - acc: 0.5528 - val_loss: 0.0215 - val_acc: 0.5809\n",
      "Epoch 865/880\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0211 - acc: 0.5504 - val_loss: 0.0210 - val_acc: 0.5110\n",
      "Epoch 866/880\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0214 - acc: 0.5511 - val_loss: 0.0212 - val_acc: 0.5750\n",
      "Epoch 867/880\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0215 - acc: 0.5522 - val_loss: 0.0214 - val_acc: 0.5419\n",
      "Epoch 868/880\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0206 - acc: 0.5529 - val_loss: 0.0195 - val_acc: 0.5984\n",
      "Epoch 869/880\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0208 - acc: 0.5555 - val_loss: 0.0216 - val_acc: 0.5126\n",
      "Epoch 870/880\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0203 - acc: 0.5404 - val_loss: 0.0205 - val_acc: 0.5766\n",
      "Epoch 871/880\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0211 - acc: 0.5417 - val_loss: 0.0204 - val_acc: 0.5440\n",
      "Epoch 872/880\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0209 - acc: 0.5532 - val_loss: 0.0220 - val_acc: 0.5603\n",
      "Epoch 873/880\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0209 - acc: 0.5552 - val_loss: 0.0212 - val_acc: 0.5218\n",
      "Epoch 874/880\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0206 - acc: 0.5103 - val_loss: 0.0204 - val_acc: 0.4519\n",
      "Epoch 875/880\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0207 - acc: 0.4915 - val_loss: 0.0208 - val_acc: 0.5505\n",
      "Epoch 876/880\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0209 - acc: 0.4897 - val_loss: 0.0214 - val_acc: 0.4267\n",
      "Epoch 877/880\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0209 - acc: 0.4897 - val_loss: 0.0214 - val_acc: 0.5326\n",
      "Epoch 878/880\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0208 - acc: 0.4869 - val_loss: 0.0207 - val_acc: 0.4376\n",
      "Epoch 879/880\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0209 - acc: 0.5233 - val_loss: 0.0217 - val_acc: 0.5801\n",
      "Epoch 880/880\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0205 - acc: 0.5475 - val_loss: 0.0214 - val_acc: 0.4317\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 861/880\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0169 - acc: 0.3926 - val_loss: 0.0107 - val_acc: 0.3179\n",
      "Epoch 862/880\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0168 - acc: 0.3889 - val_loss: 0.0239 - val_acc: 0.3177\n",
      "Epoch 863/880\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0169 - acc: 0.3738 - val_loss: 0.0098 - val_acc: 0.3200\n",
      "Epoch 864/880\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0166 - acc: 0.4086 - val_loss: 0.0239 - val_acc: 0.4552\n",
      "Epoch 865/880\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0170 - acc: 0.3900 - val_loss: 0.0103 - val_acc: 0.5371\n",
      "Epoch 866/880\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0166 - acc: 0.3973 - val_loss: 0.0228 - val_acc: 0.4615\n",
      "Epoch 867/880\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0170 - acc: 0.4153 - val_loss: 0.0104 - val_acc: 0.3276\n",
      "Epoch 868/880\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0170 - acc: 0.3789 - val_loss: 0.0241 - val_acc: 0.3219\n",
      "Epoch 869/880\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0169 - acc: 0.4521 - val_loss: 0.0101 - val_acc: 0.5402\n",
      "Epoch 870/880\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0168 - acc: 0.4389 - val_loss: 0.0224 - val_acc: 0.3134\n",
      "Epoch 871/880\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0170 - acc: 0.3829 - val_loss: 0.0102 - val_acc: 0.3198\n",
      "Epoch 872/880\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0165 - acc: 0.3621 - val_loss: 0.0230 - val_acc: 0.3192\n",
      "Epoch 873/880\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0170 - acc: 0.3891 - val_loss: 0.0101 - val_acc: 0.3229\n",
      "Epoch 874/880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0167 - acc: 0.4008 - val_loss: 0.0228 - val_acc: 0.3085\n",
      "Epoch 875/880\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0169 - acc: 0.3910 - val_loss: 0.0107 - val_acc: 0.3223\n",
      "Epoch 876/880\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0168 - acc: 0.3879 - val_loss: 0.0228 - val_acc: 0.3110\n",
      "Epoch 877/880\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0170 - acc: 0.3942 - val_loss: 0.0098 - val_acc: 0.5380\n",
      "Epoch 878/880\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0170 - acc: 0.3852 - val_loss: 0.0232 - val_acc: 0.3137\n",
      "Epoch 879/880\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0170 - acc: 0.3597 - val_loss: 0.0110 - val_acc: 0.3203\n",
      "Epoch 880/880\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0170 - acc: 0.3778 - val_loss: 0.0238 - val_acc: 0.3211\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 861/880\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0137 - acc: 0.3766 - val_loss: 0.0143 - val_acc: 0.3052\n",
      "Epoch 862/880\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0138 - acc: 0.3704 - val_loss: 0.0136 - val_acc: 0.4459\n",
      "Epoch 863/880\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0139 - acc: 0.3783 - val_loss: 0.0138 - val_acc: 0.3101\n",
      "Epoch 864/880\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0137 - acc: 0.3754 - val_loss: 0.0143 - val_acc: 0.4530\n",
      "Epoch 865/880\n",
      "10976/10976 [==============================] - 4s 363us/step - loss: 0.0138 - acc: 0.3818 - val_loss: 0.0140 - val_acc: 0.3095\n",
      "Epoch 866/880\n",
      "10976/10976 [==============================] - 4s 358us/step - loss: 0.0137 - acc: 0.3714 - val_loss: 0.0131 - val_acc: 0.4450\n",
      "Epoch 867/880\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0139 - acc: 0.3770 - val_loss: 0.0126 - val_acc: 0.3273\n",
      "Epoch 868/880\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0138 - acc: 0.3759 - val_loss: 0.0137 - val_acc: 0.4468\n",
      "Epoch 869/880\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0136 - acc: 0.3838 - val_loss: 0.0137 - val_acc: 0.3099\n",
      "Epoch 870/880\n",
      "10976/10976 [==============================] - 4s 355us/step - loss: 0.0138 - acc: 0.3768 - val_loss: 0.0133 - val_acc: 0.4392\n",
      "Epoch 871/880\n",
      "10976/10976 [==============================] - 4s 356us/step - loss: 0.0138 - acc: 0.3738 - val_loss: 0.0140 - val_acc: 0.3059\n",
      "Epoch 872/880\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0138 - acc: 0.3736 - val_loss: 0.0140 - val_acc: 0.4408\n",
      "Epoch 873/880\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0138 - acc: 0.3723 - val_loss: 0.0134 - val_acc: 0.3088\n",
      "Epoch 874/880\n",
      "10976/10976 [==============================] - 4s 355us/step - loss: 0.0138 - acc: 0.3696 - val_loss: 0.0133 - val_acc: 0.4448\n",
      "Epoch 875/880\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0138 - acc: 0.3785 - val_loss: 0.0136 - val_acc: 0.3145\n",
      "Epoch 876/880\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0138 - acc: 0.3753 - val_loss: 0.0139 - val_acc: 0.4520\n",
      "Epoch 877/880\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0137 - acc: 0.3759 - val_loss: 0.0139 - val_acc: 0.2992\n",
      "Epoch 878/880\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0138 - acc: 0.3802 - val_loss: 0.0138 - val_acc: 0.4509\n",
      "Epoch 879/880\n",
      "10976/10976 [==============================] - 4s 356us/step - loss: 0.0138 - acc: 0.3734 - val_loss: 0.0151 - val_acc: 0.3092\n",
      "Epoch 880/880\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0137 - acc: 0.3668 - val_loss: 0.0137 - val_acc: 0.4861\n",
      "start training round 44\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 881/900\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0182 - acc: 0.4461 - val_loss: 0.0170 - val_acc: 0.3898\n",
      "Epoch 882/900\n",
      "10976/10976 [==============================] - 4s 344us/step - loss: 0.0180 - acc: 0.4370 - val_loss: 0.0195 - val_acc: 0.5002\n",
      "Epoch 883/900\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0180 - acc: 0.4451 - val_loss: 0.0172 - val_acc: 0.3942\n",
      "Epoch 884/900\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0181 - acc: 0.4395 - val_loss: 0.0191 - val_acc: 0.5037\n",
      "Epoch 885/900\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0183 - acc: 0.4410 - val_loss: 0.0169 - val_acc: 0.3909\n",
      "Epoch 886/900\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0181 - acc: 0.4404 - val_loss: 0.0186 - val_acc: 0.4912\n",
      "Epoch 887/900\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0182 - acc: 0.4404 - val_loss: 0.0167 - val_acc: 0.3913\n",
      "Epoch 888/900\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0179 - acc: 0.4408 - val_loss: 0.0193 - val_acc: 0.5034\n",
      "Epoch 889/900\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0180 - acc: 0.4434 - val_loss: 0.0157 - val_acc: 0.4052\n",
      "Epoch 890/900\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0181 - acc: 0.4422 - val_loss: 0.0190 - val_acc: 0.4932\n",
      "Epoch 891/900\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0181 - acc: 0.4374 - val_loss: 0.0168 - val_acc: 0.3910\n",
      "Epoch 892/900\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0180 - acc: 0.4397 - val_loss: 0.0195 - val_acc: 0.5036\n",
      "Epoch 893/900\n",
      "10976/10976 [==============================] - 4s 344us/step - loss: 0.0181 - acc: 0.4427 - val_loss: 0.0177 - val_acc: 0.3898\n",
      "Epoch 894/900\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0181 - acc: 0.4390 - val_loss: 0.0191 - val_acc: 0.4952\n",
      "Epoch 895/900\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0181 - acc: 0.4440 - val_loss: 0.0169 - val_acc: 0.3890\n",
      "Epoch 896/900\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0179 - acc: 0.4421 - val_loss: 0.0193 - val_acc: 0.5040\n",
      "Epoch 897/900\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0180 - acc: 0.4446 - val_loss: 0.0178 - val_acc: 0.3904\n",
      "Epoch 898/900\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0181 - acc: 0.4393 - val_loss: 0.0171 - val_acc: 0.4729\n",
      "Epoch 899/900\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0179 - acc: 0.4427 - val_loss: 0.0164 - val_acc: 0.3932\n",
      "Epoch 900/900\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0179 - acc: 0.4414 - val_loss: 0.0187 - val_acc: 0.4906\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 881/900\n",
      "10976/10976 [==============================] - 4s 344us/step - loss: 0.0206 - acc: 0.4888 - val_loss: 0.0210 - val_acc: 0.5381\n",
      "Epoch 882/900\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0210 - acc: 0.4896 - val_loss: 0.0203 - val_acc: 0.4559\n",
      "Epoch 883/900\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0209 - acc: 0.4956 - val_loss: 0.0213 - val_acc: 0.4824\n",
      "Epoch 884/900\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0207 - acc: 0.4933 - val_loss: 0.0211 - val_acc: 0.4845\n",
      "Epoch 885/900\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0207 - acc: 0.4974 - val_loss: 0.0203 - val_acc: 0.4945\n",
      "Epoch 886/900\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0209 - acc: 0.4907 - val_loss: 0.0215 - val_acc: 0.5235\n",
      "Epoch 887/900\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0207 - acc: 0.4952 - val_loss: 0.0209 - val_acc: 0.4890\n",
      "Epoch 888/900\n",
      "10976/10976 [==============================] - 4s 344us/step - loss: 0.0210 - acc: 0.4905 - val_loss: 0.0211 - val_acc: 0.5223\n",
      "Epoch 889/900\n",
      "10976/10976 [==============================] - 4s 343us/step - loss: 0.0208 - acc: 0.4907 - val_loss: 0.0213 - val_acc: 0.4801\n",
      "Epoch 890/900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0208 - acc: 0.4911 - val_loss: 0.0213 - val_acc: 0.5549\n",
      "Epoch 891/900\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0207 - acc: 0.4946 - val_loss: 0.0215 - val_acc: 0.4820\n",
      "Epoch 892/900\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0209 - acc: 0.4911 - val_loss: 0.0198 - val_acc: 0.4835\n",
      "Epoch 893/900\n",
      "10976/10976 [==============================] - 4s 344us/step - loss: 0.0207 - acc: 0.4906 - val_loss: 0.0208 - val_acc: 0.5428\n",
      "Epoch 894/900\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0209 - acc: 0.4916 - val_loss: 0.0211 - val_acc: 0.5029\n",
      "Epoch 895/900\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0210 - acc: 0.4945 - val_loss: 0.0207 - val_acc: 0.4855\n",
      "Epoch 896/900\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0207 - acc: 0.4949 - val_loss: 0.0198 - val_acc: 0.4605\n",
      "Epoch 897/900\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0207 - acc: 0.4919 - val_loss: 0.0205 - val_acc: 0.4935\n",
      "Epoch 898/900\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0205 - acc: 0.4906 - val_loss: 0.0208 - val_acc: 0.4451\n",
      "Epoch 899/900\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0206 - acc: 0.4941 - val_loss: 0.0205 - val_acc: 0.4883\n",
      "Epoch 900/900\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0208 - acc: 0.4932 - val_loss: 0.0211 - val_acc: 0.4759\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 881/900\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0169 - acc: 0.3809 - val_loss: 0.0100 - val_acc: 0.4666\n",
      "Epoch 882/900\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0168 - acc: 0.3946 - val_loss: 0.0234 - val_acc: 0.3297\n",
      "Epoch 883/900\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0170 - acc: 0.4213 - val_loss: 0.0119 - val_acc: 0.3109\n",
      "Epoch 884/900\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0167 - acc: 0.4586 - val_loss: 0.0217 - val_acc: 0.3036\n",
      "Epoch 885/900\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0169 - acc: 0.3721 - val_loss: 0.0104 - val_acc: 0.4569\n",
      "Epoch 886/900\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0166 - acc: 0.3937 - val_loss: 0.0227 - val_acc: 0.4431\n",
      "Epoch 887/900\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0171 - acc: 0.3712 - val_loss: 0.0096 - val_acc: 0.5386\n",
      "Epoch 888/900\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0164 - acc: 0.4519 - val_loss: 0.0209 - val_acc: 0.3005\n",
      "Epoch 889/900\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0168 - acc: 0.3744 - val_loss: 0.0100 - val_acc: 0.3258\n",
      "Epoch 890/900\n",
      "10976/10976 [==============================] - 4s 344us/step - loss: 0.0166 - acc: 0.4078 - val_loss: 0.0226 - val_acc: 0.3190\n",
      "Epoch 891/900\n",
      "10976/10976 [==============================] - 4s 343us/step - loss: 0.0168 - acc: 0.3912 - val_loss: 0.0098 - val_acc: 0.4590\n",
      "Epoch 892/900\n",
      "10976/10976 [==============================] - 4s 343us/step - loss: 0.0167 - acc: 0.3866 - val_loss: 0.0231 - val_acc: 0.3159\n",
      "Epoch 893/900\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0170 - acc: 0.4133 - val_loss: 0.0105 - val_acc: 0.5391\n",
      "Epoch 894/900\n",
      "10976/10976 [==============================] - 4s 344us/step - loss: 0.0166 - acc: 0.3888 - val_loss: 0.0232 - val_acc: 0.3231\n",
      "Epoch 895/900\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0170 - acc: 0.4004 - val_loss: 0.0103 - val_acc: 0.4668\n",
      "Epoch 896/900\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0168 - acc: 0.4221 - val_loss: 0.0224 - val_acc: 0.3179\n",
      "Epoch 897/900\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0167 - acc: 0.4351 - val_loss: 0.0101 - val_acc: 0.3231\n",
      "Epoch 898/900\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0168 - acc: 0.4049 - val_loss: 0.0236 - val_acc: 0.3209\n",
      "Epoch 899/900\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0169 - acc: 0.3713 - val_loss: 0.0104 - val_acc: 0.3276\n",
      "Epoch 900/900\n",
      "10976/10976 [==============================] - 4s 344us/step - loss: 0.0167 - acc: 0.4218 - val_loss: 0.0224 - val_acc: 0.3106\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 881/900\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0139 - acc: 0.3783 - val_loss: 0.0138 - val_acc: 0.3191\n",
      "Epoch 882/900\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0137 - acc: 0.3768 - val_loss: 0.0131 - val_acc: 0.4465\n",
      "Epoch 883/900\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0139 - acc: 0.3726 - val_loss: 0.0139 - val_acc: 0.3066\n",
      "Epoch 884/900\n",
      "10976/10976 [==============================] - 4s 360us/step - loss: 0.0138 - acc: 0.3794 - val_loss: 0.0133 - val_acc: 0.4468\n",
      "Epoch 885/900\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0138 - acc: 0.3823 - val_loss: 0.0131 - val_acc: 0.3192\n",
      "Epoch 886/900\n",
      "10976/10976 [==============================] - 4s 357us/step - loss: 0.0137 - acc: 0.3807 - val_loss: 0.0139 - val_acc: 0.4798\n",
      "Epoch 887/900\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0138 - acc: 0.3765 - val_loss: 0.0133 - val_acc: 0.3184\n",
      "Epoch 888/900\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0138 - acc: 0.3785 - val_loss: 0.0141 - val_acc: 0.4445\n",
      "Epoch 889/900\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0137 - acc: 0.3824 - val_loss: 0.0134 - val_acc: 0.3153\n",
      "Epoch 890/900\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0138 - acc: 0.3791 - val_loss: 0.0145 - val_acc: 0.4410\n",
      "Epoch 891/900\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0138 - acc: 0.3838 - val_loss: 0.0134 - val_acc: 0.3163\n",
      "Epoch 892/900\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0139 - acc: 0.3850 - val_loss: 0.0130 - val_acc: 0.4281\n",
      "Epoch 893/900\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0137 - acc: 0.3741 - val_loss: 0.0130 - val_acc: 0.3199\n",
      "Epoch 894/900\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0138 - acc: 0.3725 - val_loss: 0.0123 - val_acc: 0.4364\n",
      "Epoch 895/900\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0138 - acc: 0.3741 - val_loss: 0.0138 - val_acc: 0.3066\n",
      "Epoch 896/900\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0138 - acc: 0.3766 - val_loss: 0.0144 - val_acc: 0.4517\n",
      "Epoch 897/900\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0137 - acc: 0.3787 - val_loss: 0.0137 - val_acc: 0.3194\n",
      "Epoch 898/900\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0137 - acc: 0.3803 - val_loss: 0.0135 - val_acc: 0.4487\n",
      "Epoch 899/900\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0138 - acc: 0.3750 - val_loss: 0.0138 - val_acc: 0.3154\n",
      "Epoch 900/900\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0137 - acc: 0.3809 - val_loss: 0.0134 - val_acc: 0.4426\n",
      "start training round 45\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 901/920\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0181 - acc: 0.4480 - val_loss: 0.0170 - val_acc: 0.3906\n",
      "Epoch 902/920\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0178 - acc: 0.4402 - val_loss: 0.0190 - val_acc: 0.5045\n",
      "Epoch 903/920\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0182 - acc: 0.4450 - val_loss: 0.0174 - val_acc: 0.3890\n",
      "Epoch 904/920\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0178 - acc: 0.4431 - val_loss: 0.0182 - val_acc: 0.4965\n",
      "Epoch 905/920\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0182 - acc: 0.4459 - val_loss: 0.0170 - val_acc: 0.3934\n",
      "Epoch 906/920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0180 - acc: 0.4396 - val_loss: 0.0188 - val_acc: 0.5056\n",
      "Epoch 907/920\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0182 - acc: 0.4483 - val_loss: 0.0166 - val_acc: 0.3940\n",
      "Epoch 908/920\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0178 - acc: 0.4435 - val_loss: 0.0190 - val_acc: 0.5039\n",
      "Epoch 909/920\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0179 - acc: 0.4475 - val_loss: 0.0164 - val_acc: 0.3938\n",
      "Epoch 910/920\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0181 - acc: 0.4433 - val_loss: 0.0194 - val_acc: 0.5045\n",
      "Epoch 911/920\n",
      "10976/10976 [==============================] - 4s 359us/step - loss: 0.0179 - acc: 0.4454 - val_loss: 0.0166 - val_acc: 0.3925\n",
      "Epoch 912/920\n",
      "10976/10976 [==============================] - 4s 386us/step - loss: 0.0180 - acc: 0.4419 - val_loss: 0.0191 - val_acc: 0.4933\n",
      "Epoch 913/920\n",
      "10976/10976 [==============================] - 4s 384us/step - loss: 0.0180 - acc: 0.4444 - val_loss: 0.0176 - val_acc: 0.3874\n",
      "Epoch 914/920\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0181 - acc: 0.4398 - val_loss: 0.0184 - val_acc: 0.4836\n",
      "Epoch 915/920\n",
      "10976/10976 [==============================] - 4s 357us/step - loss: 0.0179 - acc: 0.4427 - val_loss: 0.0178 - val_acc: 0.3840\n",
      "Epoch 916/920\n",
      "10976/10976 [==============================] - 4s 387us/step - loss: 0.0181 - acc: 0.4424 - val_loss: 0.0203 - val_acc: 0.4972\n",
      "Epoch 917/920\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0181 - acc: 0.4421 - val_loss: 0.0169 - val_acc: 0.3889\n",
      "Epoch 918/920\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0180 - acc: 0.4372 - val_loss: 0.0193 - val_acc: 0.5002\n",
      "Epoch 919/920\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0179 - acc: 0.4457 - val_loss: 0.0174 - val_acc: 0.3863\n",
      "Epoch 920/920\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0180 - acc: 0.4415 - val_loss: 0.0189 - val_acc: 0.4947\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 901/920\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0206 - acc: 0.5002 - val_loss: 0.0207 - val_acc: 0.4889\n",
      "Epoch 902/920\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0210 - acc: 0.4878 - val_loss: 0.0204 - val_acc: 0.5107\n",
      "Epoch 903/920\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0207 - acc: 0.4954 - val_loss: 0.0206 - val_acc: 0.4857\n",
      "Epoch 904/920\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0206 - acc: 0.4928 - val_loss: 0.0200 - val_acc: 0.4623\n",
      "Epoch 905/920\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0209 - acc: 0.4992 - val_loss: 0.0220 - val_acc: 0.6001\n",
      "Epoch 906/920\n",
      "10976/10976 [==============================] - 4s 365us/step - loss: 0.0215 - acc: 0.5598 - val_loss: 0.0227 - val_acc: 0.5250\n",
      "Epoch 907/920\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0207 - acc: 0.5370 - val_loss: 0.0206 - val_acc: 0.5774\n",
      "Epoch 908/920\n",
      "10976/10976 [==============================] - 4s 370us/step - loss: 0.0207 - acc: 0.5196 - val_loss: 0.0220 - val_acc: 0.4515\n",
      "Epoch 909/920\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0209 - acc: 0.5127 - val_loss: 0.0204 - val_acc: 0.5800\n",
      "Epoch 910/920\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0207 - acc: 0.5216 - val_loss: 0.0211 - val_acc: 0.4809\n",
      "Epoch 911/920\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0206 - acc: 0.5418 - val_loss: 0.0204 - val_acc: 0.5999\n",
      "Epoch 912/920\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0211 - acc: 0.5545 - val_loss: 0.0218 - val_acc: 0.5238\n",
      "Epoch 913/920\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0211 - acc: 0.5516 - val_loss: 0.0205 - val_acc: 0.5844\n",
      "Epoch 914/920\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0210 - acc: 0.5594 - val_loss: 0.0210 - val_acc: 0.5371\n",
      "Epoch 915/920\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0210 - acc: 0.5552 - val_loss: 0.0195 - val_acc: 0.5885\n",
      "Epoch 916/920\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0208 - acc: 0.5560 - val_loss: 0.0213 - val_acc: 0.5601\n",
      "Epoch 917/920\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0212 - acc: 0.5569 - val_loss: 0.0198 - val_acc: 0.5832\n",
      "Epoch 918/920\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0212 - acc: 0.5565 - val_loss: 0.0220 - val_acc: 0.5593\n",
      "Epoch 919/920\n",
      "10976/10976 [==============================] - 4s 356us/step - loss: 0.0210 - acc: 0.5510 - val_loss: 0.0224 - val_acc: 0.5876\n",
      "Epoch 920/920\n",
      "10976/10976 [==============================] - 4s 374us/step - loss: 0.0206 - acc: 0.5382 - val_loss: 0.0221 - val_acc: 0.5263\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 901/920\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0169 - acc: 0.4137 - val_loss: 0.0120 - val_acc: 0.3246\n",
      "Epoch 902/920\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0167 - acc: 0.4377 - val_loss: 0.0227 - val_acc: 0.3269\n",
      "Epoch 903/920\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0169 - acc: 0.4333 - val_loss: 0.0108 - val_acc: 0.5410\n",
      "Epoch 904/920\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0167 - acc: 0.4298 - val_loss: 0.0235 - val_acc: 0.3192\n",
      "Epoch 905/920\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0169 - acc: 0.4020 - val_loss: 0.0103 - val_acc: 0.5390\n",
      "Epoch 906/920\n",
      "10976/10976 [==============================] - 4s 355us/step - loss: 0.0165 - acc: 0.4195 - val_loss: 0.0230 - val_acc: 0.5387\n",
      "Epoch 907/920\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0170 - acc: 0.4356 - val_loss: 0.0118 - val_acc: 0.4594\n",
      "Epoch 908/920\n",
      "10976/10976 [==============================] - 4s 377us/step - loss: 0.0166 - acc: 0.3924 - val_loss: 0.0210 - val_acc: 0.3003\n",
      "Epoch 909/920\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0169 - acc: 0.3773 - val_loss: 0.0107 - val_acc: 0.5360\n",
      "Epoch 910/920\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0167 - acc: 0.4091 - val_loss: 0.0229 - val_acc: 0.5400\n",
      "Epoch 911/920\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0170 - acc: 0.4147 - val_loss: 0.0104 - val_acc: 0.5411\n",
      "Epoch 912/920\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0167 - acc: 0.3901 - val_loss: 0.0225 - val_acc: 0.3171\n",
      "Epoch 913/920\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0170 - acc: 0.3816 - val_loss: 0.0108 - val_acc: 0.5388\n",
      "Epoch 914/920\n",
      "10976/10976 [==============================] - 4s 343us/step - loss: 0.0166 - acc: 0.4123 - val_loss: 0.0236 - val_acc: 0.4590\n",
      "Epoch 915/920\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0171 - acc: 0.3756 - val_loss: 0.0102 - val_acc: 0.5363\n",
      "Epoch 916/920\n",
      "10976/10976 [==============================] - 4s 355us/step - loss: 0.0165 - acc: 0.4262 - val_loss: 0.0225 - val_acc: 0.3124\n",
      "Epoch 917/920\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0170 - acc: 0.3946 - val_loss: 0.0100 - val_acc: 0.5373\n",
      "Epoch 918/920\n",
      "10976/10976 [==============================] - 4s 363us/step - loss: 0.0166 - acc: 0.3915 - val_loss: 0.0225 - val_acc: 0.4617\n",
      "Epoch 919/920\n",
      "10976/10976 [==============================] - 4s 363us/step - loss: 0.0168 - acc: 0.4085 - val_loss: 0.0104 - val_acc: 0.5399\n",
      "Epoch 920/920\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0165 - acc: 0.3777 - val_loss: 0.0238 - val_acc: 0.3185\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 901/920\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0137 - acc: 0.3819 - val_loss: 0.0142 - val_acc: 0.3072\n",
      "Epoch 902/920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 4s 357us/step - loss: 0.0138 - acc: 0.3757 - val_loss: 0.0140 - val_acc: 0.4821\n",
      "Epoch 903/920\n",
      "10976/10976 [==============================] - 4s 378us/step - loss: 0.0138 - acc: 0.3820 - val_loss: 0.0134 - val_acc: 0.3204\n",
      "Epoch 904/920\n",
      "10976/10976 [==============================] - 4s 355us/step - loss: 0.0137 - acc: 0.3794 - val_loss: 0.0134 - val_acc: 0.4444\n",
      "Epoch 905/920\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0138 - acc: 0.3800 - val_loss: 0.0132 - val_acc: 0.3140\n",
      "Epoch 906/920\n",
      "10976/10976 [==============================] - 4s 355us/step - loss: 0.0136 - acc: 0.3815 - val_loss: 0.0135 - val_acc: 0.4514\n",
      "Epoch 907/920\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0138 - acc: 0.3787 - val_loss: 0.0134 - val_acc: 0.3140\n",
      "Epoch 908/920\n",
      "10976/10976 [==============================] - 4s 356us/step - loss: 0.0138 - acc: 0.3769 - val_loss: 0.0133 - val_acc: 0.4400\n",
      "Epoch 909/920\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0137 - acc: 0.3798 - val_loss: 0.0138 - val_acc: 0.3117\n",
      "Epoch 910/920\n",
      "10976/10976 [==============================] - 4s 358us/step - loss: 0.0139 - acc: 0.3826 - val_loss: 0.0143 - val_acc: 0.4482\n",
      "Epoch 911/920\n",
      "10976/10976 [==============================] - 4s 356us/step - loss: 0.0138 - acc: 0.3803 - val_loss: 0.0139 - val_acc: 0.3126\n",
      "Epoch 912/920\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0137 - acc: 0.3751 - val_loss: 0.0140 - val_acc: 0.4594\n",
      "Epoch 913/920\n",
      "10976/10976 [==============================] - 4s 358us/step - loss: 0.0137 - acc: 0.3791 - val_loss: 0.0143 - val_acc: 0.3091\n",
      "Epoch 914/920\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0138 - acc: 0.3789 - val_loss: 0.0129 - val_acc: 0.4446\n",
      "Epoch 915/920\n",
      "10976/10976 [==============================] - 4s 357us/step - loss: 0.0136 - acc: 0.3780 - val_loss: 0.0133 - val_acc: 0.3222\n",
      "Epoch 916/920\n",
      "10976/10976 [==============================] - 4s 378us/step - loss: 0.0137 - acc: 0.3733 - val_loss: 0.0137 - val_acc: 0.4494\n",
      "Epoch 917/920\n",
      "10976/10976 [==============================] - 4s 376us/step - loss: 0.0138 - acc: 0.3785 - val_loss: 0.0127 - val_acc: 0.3283\n",
      "Epoch 918/920\n",
      "10976/10976 [==============================] - 4s 373us/step - loss: 0.0136 - acc: 0.3767 - val_loss: 0.0129 - val_acc: 0.4454\n",
      "Epoch 919/920\n",
      "10976/10976 [==============================] - 4s 372us/step - loss: 0.0136 - acc: 0.3801 - val_loss: 0.0135 - val_acc: 0.3212\n",
      "Epoch 920/920\n",
      "10976/10976 [==============================] - 4s 376us/step - loss: 0.0137 - acc: 0.3746 - val_loss: 0.0133 - val_acc: 0.4485\n",
      "start training round 46\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 921/940\n",
      "10976/10976 [==============================] - 4s 371us/step - loss: 0.0181 - acc: 0.4425 - val_loss: 0.0168 - val_acc: 0.3899\n",
      "Epoch 922/940\n",
      "10976/10976 [==============================] - 4s 382us/step - loss: 0.0180 - acc: 0.4432 - val_loss: 0.0202 - val_acc: 0.4993\n",
      "Epoch 923/940\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0180 - acc: 0.4433 - val_loss: 0.0180 - val_acc: 0.3839\n",
      "Epoch 924/940\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0181 - acc: 0.4422 - val_loss: 0.0193 - val_acc: 0.5003\n",
      "Epoch 925/940\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0179 - acc: 0.4438 - val_loss: 0.0168 - val_acc: 0.3966\n",
      "Epoch 926/940\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0179 - acc: 0.4413 - val_loss: 0.0189 - val_acc: 0.5011\n",
      "Epoch 927/940\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0180 - acc: 0.4452 - val_loss: 0.0165 - val_acc: 0.3969\n",
      "Epoch 928/940\n",
      "10976/10976 [==============================] - 4s 357us/step - loss: 0.0181 - acc: 0.4391 - val_loss: 0.0192 - val_acc: 0.4959\n",
      "Epoch 929/940\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0180 - acc: 0.4453 - val_loss: 0.0186 - val_acc: 0.3822\n",
      "Epoch 930/940\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0179 - acc: 0.4412 - val_loss: 0.0193 - val_acc: 0.5042\n",
      "Epoch 931/940\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0180 - acc: 0.4425 - val_loss: 0.0173 - val_acc: 0.3903\n",
      "Epoch 932/940\n",
      "10976/10976 [==============================] - 4s 377us/step - loss: 0.0181 - acc: 0.4415 - val_loss: 0.0189 - val_acc: 0.5051\n",
      "Epoch 933/940\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0182 - acc: 0.4420 - val_loss: 0.0167 - val_acc: 0.3938\n",
      "Epoch 934/940\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0179 - acc: 0.4397 - val_loss: 0.0158 - val_acc: 0.4702\n",
      "Epoch 935/940\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0180 - acc: 0.4437 - val_loss: 0.0181 - val_acc: 0.3849\n",
      "Epoch 936/940\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0180 - acc: 0.4459 - val_loss: 0.0195 - val_acc: 0.4993\n",
      "Epoch 937/940\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0180 - acc: 0.4437 - val_loss: 0.0171 - val_acc: 0.3902\n",
      "Epoch 938/940\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0180 - acc: 0.4425 - val_loss: 0.0185 - val_acc: 0.4955\n",
      "Epoch 939/940\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0182 - acc: 0.4405 - val_loss: 0.0157 - val_acc: 0.3986\n",
      "Epoch 940/940\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0178 - acc: 0.4418 - val_loss: 0.0191 - val_acc: 0.5033\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 921/940\n",
      "10976/10976 [==============================] - 4s 344us/step - loss: 0.0208 - acc: 0.5168 - val_loss: 0.0211 - val_acc: 0.5422\n",
      "Epoch 922/940\n",
      "10976/10976 [==============================] - 4s 344us/step - loss: 0.0206 - acc: 0.4891 - val_loss: 0.0214 - val_acc: 0.4233\n",
      "Epoch 923/940\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0206 - acc: 0.4911 - val_loss: 0.0206 - val_acc: 0.5328\n",
      "Epoch 924/940\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0205 - acc: 0.4967 - val_loss: 0.0218 - val_acc: 0.4962\n",
      "Epoch 925/940\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0206 - acc: 0.4976 - val_loss: 0.0194 - val_acc: 0.5135\n",
      "Epoch 926/940\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0208 - acc: 0.4935 - val_loss: 0.0218 - val_acc: 0.5244\n",
      "Epoch 927/940\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0205 - acc: 0.5111 - val_loss: 0.0212 - val_acc: 0.5923\n",
      "Epoch 928/940\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0205 - acc: 0.5277 - val_loss: 0.0215 - val_acc: 0.4547\n",
      "Epoch 929/940\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0204 - acc: 0.5373 - val_loss: 0.0214 - val_acc: 0.4877\n",
      "Epoch 930/940\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0207 - acc: 0.4979 - val_loss: 0.0200 - val_acc: 0.5381\n",
      "Epoch 931/940\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0210 - acc: 0.4927 - val_loss: 0.0203 - val_acc: 0.4921\n",
      "Epoch 932/940\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0207 - acc: 0.4963 - val_loss: 0.0209 - val_acc: 0.5291\n",
      "Epoch 933/940\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0206 - acc: 0.4941 - val_loss: 0.0211 - val_acc: 0.5625\n",
      "Epoch 934/940\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0204 - acc: 0.4961 - val_loss: 0.0216 - val_acc: 0.4299\n",
      "Epoch 935/940\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0208 - acc: 0.4936 - val_loss: 0.0212 - val_acc: 0.5613\n",
      "Epoch 936/940\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0207 - acc: 0.4974 - val_loss: 0.0204 - val_acc: 0.4467\n",
      "Epoch 937/940\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0208 - acc: 0.5101 - val_loss: 0.0205 - val_acc: 0.6014\n",
      "Epoch 938/940\n",
      "10976/10976 [==============================] - 4s 344us/step - loss: 0.0205 - acc: 0.5294 - val_loss: 0.0211 - val_acc: 0.4813\n",
      "Epoch 939/940\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 4s 344us/step - loss: 0.0211 - acc: 0.5529 - val_loss: 0.0199 - val_acc: 0.6002\n",
      "Epoch 940/940\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0209 - acc: 0.5589 - val_loss: 0.0196 - val_acc: 0.5529\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 921/940\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0171 - acc: 0.3557 - val_loss: 0.0105 - val_acc: 0.5424\n",
      "Epoch 922/940\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0168 - acc: 0.4120 - val_loss: 0.0233 - val_acc: 0.4593\n",
      "Epoch 923/940\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0171 - acc: 0.3994 - val_loss: 0.0101 - val_acc: 0.5408\n",
      "Epoch 924/940\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0167 - acc: 0.3922 - val_loss: 0.0236 - val_acc: 0.3182\n",
      "Epoch 925/940\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0171 - acc: 0.3812 - val_loss: 0.0093 - val_acc: 0.5414\n",
      "Epoch 926/940\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0166 - acc: 0.3824 - val_loss: 0.0234 - val_acc: 0.3295\n",
      "Epoch 927/940\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0169 - acc: 0.4151 - val_loss: 0.0104 - val_acc: 0.4684\n",
      "Epoch 928/940\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0168 - acc: 0.3880 - val_loss: 0.0236 - val_acc: 0.3279\n",
      "Epoch 929/940\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0170 - acc: 0.4577 - val_loss: 0.0112 - val_acc: 0.3197\n",
      "Epoch 930/940\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0168 - acc: 0.4178 - val_loss: 0.0231 - val_acc: 0.3206\n",
      "Epoch 931/940\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0170 - acc: 0.4252 - val_loss: 0.0111 - val_acc: 0.4633\n",
      "Epoch 932/940\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0168 - acc: 0.3942 - val_loss: 0.0240 - val_acc: 0.3239\n",
      "Epoch 933/940\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0168 - acc: 0.3979 - val_loss: 0.0106 - val_acc: 0.3121\n",
      "Epoch 934/940\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0167 - acc: 0.3915 - val_loss: 0.0231 - val_acc: 0.3199\n",
      "Epoch 935/940\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0169 - acc: 0.4397 - val_loss: 0.0108 - val_acc: 0.5426\n",
      "Epoch 936/940\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0166 - acc: 0.4460 - val_loss: 0.0244 - val_acc: 0.3248\n",
      "Epoch 937/940\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0171 - acc: 0.3789 - val_loss: 0.0117 - val_acc: 0.3207\n",
      "Epoch 938/940\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0167 - acc: 0.3606 - val_loss: 0.0236 - val_acc: 0.3241\n",
      "Epoch 939/940\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0170 - acc: 0.3818 - val_loss: 0.0109 - val_acc: 0.4610\n",
      "Epoch 940/940\n",
      "10976/10976 [==============================] - 4s 344us/step - loss: 0.0166 - acc: 0.3521 - val_loss: 0.0236 - val_acc: 0.3154\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 921/940\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0137 - acc: 0.3775 - val_loss: 0.0131 - val_acc: 0.3223\n",
      "Epoch 922/940\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0137 - acc: 0.3762 - val_loss: 0.0134 - val_acc: 0.4491\n",
      "Epoch 923/940\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0138 - acc: 0.3732 - val_loss: 0.0133 - val_acc: 0.3198\n",
      "Epoch 924/940\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0137 - acc: 0.3798 - val_loss: 0.0141 - val_acc: 0.4503\n",
      "Epoch 925/940\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0138 - acc: 0.3809 - val_loss: 0.0133 - val_acc: 0.3176\n",
      "Epoch 926/940\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0137 - acc: 0.3791 - val_loss: 0.0140 - val_acc: 0.4463\n",
      "Epoch 927/940\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0138 - acc: 0.3814 - val_loss: 0.0136 - val_acc: 0.3153\n",
      "Epoch 928/940\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0137 - acc: 0.3715 - val_loss: 0.0124 - val_acc: 0.4376\n",
      "Epoch 929/940\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0138 - acc: 0.3728 - val_loss: 0.0135 - val_acc: 0.3204\n",
      "Epoch 930/940\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0136 - acc: 0.3820 - val_loss: 0.0140 - val_acc: 0.4543\n",
      "Epoch 931/940\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0138 - acc: 0.3844 - val_loss: 0.0142 - val_acc: 0.3166\n",
      "Epoch 932/940\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0136 - acc: 0.3805 - val_loss: 0.0133 - val_acc: 0.4508\n",
      "Epoch 933/940\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0138 - acc: 0.3784 - val_loss: 0.0142 - val_acc: 0.3078\n",
      "Epoch 934/940\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0137 - acc: 0.3777 - val_loss: 0.0133 - val_acc: 0.4448\n",
      "Epoch 935/940\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0137 - acc: 0.3842 - val_loss: 0.0137 - val_acc: 0.3168\n",
      "Epoch 936/940\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0138 - acc: 0.3790 - val_loss: 0.0132 - val_acc: 0.4469\n",
      "Epoch 937/940\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0137 - acc: 0.3788 - val_loss: 0.0142 - val_acc: 0.3018\n",
      "Epoch 938/940\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0138 - acc: 0.3788 - val_loss: 0.0136 - val_acc: 0.4455\n",
      "Epoch 939/940\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0138 - acc: 0.3789 - val_loss: 0.0146 - val_acc: 0.3112\n",
      "Epoch 940/940\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0138 - acc: 0.3760 - val_loss: 0.0134 - val_acc: 0.4433\n",
      "start training round 47\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 941/960\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0179 - acc: 0.4464 - val_loss: 0.0163 - val_acc: 0.3976\n",
      "Epoch 942/960\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0179 - acc: 0.4415 - val_loss: 0.0189 - val_acc: 0.4987\n",
      "Epoch 943/960\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0178 - acc: 0.4464 - val_loss: 0.0187 - val_acc: 0.3828\n",
      "Epoch 944/960\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0179 - acc: 0.4426 - val_loss: 0.0186 - val_acc: 0.4995\n",
      "Epoch 945/960\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0180 - acc: 0.4428 - val_loss: 0.0179 - val_acc: 0.3876\n",
      "Epoch 946/960\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0182 - acc: 0.4437 - val_loss: 0.0193 - val_acc: 0.5030\n",
      "Epoch 947/960\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0180 - acc: 0.4424 - val_loss: 0.0163 - val_acc: 0.3971\n",
      "Epoch 948/960\n",
      "10976/10976 [==============================] - 4s 344us/step - loss: 0.0180 - acc: 0.4414 - val_loss: 0.0193 - val_acc: 0.5025\n",
      "Epoch 949/960\n",
      "10976/10976 [==============================] - 4s 344us/step - loss: 0.0179 - acc: 0.4439 - val_loss: 0.0178 - val_acc: 0.3861\n",
      "Epoch 950/960\n",
      "10976/10976 [==============================] - 4s 344us/step - loss: 0.0180 - acc: 0.4422 - val_loss: 0.0191 - val_acc: 0.5020\n",
      "Epoch 951/960\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0180 - acc: 0.4478 - val_loss: 0.0172 - val_acc: 0.3891\n",
      "Epoch 952/960\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0180 - acc: 0.4438 - val_loss: 0.0190 - val_acc: 0.5027\n",
      "Epoch 953/960\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0181 - acc: 0.4442 - val_loss: 0.0162 - val_acc: 0.3957\n",
      "Epoch 954/960\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0180 - acc: 0.4387 - val_loss: 0.0194 - val_acc: 0.5006\n",
      "Epoch 955/960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 4s 343us/step - loss: 0.0180 - acc: 0.4408 - val_loss: 0.0160 - val_acc: 0.3989\n",
      "Epoch 956/960\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0179 - acc: 0.4442 - val_loss: 0.0192 - val_acc: 0.5053\n",
      "Epoch 957/960\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0180 - acc: 0.4409 - val_loss: 0.0172 - val_acc: 0.3910\n",
      "Epoch 958/960\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0180 - acc: 0.4428 - val_loss: 0.0197 - val_acc: 0.4950\n",
      "Epoch 959/960\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0181 - acc: 0.4430 - val_loss: 0.0181 - val_acc: 0.3857\n",
      "Epoch 960/960\n",
      "10976/10976 [==============================] - 4s 343us/step - loss: 0.0180 - acc: 0.4406 - val_loss: 0.0178 - val_acc: 0.4797\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 941/960\n",
      "10976/10976 [==============================] - 4s 344us/step - loss: 0.0205 - acc: 0.5583 - val_loss: 0.0206 - val_acc: 0.5997\n",
      "Epoch 942/960\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0203 - acc: 0.5594 - val_loss: 0.0211 - val_acc: 0.5550\n",
      "Epoch 943/960\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0210 - acc: 0.5565 - val_loss: 0.0194 - val_acc: 0.6003\n",
      "Epoch 944/960\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0212 - acc: 0.5573 - val_loss: 0.0220 - val_acc: 0.5139\n",
      "Epoch 945/960\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0203 - acc: 0.5521 - val_loss: 0.0209 - val_acc: 0.5752\n",
      "Epoch 946/960\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0210 - acc: 0.5490 - val_loss: 0.0226 - val_acc: 0.5360\n",
      "Epoch 947/960\n",
      "10976/10976 [==============================] - 4s 343us/step - loss: 0.0211 - acc: 0.5536 - val_loss: 0.0198 - val_acc: 0.5777\n",
      "Epoch 948/960\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0209 - acc: 0.5572 - val_loss: 0.0217 - val_acc: 0.5350\n",
      "Epoch 949/960\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0209 - acc: 0.5575 - val_loss: 0.0196 - val_acc: 0.5751\n",
      "Epoch 950/960\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0209 - acc: 0.5567 - val_loss: 0.0219 - val_acc: 0.5520\n",
      "Epoch 951/960\n",
      "10976/10976 [==============================] - 4s 344us/step - loss: 0.0212 - acc: 0.5558 - val_loss: 0.0210 - val_acc: 0.5860\n",
      "Epoch 952/960\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0207 - acc: 0.5528 - val_loss: 0.0220 - val_acc: 0.5257\n",
      "Epoch 953/960\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0208 - acc: 0.5556 - val_loss: 0.0204 - val_acc: 0.5998\n",
      "Epoch 954/960\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0207 - acc: 0.5442 - val_loss: 0.0201 - val_acc: 0.4990\n",
      "Epoch 955/960\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0204 - acc: 0.5215 - val_loss: 0.0201 - val_acc: 0.5807\n",
      "Epoch 956/960\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0208 - acc: 0.5212 - val_loss: 0.0212 - val_acc: 0.4936\n",
      "Epoch 957/960\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0206 - acc: 0.5245 - val_loss: 0.0209 - val_acc: 0.5730\n",
      "Epoch 958/960\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0208 - acc: 0.5242 - val_loss: 0.0220 - val_acc: 0.4527\n",
      "Epoch 959/960\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0208 - acc: 0.5214 - val_loss: 0.0193 - val_acc: 0.5832\n",
      "Epoch 960/960\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0206 - acc: 0.5249 - val_loss: 0.0215 - val_acc: 0.4900\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 941/960\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0171 - acc: 0.4557 - val_loss: 0.0101 - val_acc: 0.5386\n",
      "Epoch 942/960\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0167 - acc: 0.4285 - val_loss: 0.0225 - val_acc: 0.3203\n",
      "Epoch 943/960\n",
      "10976/10976 [==============================] - 4s 344us/step - loss: 0.0170 - acc: 0.4534 - val_loss: 0.0104 - val_acc: 0.3235\n",
      "Epoch 944/960\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0168 - acc: 0.3949 - val_loss: 0.0234 - val_acc: 0.3298\n",
      "Epoch 945/960\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0169 - acc: 0.3849 - val_loss: 0.0117 - val_acc: 0.3299\n",
      "Epoch 946/960\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0167 - acc: 0.4057 - val_loss: 0.0232 - val_acc: 0.3247\n",
      "Epoch 947/960\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0171 - acc: 0.3881 - val_loss: 0.0110 - val_acc: 0.4486\n",
      "Epoch 948/960\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0168 - acc: 0.3702 - val_loss: 0.0230 - val_acc: 0.3207\n",
      "Epoch 949/960\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0171 - acc: 0.4039 - val_loss: 0.0116 - val_acc: 0.3118\n",
      "Epoch 950/960\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0167 - acc: 0.3521 - val_loss: 0.0246 - val_acc: 0.3231\n",
      "Epoch 951/960\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0168 - acc: 0.3701 - val_loss: 0.0097 - val_acc: 0.3251\n",
      "Epoch 952/960\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0166 - acc: 0.4271 - val_loss: 0.0223 - val_acc: 0.5399\n",
      "Epoch 953/960\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0171 - acc: 0.4285 - val_loss: 0.0098 - val_acc: 0.5374\n",
      "Epoch 954/960\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0166 - acc: 0.3697 - val_loss: 0.0222 - val_acc: 0.3084\n",
      "Epoch 955/960\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0170 - acc: 0.3679 - val_loss: 0.0100 - val_acc: 0.3273\n",
      "Epoch 956/960\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0166 - acc: 0.3903 - val_loss: 0.0220 - val_acc: 0.3101\n",
      "Epoch 957/960\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0171 - acc: 0.3573 - val_loss: 0.0099 - val_acc: 0.4621\n",
      "Epoch 958/960\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0167 - acc: 0.4129 - val_loss: 0.0217 - val_acc: 0.3139\n",
      "Epoch 959/960\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0168 - acc: 0.4364 - val_loss: 0.0111 - val_acc: 0.3194\n",
      "Epoch 960/960\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0167 - acc: 0.3739 - val_loss: 0.0229 - val_acc: 0.3166\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 941/960\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0137 - acc: 0.3777 - val_loss: 0.0136 - val_acc: 0.3023\n",
      "Epoch 942/960\n",
      "10976/10976 [==============================] - 4s 358us/step - loss: 0.0137 - acc: 0.3714 - val_loss: 0.0129 - val_acc: 0.4412\n",
      "Epoch 943/960\n",
      "10976/10976 [==============================] - 4s 355us/step - loss: 0.0135 - acc: 0.3746 - val_loss: 0.0142 - val_acc: 0.3114\n",
      "Epoch 944/960\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0137 - acc: 0.3719 - val_loss: 0.0141 - val_acc: 0.4450\n",
      "Epoch 945/960\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0137 - acc: 0.3801 - val_loss: 0.0136 - val_acc: 0.3191\n",
      "Epoch 946/960\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0137 - acc: 0.3715 - val_loss: 0.0139 - val_acc: 0.4539\n",
      "Epoch 947/960\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0137 - acc: 0.3828 - val_loss: 0.0141 - val_acc: 0.3030\n",
      "Epoch 948/960\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0136 - acc: 0.3774 - val_loss: 0.0124 - val_acc: 0.4549\n",
      "Epoch 949/960\n",
      "10976/10976 [==============================] - 4s 356us/step - loss: 0.0137 - acc: 0.3743 - val_loss: 0.0136 - val_acc: 0.3082\n",
      "Epoch 950/960\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0138 - acc: 0.3767 - val_loss: 0.0132 - val_acc: 0.4461\n",
      "Epoch 951/960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0137 - acc: 0.3772 - val_loss: 0.0142 - val_acc: 0.3107\n",
      "Epoch 952/960\n",
      "10976/10976 [==============================] - 4s 355us/step - loss: 0.0137 - acc: 0.3821 - val_loss: 0.0136 - val_acc: 0.4456\n",
      "Epoch 953/960\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0138 - acc: 0.3782 - val_loss: 0.0139 - val_acc: 0.3140\n",
      "Epoch 954/960\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0137 - acc: 0.3794 - val_loss: 0.0148 - val_acc: 0.4492\n",
      "Epoch 955/960\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0137 - acc: 0.3815 - val_loss: 0.0132 - val_acc: 0.3156\n",
      "Epoch 956/960\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0137 - acc: 0.3788 - val_loss: 0.0138 - val_acc: 0.4553\n",
      "Epoch 957/960\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0136 - acc: 0.3793 - val_loss: 0.0137 - val_acc: 0.3169\n",
      "Epoch 958/960\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0137 - acc: 0.3796 - val_loss: 0.0135 - val_acc: 0.4448\n",
      "Epoch 959/960\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0137 - acc: 0.3759 - val_loss: 0.0137 - val_acc: 0.3170\n",
      "Epoch 960/960\n",
      "10976/10976 [==============================] - 4s 356us/step - loss: 0.0138 - acc: 0.3728 - val_loss: 0.0146 - val_acc: 0.4433\n",
      "start training round 48\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 961/980\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0180 - acc: 0.4467 - val_loss: 0.0165 - val_acc: 0.3914\n",
      "Epoch 962/980\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0180 - acc: 0.4415 - val_loss: 0.0197 - val_acc: 0.4999\n",
      "Epoch 963/980\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0181 - acc: 0.4407 - val_loss: 0.0173 - val_acc: 0.3890\n",
      "Epoch 964/980\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0179 - acc: 0.4386 - val_loss: 0.0187 - val_acc: 0.5036\n",
      "Epoch 965/980\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0178 - acc: 0.4471 - val_loss: 0.0173 - val_acc: 0.3881\n",
      "Epoch 966/980\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0179 - acc: 0.4388 - val_loss: 0.0190 - val_acc: 0.5011\n",
      "Epoch 967/980\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0180 - acc: 0.4451 - val_loss: 0.0172 - val_acc: 0.3897\n",
      "Epoch 968/980\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0179 - acc: 0.4442 - val_loss: 0.0180 - val_acc: 0.5044\n",
      "Epoch 969/980\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0178 - acc: 0.4482 - val_loss: 0.0171 - val_acc: 0.3902\n",
      "Epoch 970/980\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0178 - acc: 0.4437 - val_loss: 0.0190 - val_acc: 0.5048\n",
      "Epoch 971/980\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0179 - acc: 0.4430 - val_loss: 0.0174 - val_acc: 0.3867\n",
      "Epoch 972/980\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0179 - acc: 0.4405 - val_loss: 0.0184 - val_acc: 0.4962\n",
      "Epoch 973/980\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0180 - acc: 0.4437 - val_loss: 0.0174 - val_acc: 0.3920\n",
      "Epoch 974/980\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0179 - acc: 0.4419 - val_loss: 0.0189 - val_acc: 0.4949\n",
      "Epoch 975/980\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0180 - acc: 0.4434 - val_loss: 0.0169 - val_acc: 0.3889\n",
      "Epoch 976/980\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0180 - acc: 0.4442 - val_loss: 0.0148 - val_acc: 0.4760\n",
      "Epoch 977/980\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0178 - acc: 0.4449 - val_loss: 0.0174 - val_acc: 0.3912\n",
      "Epoch 978/980\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0178 - acc: 0.4453 - val_loss: 0.0191 - val_acc: 0.5001\n",
      "Epoch 979/980\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0179 - acc: 0.4437 - val_loss: 0.0170 - val_acc: 0.3881\n",
      "Epoch 980/980\n",
      "10976/10976 [==============================] - 4s 344us/step - loss: 0.0180 - acc: 0.4378 - val_loss: 0.0152 - val_acc: 0.5044\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 961/980\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0210 - acc: 0.5394 - val_loss: 0.0218 - val_acc: 0.5760\n",
      "Epoch 962/980\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0209 - acc: 0.5561 - val_loss: 0.0210 - val_acc: 0.5510\n",
      "Epoch 963/980\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0206 - acc: 0.5590 - val_loss: 0.0199 - val_acc: 0.5509\n",
      "Epoch 964/980\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0212 - acc: 0.5588 - val_loss: 0.0193 - val_acc: 0.5764\n",
      "Epoch 965/980\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0209 - acc: 0.5602 - val_loss: 0.0214 - val_acc: 0.5298\n",
      "Epoch 966/980\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0207 - acc: 0.5564 - val_loss: 0.0201 - val_acc: 0.5861\n",
      "Epoch 967/980\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0207 - acc: 0.5583 - val_loss: 0.0215 - val_acc: 0.5734\n",
      "Epoch 968/980\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0209 - acc: 0.5527 - val_loss: 0.0213 - val_acc: 0.5748\n",
      "Epoch 969/980\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0207 - acc: 0.5403 - val_loss: 0.0214 - val_acc: 0.5028\n",
      "Epoch 970/980\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0207 - acc: 0.5589 - val_loss: 0.0204 - val_acc: 0.5817\n",
      "Epoch 971/980\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0206 - acc: 0.5563 - val_loss: 0.0218 - val_acc: 0.5330\n",
      "Epoch 972/980\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0208 - acc: 0.5524 - val_loss: 0.0188 - val_acc: 0.5917\n",
      "Epoch 973/980\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0208 - acc: 0.5535 - val_loss: 0.0213 - val_acc: 0.5209\n",
      "Epoch 974/980\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0210 - acc: 0.5533 - val_loss: 0.0195 - val_acc: 0.5861\n",
      "Epoch 975/980\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0204 - acc: 0.5530 - val_loss: 0.0209 - val_acc: 0.4664\n",
      "Epoch 976/980\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0209 - acc: 0.5462 - val_loss: 0.0213 - val_acc: 0.5929\n",
      "Epoch 977/980\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0204 - acc: 0.5270 - val_loss: 0.0207 - val_acc: 0.4632\n",
      "Epoch 978/980\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0206 - acc: 0.5204 - val_loss: 0.0196 - val_acc: 0.5808\n",
      "Epoch 979/980\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0206 - acc: 0.5283 - val_loss: 0.0215 - val_acc: 0.4791\n",
      "Epoch 980/980\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0206 - acc: 0.5192 - val_loss: 0.0209 - val_acc: 0.5747\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 961/980\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0168 - acc: 0.3882 - val_loss: 0.0119 - val_acc: 0.3171\n",
      "Epoch 962/980\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0167 - acc: 0.4296 - val_loss: 0.0226 - val_acc: 0.3164\n",
      "Epoch 963/980\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0170 - acc: 0.3725 - val_loss: 0.0100 - val_acc: 0.5387\n",
      "Epoch 964/980\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0168 - acc: 0.3887 - val_loss: 0.0240 - val_acc: 0.3232\n",
      "Epoch 965/980\n",
      "10976/10976 [==============================] - 4s 344us/step - loss: 0.0169 - acc: 0.4575 - val_loss: 0.0118 - val_acc: 0.4665\n",
      "Epoch 966/980\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0167 - acc: 0.4522 - val_loss: 0.0241 - val_acc: 0.3243\n",
      "Epoch 967/980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0169 - acc: 0.4652 - val_loss: 0.0103 - val_acc: 0.5426\n",
      "Epoch 968/980\n",
      "10976/10976 [==============================] - 4s 344us/step - loss: 0.0168 - acc: 0.4361 - val_loss: 0.0223 - val_acc: 0.3210\n",
      "Epoch 969/980\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0168 - acc: 0.4033 - val_loss: 0.0115 - val_acc: 0.5347\n",
      "Epoch 970/980\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0166 - acc: 0.3820 - val_loss: 0.0235 - val_acc: 0.4539\n",
      "Epoch 971/980\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0169 - acc: 0.4184 - val_loss: 0.0108 - val_acc: 0.4610\n",
      "Epoch 972/980\n",
      "10976/10976 [==============================] - 4s 342us/step - loss: 0.0166 - acc: 0.3970 - val_loss: 0.0229 - val_acc: 0.3153\n",
      "Epoch 973/980\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0168 - acc: 0.4495 - val_loss: 0.0100 - val_acc: 0.4700\n",
      "Epoch 974/980\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0167 - acc: 0.3964 - val_loss: 0.0231 - val_acc: 0.3216\n",
      "Epoch 975/980\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0168 - acc: 0.3990 - val_loss: 0.0129 - val_acc: 0.5332\n",
      "Epoch 976/980\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0167 - acc: 0.4548 - val_loss: 0.0234 - val_acc: 0.5400\n",
      "Epoch 977/980\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0167 - acc: 0.4048 - val_loss: 0.0103 - val_acc: 0.4663\n",
      "Epoch 978/980\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0166 - acc: 0.4371 - val_loss: 0.0228 - val_acc: 0.3193\n",
      "Epoch 979/980\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0172 - acc: 0.3956 - val_loss: 0.0104 - val_acc: 0.4648\n",
      "Epoch 980/980\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0166 - acc: 0.3841 - val_loss: 0.0234 - val_acc: 0.3241\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 961/980\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0138 - acc: 0.3789 - val_loss: 0.0136 - val_acc: 0.3217\n",
      "Epoch 962/980\n",
      "10976/10976 [==============================] - 4s 360us/step - loss: 0.0138 - acc: 0.3750 - val_loss: 0.0136 - val_acc: 0.4496\n",
      "Epoch 963/980\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0137 - acc: 0.3763 - val_loss: 0.0129 - val_acc: 0.3141\n",
      "Epoch 964/980\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0138 - acc: 0.3725 - val_loss: 0.0130 - val_acc: 0.4486\n",
      "Epoch 965/980\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0137 - acc: 0.3712 - val_loss: 0.0142 - val_acc: 0.3130\n",
      "Epoch 966/980\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0136 - acc: 0.3781 - val_loss: 0.0136 - val_acc: 0.4438\n",
      "Epoch 967/980\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0136 - acc: 0.3826 - val_loss: 0.0140 - val_acc: 0.3115\n",
      "Epoch 968/980\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0138 - acc: 0.3784 - val_loss: 0.0141 - val_acc: 0.4465\n",
      "Epoch 969/980\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0137 - acc: 0.3789 - val_loss: 0.0131 - val_acc: 0.3207\n",
      "Epoch 970/980\n",
      "10976/10976 [==============================] - 4s 358us/step - loss: 0.0137 - acc: 0.3701 - val_loss: 0.0139 - val_acc: 0.4435\n",
      "Epoch 971/980\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0137 - acc: 0.3799 - val_loss: 0.0140 - val_acc: 0.2895\n",
      "Epoch 972/980\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0137 - acc: 0.3771 - val_loss: 0.0139 - val_acc: 0.4446\n",
      "Epoch 973/980\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0137 - acc: 0.3813 - val_loss: 0.0136 - val_acc: 0.3103\n",
      "Epoch 974/980\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0136 - acc: 0.3699 - val_loss: 0.0139 - val_acc: 0.4480\n",
      "Epoch 975/980\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0137 - acc: 0.3764 - val_loss: 0.0144 - val_acc: 0.3096\n",
      "Epoch 976/980\n",
      "10976/10976 [==============================] - 4s 354us/step - loss: 0.0138 - acc: 0.3767 - val_loss: 0.0142 - val_acc: 0.4445\n",
      "Epoch 977/980\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0136 - acc: 0.3845 - val_loss: 0.0136 - val_acc: 0.3144\n",
      "Epoch 978/980\n",
      "10976/10976 [==============================] - 4s 356us/step - loss: 0.0137 - acc: 0.3767 - val_loss: 0.0143 - val_acc: 0.4446\n",
      "Epoch 979/980\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0137 - acc: 0.3774 - val_loss: 0.0138 - val_acc: 0.3128\n",
      "Epoch 980/980\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0136 - acc: 0.3763 - val_loss: 0.0126 - val_acc: 0.4645\n",
      "start training round 49\n",
      "training gyro model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 981/1000\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0179 - acc: 0.4470 - val_loss: 0.0168 - val_acc: 0.3945\n",
      "Epoch 982/1000\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0178 - acc: 0.4418 - val_loss: 0.0180 - val_acc: 0.5025\n",
      "Epoch 983/1000\n",
      "10976/10976 [==============================] - 4s 342us/step - loss: 0.0182 - acc: 0.4476 - val_loss: 0.0165 - val_acc: 0.3957\n",
      "Epoch 984/1000\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0180 - acc: 0.4397 - val_loss: 0.0173 - val_acc: 0.4953\n",
      "Epoch 985/1000\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0180 - acc: 0.4462 - val_loss: 0.0170 - val_acc: 0.3907\n",
      "Epoch 986/1000\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0179 - acc: 0.4408 - val_loss: 0.0189 - val_acc: 0.5024\n",
      "Epoch 987/1000\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0180 - acc: 0.4437 - val_loss: 0.0166 - val_acc: 0.3978\n",
      "Epoch 988/1000\n",
      "10976/10976 [==============================] - 4s 342us/step - loss: 0.0180 - acc: 0.4431 - val_loss: 0.0187 - val_acc: 0.4985\n",
      "Epoch 989/1000\n",
      "10976/10976 [==============================] - 4s 344us/step - loss: 0.0179 - acc: 0.4429 - val_loss: 0.0180 - val_acc: 0.3821\n",
      "Epoch 990/1000\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0179 - acc: 0.4396 - val_loss: 0.0185 - val_acc: 0.5056\n",
      "Epoch 991/1000\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0179 - acc: 0.4443 - val_loss: 0.0178 - val_acc: 0.3888\n",
      "Epoch 992/1000\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0180 - acc: 0.4457 - val_loss: 0.0185 - val_acc: 0.5045\n",
      "Epoch 993/1000\n",
      "10976/10976 [==============================] - 4s 343us/step - loss: 0.0178 - acc: 0.4471 - val_loss: 0.0167 - val_acc: 0.3920\n",
      "Epoch 994/1000\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0178 - acc: 0.4473 - val_loss: 0.0189 - val_acc: 0.5054\n",
      "Epoch 995/1000\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0180 - acc: 0.4443 - val_loss: 0.0178 - val_acc: 0.3864\n",
      "Epoch 996/1000\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0179 - acc: 0.4405 - val_loss: 0.0186 - val_acc: 0.4948\n",
      "Epoch 997/1000\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0180 - acc: 0.4445 - val_loss: 0.0165 - val_acc: 0.3969\n",
      "Epoch 998/1000\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0179 - acc: 0.4385 - val_loss: 0.0182 - val_acc: 0.4994\n",
      "Epoch 999/1000\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0179 - acc: 0.4456 - val_loss: 0.0178 - val_acc: 0.3866\n",
      "Epoch 1000/1000\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0180 - acc: 0.4392 - val_loss: 0.0192 - val_acc: 0.5058\n",
      "training linearAcc model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 981/1000\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0206 - acc: 0.5252 - val_loss: 0.0219 - val_acc: 0.4754\n",
      "Epoch 982/1000\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0206 - acc: 0.5200 - val_loss: 0.0198 - val_acc: 0.5799\n",
      "Epoch 983/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0204 - acc: 0.5268 - val_loss: 0.0211 - val_acc: 0.4891\n",
      "Epoch 984/1000\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0205 - acc: 0.5239 - val_loss: 0.0206 - val_acc: 0.5781\n",
      "Epoch 985/1000\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0207 - acc: 0.5435 - val_loss: 0.0229 - val_acc: 0.5652\n",
      "Epoch 986/1000\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0208 - acc: 0.5563 - val_loss: 0.0206 - val_acc: 0.5868\n",
      "Epoch 987/1000\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0211 - acc: 0.5556 - val_loss: 0.0216 - val_acc: 0.5408\n",
      "Epoch 988/1000\n",
      "10976/10976 [==============================] - 4s 344us/step - loss: 0.0209 - acc: 0.5541 - val_loss: 0.0212 - val_acc: 0.5808\n",
      "Epoch 989/1000\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0209 - acc: 0.5546 - val_loss: 0.0223 - val_acc: 0.5647\n",
      "Epoch 990/1000\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0212 - acc: 0.5557 - val_loss: 0.0209 - val_acc: 0.5842\n",
      "Epoch 991/1000\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0206 - acc: 0.5602 - val_loss: 0.0221 - val_acc: 0.5737\n",
      "Epoch 992/1000\n",
      "10976/10976 [==============================] - 4s 344us/step - loss: 0.0210 - acc: 0.5553 - val_loss: 0.0198 - val_acc: 0.5756\n",
      "Epoch 993/1000\n",
      "10976/10976 [==============================] - 4s 343us/step - loss: 0.0205 - acc: 0.5579 - val_loss: 0.0213 - val_acc: 0.6005\n",
      "Epoch 994/1000\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0200 - acc: 0.5545 - val_loss: 0.0216 - val_acc: 0.5534\n",
      "Epoch 995/1000\n",
      "10976/10976 [==============================] - 4s 343us/step - loss: 0.0206 - acc: 0.5631 - val_loss: 0.0189 - val_acc: 0.5848\n",
      "Epoch 996/1000\n",
      "10976/10976 [==============================] - 4s 344us/step - loss: 0.0210 - acc: 0.5544 - val_loss: 0.0230 - val_acc: 0.5468\n",
      "Epoch 997/1000\n",
      "10976/10976 [==============================] - 4s 343us/step - loss: 0.0206 - acc: 0.5571 - val_loss: 0.0232 - val_acc: 0.5801\n",
      "Epoch 998/1000\n",
      "10976/10976 [==============================] - 4s 343us/step - loss: 0.0205 - acc: 0.5562 - val_loss: 0.0199 - val_acc: 0.5411\n",
      "Epoch 999/1000\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0210 - acc: 0.5566 - val_loss: 0.0211 - val_acc: 0.5803\n",
      "Epoch 1000/1000\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0212 - acc: 0.5553 - val_loss: 0.0214 - val_acc: 0.5476\n",
      "training gravity model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 981/1000\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0169 - acc: 0.3986 - val_loss: 0.0105 - val_acc: 0.4598\n",
      "Epoch 982/1000\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0166 - acc: 0.4130 - val_loss: 0.0213 - val_acc: 0.3123\n",
      "Epoch 983/1000\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0167 - acc: 0.4005 - val_loss: 0.0122 - val_acc: 0.3111\n",
      "Epoch 984/1000\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0167 - acc: 0.4004 - val_loss: 0.0231 - val_acc: 0.3156\n",
      "Epoch 985/1000\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0167 - acc: 0.3489 - val_loss: 0.0112 - val_acc: 0.3228\n",
      "Epoch 986/1000\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0168 - acc: 0.3861 - val_loss: 0.0238 - val_acc: 0.3322\n",
      "Epoch 987/1000\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0168 - acc: 0.4073 - val_loss: 0.0110 - val_acc: 0.5424\n",
      "Epoch 988/1000\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0165 - acc: 0.4414 - val_loss: 0.0215 - val_acc: 0.3135\n",
      "Epoch 989/1000\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0169 - acc: 0.3864 - val_loss: 0.0100 - val_acc: 0.5407\n",
      "Epoch 990/1000\n",
      "10976/10976 [==============================] - 4s 346us/step - loss: 0.0168 - acc: 0.4115 - val_loss: 0.0237 - val_acc: 0.4534\n",
      "Epoch 991/1000\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0169 - acc: 0.3759 - val_loss: 0.0106 - val_acc: 0.4587\n",
      "Epoch 992/1000\n",
      "10976/10976 [==============================] - 4s 343us/step - loss: 0.0168 - acc: 0.4359 - val_loss: 0.0236 - val_acc: 0.5406\n",
      "Epoch 993/1000\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0169 - acc: 0.4130 - val_loss: 0.0103 - val_acc: 0.5415\n",
      "Epoch 994/1000\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0168 - acc: 0.4312 - val_loss: 0.0200 - val_acc: 0.3017\n",
      "Epoch 995/1000\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0169 - acc: 0.3674 - val_loss: 0.0115 - val_acc: 0.3167\n",
      "Epoch 996/1000\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0167 - acc: 0.4272 - val_loss: 0.0227 - val_acc: 0.3252\n",
      "Epoch 997/1000\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0170 - acc: 0.4255 - val_loss: 0.0104 - val_acc: 0.3224\n",
      "Epoch 998/1000\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0167 - acc: 0.3946 - val_loss: 0.0222 - val_acc: 0.3105\n",
      "Epoch 999/1000\n",
      "10976/10976 [==============================] - 4s 345us/step - loss: 0.0168 - acc: 0.3898 - val_loss: 0.0101 - val_acc: 0.3198\n",
      "Epoch 1000/1000\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0168 - acc: 0.3918 - val_loss: 0.0231 - val_acc: 0.3216\n",
      "training gameVec model\n",
      "Train on 10976 samples, validate on 1220 samples\n",
      "Epoch 981/1000\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0137 - acc: 0.3867 - val_loss: 0.0134 - val_acc: 0.3207\n",
      "Epoch 982/1000\n",
      "10976/10976 [==============================] - 4s 347us/step - loss: 0.0137 - acc: 0.3812 - val_loss: 0.0136 - val_acc: 0.4821\n",
      "Epoch 983/1000\n",
      "10976/10976 [==============================] - 4s 352us/step - loss: 0.0137 - acc: 0.3808 - val_loss: 0.0141 - val_acc: 0.3145\n",
      "Epoch 984/1000\n",
      "10976/10976 [==============================] - 4s 353us/step - loss: 0.0138 - acc: 0.3760 - val_loss: 0.0136 - val_acc: 0.4552\n",
      "Epoch 985/1000\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0136 - acc: 0.3789 - val_loss: 0.0135 - val_acc: 0.2975\n",
      "Epoch 986/1000\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0137 - acc: 0.3741 - val_loss: 0.0135 - val_acc: 0.4472\n",
      "Epoch 987/1000\n",
      "10976/10976 [==============================] - 4s 351us/step - loss: 0.0137 - acc: 0.3797 - val_loss: 0.0134 - val_acc: 0.3177\n",
      "Epoch 988/1000\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0136 - acc: 0.3755 - val_loss: 0.0139 - val_acc: 0.4462\n",
      "Epoch 989/1000\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0137 - acc: 0.3789 - val_loss: 0.0135 - val_acc: 0.3186\n",
      "Epoch 990/1000\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0136 - acc: 0.3808 - val_loss: 0.0140 - val_acc: 0.4472\n",
      "Epoch 991/1000\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0136 - acc: 0.3822 - val_loss: 0.0137 - val_acc: 0.3172\n",
      "Epoch 992/1000\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0138 - acc: 0.3775 - val_loss: 0.0135 - val_acc: 0.4415\n",
      "Epoch 993/1000\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0136 - acc: 0.3815 - val_loss: 0.0129 - val_acc: 0.3211\n",
      "Epoch 994/1000\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0136 - acc: 0.3750 - val_loss: 0.0140 - val_acc: 0.4461\n",
      "Epoch 995/1000\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0136 - acc: 0.3740 - val_loss: 0.0138 - val_acc: 0.3049\n",
      "Epoch 996/1000\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0137 - acc: 0.3758 - val_loss: 0.0130 - val_acc: 0.4479\n",
      "Epoch 997/1000\n",
      "10976/10976 [==============================] - 4s 350us/step - loss: 0.0137 - acc: 0.3736 - val_loss: 0.0140 - val_acc: 0.3144\n",
      "Epoch 998/1000\n",
      "10976/10976 [==============================] - 4s 349us/step - loss: 0.0136 - acc: 0.3761 - val_loss: 0.0134 - val_acc: 0.4564\n",
      "Epoch 999/1000\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0137 - acc: 0.3805 - val_loss: 0.0139 - val_acc: 0.3183\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1000/1000\n",
      "10976/10976 [==============================] - 4s 348us/step - loss: 0.0138 - acc: 0.3722 - val_loss: 0.0134 - val_acc: 0.4492\n"
     ]
    }
   ],
   "source": [
    "train_round =50\n",
    "epochs_per_round=20\n",
    "for i in range(train_round):\n",
    "    print('start training round '+str(i))\n",
    "    print('training gyro model')\n",
    "    gyroModel.fit(gyro_train,gyro_train,batch_size=256,epochs=(i+1)*epochs_per_round,validation_split=0.1,verbose=1,initial_epoch=i*epochs_per_round)\n",
    "    print('training linearAcc model')\n",
    "    linearAccModel.fit(linearAcc_train,linearAcc_train,batch_size=256,epochs=(i+1)*epochs_per_round,validation_split=0.1,verbose=1,initial_epoch=i*epochs_per_round)\n",
    "    print('training gravity model')\n",
    "    gravityModel.fit(gravity_train,gravity_train,batch_size=256,epochs=(i+1)*epochs_per_round,validation_split=0.1,verbose=1,initial_epoch=i*epochs_per_round)\n",
    "    print('training gameVec model')\n",
    "    gameVecModel.fit(gameVec_train,gameVec_train,batch_size=256,epochs=(i+1)*epochs_per_round,validation_split=0.1,verbose=1,initial_epoch=i*epochs_per_round)\n",
    "    gyroModel.save('gyroNorModelFFT_64.h5')\n",
    "    linearAccModel.save('linearAccNorModelFFT_64.h5')\n",
    "    gravityModel.save('gravityNorModelFFT_64.h5')\n",
    "    gameVecModel.save('gameVecNorModelFFT_64.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJkAAAGfCAYAAAD8qwORAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAIABJREFUeJzs3XuUZOdZ3/vv01Vd3dPd03PtuegyHskeXyQby+gCCaAk5thInCzbWZGJjIktTogCOQayEsAyIfaKgNjAAROCl7Gwje2AIhtYxjoncoRZyyFZIMSMZFny6MKM7qPRaHpGM9PXur/nj3e/Xburd1VXdddlV83vs9asrq5b16zu2rv2s3/P85pzDhERERERERERkc0Y6fcLEBERERERERGRwacik4iIiIiIiIiIbJqKTCIiIiIiIiIismkqMomIiIiIiIiIyKapyCQiIiIiIiIiIpumIpOIiIiIiIiIiGyaikwiIiIiIiIiIrJpKjKJiIiIiIiIiMimqcgkIiIiIiIiIiKblu33C+ik3bt3u4MHD/b7ZYiIpM5DDz10xjk30+/X0W/aT4iIJNN+wtN+QkQkWav7iaEqMh08eJAjR470+2WIiKSOmT3f79eQBtpPiIgk037C035CRCRZq/sJtcuJiIiIiIiIiMimqcgkIiIiIiIiIiKbpiKTiIiIiIiIiIhsmopMIiIiIiIiIiKyaSoyiYiIiIiIiIjIpqnIJCIiIiIiIiIim6Yik4iIiIiIiIiIbJqKTCIiIiIiIiIismkqMomISNeZ2U1m9pSZHTezOxJuv9HMHjazspndErv+H5nZI7F/eTN7T3TbF8zs2dht1/Ty/yQiIiIiIqtl+/0CRERkuJlZBvgU8A7gBHDYzO51zj0eu9sLwG3Az8Uf65z7JnBN9Dw7gePAn8fu8vPOuT/p3qsXEREREZFWqcgkIiLddgNw3Dn3DICZ3QO8G1gpMjnnnotuqzZ5nluArzvnlrr3UkVEREREZKPULiciIt12KfBi7PsT0XXtuhX4b3XX/aqZPWpmnzSzsaQHmdntZnbEzI7Mzs5u4MeKiIiIiEgrVGQSEZFus4TrXFtPYLYfeAtwf+zqjwBvBK4HdgIfTnqsc+4u59x1zrnrZmZm2vmxIiIiIiLSBhWZRESk204Al8e+vww42eZz/AjwVedcKVzhnHvZeQXgD/BteSIiIiIi0icqMvXZuXP9fgUishGlEszP9/tVDIzDwCEzu8LMcvi2t3vbfI73UdcqF6WbMDMD3gN8pwOvdcMqFZib6+crEJFhVy5r3zNILlzo9ysQEYGFBb//6JWOFJm0NPXGHD8Ou3fDgw/2+5WISLt+4zfgmqHbKnWHc64MfAjf6vYE8BXn3FEzu9PM3gVgZteb2QngvcBnzOxoeLyZHcQnof6y7qn/yMweAx4DdgO/0u3/SzNf/CJceWVvd+IicnH53d+Fq6/u96uQVhw5Ajt3wrPP9vuViMjF7m1vg9/6rd79vE2vLqelqTfuhRegWoUnnoDv+Z5+vxoRacfRo/Dcc+AcWNLEIVnFOXcfcF/ddR+NXT6Mb6NLeuxzJAwKd869vbOvcnNOnICzZ6FQgKzWbhWRLnjxRXjppX6/CmnFyZP+c/4rr8AVV/T71YjIxeyll/w2qVc6kWRaWZraOVcEwtLUK5xzzznnHgW0NHXM4qL/eupUf1+HiLTv1Cn/4bFQ6PcrkbQoRdOiisX+vg4RGV7Fot/3VJt9opZUCL8jpVtFpN/K5d7uNzpRZNLS1BukIpPI4Arv2/A+FgkHEioyiUi3hO2LChfpFw7oKpX+vg4RkUpl8IpMWpp6gxYW/NdXXunv6xCR9oX3bXgfi4Qkk9JtItItochUKjW/n/SfkkwikgbO9T4B24kik5am3iAlmUQGU7HoZ++AkkxSoySTiHRbKGKrcJF+IcGkJJOI9FM/tkWdKDJdFEtTd4OKTCKD6fTp2mUVmSTQTCYR6TYlmQaHkkwikgZhGzRQSaaLZWnqblC7nMhgir9n1S4ngZJMItJtmsk0ODSTSUTSIGyDellk6sgiyxfD0tTdEBIQ5875+PNY4mhzEUmbePpQSSYJlGQSkW5TkmlwKMkkImkwkEkm2bj4wanSTCKDQ0UmSaIkk4h0m5JMg0NJJhFJg34kmVRk6qN4m42KTCKDQ+1ykkRJJhHpNiWZBkc4sFNBUET6KWyDBm3wt2zQ4iKMRL8BDf8WGRynTtXeu0oySaAkk4h0m5JMg0NJJhFJAyWZhshXvwr//t83v8/iIlx+ub+sIpPI4Dh1qvbebVZkWl6GW26BZ5/tzeuS/lKSSUS6TUmmwaGZTCKSBprJNET+7M/gs59tfp+FBbjySn9Z7XIig+OVV+DAAchkmrfLHTsGf/qn8Fd/1bvXJv2jJJOIdJuSTINDSSYRSQMlmYZIoeBTDM0sLsKOHf6fkkwig+PUKdi/HyYn108ygd8eyPBTkklEuk1JpsGhmUwikgaayTREWi0yTU3Bvn0qMokMklOnYO9e//5VkUkCJZlEpNuUZBocSjKJSBooyTRECgX/AaDZh4CFBZ+E2LdP7XIig2J5Gebm/Pt2crJ5u5yKTBcXJZlEpNuUZBocmskkImmgmUxDJBxUNkszLS76g9S9e5VkEhkUoSAcikxKMkkQduL6fYtItyjJNDiUZBKRNFCSaYisV2SqVCCfV7ucyKAJ71W1y0k9JZlEpNuGOclkZjeZ2VNmdtzM7ki4/UYze9jMymZ2S+z6f2Rmj8T+5c3sPdFtXzCzZ2O3XdOr/4+STCKSBv1IMmV796MuLusVmcKB6eQkjI/7lpuQbBKR9ApFppBkOneu8X1VZLq4aCaTiHTbsCaZzCwDfAp4B3ACOGxm9zrnHo/d7QXgNuDn4o91zn0TuCZ6np3AceDPY3f5eefcn3Tv1ScL6QElmUSkn/qxLVKSqUvCQWU+n3x7KDJNTflEBGguk8ggaKddLrz/VWS6OCjJJCLdVK3WtjNDmGS6ATjunHvGOVcE7gHeHb+Dc+4559yjQLPz8bcAX3fOLXXvpbZGSSYRSQPNZBoi7SSZ9u3zl9UyJ5J+4X26Z4/a5WQ1JZlEpJvihaUhLFxcCrwY+/5EdF27bgX+W911v2pmj5rZJ81sLOlBZna7mR0xsyOzs7Mb+LFrqcgkImmgmUxDZL0iU1iRKgz+BhWZRAbBqVOwaxeMjmp1OVlNSSYR6ab4tmUIk0yWcJ1r6wnM9gNvAe6PXf0R4I3A9cBO4MNJj3XO3eWcu845d93MzEw7P7YhDf4WkTRQkmmItJpkCoO/Qe1yIoPglVdq71mtLidxSjKJSDfFty1DmI45AVwe+/4y4GSbz/EjwFedcyslOOfcy84rAH+Ab8vrCSWZRCQNNJNpiLTTLjczA2ZKMokMglOnakWmqSn/Hm+00VaR6eKSlGT64hfhy1/uz+sRkcG2uAj/8l/WFpgY8iTTYeCQmV1hZjl829u9bT7H+6hrlYvSTZiZAe8BvtOB19oSDf4WkTRQkmmItNMul83C7t3w8su9eW0isnGnTtVaXMNqkEsNxouqyHRxSUoy/e7vwmc/25/XIyKD7Vvf8tuPBx7w3w9zksk5VwY+hG91ewL4inPuqJndaWbvAjCz683sBPBe4DNmdjQ83swO4pNQf1n31H9kZo8BjwG7gV/p9v8lUJJJRNKgHzOZsr37UReXdtrlAF77Wnjqqe6/LhHZuKUleP55+LEf89+HItPiImzduvb+KjJdXJKSTPm8zmKLyMaE4kRYqXTIk0w45+4D7qu77qOxy4fxbXRJj32OhEHhzrm3d/ZVtk4zmUQkDZRkGhKVSm2H0kq7HMC11/ozVr385YtIe779bf8evfZa/30oEjeay6Qi08UlKcmUz+sstohsTLMik7Yr6ackk4ikgYpMQyJ+QNlKuxzAd383zM/DsWPdfW0isnEPPeS/fvd3+6/h/dtohTkVmS4ujZJMOsAQkY242JJMw0YzmUQkDTT4e0jEDyjDB4N6SUkmqB3Eikj6PPSQH9R/WRTWj7fLJQnvfxWZLg7hgLB+H6Aik4hshJJMg01JJhFJAyWZhkQrSabFRRgfh0zGf3/VVTA2Bg8/3P3XJyIb8/DDviBs5r9Xu5wEztXOECnJJCKdoCTTYNNMJhFJg34M/laRqQtabZcLKQiA0VF461uVZBJJq+VlOHq0ljoEtctJTbyQpCKTiHSCkkyDTUkmEUkDJZmGRKtJppCCCK691iclNPxbJH0efdSfCUgqMinJJPFUQTgQLJf9P53FFpGNCNsOJZkGk5JMIpIGSjINifiHgGZFpniSCfww4bk5eOaZ7r02EdmY0Moahn5D6+1y8W2CDKekJFMoLuostohshJJMgy0c2Ol3JSL9FLZBGvw94DbSLgca/i2SZg89BLt2wYEDtevULidBUpIpHBjqAENENkIzmQabkkwikgZKMg2JjbbLXX015HIqMomk0UMPrR76DWqXkxoVmUSk0+qLTPF9iYpM6aeZTCKSBprJNCRaLTLVJ5lyOfiu71KRSSRt8nn4zndWz2MCvzrk+LiKTJLcLqcik4hshtrlBpuSTCKSBkoyDYlwQJnNNm+Xq08ygZ/38tBDsLTUvdcnIu3527/1H+jj85iCycnkdrkw9Dmb9Rv3i/1DppndZGZPmdlxM7sj4fYbzexhMyub2S11t1XM7JHo372x668wswfN7JiZfdnMcr34vyQJqYJcTkUmEemMRkWmXK4uyfS1r8Ff/VVPX5usTzOZRCQNNJNpSIQi0/bttQ8G9ZKSTADvfz9cuACf+ET3Xp+ItK5ahTvugJkZeMc71t4+OZmcZArv/e3b/deLOc1kZhngU8DNwFXA+8zsqrq7vQDcBtyd8BTLzrlron/vil3/a8AnnXOHgHPAv+j4i2+gUoFf+RW/vYbaDnxysnYgGE4yXOwFRhHZmLDtqF9EYnJydeFi/v++gzO/+Ju9fXGyLiWZRCQNlGQaEvEiUzvtcgA33gg/+qPwa78Gx4937zWKSGu++EV44AH49V+HbdvW3j41lVxkCu99FZkAuAE47px7xjlXBO4B3h2/g3PuOefco0BLu0AzM+DtwJ9EV30ReE/nXnJzjz8O/+E/wJ//uf8+pAomJpRkEpHOaJRkmphYnWS6cDrPC081OKspfaOZTCKSBprJNCTWKzI517hdDuA3fgPGxuBnfsbfV0T649w5+IVfgL//9+EDH0i+T6N2ORWZVrkUeDH2/YnoulaNm9kRM/sbMwuFpF3Aeedc+Pje8DnN7Pbo8UdmZ2fbfe2JwgFe+L0mJZlUZBKRzWhUZKpPMuWqebIVFZnSRkUmEUkDJZmGxHpFpmLR/7KTkkwAl1wC//E/wte/Dv/7f3fvdYpIc5/8JLz6KnzqUzDSYGvZqF1ORaZVLOG6dkroB5xz1wE/Cvy2mb22ned0zt3lnLvOOXfdzMxMGz+2sbDDDgd99Ukm51RkEpHNaVRk2rJldZIpV80zWlaRKW3ULiciaaAk05BYr8gUDkgbFZkA/vE/9l9feKGzr01EWvf883D55XDNNY3vo3a5lpwALo99fxlwstUHO+dORl+fAf4n8DbgDLDdzLIbec7NCjvscNAXTzI5579XkUlENiOpyDQ66gd/x7crYy7PqJJMqaPB3yKSBmFbpMHfAy4cTG7bllxkCq01jdrlwC+LHn8uEem9QqH2XmxkvXa5MMfpIn8vHwYORavB5YBbgXvXeQwAZrbDzMaiy7uB7wMed8454JtAWInug8DXOv7KGwgHDeH3Gk8ygT8YDAeGlYpan0WkfUlFplzOr1q6kmRyji2oyJRGSjKJSBoMbJJp2Jembld9kqn+4KKVJNPY2OrnEpHeKxRq78VG1C63vmhu0oeA+4EngK84546a2Z1m9i4AM7vezE4A7wU+Y2ZHo4e/CThiZt/GF5U+4Zx7PLrtw8C/NbPj+BlNn+vV/6m+XS6eZArXx1cX1UGGiLQrbDfqi0yjo7F0TFRtGq2qyJQ2mskkImnQj5lM2fXv0lxsaep34FsiDpvZvbGDAKgtTf1zCU+x7JxLakYJS1PfY2a/h1+a+tObfb29UCjAB/kC7//m/+Y3q5+jVPIfCoJ2ikx5fWYQ6Zt8fv0ik9rlWuOcuw+4r+66j8YuH8a3vNU/7q+BtzR4zmfwK9f1XH27XLMkE/gdfHbTe1wRuZg0SzKt7FOiG3MqMqWOkkwikgaDmmQauqWpN6tQgB+yb/DWb32BCRbXtMypXU5kMHSiXU5FpuFU3y63XpJJZ7JFpF2NikzxJFN5wd84WtVOJm00k0lE0mBQZzL1dWnqNCoUYNvIPCOuylv59poiUytJppB8UpJJpH9aSTKFpaRDoiVQkWm4NVtdLlyvIpOIbEYrM5lK80oypZWSTCKSBoOaZOrr0tRmdntUpDoyOzvbxo/tnkIBtpqPNlzLQ2sKRa0Umcz8wa0OTEX6p5UkU0gk1rfMhfe9ikzDqdnqcuF6FZlEZDNW0krl2smM+iTTqiKTVhhIFc1kEpE06MdMpk4Umfq6NLVz7i7n3HXOuetmZmbaf/Vd4ItM84AvMm2kXQ5UZBLpt1YHf8PaljklmYZbO6vLxe8vItKq+HajUGieZMpQ1YYmZZRkEpE0GNQk09AtTb1ZhQJM4Y84v5uHN9QuB/7gVu1yIv3TarscrE0yqcg03NpdXU7HfiLSrnhxIp9vMJNpsbD6TpIaSjKJSBoMZJJpGJem3qxCAaacTzJdxeMUzq+uMrVaZBof14GpSD9tpl0uFJm2bas9lwyPVpJM8RMMOpMtIu2KFyeWl/32pj7JFAZ/AyoypUw/hu2KiNQL+5Jebos6sqDysC1NvVmFAkxWF8jvPcD4Ky+Q+c634e3fu3L7wgKMjKx/8Kokk0h/tZNkSmqXGxvTSpHDSjOZRKTb4tuNhkkmFZlSS0kmEUmDeHHJOT/7uds60S4ndQp5x0R1noVr/wEA4088vHLbk0/C174GO3eu/wtWkkmkv1pJMu3c6b9++tNw4ULt+uVl2LKlVqTSe3m4NFpdTkUmEemUpCLT2NjqJFNlMbah0Y4mVTSTSUTSIL4v6VXLnIpMXeCW82SoUj70JmbZzdRTDwHwuc/B294GJ0/CZz+7/vMoySTSX60kmd76VvilX4I//mN4y1vgyBF//fKyL1CpyDSc6tvl6pNMhYKKTCKyOa0kmSpLSjKllZJMIpIGKjINieyyn8eU2baVh7iW7ccfguVlnv/Qb3DroYc4ehTe/e71n0dJJpH+qVb9meL1kkxm8Mu/DA88AEtL8Ou/7q8PSaZczn+v9/JwUZJJRLqtWZEpbHOqiyoypZWSTCKSBvFtUK+2RyoydUFm2Q9nyW6f4iGuZdtLR6l+97Xcmf8Ffs7+H/bta+15xsZ0YCrSL6F4sF6SKbjhBp9kevll/30oMpn5gwK9l4dLo5lM8cHfKjKJyGY0Wl0um40tSb2s1eXSKvz+tP0XkX5SkmlIZPO+yDS60yeZMtUy1fNzPMXr2XvhWMvPo3Y5kf4J771Wi0wA+/bBqVO1x2/ZUnsOFZmGS6PV5eqTTGH2ng4yRKRd5XJtG9IwybSsJFNaKckkImkQ3wapyDTARvO+XS63c4p7eRf/7y1f5Ftf+g7f4B1smz3mx7q3QO1yIv0T3nvrtcvFxYtMIckEKjINo/p2uUZJpqmp1fcXEWlVuVzbhjRKMjnNZEqtcDBXrfbuwE5EpJ6STENitFBLMlUty+E3fYCTS9s5xiFGl+bgzJmWnkdJJpH+2UiSae9eWFiAxUUVmYZdUpIpk6n9vdQXmZRkEpF2NSoyjY7685WVil9sZoV2NKkSP5jTiQYR6RclmYbEWNEnmWzrFOPj/mDz1Ck4xiF/h2OttcwpySTSPxtNMgG88oqKTMMuaSZTNlsb9K4ik4hsVrMkU7h91dlInZlMlX4M2xURqRf/DKrB3wNsrOSTTGzdypYttSLTcV7nr2+xyKQkk0j/tJxkqlbhnnugVFopMp06pSLTsEtaXW50VEUmEemcchm2bvWX65NM4Lc7rqDB32kVTwxoHyAi/aIk05AYL/kkE1NTK0WmV16B+V1X+H6KNopMOjAV6Y/w3lu3yPTNb8L73gdf/7qSTBeR+na5kGQKB3+hyBQOEHWAISLtKpdriwksLycnmSxWWFo1BHwAmdlNZvaUmR03szsSbr/RzB42s7KZ3VJ3W8XMHon+3Ru7/goze9DMjpnZl80s14v/C6hdTkTSQTOZhoBzMF5OTjLt2jcKBw+qXU5kALTcLvfgg/7rqVPs3btyUUWmIVffLheSTJmM/1co+CJTOEBUkUlE2lWp1NKQC9FHy/okE4U88/g7rRoCPmDMLAN8CrgZuAp4n5ldVXe3F4DbgLsTnmLZOXdN9O9dset/Dfikc+4QcA74Fx1/8Q0oySQiffHnfw533bXyrZJMQ6BchinmqY74CbDxmUz79gGHDrXdLtfiYnQi0kEtt8sdPuy/nj7NzIxfbjoUmUKBSkWm4ZO0ulxIF+Ryfvh7tarV5URk48plvx/JZODCBX9dfZJppJDnPNuBgU8y3QAcd84945wrAvcA747fwTn3nHPuUaClwyQzM+DtwJ9EV30ReE/nXnJzSjKJSF98/vPw8Y+vfKuZTEOgUIApFiiObQWzVe1yq4pMLVSOwgFqqdTd1ywia7WcZIoVmbJZmJlRkulikLS6XEgX5HIwN+cvayaTiGxUKF6Pj9e2KfVJJivmWWCKCiMDnWQCLgVejH1/IrquVeNmdsTM/sbMQiFpF3DeORe2wO0+56bED+a0DxCRnikW/YFIREmmIVAowFbmKY35I4t4u9zevfgi08KCrzqtIyQoNMdRpPdaSjK9/DK89JK/fPo04N/nJ074jbiKTMMrHDCUy/53XZ9kUpFJRDYrXmRqmGQq5skzToExqssDvaOxhOvayfIfcM5dB/wo8Ntm9tp2ntPMbo+KVEdmZ2fb+LGNKckkIn1RV2TSTKYhEJJM5XE/7XXLFpidhaWlWJIJ4PjxdZ8rJCh0cCrSey0lmUKKaXzcv9Hx7/Nnn/VXq8g0vOIHDMXi6iTT2FjtgFBFJhHZqKQk09jY6iTTSKlAnnHyjOMG+6zkCeDy2PeXASdbfbBz7mT09RngfwJvA84A280su95zOufucs5d55y7bmZmpv1Xn0AzmUSkL8LqMxElmYZASDKVt9SSTM89529bVWRqYS6Tkkwi/dNSkunwYT8s48YbV5JM+/bV3vMqMg2v+AFDKDIpySQinVQu+11Mfbtc2NaUSpAp5WtFpsGeyXQYOBStBpcDbgXuXecxAJjZDjMbiy7vBr4PeNw554BvAmElug8CX+v4K2+gWq0VBJVkEpGeKZX8h9Now1Mu17ZFKjINqJBkqkzUkkwhrbZ3L/Ca1/hPB20UmXRwKtJ74X23bpHp6qv9qpGxdrnwnleRaXjVF5niO/B4kUmry4nIRlUqzWcylcuri0wMcJEpmpv0IeB+4AngK865o2Z2p5m9C8DMrjezE8B7gc+Y2dHo4W8CjpjZt/FFpU845x6Pbvsw8G/N7Dh+RtPnevV/qlRW/65ERHoirEoTnTGPb4t6VfDOrn8XaUex6JNM1YldQO0gE6IkUzYLV17ZUpFJ7XIi/bNuu5xzvsj0T/4J7NkDZ85Atcq+fbXafbzIFLb3MhziO+lCYW2S6cwZf1lJJhHZqHi73Msv++vqk0zZeJJpwKPvzrn7gPvqrvto7PJhfMtb/eP+GnhLg+d8Br9yXc9Vq/73tbSkJJOI9FA46FhehslJyuXatqhXSSYVmTqsUICdLOAma0mmYN++6MLrXqd2OZGUW7dd7tln4dVX4frr/Ru/WoVXX2Xfvt0rd1GSaXi1mmQKRSYdYIhIuxqtLpfJxG4vx5JM+sCYKqHIBDrRICI9FC8y4T+Dhm2R2uUGVJjJ5CZrM5nAfyDYtSu606FDfvC3a75ohpJMIv2zbrtcGPp9/fU+yQRw+nStmIyKTMMsafB3PMm0sOAvK8kkIhsVLzKFbcqamUyVAgXGVGRKoXiRaWWf8fTTcPfd+lAgIt1TKvmv0T4hJJlARaaBFWYysXV1kmlmpnbmiUOHYHGxln1uQEkmkf7J530yZaTRVvJb3/J3eMtbVhWZ9u6t3aW+yLROXVkGSLxoVCisTTIFKjKJyEbFi0xB/Uym0YpPMhXQ2Yy0SUwy/eZvwvvfD699LXzmM317bSIyxJRkGj7FpTITLMNWf2QRPhjE0w2trjCnJJNI/xQKTeYxAZw9C7t3+0/7Ybnj2dlV7/Xw+LExX2BSoWF4rLe6XKAik4hsVCgyxUcv1CeZQpEpzzims5Kpkjhsd3ERtm+HAwfgJ38SHnywb69PRIZUXZEpfiK0V+MbVGTqsPKFRQBsenWSKbHIdPx40+dSkkmkf/L5dVaWm5tbSSzGk0w7d9YOAOJJJlDBeJjUt8spySQinVap+BR8K0mmPONYQR8Y0yQxyVQs+hNTv/3b/vvZ2b68NhEZYkoyDZ/qhXkARqZXz2SKt9Bw4ID/Ta+TZNKBqUj/FArrFJnm52F62l/etQvM4PRpRkZqNScVmYZXfbtcoyTT5OTa+4uIrMc5f2CQ1C63kmQqVMm5oopMKZU4k6lY9FeGDwjRQaCISMeEmUyxJJOKTAOuOucnM2a2NUkyZTJw5ZVqlxNJsXXb5eJJpmzWF5qiM5Lh/a4i0/Bab3W5YGLC1x+1upyItCNsM5rNZKou+51KGPxtRRWZ0iQxyRTOYKnIJCLdEpJM+TzVqj9poSLTgHPzoci0Osm0qsgEvmWuxSST2uVEem/ddrl4kgl8/P30aUBFpotBpbL6RECjJNP4uL9eSSYRaUfYZjRLMoUiUyWrJFMaVaurWxsBJZlEpPti7XLhhIVmMg26ed8ul92xOsm0ql0OfJHp+PGm5UQlmUT6p60kE/geuajIFN7vKjINr3LZp5SgeZJpbExFJhFpX7MiU9jWuGVfVHJjfnW5ESWZUiU+B0XtciLSE86tapcL2x4lmQbdok8yje7wSaZdu/zVr3lN3f1e9zq/Yzl5suFTKckk0j8NZYJSAAAgAElEQVRtJ5liRaaDB30BIjxeRabhU19kSkoyjY767mgVmUSkXa0kmUKRqZqLkkxF7WTSRO1yItJzlYovNAEsL69se1RkGnAjC6uTTH/v78H/+l/+6yphhbkmLXM6MBXpn5YGf8eTTDMzKzOZ/s2/gb/6Kz+LB2ob9ov5vWxmN5nZU2Z23MzuSLj9RjN72MzKZnZL7PprzOwBMztqZo+a2T+L3fYFM3vWzB6J/l3Tq/9PpVIrMhUKq5NM4e8mHEOoyCQi7Qpnn+tXlxsbiyWZ8rUkU55xMqVC7eBC+q7p4O/wS1WRSUQ6KbTKAeTzKjKl3oMP+njC+fNN72ZLq5NMZvADP1A72FzRQpEpm/UfLpRkEum9fL5Ju1yh4Dfi9UmmV1+FUonpabgmVu642AvGZpYBPgXcDFwFvM/Mrqq72wvAbcDdddcvAR9wzl0N3AT8tpltj93+8865a6J/j3TlP5CglSRT+PvJZFRkEpH2tNIuR5RkYtwXmYCLd0eTMqHW13Amk5n/xarIJCKdFC8yJcxkUpEpbf72b+H55+HEiaZ3yyz6JJNNb216Py6/3B95tjD8W58XRHqvaZIpmr22ZiYTwJkza+5+sReZgBuA4865Z5xzReAe4N3xOzjnnnPOPQpU667/O+fcsejySeA0MNObl91YKzOZwoGhkkwi0q5GRabR0Vi7XN7vVEa2jNWKTDozmQr1c1BWkkzxDxdbtqjIJCKdFeYxQWK7nAZ/p000a2W9nUFm2SeZmJxs/nwjI/Da1/rh302Mj1/UB6YifdN08PfcnP9av7ocrLTMxanIxKXAi7HvT0TXtcXMbgBywNOxq381aqP7pJkllgXN7HYzO2JmR2YTfj8bUanUNvNhdblmRaZe7dRFZDgkFZmyWf/xcSXJFBWURibGVWRKmZAWaJhkAhWZRKTzlGQaMOHAZJ2dQTY/zyITvj9iPYcOtZRk0ucFkd5rOvi7WZIpFKRjVGSivmEYoK3BIWa2H/ivwI8758Iu8iPAG4HrgZ3Ah5Me65y7yzl3nXPuupmZzoSgyuXazKWQZGrULqckk4i0K6nIFLYtYVtjBf8B0bb41eUAfWhMiXAg13AmE/g4rIpMItJJdUUmzWRKuxaTTNn8AksjU60956FD8PTTTX/bSjKJ9EfbSSYVmZo5AVwe+/4yoPHSmnXMbBr478AvOef+JlzvnHvZeQXgD/BteT0RLzK1kmRSkUlE2tGsyLQy+Hs5Icl0Ee9o0qS+yLRmdTlQkklEOq9u8Hd9666KTGnTYpIpl59naWSdeUzBoUP+jFOTOU9KMon0h5JMHXUYOGRmV5hZDrgVuLeVB0b3/yrwJefcH9fdtj/6asB7gO909FU3Uan4A71czv+tOKckk4h0Tnx1uVDQDtuWkejTe2XRf0DMTqldLm1aSjKpyCQinTZMM5mGbWnqRK0WmYoLLGfaSDJB05Y5Df4W6Y+mg7+Tkkzbt/ujAc1kWsM5VwY+BNwPPAF8xTl31MzuNLN3AZjZ9WZ2Angv8BkzOxo9/EeAG4HbEvYHf2RmjwGPAbuBX+nV/6lc9r/uXA4WF/11SjKJSKc0SzKZ+e1NdckXlDKTKjKlTf0cFM1kEpGeSMlMpuxmnyC2NPU78C0Rh83sXufc47G7haWpf67u4WFp6mNmdgnwkJnd75w7H93+8865P9nsa+yIFtvlcsV5lrMtJpmuvNJ/fe65hncZH9fnBZFec86/7xq2yyUlmUZG/PDvV15Zc/eLvcgE4Jy7D7iv7rqPxi4fxrfR1T/uD4E/bPCcb+/wy2xZmMEULzI1SjJlMioyiUh7mhWZwvWVJb9TGZ3S6nJpk5hkcs4fAMbb5c6e7cvrE5EhlZKZTJsuMhFbmhrAzMLS1CtFJufcc9Fta5amjl0+aWZhaerzpEmpBOfO+cvrFJnGSgucy+5q7XkvvdQffTz7bOPnU5JJpOfKZf9ZsK0kE8Dll8Pzz6+5u4pMw6dS8Qd5Y2OtJZm0upyItGO9ItPoaG0mU25aSaa0SZzJFNpYlGQSkW4JRSazVUmmQZzJ1NelqXsifpZhnZ3BltI8+VyLSaZs1h+UrpNk0oGpSG+F99y6SaaputbYN7wBnnpqzd1VZBo+8Xa5pSV/nWYyiUintJJkCkWm0a0qMqVNYpIpOvj7/S+qyCQiXRIVsxczWyGfH+jV5fq6NLWZ3W5mR8zsyGzCLJSOiA/yDUcTDYxXFiiOtjiTCeDgwaZFJg3+Fum98J5rmmSamqpNXw1e/3p48cVatCWSzfoTCioyDY+QZNJMJhHphlaSTGFnlds6RoFoh6UPjamQOJMp+hDwzEtaXU5EuiQqZl9w04kzmQZp8Hdfl6Z2zt3lnLvOOXfdzMzMhv4D64oXr9bZGUyU5ymMtZhkgpaKTDowFemt8J5rurrc1oT3+Rve4L/WDfM303t52ISZTPF2OSWZRKRTwjYjk2mcZLJinhJZJqaztSSTdjSp0CzJlK8qySQiXRJtZ+bY1teZTJ0oMg3d0tRrxJNMzXYGi4tsrV5gcWJP6899xRVw8mTDDwUa/C3Se+E917Bdbm5u7TwmqBWZGrTM6bP/8NDqciLSTeFsc7MkkxUKFBhjyxbULpcy4UBuVZJJRSYR6bZoO3M+KjIN7EymYVyaeo2QZMpmm+8M/s7PMT+98w2tP/fBg37C8AsvJN6sA1OR3ttwkunQIf9VRaahl9Qup9XlRKRT4u1yYV9Un2QaKebJM87EhIpMaRMO5LJZ31lfqbDyIWCpEmuXy+f9cYCISCdEM5kuuOm+zmTqxOpyQ7c09Rqzs34PsX9/8yLTk08CcHb3G1t/7oMH/dfnnqsdoMYoySTSextOMk1MwIEDKwXnOBWZhktSu1w4Yx0OCJVkEpGNiheZMhm/fYmf+BgdhUzJF5kmJ1VkSptwIDcyEtsHRAmDpUoO58C2bPF3yud9wUlEZLNCkqk6jUtIMg3STKbhd/o07N4Nk5PNi0xPPUUV4/zu17X+3PEiUwIdmIr03oaTTNB0hTm9l4dDtepPPNe3yzWbydSrnbqIDId4kQn89qQ+yZQp+yLT+Dga/J0yYZs/MhJLs0YHf0VyvggVCktqmRORTgmDv9mGFQqUi77iHU6EDky73EVhdhZmZtbvnX7ySZ63K8hMNoo/JLjkEv9JoUGRaXxcB6YivRbec20nmaBWZKqLv6vINDzis1JyudpuQTOZRKRT1isyjY7CaMUXmcbGwDIZKiNZFZlSoj7JFB/8XWDMd7SoyCQinRYrMgG45WgV0kGbyXRROH26pSKTe+opnnRvYGqqjefOZuHyy5smmcplnQUX6aXwGX1DSabXv97ffurUqqunpvzVMviSZqWE78HXH81g587a9SoyiUg74qvLAezaBdu3127PZmGcWpEpm4VyVjMW0iIcyGUysSRTdKapSE5FJhHpjmgm0xz+ZLhb8tuXgZzJNPRmZ+Gaa+DsWVhaSr5PtQpPPcUT/CP2tLG4HOBXmHv22cSbwgFMoeDHvYhI963bLrdekgl8mmn//pWr32F/wTPPbQOu79jrlP6IH/zVJwvA/9offNDvNkBFJhFpXzwxCXDvvauLTKOjMIZfXW4657dHJTfOmCKzqdAsyaQik4h0TbSdCUWmcOJBSaY0aqVd7sUXseVlnuIN7N3b5vMfPNi0XQ50Ykqkl5oO/i4U/FmCZjOZYPXw72KRO771I/zU8x/u6OuU/qhvlwuysdM2119fKzqpyCQi7apvlzt0yH8UDeJJplzOf1/KKMmUFs1mMhUY89+ryCQinVbXLhe2L+EzqQZ/p0WpBOfOrV9kigb9Pskb208yHTwIL7+c+MEgnmQSkd5ommSam/NfGyWZLr/cbyviw7//4i+YLJzjNcVjPTuDIN0TTzLVr/aUZOUAQ0SkRfVFpnqjo2vb5VRkSo/EJJPa5USk24pFqhgLRPN7lvvTLqci03rOnPFf9+xpXmR68kkAnuINGysyAbzwwpqbQpJCRSaR3mk6+DsMVmqUZBoZ8aec40WmL38ZgMs5wbmXGrTcysCIH/zFk0wTp5+FT396zf2VZBKRdq1XZKpPMmUyKjKlSbzIlLS6nIpMItIVpRIly7HM6u2LikxpMzvrv7aQZCps2cYr7N1YuxwktsyNaUVakZ5rOvh7vSQT1FaYC0/2Z39GYWoXABcefrpzL1T6olG73N77/yv8638NL7206v4rZ7FFRFrUTpJppV1uZEwfGFMiPvhbq8uJSM8UixSpFZksryJTOp0+7b+2kGQ6veMNZDLGjh1t/oxQZEoY/q12OZHea9out16SCeC7vguefhruuw/uvx/m5jjxI/8WgOVHj3X2xUrPNWyXWzzvL3z726vurySTiLSrlSTTGIWVdrlMBkojSjKlRVKSqbpca5fTTCYR6YpikRI58qwe7KyZTGkTTzJNTDRNMr04+UZmZvwOpS2XXOJ/8wlJJg3+Fum9poO/W0ky/ezP+qXF3vte+PjHYdcuiv/XTwJQfvJ4Z1+s9Fyjdrns4gV/QUUmEdmkcCCQySTfHpJMBcYYHfXbmaKKTKkRH/wdkkyVZbXLiUiXFYuUGF1JMo0UlGRKp5BkCu1ylYofBh43Pw8vvcTTmQ3MYwL/CeLAgabtckoyifROoQBmDc4gt5Jk2rrVp5j27PFr2f/Tf8quQzt5hT1kn1WSadCFg4cD37mP8Uxtf5BdigqQjzyy6v4qMolIu1qdyVQcGV/ZXxVHxgf6A6OZ3WRmT5nZcTO7I+H2G83sYTMrm9ktseuvMbMHzOyomT1qZv8sdtsXzOxZM3sk+ndNL/4vSUmm8pLa5USky0olCgntciHJpCJTWszO+j3Ezp2NdwbRUuVHq29sfx5TcPCgkkwiKZHP+/eeWcKNrSSZAPbt861yP/iD8NM/za5dcIxDTJxUkWnQlcvwRp7gH/7G/8mbnv7/Vq7PhCRTXZFJq8uJSLtanclUzvgPipkMFAY4yWRmGeBTwM3AVcD7zOyquru9ANwG3F13/RLwAefc1cBNwG+b2fbY7T/vnLsm+vcIPZC0ulx5SavLiUiXaSbTgJidhd27/V6i0c4gmqX06MJrN5ZkgoZFJiWZRHqvUGgwjwlaSzIFr389/MVfwJvfTCYDJ8Zfx/ZZFZkGXbkMu/Erj24tzK5cn5mPikzHjsHCwsr1SjKJSLvis9+S5EbKjFKmlPVFpmwWCja4RSbgBuC4c+4Z51wRuAd4d/wOzrnnnHOPAtW66//OOXcsunwSOA3M9OZlJ4sP/g4nGipLtXY5zWQSka4oFim6WpFppOj3CSoypc3p075VDhrvDM74g41j53Zvrsh06tSa5w5JJhWZRHqnUGgwjwlqSaapqbaf9/T0IXYsnYTFxY2/OOm7SgWm8X8Hk4VzK9ePLM7B5CQ4B489tnJ9Nuuv6tWOXUQGX7nsz2+umfN5zz3wfd/HtPNF7ZBkymahZAO9utylwIux709E17XFzG4AckB8KddfjdroPmlmiaeQzOx2MztiZkdmZ2eT7tKWpCRTZblImQxVMj7JlMn4SJqKTCLSIa5YpMjoyuDvMJMpm/UdGhr8nRZnzqxfZHr1VQBeXN61uXY5gOefX3V1SFMM7mcGkcGTz6+TZJqa2sCEf7iw55C/cFzDvwdZuVwrMk0Uz69cPzJ/Ab7v+/w3sZa50O7Sqx27iAy+crlBq9xf/zX89V9z06O/DkB11O+sMhnID3aSKalB3bX1BGb7gf8K/LhzLpT1PwK8Ebge2Al8OOmxzrm7nHPXOeeum5nZfAgqPvg7vrpcER8nWBnv2mzlahGRNrliaVW7XCgyZTJ+e6QkU1qcPevb5aBxkensWapbJigwvvEk0xVX+K91LXNqlxPpvabtcnNz689jamDpUhWZhkE8yTReqBWZbO4CvPnNsGNHYpFJLXMi0qpKpUGr3Hm/zfmHj3wSgPJorF2OgS4ynQAuj31/GXCy1Qeb2TTw34Ffcs79TbjeOfey8wrAH+Db8rouKclUzRdVZBKRrnIFv50pM0p1JEOmWEsyqciUJmfOwK5d/nKTIlNp2t9nU+1ysKbIpMHfIr0XBn8nmp9vbR5TguqVr/MXjmku0yCLJ5m2LPt2uQxlbGkJtm2Da65RkUlENqVhkunCBdi9G4uCOtXRuplMg3tW8jBwyMyuMLMccCtwbysPjO7/VeBLzrk/rrttf/TVgPcA3+noq24gaSZTNV+kgD+DtbI/UJFJRDooFJkAStktK0UmJZnSpFptOcmUn9hkkWn/ft+XrSSTSN91K8m07bKtvMw+yk+oyDTIymXYih8AP7bsUwWh6LRSZHr00ZWjCBWZRKRdTYtMV13FkWv+JQCV0drqcnnG/QMHcGPjnCsDHwLuB54AvuKcO2pmd5rZuwDM7HozOwG8F/iMmR2NHv4jwI3AbWb2SPTvmui2PzKzx4DHgN3Ar/Ti/5OUZHJ5tcuJSJcVipQYBXyRaaSYZ2TEz2PKZHpXZGqwMKoAfkderbZUZFoY90WmDc9kGhmB17xmZaW6QEkmkd7rVpJp7144xiF2PHlMG98BFm+Xyy35ItM2opXlpqd9kSmf94m1N71ppeVlAI/7RKRPGhaZzp+HAwf4Xzf8R0586zR/t913f2Wz8J3J74Gf+ZmBXWXAOXcfcF/ddR+NXT6Mb6Orf9wfAn/Y4Dnf3uGX2ZLEmUwFtcuJSHe5Uqm2ncmMkyktr3wOHRnR4O90iFaNW9Mut7S0+n5nz3Ih6++zqVmBBw+uSTKF5QaVZBLpnW4lmfbsgeO8jpGnlWQaZPF2udyib5fbmYklmd76Vn/5298GlGQSkfY1TTJt20Zh2x5u4U95dZuf6ZnNwt9MvxP+83+ufXiUvqlPMpXLvo1F7XIi0lWxYnZxZAvZ4vLKvkTtcmkRikwtJJleZRdbt9busiFXXLGmyGTmPyuoyCTSO02LTJtIMu3ZA49zFbmzp+DBBzf+AqWv4kWm7KJPMu3MREmmbdvg0OoB71pdTkTa1bTItH07o74bYqWeFNIykg7xIlMmE23/C2qXE5EuK9aKTIXMFrJ1SSYVmdLg7Fn/tVmRqVqFc+c4Xdm18Va54OBBOH16TVJqfKAXCxEZPA3b5c6cgRMn4MCBDT3v3r3wWX6ChZ0H4P3vh4WFzb1Q6Yt4u1x24Tzg2BGSTNPTMDHhf9lR+7OSTCLSrsTV5ZxbSTKF7UooMoW0jKRDfPD3yu+mqHY5EemyUm0mU8F8kSnsL3o5k0lFpmYatcvFdwbR3KaXS7s2PvQ7CCvMPf/8qqvHxpRkEumlhkmmP/1T/0nxlls29LwzM3CB7fzZLX/oCxA/+7Obe6HSF/HB3yOlIltYZsdILMkEcOWVKjKJyIYlJpkWFvwRwrZtK0mmsK8Kw6UlHZIGf1ux1i6nIpOIdEVsJlN+ZAvZcn5Vu5xmMqVBK+1yUdrppaWdnSsyJQz/VpJJpHcaJpnuuQfe8IbazJ02TU76fw9P/gB85CPw+c/Dxz9e+zT6zW/Cf/pP/my1pFa8XQ5gO+fZbnVFpiuuUJFJRDYssch0IdrObN++Jsmkdrl0SRr8bcVau5xmMolIN1i8XY5xsuX+tMtpgaNmzp6F0dHa/JVczg9JSigyPTffoXY5WDOXSUkmkd5KTDKdPAl/+ZfwsY/57cAG7d0Lr7wCfOFj8Hd/B7/4i/DAA1Ev3Wf9nf75P4fLL9/wz5DuCu1y1YlJRpYWfZFpJNYuB77IdM89UCqRyfjIgQ4ARaRViUWm834GHNu2MRrVm+LtckoypUdikqlcpIg/plCSSUS6ItYut8wWRkvLZCf9TZrJlBZnzvhWuXBAabZ2ZxAVmZ6d60C73L59/si2rsikJJNIb+XzCUWmr3zFJ4xuvXVTz71njx+9xugofPnL8Du/A//jf/hU0/d/v7/T/PymfoZ0V0gylff72Vw7OMc2LvjfaYjAXXGF35O/+KKSTCLStqZJpthMpni7nLYx6RGfybSSZCppJpOIdFd8O7PMFkb7lGRSkamZs2drrXJBgyLTGTpQZBoZgde8Zk2RaXISFhc3+dwi0hLn/Pttaqruhnvugbe9zbfLbcJKkQl84fqnfxq+9S2/3P1HPuKvV5Ep1colx1bmqVzqi0zbOe+LTNu21U5KXOGXFefZZ9esLve5z8X+BkREIl/9Kjz5pL+8XrucVpdLt6QkU6ZUoGSaySQi3WNlP5NpdBSW3BaylbwGf6fOmTMtF5nOdqLIBL5lrq7IND0Nc3OJ9xaRDltc9IWm0PUE+Pfkgw9uOsUEvsj0yit1V159Nbz5zbXWXBWZUs2WFhnBUbmsVmTaytzqP5orr/RfY0WmchlmZ+EnfsKH2ERE4n7iJ+C//Bd/uVxOWF0u1i6XtLqc2uXSI2km00ilSCWbw6xuJlOlEqs6iYhsULXKSKVMkRxTU7BY3UKusjrJpMHfaRDa5eISikxuZITzbN/8TCbwRaa6wd8qMon0zlzdaB0AHn/cf73xxk0//969vtCQeCZhiItMZnaTmT1lZsfN7I6E2280s4fNrGxmt9Td9kEzOxb9+2Ds+mvN7LHoOX/HbBPDstowsuD/SNxlsXY5d6E29Bvgssv8kUVdkSn8anXiWkTqLS35BeTAHwg0a5dLWl1OSab0SEoyjZSLVDM+YbAqyQTaKYjI5kUblhKjbN0KS26cXHl51epySjKlQYvtcsXJHThG2LGjAz/zwAFf3Ir9DBWZRHonscgUzh5v377p59+xw2/gE+tI4YcO2RvezDLAp4CbgauA95nZVXV3ewG4Dbi77rE7gY8B3wPcAHzMzMLW9tPA7cCh6N9NXfovrLJSZLo8lmSqXlj9R5PN+u15XZFpaclf1vGEiMQ55+cBhm2EVpcbbPEiU/jdZCsFytkxFZlEpDuiDctKkqmyhVw1T2bEr1qtIlMaVKutFZlefZXC5K6Vmzbtkkv815dfXrlKRSaR3kksMsU+2G9W2E4kDvMf3iTTDcBx59wzzrkicA/w7vgdnHPPOeceBep3fz8EfMM596pz7hzwDeAmM9sPTDvnHnDOOeBLwHu6/j8Bskv+j8T27IaJCXbaeabc3OokE/i5TM88s6rIFObraTEHEYkLqwiHbUTD1eWiBQbqZzKpXS5d4oO/V2YyVYpUs0oyiUiXFIv+S1RkWqj47ctkxn/oVJGpz6pV+PnbL/g9QgvtcssT/j5hUaFNaVBkmp/3Z7lEpLtCfScxyVRfRNiAsJ1I/DyZUGS6+274vd/b9I/tt0uBF2Pfn4iu28xjL40ur/ucZna7mR0xsyOzs7Mtv+hGMou+yDQyvRW2b2fnyHmmKheSi0zPPrvSCx9PMqnIJCJxYZuwbpIpWmCgfnU5JZnSJTHJVC3isrnVrY0qMolIp9QVmebLfvuyxfwOJpPRTKa+On8evvo5P9C7lXa5pS07V27atFBkOnly5aqtW2srXolId4UkU6j3AP6DfS7XkUpy0yTT2Jg/qogVmb70Jfj939/0j+23pFlJrZbNGz225ed0zt3lnLvOOXfdzMxMiz+2scyS//2MbJ+OikznmKzUDf4GP/z79GlyJb/xVpJJRBoJ24SmSaYLF1YStUlJJhWZ0iM++DskmbKVApVRtcuJSJdERaYwk2klyTTity9KMvVZqQS7OeO/aaHItDjWwSTT/v3+a6zINKRjWkRSqeFMpvjy9JvQ9POkma9uxYpM5fJQLDpzArg89v1lwMkG9231sSeiyxt5zk0ZXfZ/JJkdUZGJc42TTMDE6ecAf5ChJJOIJElKMiWuLhdtZ7S6XLrVJ5mq5QoZqjCqdjkR6ZK6mUzzZV+cmDAVmVKhWIwVmVpol1sY6+BMpl27/OkpFZlE+qLhTKYOzGOCWjG6YZEhocgUnZgYZIeBQ2Z2hZnlgFuBe1t87P3AO81sRzTw+53A/c65l4F5M/veaFW5DwBf68aLrxdmMmV2TMOOHexzJ8m4SsMi05ZTfsXQeJJJxxMiEhe2CWEb0XB1uWg7U7+6nNrl0qV+JlMOvyN3KjKJSLfUtcst47cvKjKlRKkEu2ihXa5QgMVFLozuYnQ04YzTRpj5lrm6mUygIpNILyS2y8XOHm/Wup8nt25d9WYvlQY/yeScKwMfwheMngC+4pw7amZ3mtm7AMzsejM7AbwX+IyZHY0e+yrwy/hC1WHgzug6gJ8CPgscB54Gvt6L/08uH81k2uZnMl1aecHfUN8uFxWZxmNFJiWZRCRJyzOZohMeSUmmalXzO9OiPsk0hp/s7nJjmskkIt0RKzJt3QqLTAIw4fzZi0xmwIpMZnaTmT1lZsfN7I6E2280s4fNrGxmt9Td9kEzOxb9+2Ds+mvN7LHoOX8nOlPdE6uSTM2KTGd9IepCZldnWuWCSy5RkkmkT+bm/JnhcHYY6EqSqeHnyTDpPzIkSSacc/c5517vnHutc+5Xo+s+6py7N7p82Dl3mXNu0jm3yzl3deyxn3fOvS769wex6484594cPeeHolXmum50eY48Y9hYDrZvZ4uLfpn1hcg9e2BigvGXngE0k0lEGmtpJlPshEfYl4QaRbivWubSoX4mU0gy2ZiSTCLSJbGZTFNTcBbfbbWz6usaIyMDNPjbzDLAp4CbgauA95nZVXV3ewG4Dbi77rE7gY8B34Nf4vpjUTsEwKeB24FD0b+bNvtaWxVmMlVGsnVxBlYXmV71J9PPZ3Z1plUu2L8/scg0fKuai6TP/PzaQEo3kkzttMsNepJp2Izm55gj+iPZsaN2Q/3fiBkcPEjuJSWZRKS5eJLJuXVWlwOuvtqvPHrzzf6m+CqW0n/1SaZQZCKnIpOIdEndTKZZ/GI3O8p+ZeVBa5e7ATjunHvGOVcE7gHeHb+Dc+4559yjQP1/64eAbzjnXnXOnQO+AdxkZvuBaefcA9GZ6S8B7+nAa21Jsejb5f34y0AAACAASURBVBbHd68d9Dsx4e9QqawkmV41JZlEhsVcwiJhnUwytdQuFysylUrDkWQaJrnCPPMW/ZHE/y7W/OEAe/eSueD3FUoyiUgj8W3C8nJCkalchoWFlW2OGfyrf7U2yaQiUzrEi0zZbK1djjGtLiciXVI3k2mlyFQZzCLTpcCLse9PRNdt5rGXRpfXfU4zu93MjpjZkdnZ2ZZfdDMhybSwZffaG+MxhFiRqaNJpksu8Qe10SnvEKZSkUmk++bm1gYY42ePN2sjg7+VZEqXsfwcC0lFpqS/ka1bySz4jbdWlxORRuLbhKWlhCJT+BDYYF+kdrl0iQ/+jieZbCynmUwi0h11M5kWmCLPGDtKg1lkSpqV1OpcjEaPbfk5nXN3Oeeuc85dNzMz0+KPbS7MZAqrxq0S3xlERabZaheKTLAy/FtFJpHeWZNkKpV8/KRPSaZhmck0TMYKcyyMRBvmeLtcUpJpehpb9L9PrS4nIo3EtwmLi75YtGpBmQsX/NcGRSa1y6VLfZIpFJlGxtUuJyJdUjeTCYzT7GFbVGTKZAZoJhM+ZXR57PvLgJMN7tvqY09ElzfynJsWVpebH2uSZIoXmSo7O98uBystc2EIsYpMIt23psi0ztnjdq07+DusLhfNsA5JJq0YlB5jxTkWR1pMMk1PMxIlmTSTSUQaWTfJFIpMDU54qF0uXeKDv+Ory9l4XbtcLud7H1VkEpHNijYsZcsxMeGvmmWGbcXBTDIdBg6Z2RVmlgNuBe5t8bH3A+80sx3RwO93Avc7514G5s3se6NV5T4AfK0Dr7UlIck0N9qkyLS05ItM4+NcKE10fvA3rJnLpCKTSPetKTKdP++/djjJ1LDIMD3t9wDRB85QYFILRHqMF+dYyCQUmdb0WeKTTPNzgNNMJhFpKL5NWFxMKDKFfZHa5QZCoyRTZotvl1spMpmtXlRIRGSjoiRTNZtbWSV7lhm2F04DA1Zkcs6VgQ/hC0ZPAF9xzh01szvN7F0AZna9mZ0A3gt8xsyORo99FfhlfKHqMHBndB3ATwGfBY4DTwNf3+xrbVWp6NjFWeZyLbTL7drF8jJdTTLBmlXNRaRL1qwut06LQrsyGRgdXSfJFF4ItbPSmsuUHmPF+bVJpqmput6WyPQ0VioxRkFJJhFpqOUkk9rlBkK16utHZqtnMoV2uVW/JxWZRKQTYkWmUJuYZYbpQu+TTPWLo26Ic+4+4L666z4au3yY1e1v8ft9Hvh8wvVHgDd34vW1q3p+jiwV5rM7194YLzI9+ihceSX5V+lskmnHDt8fF81kAiWZRHql20km8EXppoO/wReZ9u5d+SBaLHZ4OyMbtqU0x9JY9EcSZjI1KkJGf0zTzFEujyvJJCKJ1k0ytdgupyRTOlSr/oAO6pJME3XtcgCTk7WYq4jIRoUhrqOjq5JMWwu1mUwDk2QaSgsL/stIwhDXcJT37LPw0ENw882dTzKZ+TRTLMkUxrSISPcUi/6D/qqupw4nmWCdk5Z1SabwQVRJppQoFslV8ixlot9TqEi2VGSqJZmKRR0MikhNfJ+QmGRqsV1OSaZ0iBeZ4jOZMltya4tM27bVPmuIiGxUtGFxo6uTTOPlRVheZmRksAZ/Dx0374tMSza59sZQZPrqV/3XH/5h8vkuJAzqikxKMol0X2hJTUWSaa42LBq0wlxqRH8kS9nojyST8X8wSSvLwaoiU6Wy+mR1odDNFyoigyS+T1hY8LP4tLrc4KpUkpNM2Ym6mUygIpOIdEaDmUz+wuxgzWQaSotRksmm1t4Wqkn33ecLQd/1XSwvd6HItH+/ikwiPRbeY92cyQTtJZk0kylloj+S5dHYH8mOHY3/PqLf5/aR+VVJJlDLnIjUxLcHYV+0pl1uYsIP9Uugdrl0adQul50cWzuTSUUmEemEWLtcPMnkL6jI1H8L/lTzIk2KTEtL8MM/DGadb5cDX8AKM5m++EVuuPANFZlEuiyxyBSSTI2SKhswPt5+kUlJppSoTzIBvP3t8P3fn3z/6O9m+8jcyupyU9GuRUUmEQny+dq2IdQb1rTLNTnZoXa5dKlWa+myeLtcdiKhXW779tpnDRGRjQoHC7lakuk0e6ILpwdv8PewGVmKkkyuSbscwA//MM7RvXa5uTm491647TZuPvhO7ph/R4d/iIjEJbbLXbjgCz9JK4dt0JYtTQoM4YfPz+Nc7ay0kkwpEVUi87nYH8nn16xdUROKTJk5SiV/fuLAAd8OoyKTiAT5POzc6bcNDZNMTYpMapdLl0ZJptyUZjKJSJeEmUzZ5CRTJqOZTH1loV2uWZJpdBR+8AdXZmp0JckE8GM/BsB0YZZ8XmkGkW5qmGTq4DwmaL1dLn6woPd+SkR/JIXc1nXuGAlFJptbmbOya5e/SUUmEQnCohO5XIMk04ULTfdFapdLl/hMpkxmdbtcw5lMzvX+hYrI8CgWKVuWbG5EM5nSaGTZt8vNuyZFph/4AZieXjlI6EqSCfynjhtuYGrptH9N8x3+OSKyouFMpg7OY4J1Bn+Hfom6IpOSTCmRNJOpmeiPaZvNrfx97dzpvzYsNIrIRSeMXpiYaJBkUrvcQKlPMoV2uZBkWvV72r6dNUP7RETaVSxSsVFGR/1i9bkczDFNeWRURaY0WLdd7m1vgx//caB2kNDxItOhQ37v9PGPw9vfzpaF04DTXCaRLgrvr63xkEqvk0wjIzA5CXNzqwpLSjKlREgyjbVYZNqyBTKZVUUmJZlEpF4+74tM0eYfSFhdTu1yA6N+JtO67XKgljkR2ZxikdJIbmV9CN9pZSxNzvS8yKSZTAkyeZ9kmqsmJJnM4OGHV74NB4odb5c7cABeeQV274ZPfpJMpcQ2LjA319mDXRGpaZhk2r+/oz+n6eBv8FUuJZnSKYqTtlxkMoPpaaaLa5NMKjKJSBCKTA2TTPPzdWdAVlO7XLo0msk0NjW6UmRyzu8iVhWZQieDiEi7SiVKllvZH4SWuaXJGaZnZxmZUpKprzL5BcpkWKqMrXvfrrXLgS8wAezxU+H3cFpJJpEumpvzH/gm4yHGLiWZmhYYEopMSjKlRLQRLuUSkq6NbN3KVuZXTlIrySQi9eJJpsSZTEtLdTun1YYhyWRmN5nZU2Z23MzuSLj9RjN72MzKZnZL3W0fNLNj0b8Pxq6/1swei57zd8zMevF/iReZwupyBXKMjdvagmD4jKEkk4hsRrFIyeqTTKwkmTT4u89G8wssMkm5sv5+qGtJprhYkanRTKZnnoGPfKR31UmRQXTqFPy7f9c4FRROFI/Et4xdmMnUtF0OfJRKSaZ0yucp2BiZ0TZ2n9PTTDu1y4lIY+smmRYX/Y0NDPpMJjPLAJ8CbgauAt5nZlfV3e0F4Dbg7rrH7gQ+BnwPcAPwMTPbEd38aeB24FD076Yu/RdWiQ/+DkmmIjnGx1k5AFz5XYXPGOfP9+KliciwKhYpRzOZoJZkyk/NwOnTmsnUb5nCIgtMtXRQ19UkU9BCkunuu+ETn4ATJ7r4OkQG3Ne/Dr/1W6s6XleZm6trlXOuK0mmpoO/YSXJpJlMKVSpULXM6oO/9UxPs5W5NUkmDf4WkWB52X+WTEwylUq+ItEkyTQE7XI3AMedc88454rAPcC743dwzj3nnHsUqD9M+iHgG865V51z54BvADeZ2X5g2jn3gHPOAV8C3tP1/wlrk0w5ihQYY2ysVmRa2cdrJpOIdEKxSIm1SablrXs0+DsNcoUFFphq6WxQr5NMjYpMx4/7r9o/iTQWThKG90u9NUWmpSX/ib1LSaaGqxVrJlN6VSpUyKweyLue6WmmqprJJCKNNU0yLfpZoc2STEPQLncp8GLs+xPRdZt57KXR5Y0856bEB3+H1eXqk0wr+3W1y4lIJ5RKFBNmMuWnZ2B+npwr8H8sfg1+7de6fvZaRaYEo8WoXa6FHXVPkkzRbKZWikxK2oo0Ft4fx44l376myBQe0IUkEzTZvm/dCnNzmsmURlGRqe0kk5tbOXukdjkRqRefyRS2FSvF7LC0fQtJpgEuMiXNqGh0KqbVx7b8nGZ2u5kdMbMjs7OzLf7YxpKSTEVyjI3Vfldrkkz6EC8im1H025n6JFNh6wwA24qz/Ou5T8AXvlCrdneJikwJcsXFtpNMXS0yjY7idu5kb5MiUzho1v5JpLFWkkyrFu8JZxW7kGSCJu1SCe1ySjKlRKVCdQNFpqlKbeOtJJOI1IsnmYKV7UwoMrUwk2mA2+VOAJfHvr8MOLnJx56ILq/7nM65u5xz1znnrpuZmWn5RTeSNJOpwBi5XMJMpokJX4lSkklENqNYpMTomiJTcZvfpl198htcW/ob+MmfjJa27B4VmRLkSgstz2TqSbscYHv2cEk2ucg0NwenT/vLKjKJNNZ2u1z4wNeF1eVg/SKTkkwpVKlQsfbb5SartY23kkwiEufc6iRTsKZdbrhXlzsMHDKzK8wsB9wK3NviY+8H3mlmO6KB3+8E7nfOvQzMm9n3RqvKfQD4WjdefL2k1eVKlsMsoV3OzJ/MUpFJRDajLskU2uUKUZHp5kf+E0tsgQ98oOsvRUWmBOOllLXLAezZw76R5NXlnn66dllFJpHG1isyzc83aJfrcJIpFKUbFhmmpyGfp5yvbYSUZEqJDbbLTVQXGaESviWTUZFJRLxy2RclOpFkGtQik3OuDHwIXzB6AviKc+6omd1pZu8CMLPrzewE8F7gM2Z2NHrsq8Av4wtVh4E7o+sAfgr4LHAceBr4ei/+P/EiU0gylS0HJBSZwJ/M0od4EdmMUomiq81kCscbpe2+yLTnwnG+OnYr7NjR4Ak6p52PyReNXLn9drluJ5nYs4c9HE1MMsUPmP9/9t48TK6qXNu/V3X1mHmeJzIQwpRAEmZQgggfs4rAQUQPnKiIHw6ffug5cPxQj6AeceKoKHpQQVD8CcEDRGZBGZJACAkZ6CRAOoSM3UnP3VW1fn+8e1fvqtpVXdVdVb2r+r2vq66q2nvt6t3dtYf1rOd9ll6fFCU97vGxfz80NqaeYwPlZALsoWZAdlKdTAEhEsldZHL+n0Npob1yBJWVPeHviqIo6aIXcnEylUG5HNbaR4BHkpbd7Hm9isTyN2+7XwG/8lm+Gjgqv3vaO97gbzeTqTsktoKUTCZQJ5OiKP2nq4tOOzzFydQ9oqcE+FdVn+HKIuyKOpl8qI1kXy5XTCfTmJh/uZwrMlVW6vVJUTLR1NQzgpjsZrI2Q/B3sZ1Mrsh0sOeAVydTQIhGidhwzuVyAMM5FDci1NSok0lRFME9F/THyVQG5XJlRbKTqZpOIhWJTqaE/5WKTIqi9JeuLrpsaiZTbPhIqKhgx/jjWW2WFGVXVGTyoSaafblcMZ1MIyP7aT2YulP19TBpEowbp04mRclEUxMce6y8ThaZWltFaAqSk8lbH6tOpoDQx3I5EJHJNSKoyKQoiotXZPKaleJitutkKuNyuXLDG/ztOpmiFVoupyhKAenqotNTLuc6mSoqQ/CNb/DQ+26Pz15aaFRkSqa7myrbRQtDicXo9R/R3g5VVT0XkoIxfjwAFY37Ula9+SbMmaPXJ0XpjYMH4bjj5HWyyOS6BBNml3OtT3lWkfsiMqmTKSBEo0TIPfgb1MmkKIo/WTuZyrxcrpzwy2SKVkiPz1dkUieToij9pbubTluV4mQKh4GvfpW3p5+mItOA4YwWtSIX8t4u1h0dRSiVg7jIVNW0J2VVfb2ITCNGqMikKOmIxURImjgRpk0TcdaLKzIlOJn27RP1Ns/TfGZbLqdOpgASjRK16mRSFCV/pHMyablc6ZKcyVRNJ9GwOJl8XWcqMimK0l+6uuiIpc4u556LQqHeDTT5QkWmZFpa5ImhQO/ugfb2IpTKQVxkqmtNFJlaW2HXLnUyKUpvHDok5XAjR8rxkuxkcvWcuMgUjcKjj8IJJ+R9X7J1MpkWdTIFDsfJ1FeRSZ1MiqIk06uTaZAEf5cTfk6mWG/lcocOFa8HqChK2WG7uuj0yWRyrw8VFcU7xejscsk4F3JXZOptRKjYTqahrXv45jelRO+aa2DnTlk9dy6sXw9bthRhXxSlBHEFWFdkevBBeb9+Pfz5z/D22/I+LjL97W9ygH3ve3nfl16dTM5OeEUmdTIFBMfJ1JdyuWE0x/uIOrucoigu3tnl3M4BJDmZjMk4qqmZTMHCKzK5mUyxyl7K5ayVEa88TzaiKMogoauLLnoymdxLhtfJVKyBCBWZknGcTG65XG8X62I7mcazh5tukkVPPw3XXiuv1cmkKJlJFpn27oWtW+Gss2D3bllXVwezZjkb3HMPDB0KF16Y933p1cnkiBLh5sb4InUyBYS+OJkcZ1qyk8m53CiKMsjxOpnc8gZIcjLV1WUs3dZyuWDhDf52Z5eLhTM4mVxh6eBBFZkURekb3d10kVou515LtFxuIEkqlwuMk2nkSAiH+d5X9tDdDT/+MTz2GNxyi6yePbtHZLK2CPujKCVGssgE8KEPwYEDsHatOIUOHoSpU5ED+4EH4JJLMmZg9BX3nJHWyTR6NEycyMg3V8UXqZMpGNhIH2aX84hMmsmkKEoyvc4u19bW67XIbavlcsHA62QKhcTJZCszZDK5s9jqaLGiKH2lq4tu0pfLhUKiExRDK1CRKZl48Hf2mUxFEZmMgXHjqNi/h3AYrrsOli2DdevE5DR8uFyfotGe0n1FUXrwikxz58rrdetEqD32WBlZjAsHjzwiitPHPlaQfXFP+mmdTMbAmWcyYcNTgMUYdTIFBRsVkSmncrlwmM5wnWYyKYriS1aZTBnymEAuGxUV6mQKCt7gb2PEyWSreimXAw3/VhSlb1gbL5fLFPztNi00KjIl4ziZYrUBK5cDUZP2SPB3KAS//rWIS/PmyWp3EESvT4qSildkmj1bjqGTToIvf9mn8T33wIQJcOaZBdmXXkUmgGXLqD20myPYSF1d6TuZjDHnGGM2G2PqjTE3+qyvNsbc76x/yRgz01l+pTFmrecRM8YsdNY943ymu258oX+PPjmZgPbK4epkUhTFl6xml8vCVasiU3DwOpmwllraiVXLqLSKTIqi5J32doy1tDIkJZPJG/wNxXG8aiZTMo7IFK0dCu3ZlcuNGlWE/YIEkQlkGvann5YQcEh02k6ZUqR9UpQSwSsy1dWJWWnhQlIdKV1dsvLaa8lZSciSUEiO24wigyNwLeNJ7q9bUNJOJmNMBXAH8AGgAVhljFlhrX3D0+waoNFaO8cYczlwG3CZtfYe4B7nc44GHrLWrvVsd6W1dnVRfhGA7kifRKaOquEMb090Mmnwt6Io0HMuSOtkylJkCoe1XC4oRKM99+d0d1NBLLPIpOVyiqL0B2ea7GaG9epkKkYukzqZknFqzaK12ZfLDYSTyeW44+Coo+S1Xp8UJT3uceHOHvfBD4pZKYV160T9Of30gu5Pr7OLzZzJwTGzOJOnqK0teSfTUqDeWrvNWtsF3AdclNTmIuBu5/UDwDJjUlJurwB+X9A97QUbiRIhnFu5HNBZnehkqq1VJ5OiKIJ7LqitlU6BNzAayKpczm2vTqZgkOBkci72w8ZJh8E3k0mdTIqi9AcfkWnWLDnfTJok71VkGkBsszPdz5DsyuWKFvwNviKTFxWZFCU9TU0iMPUqDrz8sjwvXVrQ/clGZNgx50zexzMMrY2WtJMJmALs8LxvcJb5trHWRoCDwJikNpeRKjL92imVu8lHlMo7biZTrk6mzqrhDKNZM5kURUnBWy5nTI9pScvlShdvJpP7Dz7jXC2XUxSlQPiITMcdB4cOwYwZ8l5FpgHENrfI9NRDxF+WTSZTUUWm1ta081671ycVmRQllaamLGcFfvllOdamTy/o/mRTLvXW7DMZRRMLzdpSdzL5iT/JsYMZ2xhjTgDarLXrPeuvtNYeDZzmPK7y/eHGLDfGrDbGrN67d29ue55MHzOZkp1Mrsiks4EqiuKKTG5pg3ue6IuTScvlgoGfkylUl0Fkqq6Wh97EK4rSFzwik/ce1atTqMg0gMSaW2lhKHVDpL8TqODvyZPledcu39XqZFKU9Bw82HOMZOTll+GEE2Q4uYD0Wi4HbJshuUyndDxV6k6mBmCa5/1U4N10bYwxYWAEcMCz/nKSXEzW2p3OczNwL1KWl4K19k5r7WJr7eJx48b149fo4+xyQFfNsJTZ5aDkyyAVRckDHR2iL7iXHfc8ET/P5JDJpE6mYOAnMrm9PV+RCeQmRZ1MiqL0BR8nUzLFDP5WkSmJ2KEWWhkSv5b31rErarlcLyKTOpkUJT1NTVmITAcPwqZNBS+Vg+zKpQ7WTmQDCzhv76+Z2rQ+c+NgswqYa4yZZYypQgSjFUltVgBXO68/Ajxlrfh8jDEh4FIkywlnWdgYM9Z5XQmcDxT+j9THcrnumlQnE2jJnKIoch7wDlj21cmk5XLBIRpNLzL5ZjKB3MiryKQoSl9wKp0yiUzqZBpAbHOLOJkckSnTxToWg87OAXAyvZtsABBqauShIpOipJKVyLR6tdQvFUFkysbJFInATXyTMV27uGfDsfCZz8C+fQXft3zjZCxdD6wENgJ/sNZuMMbcYoy50Gl2FzDGGFMPfBG40fMRpwMN1tptnmXVwEpjzDpgLbAT+EWBf5U+l8t1OSJTXa3Ux7nXDZ1hTlGUZFd8f5xMWi4XDHydTM4/Oa2TacQIvYlXFKVvpCmX81JyIpMx5hxjzGZjTL0x5kaf9dXGmPud9S8ZY2Y6y690AlvdR8wYs9BZ94zzme668fnY196wLVIu57qTMolMnZ3yXDQnkxsNn0ZkAulE6/VJUVLJSmRyQ7+XLCn4/mQT/N3dDX+pvITrP1jP70d/Fn73u7SZbEHHWvuItXaetXa2tfZbzrKbrbUrnNcd1tpLrbVzrLVLvYKStfYZa+2JSZ/Xaq093lp7jLX2SGvtDdbagnev+lou1107nEoiDKuUf7p73VAnk6Ioya74IUNEYDIG6Q20tamTqcTwC/7WcjlFUQpGFuVyJSUyGWMqgDuAc4EFwBXGmAVJza4BGq21c4DbgdsArLX3WGsXWmsXIoGtb1lr13q2u9Jdb61NP61aPmnJvlwuaWCi8IwcKT+sF5FJr0+KkkrWItO8eTBqVMH3J5vg70hERqa7ho3hljE/gnfegZkzC75vSgb6OrvcsLEADG97D9ByOUVRekgul6ur85TKuScJzWQqKTJlMoVCIiD6Opn0Jl5RlL7giEwtDC2bTKalQL21dpu1tgvJzLgoqc1FwN3O6weAZT5TTV9B6tTUxacl+3K5pGtG4TFGSubUyaQoORGLZRn8/fLLRSmVg+zL5cJhGfXs6qIo4pfSC30UmZomHg7AyN2bARWZFEXpwS+TKX6OaWuTZy2XKykyZTKBXNc1k0lRlLzR3Ey0upYo4fJwMgFTgB2e9w3OMt82TjbHQWBMUpvLSBWZfu2Uyt3kI0oBeZ6aGjBtrVmLTEnu1+IweXLa4G9QkUlR/GhulqiljCJTQ4MIuEUSmbIJ/o5E5Ea0qqr3SQiUItHHcrmDE0RkGrpTRSZFURLxczLFzzGtrfKs5XIlRSYnE8i13bdcTm/iFUXpC83NRGqHAZRNJpOf+GNzaWOMOQFos9Z6Zwa60lp7NHCa87jK74fnc2pqANMm5XLZZDIVvVwOJJdJnUyKkhPuMZFRZHrqKXk+4YSC7w9k52Tq7k5yMikDjolE+uRkqpsxjkZGMqQhUWTS4G9FUZKDvydOhNGjnTc5OplUZAoGCZlM2YpMo0bJ/9sNfVUURcmW5mYi1UMBysbJ1ABM87yfCiSrIPE2xpgwMAI44Fl/OUkuJmvtTue5GbgXKcsrOKG2xHK5TO6BAXMyqcikKDmRlcj0y1/CnDmweHFR9imb4G+3XE6dTAEiFiVCOGcn0xX/ZKhZOJ+qbZsAdTIpitJDspPpa1+DZ5913uTgZNJyueCQ4GRyT/Sef3I47HNdd5XFxsaC75+iKGVGczPdNeJkKheRaRUw1xgzyxhThQhGK5LarACudl5/BHjKWmsBjDEh4FIkywlnWdgYM9Z5XQmcD6ynCITasy+XGxAn0+TJMruUE+6VjDv7qU32kinKIMYVmUaMcBbs2wePP96TfbBxIzz3HCxf7rkrLCzZBn9XVqqTKVD0MZOpqgpqjz0cNouTSWeXUxTFJXl2uWHDYPp0500OTiYtlwsO2ZTLpfyvXJHpwAEURVFyormZrl5EppIK/nYylq4HVgIbgT9YazcYY24xxlzoNLsLGGOMqQe+CNzo+YjTgQbvdNVANbDSGLMOWAvsBH7R333tla4uQt1dCbPLBTKTCdLmMo0cKSMjWoKhKD24ItPE/Rvg8MNh3Dg4+2y46CI5YO68U87IV1+d+YPySG2tOOIzCcJuuZw6mYKD6aPIBMh37913oblZnUyKosRJdjIloE6mksQ3+NvzT05bLgfqZFIUJXeam+muDk4mU19uk1Ow1j4CPJK07GbP6w7EreS37TPAiUnLWoHj87FvOeFcyFsYGuxMJpCOyrx5KavdcqCmpqwGvRRlUOCKTOM3PwdbtsDXvy7Kzde+BjfcAPfdBx/6EIwfX7R98ooM6YRq7+xykYgIUv5TIChFI9a34G8A5s+X582bqRknZZkqMimKklFk0kymkiTFyVRTk3AB9xWZ1MmkKEpfaW6mc8RsIBjlcnkRmcoGR2TyOpkyuQd83K+Fx3Uypcll8opMblNFGey4VXFDWnbLi699Tc7A770HP/qRLFu+vKj75J432tt7F5mqquR9d3fPa2VgMLF+OplARKZpuYlMO3fCHXfAZZfBscf24WcrilIUrIVbb4XZs+HSS7MbGMhKZMpydjkVroNBSvB30oXeN5PJdTKpyKQoSq40N9M5rrwymcqHlhZ5yjKTaUDL5XoRmdxOtaIoPU6m3MsGxwAAIABJREFUmoO7YcyYnrPv974Hy5bBwoXwvvcVdZ+yyeTxZjKB5jIFgv6Uy82eLb2OTZuynl2uqwv+/d/FuPrtb8NPftKHn6soStHYuVPGMS67DE4+Gdas6X2b5NnlEnDL5bJ0Mmm5XDDwdTJ5yJjJpOVyiqLkSksLnZVllMlUVuQoMg1Iudzw4XKjkSGTCXSGOUXx0tQEQ4dCaO9umDChZ0VlpQSAv/BC0QK/XbIRGbyZTO57ZWAx/SmXq66GWbPEyZRlJtOtt8Itt8AFF8CiRbBuXR9+rqIoRcM9Rm+4Ad5+W+L/ersn03K58iMhk8mnLt63XG7ECLG+qZNJUZRcsBZaWuioCk4mk4pMXnIslxsQJ5Mx4mbKolxOURShqck5NnbvTs1dMqbISrGQrZPJzWQCdTIFgX6Vy4GUzG3eTFWVfPUy/f9374bvfEfiwu67D844A9avV6eCogQZV2T6f/8PHnlETCm33pq+fSQij3w4mXR2ueCQ4mTKRmQKheRmRZ1MiqLkQmsrWEtHWMvlgkkpOJlAwr97EZn0+qQoPTQ2ygAhu5OcTANINk4mt1xOnUzBod8i0/z5sGULxsaoqcksMt1yi6z/9rfl/THHiKlh27b02yiKMrCsWwczZsg1Z+FC+NjH4Ac/gHfe8W/f2SnPaQcs29oS66YzoOVywaFPmUwgJXPqZFIUJReamwFoV5EpoHhEpsDOLgcZnUxjx8pFLc1qRRmUvPuuMzFjgEQmb/B3OtxyOXUyBYRYDGNt38vlQJxMHR3wzjsZRabNm+HnP4dPfapnItFjjpHn117r489WFKXgvPZaz7EK8I1vyPPNN/u3d88BGZ1MWYR+g5bLBYlsnEy+/6tRo1RkUhQlN1yRqVLL5YKJp1zOvdj3FvydNCNpcZg8WTKZrE1ZFQ7D1Knw1ltF3idFCTBvvQVzp7bLSThgIlM25XLqZAoIjkWg304mgO98hx93/AuL1t3t2+w//1OuL96O6YIFcoOguUyKEkw6OkQg9opMM2bA5z4Hv/kNNDT4bwO9ZDJlUSoHWi4XJPpULgfiZNJyBEVRcsEVmSqGYQxpB0I1+HugcK70HdRQVZXByuqQcTaQQjJ5sghizhcqmVmzVGRSFJe2NtizBxaM3SMLAiIyZVsup06mAOFclSOE+y4yLVgg/9Cf/pTL23/F+av/3bdZQwMccUTi17W2VoxQKjIpSjDZuFFOE8cem7j8jDNkXNBvzpZeXfFtbTk5mbRcLhikBH/7zC6n5XKKouQFRxNoDQ3LWFmtTqaBwhn+6aaSqqoMVlYHn8kiisOkSfKcpiZu5kzYvr14u6MoQcYVXOcO3y0vAiIyZetk0kymAOFxMvW5XG7MGHjjDWho4PbxtzKu9W3Yty+lWXu7v3nhmGNUZFKUoOIem14nE/Qcy36DClmVy2XpZNJyueDQm5Mp7UD2qFHqZFIUJTdUZAo4zpU5QpjKyt4v1gPqZIKMItO77/aESSrKYMYVmaZXB0tkysbJpJlMASMf5XIAc+bAlClsGb5Y3q9Zk9Kkrc1/EOOYY2QQ4dChfvx8RVEKwmuvybl9zpzE5e6x3NaWuk1W5XJZOpm0XC449Bb8nXYg23UyFaMXqChKeeCITG2hoRnvT1VkGiics32UMBUVvZfL7dwJ48YVad+8uCLTjTfKtENJtqVZs+Q53UwmijKYcEWmSaFgiUxjxsizX0aHi2YyBYx8iUwOb40+Tl6sXp2yLpOTCeD11/v/8xVFyS/r1sFRR6XmYWTjZErrjM/RyVTK5XLGmHOMMZuNMfXGmBt91lcbY+531r9kjJnpLL/SGLPW84gZYxY6655xPtNdN74Yv0u/MplisbSRGIqiKCk454sWo06mYOKITKYyjDGZnUyxmAw+H398EffPZe5cEZg6OuBrX4NPfCJh9cyZ8qy5TIoix0F1NQzvcESm8UW5v+yVESNktNvHxBJHM5kCRj7K5bwfN3QEO2rm5uRkcrNetGROUYKFteJkSs5jgsxOJld4qq5O88E5BH+XcrmcMaYCuAM4F1gAXGGMWZDU7Bqg0Vo7B7gduA3AWnuPtXahtXYhcBXwlrV2rWe7K9311to9hf5drJVHn0SmUaPkWUvmFEXJlixFJg3+HiicK3OoSoaoM2Uyvfmm/D+XLCnWznkIhcTBtG4d3HADrFqVsKOuyKS5TIoix8GMGRDasxuGDx+gGld/lizxNbHE6e7WTKZAkWcnU20tbKhd7PslSNevnDoVRo5UkUlRgsbu3RKvlpzHBD3HciaRKa2O1No6WMrllgL11tpt1tou4D7goqQ2FwHulJwPAMuMSZnj+Qrg9wXd015wXQKZgr/TVkuMHi3PGv6tKEq2uCITQ9XJFEgiEWIYwlXyZ8k0IuT2CRYvLtK+pWPpUrlD2bAhvmjKFNl3dTIpihwHM2ciPYCAlMq5LF4MO3bIrvmhTqaAUQCR6bVK50uwJ3FwPV25nDHSiX3ttf7/fEVR8od7TGYSmfzK5dxlacvlcnQylXC53BRgh+d9g7PMt421NgIcBMYktbmMVJHp106p3E0+ohQAxpjlxpjVxpjVe/fu7evvAPR04CoqECUpGs0+k8l1MqnIpChKtrS0wJAhdEdDmskUSCIRbKgi7hrIlMm0erVcL444oni758vSpfK8alV8UUUFTJ+uIpOigBwHs2YhnfgAikyQvmROM5kCRp7L5Wpr4dWQU3Od9CVIVy4HUjGt53dFCRbuMZkc+g3ZlctlFJkGh5PJT/yxubQxxpwAtFlr13vWX2mtPRo4zXlc5ffDrbV3WmsXW2sXj+tn4GqCkynNP9hbLvfqq565fFwnk5bLKYqSBV1d0LCxGYYNi1dApENFpoEiEiFqwvF/TqZyudWrYdEi8jKa3S9mz5ZRj5dfTlg8c6Z2QhSlpUXKF4LqZFq0SJwp6UrmIhE5D6mTKSDk2clUVwerIqlfgmhU/tfpzAujRkFTU/9/vqIo+cM9Jl2NwItbKeUnMrnLMpbLDYJMJsS5NM3zfiqQPI1yvI0xJgyMALyWn8tJcjFZa3c6z83AvUhZXkHJVWQ691z45jedFVoupyhKDqxYAc892kxnde8ik2YyDRSOyOR1MvldrKNReOWVAJTKgXROlixJcDKBdKo1k0kZ7LhCa1BFpmHDYP789CJTd7c6mQKFR2QK5eHqWVsLezuHw+GHJ3wJestoGTVK2nR29n8fFEXJD42NcnPv50gyRo7nnMvlurvlkaWTKRwWgcMm+39Kg1XAXGPMLGNMFSIYrUhqswK42nn9EeApa+W3NcaEgEuRLCecZWFjzFjndSVwPrCeAuN24DKJTG61RFOT3J7ENSUN/lYUJQcOHIBhNNNWoU6m4BKNJjiZ0pXLbdokI0+BEJlASuZefz1hiGzWLHjvPf8bGkUZLDQ99gK1tHHYtG45CwdMZAI5j2RyMmkmU4BwRh1iRmYg7S91dc5p+/jjE8rl3FN5uvIZ7YMoSvBoapJjM925obY2s5PJ93jv1eaUSDFHqfONk7F0PbAS2Aj8wVq7wRhzizHmQqfZXcAYY0w98EXgRs9HnA40WGu3eZZVAyuNMeuAtcBO4BcF/lUSnUwdHfImTSbTNmdvW1roaVdTo04mRVGyoqVFRKZmhsX7DelQkWmgyNLJFJjQb5clS+SOYm3PbK3uDHPvvDMwu6QoA87+/Zz8lVO5iW8wa4gTqhxQkWnXLk8egwfNZAoYbs8tH4FMSF8iGoXoosWwc6cMFtC7k2nkSHlWkUlRgkNjY8+x6UcmJ5N3MCEBV2TK0sk0YQIceWRpikwA1tpHrLXzrLWzrbXfcpbdbK1d4bzusNZeaq2dY61d6hWUrLXPWGtPTPq8Vmvt8dbaY6y1R1prb7DWFvyvkxD87f7Tk2aXc8vltm6V962tnpWjRqnIpChKVrS2isjUFFEnU3DJMpNp9WoYOlQqHALBkiXy7MllckUmzWVSBi2bNhGyMf7J/J6xkfdkWQBFJvfw9XMzuRcLdTIFBKfnFqrMn8gE0HbJlTB2LFx7LUQi6mRSlBLEdTKlI52Tqb09Q+i3qzxk6WRavhzWr4fq6qyaKwUi20wmgC1b5DnuZALJZdITvKIoWeA6mfZ1qcgUXCIRomTnZDr+ePKSyZEXJk2CqVMTcplmzZJnzWVSBi2bNwMww76NediJdQigyHTssTLamSwyubka6mQKEI7IZML5EZncfmNr3Tj4yU9koOD73++1QsbtyGr4t6IEh2ycTOnK5dJqSDmWyynBIBuRyS1pcW5VUkUmdTIpipIFrpNpd+uweJZrOjT4e6CIRIj0kskUi0lV2nHHFX/3MrJ0aYKTadIkUTLVyaQMWrZsodtU0hWqhp/9TJaNHz+w++RDXR0sWCCTCXhxBW7NZAoQrsiUZydTezvw0Y/Chz8MN98swX/0LjLpQLeiBIempr6Xy6V1MjU3y3OW5XJKMMgm+Nu9rjuney2XUxSlT7hOpl0tw2hvVydTMIlEiHicTH7lcu3tkuE3aVLxdy8jS5dCfT3s3QvIl2jGDBWZlEHM5s1sM3N4Y8b/gj3BzWQCmDgx9X7SPfd4y+XUyTTAuOVyeXYytbUhacF33AHAmD//EtByOUUpJRob+1Yul9HJtHGjPM+d2+/9U4pHLuVyaZ1MeoJXFCUL2luiDKGNg3YY27apyBRMHJHJ62RKFplyLI8vHmedJc+PPRZfNGMGvP32AO2PogwwsY2beSN2OG+fdLksqKuTMLUAUleXNIpJj6AUDstFoaJCnUwDToEymeLuhgkT4LjjGLbhRUCDvxWlVLA2OydTzplMa9fCsGE9GQhKSZAQ/O3OLucT/A1w6JCMMSTcA2i5nKIoWRJpEoW6haEcOqQiUzBJcjL5iUw5TvRRPBYtEnvVX/4SXzR2rHZClEFKNIrZVs8W5tF0ynlywAbUxQSye8mdD2+5HPTMRKMMIIUWmQBOPJERW9dQSVfajmdlpeilen5XlGDQ1ibn7N6cTDmXy61dK8F9gQkBVbIhl0wmgNmzZRApfo0fNUpUJx1ZUhSlN5yy6maGAZkzmdxLiWYyFRsfJ1Nypy6wTqZQCM47T5xMzkVp5EjthCiDlLfewnR3s5nDGTphCFx3HXzgAwO9V2nxczIli0xVVXq/OeDkWWRKKJdzOekkwt0dHMO6jNcZPb8rSnBwj8W8Bn/HYvDaa7BwYV72USkeuWQyARxzjDzH7wNGj5ZnPckritILtlmcTB1hEZkyOZnc4G91MhWbSISIrciYyRToiT4uuEB8t88/D8jNTlOT2LgVZVDhhBxsYZ7c9H/nO/Dznw/sPmXAr/PhCtzuxUKdTAGgSE4mgBN5MeN1ZtQo7X8oSlBwZ3rM5GTKOfh72zYJ6lGRqeTIJZMJekSmeC6T+0XSkjlFUXoh1CpOpiETexeZtFxuoIhEiNjsMpkCVy4HsGwZVFfDww8Dco3q6vK/qVGUsmbLFgA2c3jGm/6gMGRI+TuZjDHnGGM2G2PqjTE3+qyvNsbc76x/yRgz01k+0xjTboxZ6zx+5tnmeGPM6842PzLGmIL+EsVwMk2dSvPwyZzIi+lLaJDzu9uxVRRlYMnGyZRz8PfatfKsIlPJkZDJ5N6Ep8lkqq2FOXPktTqZFEXJFVdkGjFVRabgEonQnZTJlOwcCLSTacgQEZoefhisjd/saEdEGXRs3kznkFHsY2zGm/6gUFcnopL3fFNOmUzGmArgDuBcYAFwhTFmQVKza4BGa+0c4HbgNs+6rdbahc7j057lPwWWA3OdxzmF+h2A+D8lVJWh4D0HfJ1MxrBjyolZiUza/1CUYJCtk6mtLdVdntbJtHatqBRHHpm3/VSKQ4KTqaNDBKakMRD32n7YYZLtDh4nkysy7d9f8H1VFKW0CbeLyDR6RvaZTCoyFZtolO5YZidTYIO/Xc4/H7Zuhc2b4zc7KjIpg47Nmzkwdh5gSsbJBImj3GXmZFoK1Ftrt1lru4D7gIuS2lwE3O28fgBYlsmZZIyZBAy31r5grbXAb4CL87/rHopRLgdsH38ic9hKaP/etNuqyKQowSHbTCaAzs7E5RlFpiOOSHHAKMEnpVzO53/o9jVmz+65B0hxMmm5nKIoGbAWKtsPATB+znAgu0wmDf4uNpEI3TacMZMpsMHfLuedJ89/+YtOc60MXrZsYffIwwEYPnyA9yUL3POJt2SuzDKZpgA7PO8bnGW+bay1EeAgMMZZN8sY86ox5lljzGme9g29fGZ+KUa5HPDmGMll4qWX0m6rIpOiBIdsnEyukJR8vGcsl9NSuZIkJfjbR0V0r+2HHSazhYLHyTTGufSpyKQoSga6umCElZvBSQvkAqTlckHEEZlK2sk0fbpMd/vww1oupwxOWlpg50521s1j+PAe1T7IDAInk58jKXlKgnRtdgHTrbWLgC8C9xpjhmf5mfLBxiw3xqw2xqzeuze9O6hXnJ5DRVVhnUybhx5PhAp48cW0244cKV/1EhYeFaVscAXfESPSt/ETlWMxqaZK0SD27oWdO2HRorzup1IcUpxMGUQmXyfTiBFy86LlcoqiZKClBUYiHf3pR8sFSEWmIBKJ0GUzZzIF3skEUjL3978zxsgIiI52K4OKN98EYFtVaYR+g7+TqZwymRCX0TTP+6nAu+naGGPCwAjggLW201q7H8BauwbYCsxz2k/t5TNxtrvTWrvYWrt43Lhxff8t8iwyhUIyV0Oys+Fgdx2bqo/NKDJpObSiBIemJsnVyZSF4Scqd3TIc8o95WuvybM6mUqSlOBvH5HJFZbmzfNxMhkjJ3kVmRRFyUBrK4yika6aYcyaGyYczmyEUZFpgLBJTia/crlAB3+7nH8+RKOMe2UloJ0QZZCxaRMAb5rDSyL0G/ydTMnlciXuZFoFzDXGzDLGVAGXAyuS2qwArnZefwR4ylprjTHjnOBwjDGHIQHf26y1u4BmY8yJTnbTx4GHCvpb5FlkAul7JDuZ2tpgW91RUF+fdjtXZNJBBEUZeJqaMucxgb+TKc3s9j0zyx17bF72TykuKcHfPiLTiSfCihVw1lk+TiaQkjkVmRRFyUBLi4hM3UNHUVMDTz8Nn/lM+vauyFSMTKb8TJFTJtjuCJGk2eX8MpmqqwNegrN0KYwbx9CnHwauUJFJGVw89hiMGMGGSOmITOXuZLLWRowx1wMrgQrgV9baDcaYW4DV1toVwF3Ab40x9cABRIgCOB24xRgTAaLAp621blDFZ4D/BmqBR51H4SiSyNTeDk21k+Hdd6W3EkodD1Ink6IEh8bG7EUm7/GeVmTatAkmTOjJ5lFKipRMJp/gb2PgggvkdYqTCVRkUhSlV1pbpVwuOlQuQKeemrm9q18Uw8mUF5HJGHMO8EOk8/BLa+2tSeurkZl/jgf2A5dZa98yxswENgKbnaYvutNTG2OOp6fz8AhwgzODUOFwRCZvJlNyp66tLcB5TC6hEJx3HqEHH2R4XYTGRtUSlUFCZyc89BBcfDF7X6li9uyB3qHsyDaTqbm5uPuVT6y1jyDncu+ymz2vO4BLfbb7E/CnNJ+5Gjgqv3uaAafnEK7On8jkTmvupa0NmoZMgXe7pZPhU+KnTiZFCQ5NTZlDv8E/+DutO76hQTI2lZIkJZNp2LCM7WtqRHRKcTLt2JF2G0VRFNfJFBuRXT5ISZXLOWUMdwDnAguAK4wxC5KaXQM0WmvnALcDt3nWbbXWLnQen/Ys/ymwHCmNmAuc09997Q0bSXUyWZv4j2htDXipnMsFF0BTEx+o+7uOdCuDh8cfh4MH4aMfzap8ISiUu5OpbChiuVzzsMny5l3fmCkVmRQlQOTiZMqqXK6hAaZORSlNsslk8mKMuJkSnEyjR6uTSVGUjLhOJptlh6ekRCZgKVBvrd1mre0C7gMuSmpzEXC38/oBYJmToeGLMWYSMNxa+4LjXvoNcHEe9jUzSU4m99lbMlcSTiaAD3wAKiu5rvN2Tl11O/zXfxXnG6UoA8kf/yh3+medldXIclAYBJlM5UGBnEx+5XKtIxyRaedO3+3c+wkVmRRl4MnFyeRXLufrZFKRqWTJZna5ZIYM0UwmRVFyw3UymSw7PKUmMk0BvH7OBmeZbxtrbQQ4CLiF5rOMMa8aY541xpzmad/Qy2fmHRuJEKUiwckEiSJTyTiZhg2Dc87hzOaH+OTrX4TPfhaefz6l2fr1cNVVSaMnihJwXnkFrr02qXyssxMefBAuvphIqIrmZnUyKfnFRvIvMtXW+pfLtY9SJ5OilAp9dTK5rxM0iOZmceSqyFSyZBP8nUyKk2nMGPmCuFMQKoqiJOHOLlcxNjuRyc1kKkbwdz5EJj9HUnJ2Uro2u4Dp1tpFwBeBe40xw7P8TPlgY5YbY1YbY1bv3bs3h932IZKayQSJHbu2thIRmQDuv59rz9zGB49sEBvEww8nrG5ogHPOgd/9TjrtilIq/PnPcNdd8NGPeo7Pv/4VDh2Cj36UgwdlUamJTL1lMqmTaWCJdso/paI6fzl36crlusZMkjdpRKaaGnmoyKQoA0skIrpQbwPJWZfLue5FFZlKlpTg7746mQAOHPBtryiK0nowwjBaqBibXYfHrSMrFSdTAzDN834qkHxXHG9jjAkDI4AD1tpOa+1+AGvtGmArMM9p7726+n0mznZ3WmsXW2sXj/MJR82JpEymki6XA6itpWvKLLa0ToH3vS9BZDp4EM49F/bskfcNDf4foShBpKFBxJfHHoNPf1qy03jgAbnLX7Ys3vEulXI5N/QzU7mcOpkGnmhXcYK/29uhemgljB+ftlwO5PutmXuKMrBkO6jhVy7nG/zt3pCpyFSypJTL+cwul4yvkwm0ZE5RlLRE98tNYNX47Ds8oVDpiEyrgLnGmFnGmCpk2ukVSW1WAFc7rz8CPGWttcaYcU5wOMaYw5CA723W2l1AszHmRCe76ePAQ3nY14yYaNTXyVSS5XIOI0c6nZALLoDNm4lt2sL998OSJbB5s0TYgE5goZQWO3bA4sVw883wq1/B2WdD+3Or4fTToaoq3vEuFSeTMXJeyVQup06mgSdSAJEp2clkrccxO3lyWicTiMikTiZFGVjc603enEwqMpU8uQZ/g4+TafRoeVaRSVGUNLgiUzhLJxOUkMjkZCxdD6wENgJ/sNZuMMbcYoy50Gl2FzDGGFOPlMXd6Cw/HVhnjHkNCQT/tLXW9YV+BvglUI84nB7t7772SjR1djkoYScT0sk+eBBi510AwJ0XPMzll0N1NTzyCFx0EYwYoU4mpbRwM1G//nX4wQ9g7SsxzPatvLhvDkDJiUwg55VM5XLqZBp4XCdTZU3hgr+7u+XiryKTopQG2V5vKivlfN5r8Ld7QzZ5ct72USkucZEp1i21c33NZAIVmRRFSYtpyr10IxQqTiZTXoIlrLWPAI8kLbvZ87oDuNRnuz8Bf0rzmauBo/Kxf9lisshkKjUn06hRMjJ+aNQMhsw/mvmbHuZLX/oSt93WE/41bZqKTErpYK18X889VxxAN9wA/3z2u9Qs6GTl1jmcCCVXLgepTib3vKNOpuBQiHK55ODvhCDgKVNgzZq0244cmVGDUhSlCORyvcl4vLs0NMC4cVmVWCnBxBWZwt1+VjV/hgxRkUlRlNzoi8hUUVEiTqayIlpmmUz0jKw1NcGuJRdwKs9z7omNcYEJxBGi5XJKqdDUJGLMNE8S3LDd9QC8tH8OsVh5OZk0kyk4xArgZEoul0vIaJk8WYLz0vzj1cmkKANPLtebujo49YXvwt13QzSavlxOS+VKGtclUNHtzAyXpZNJg78VRcmF0KHcOzwlUy5XNlhLKOrvZHI7e9aWppMJpCOybsaFhIly5NsJpjOmTlUnk1I6+MZVbN0KwMbu2ezaVR5OJs1kCh6FKpfr6urplCR0OidPlgvP7t2+26rIpCgDTy7Xm/HVB/nwS1+BT3wCFi1i/MZnCYWID24CKjKVAW4HrjLinND7EvxdWysPdTIpipKGcHPfyuVUZComzl/bL5PJHUTudkqrS9XJ9LJdQgNTGP/UfQltpk2TPox2YJVSwFdkqq8nFq5kB9PYulW+7+FwaQnC2WQyWVucOmrFn1h3YZxM0CMupTiZIO0Mc6NGSeaeficUZeDIxcl0dGiDvLj+emht5ar7/heHVe+MTysNqMhUBsQzmbpyK5fr6Eg6n48erSKToihpqWxTJ1PwcXp0XidTcrmc6zIopY6rV2Tauj3Ew8M/Rmjlowkj4+69jGZ7KKWAKzJ5y+WorycybRZRwmzbJt/3kSNJvHEPOOkymdzzkCt+qxg8cES7osQwVFXn74vlXk98RaYpU+RNmpOzO3B16FDedkdRlBxpbJSMi2wGIBdYR2T64hfh8ccJxSLcEvu3ngbt7SIqqMhU0vQlk2noUHlOKZlTkUlRlDTUtDXSbaqyOse4FCv4W0UmF4/IlG52Offmv5ScTN5yua1bYdWCq+Wbde+98TbuvYzmMimlwI4dcoKcONGzcOtWwofPJhSS73ljY2mVykF2TibQXKaBJNYdJUpFYmlLP3HvC9z/fUq5HPQqMmnJnKIMHE1NcixmM6gxr3sD7aE6mDEDDjuMx4+4gcs674ZXX5UGrmtRRaaSJp7JlKOTCXxEJs1kUhQlDdUdTbRWZXkBctDg72Lj42RKJzKVrJNpK4SPPgKWLIHf/CbexnWEaC6TUgo0NIjA5B6nWAv19YTmzmH6dOLlcqUU+g1yXskkMqmTaeAppMjk62QaN07uBtKITO53XEUmRRk4Ghuzv97M7tzAttojZaQE+OPcr9FUMQa+9KWeqVNBRaYSJ14ul2PwN/jMMKdOJkVR0jCks5H26tw6PFouV2wyOJlc54A7ulBKTqZhw0TcfOcd2LcPZs8GPv5xWLsW1q0Deu5lVGRSSoGGhqRSub17obkZZs9/DyGPAAAgAElEQVRm9mzYti23m/6gMGRIarmcMfG+iDqZAkCsO3EG0nzgDlr4OplCIZg0KWMmE6jIpCgDSS6DGjNbN/Bm1ZHx9/siI7lz0tfh6afh8cdVZCoT4uVy7c3yIouOQ1onk4pMiqKkYWh3Ix21uZVuqMhUbByRKUpF2kymUnQyhUJy87NmjbyfPRu44gr55e6+GxAhavhwLZdTSoMdO/xnlmPOHGbP7nEylVq5nJ+TKe7WQp1MQaDoTiaQXCYtl1OUwNLYKPnMvXLgAKM7drEx1CMytbfDo1P/RUpjv/vdHpHJzWNTShK3A1f9zpvyYtasXrfxdTKNHi3lctbmdwcVRSkLhkab6KrL3cmkmUzFJItyuVJ0MoGITG65/2GHISMj554Lf/pTvM20aepkUoKPtT4iU329PM+Zw2GHiWOvoaE0nUxtbT03p5FIzzkI1MkUBGwBRKaMwd8gnU8VmRQlsBw4kOWgxgYJ/V4fSxSZqoZWwf/+3/DEE/CXv8iHldqNppJAXGTavgnGj89KhUzrZIpEdHYHRVFS6O6GkbaR7qG5japrJlOxcSS9COH4yHJyuVwpOplA7lfci9bs2c7C978f3n473nmZOlVFJiX4HDok3+UUJ5MxMHNm/Pvd2lqaTiaQKYwhVWQ6/3zpoySUCipFJRYpcvA3iMik5XKKEliynmjCEZnWRXtEprY251j/1KfEyvL3v2upXBngugSqtm+CI47Iapu0mUyg4d+KoqTQ2gojaSIyXMvlgo3HyVRTI4vSOZlKTWRyHR1jxsCIEc7Ck06S5xdeAOSeRsvllKDjfkcThJb6epg+Haqre0RUSs/J5J5X3PNMd3eiyDRqFCxYANXVxd83RSiEk6nXcrnJk6X+01tLSU+bykpZrShK8bE2h3K5DRvoqBrGlvaeC1h7u3MOGDkSli+XhYNYZDLGnGOM2WyMqTfG3OizvtoYc7+z/iVjzExn+UxjTLsxZq3z+Jlnm+ONMa872/zImBymYeojbgeuatsmmD8/q23SOplAc5kURUmhpdkyikbscA3+DjY+wd/pMplKzcXsdrYPO8yzcNEi6a3+4x+AdNp379a8FyXY+Gai1tfDnDlA4ne81EQm97zinmeSM5mUgccWwMmUHPzd1iY3APGf4Waz+LiZjJHvuTqZFGVgaGkR10pWTqb169k3/ki6uk3c6dLW5hGUb7hB6himTy/U7gYaY0wFcAdwLrAAuMIYsyCp2TVAo7V2DnA7cJtn3VZr7ULn8WnP8p8Cy4G5zuOcQv0OLrEYjGEfFY37sxaZMjqZVGRSFCWJtj0thIlicyzdUJGp2DhKUqgyjDvGUS5OJve753V5UFUFixcnOJmshV27ir9/ipItviLT1q3xL/eIET33ZKVaLueeZ5LL5ZSBpxAiU7KTyXU2xMfaj3RKa155xXf7UaNUZFKUgcKtYsq2XO7AJDmek493QMSllSvhq1/N+36WCEuBemvtNmttF3AfcFFSm4uAu53XDwDLMjmTjDGTgOHW2hestRb4DXBx/nc9kVgMjmCjvOmPk8m1yKnIpChKEh3viY09NEqDv4ONoySZyp5eXbpMplJ1MiWITAAnnyzTznV2xjvtWjKnBJkdO6TzPXmys2DvXkn69ny53Zel7mRKLpdTBp5COpm85XIJAxkLF8qX4/nnfbdXkUlRBg732OtVZNq7F/bu5dA0EZm8GWwJx/uyZTBjRt73s0SYAnjvQhucZb5trLUR4CDgDC0xyxjzqjHmWWPMaZ723sRRv8/MO9EozGeTvMlSZHK/B5rJpChKNnTvkQtQaIwGfwcb18lU1dOrSy6Xa21NKmMoEdybn4RyOZBcpq4ueOWVeMZNb+HfHR2wZ0/ed1FReOed3mfpbWiAiRM9ZWQ//rE8n3tuvI37PS8HJ5OWywWLQjqZvJ3OuLMBRGk86SR47jnf7VVkUpSBI2uRae1aAFpm9DiZrPUEfysAfo6k5LuCdG12AdOttYuALwL3GmOGZ/mZ8sHGLDfGrDbGrN67d28Ou51KLCYik62pybr8MRSS+4AEJ5P7xVInk6IoSXTtFSdT5Xgtlws2PiJTcrlcW5sMKBc+MjC/pHUyecK/XSdTOpHJWvjDH2DePJg1C1asKMiuKoOQWEyqA2bMgNNPh1dfTd+2ocFTKtfYCD/8IXzoQ3D00fE25eJk0nK54GGjIjLlU/wLh+WR1skEcOqpsG4dHDyYsr2KTIoycLjHXsbg71gMvv51GDOG1qNOAOQ47+qSeysVmeI0AN5pPaYC76ZrY4wJAyOAA9baTmvtfgBr7RpgKzDPae8tsPf7TJzt7rTWLrbWLh43bly/fhFXZIrOPlx6dFkydGiSkykclpsZFZkURUkitk8uQJXjNPg72LgiU3XmcrlSy2MCyfieNi2hHy5MnCiK0T/+wfDh4srdvNn/My67TB6jR8tsrJdcAj/5iX9ba+Hxx3v+bsrg5r330sbJ0NkJV14Jt94KF14ImzbB8cfDz37m337zZo8j74c/hEOH4OabE9q8730wc2ZPXnKpkBwArSJTAHGcTBUV+f3YurrE4G9fkcnaeIael1GjdHY5RRkosspkuvNOmWTl+9+ncqxM8dvW5jOTpLIKmGuMmWWMqQIuB5KHNFcAVzuvPwI8Za21xphxTnA4xpjDkIDvbdbaXUCzMeZEJ7vp48BDhf5F4iLT3OxK5VyGDElyMoHcnO/bl7+dUxSlLIjtF5GpZpI6mYKNIzJVZHAytbaWXh4TSPTSO++kGWk7+WTpuFjL4sWwalVqk5074Y9/hM99TiKcnn0WLrhA3j/7bGr7F16As88WDUBRPvlJeP/7/UXHb34T7rsPbrsNHnwQ3nxT8ujdKjgve/bA22/Lepqa4Ac/ELXz2GMT2p11FmzfXnrHanLop2YyBZBIhKjJ/z+ltjZNELDLCSdIEb1PLpMrMvVWaqooSv7ptVxu1y648UY480y46qqEoH/3mFcnk+BkLF0PrAQ2An+w1m4wxtxijLnQaXYXMMYYU4+Uxd3oLD8dWGeMeQ0JBP+0tdYNMvoM8EugHnE4PVro38V0djCL7cTm5SYypTiZACZMkOmfFUVRvDgjjDUTc3MyVVRo8HdxcUWm6vSZTKXqZMrISSfBu+/Ca6+xdCmsX586ivLUU/L8z/8sX8whQ+B3vxMl9MknUz/yscfk+Sc/Kc6XWAkumzbJ9+HQIXjxxdT1TzwBp5wCX/lKz3Tsl14Kb7yROtOhK4AuXYoomAcPpriYShk/J5NmMgWMaBRr8mxjIlFk8r3ODB0Kxx3nKzKNHCnn2ebmvO+Woii90Ngo90Xu9PMpfP3rEmb505+CMQnneRWZUrHWPmKtnWetnW2t/Zaz7GZr7QrndYe19lJr7Rxr7VJr7TZn+Z+stUdaa4+11h5nrX3Y85mrrbVHOZ95vTPLXEEZvvtNQljs4XlwMk2apFM/K4qSgmlqJIZhyOQROW2nTqZi4yhJ4eqeDkS5OJky8pGPyCjJRz/KyQuaiEZTM3GeeALGjoVjjulZNnSoGEj+8Y/Uj/zrX6WT9Pbbmt002PnRj6C6Wk5of/1r4rqODnHGnXJK4vKzzpLnZAHz5Zflc46bfVBcTBdfLDNvlQnJTiYtlwsg0SixUP5FJm+5nK+TCaRk7qWXJMjFg+ug0FwmRSk+jY3iEk+b1blyJZx/vgRaQoKTScvlypeR7+U2s5yLr5Np0iQZDFYURfFQ0dzEQUZQU5ebnKMiU7HxcTKVSyZTRiZMgD/9Cd56i/ffeQUhogklc9aKyHTmmanZhaecIu4UV4QDueFatQo+/3kJctaSucFLUxPcfTf80z+J++jxxxPXr14tx1ayyHTssRJB8MQTictXrYIjj4Shv/qRfHgZuZgg1cmk5XIBJBolNhBOJhCRyVVmPajIpCgDx4EDGUrlduyQ0bbTTosvUifT4GDUbkdkcsTFbPF1Mk2eLM5t9wujKIoChFsaOWhG5jwhmYpMxcap66qoSV8uV5ZOJpBe/k9+QvXTj3HLyNt5+eWeVZs2yQCK6y7xcvLJ8jdZt65n2VNPyRf33HPhs5+VzKbXXiv8r6AEj7vukhvpz31OMrpWreoJSQX4+9/l2Z3k0CUUElHziSd6cmasFSfTGQsPwve/LynhixYV5xcpEhUV4vryOpm0XC5gRKPYAjmZehWZXDU2qWQuLjLtj2kwk6IUmcbGDCKTe6yeemp8kZ/IVHaDlwoj99Wzg6mEhub2z03rZAItmVMUJYHa5r0crBiT83ahkGYyFRdHSar0iEyhkFigyzqTyWX5cjjzTP6l6yeseqlH3nTdJH4ik9vnccUCkJKoYcMkp/baa+Xv9YEPiGjwb/+mfaByZ98++MQnRDi66SYZwF20SESmWKwn3wvkezNvHvjNFHzWWRI47852uH077N9v+ec9t5ali8nFWzal5XLBw8QKIzLV1mZRLjdhghwwaUSm4z9xNPzHf+R93xRFSU+vIpObLeDgVy6nTqbyI9zVSjPDUioAekNFJkVRsmV84ybeqcnNLQkyqK1OpmLiZjLVJPbqwuFEJ1PZikwAy5czvu1tZm9/PD5b6pNPypTxs2alNp8+HaZO7cllslZEpve/XxwYo0bB/feLwPDee/Ctb8ksd0r58uCDUiJXXQ1XXgm/+IUsX7pUxEe3ZM5a+d4kl8q5JOcyNfz0YV7hOBatvFVyxI4/vrC/yABRV6eZTIEmNkDB3y6nnirqrOfuYNQoGEozwxvegEcLPmmSoige3EwmX55/XkZcPCdyLZcbHFR0d9JJdc4i0/DhUhmX4DJQkUlRlGRaWxnf+hbvDFuQ86ZaLldsMohM3kymsiyXc7n4YrpGjOVf+AWrVsmf5Omn/V1MLqec0uNk2roV3npLRCWX88+Xmejuvlvee/OelPJj1SqZ7erpp0VgOvxwWV5ZKW62lStFYNqyBfbvTy8yucLmE08AL77I6d+7kGGmhcgv/xt+//ti/TpFZ8gQzWQKMiYaxVYULvg7FpPYpYwi0/79PRY/5HibxXZ5s2ZNzwVLUZSCk9bJ1NQEr7+ekMcEPYJSW5sGf5czFZFOOqgh18vFwoXQ2SkzPcdxRSYN/1YUxcFukvtAc8QROW+rIlOxccvlahN7dZWVg6RcDsR+8vGruYiHWP/kbr7zHZl6ftmy9JucfLJkW+7YAb/9rSzzikwuxxwjf0sVmcqbVatg8WL/mXbOPlsyUJ9/vkeYPPnk9J911lnifNr1b3fQUjGcTy99lfA1V5e18pJcLqeZTAGjgOVy7e092tGECWkautkunpK5YcNgtnFEpo4O6dgqilJwYrEkkam9Xey3rlXX2oQ8JpCb+/HjYeNGdTKVMxURcTLlGsjr3hN5YygYM0bue9TJpCiKw3tPvQHA+Pepkyn4pBGZ3HK5WExuCMrayQRUXXctlUQ4cPt/86//KvnKF12Uvr3rRLnkErjlFjjvPJgzJ7VddbXEEqjIVL64/dslS/zXf/jDMG2aZHR9//tSYuA6nfz48pfhmEl7Gf3kH/h19GqOPmloYXY8QHhnltFyueBhYlEoYPD3PffIxf+SS9I0nDNHeqjPPRdfFArBEbXbe9p4Z25QFKVgHDokOlJcZLr3Xhkd+cIX4G9/kxP4CSekbPfhD8PDD8OePfK+rAcvBykV0U66qM55uxkzxLjkxlAAcpKfODGjyNTdrXEUilKq7NsnZbK5sOfZN+gmzJEX+XS6e0GDv4tMrCu9yNTdPYhmAZk/n/opp3OD/QF//I83efBBEYjSceyx0jFeswa+9jV46CF/FwuI+LBmTXHUU6X4rF0rwkg6kWnCBHjlFTjjDNiwQUbsMuUVzJ0Lz179K6rp4r/4TMayzXLB62Qqt3I5Y8w5xpjNxph6Y8yNPuurjTH3O+tfMsbMdJZ/wBizxhjzuvN8pmebZ5zPXOs8xhf0d4gVplyutlbERbePOnFiuh0wUn6TFP49r3I77eGhkqL/0kt53z9FUVJpbJTneCbThg3y/MMfwg9+INmBPjeNH/tYj6gM6mQqRyoinXSa3EUmY+TeKEFkApg8OaPIdOedEjPw4os5/0hFUQaQpia5VHziE7ltF1v/BttCczn8qNxLHjT4u8hEOjKXy7nugnJ3MgHMePBHTBgT4SM/Oh2zYX3GtuGwlMn99a8S7J2p/7VkiYz8bdmS5x1WAoHrUksnMgGMHQuPPCJ5Td/4Ri8fGI1SedfP4P3v58WmIzjvvLztamBJdjKVS7mcMaYCuAM4F1gAXGGMSfb4XgM0WmvnALcDtznL9wEXWGuPBq4Gfpu03ZXW2oXOY0/BfgkgFItgK/Kv/NXWSg7H9u3SAc3IqadKw50744tms433amZJwr46mRSlKLgiU9zJtGmThOp86UtyQJ9+uu92J50kmYMbN8r7mprC76tSXCoifXMygYhM27cnaUqTJmUUmV55RZwJV13Vcw+hKErw+dznxIX46qu5bTdy10b2jl+Q8+QCoOVyRae7XUSmqjr/crnBFNBYufhYzLPPypDKGWfAtm0Z219yiZRA9YYrPmjJXHmyapU4MKZMydyuogKuvVbuxTPy6KOSJH/ddYwYka+9DDbJmUxl5GRaCtRba7dZa7uA+4DkQtyLAGeKAB4AlhljjLX2VWutm3i6Aagxpg9DxHkgVMByORCx6eKLe2nsZrx4QjumRbazIzxLSnM2bhQ1X1GUgnLggDzHRabNm2H+fPjud+EvfxF7tw/GyOyrIMd8rrk9SvAJRzrpCvVdZAJ44QXPwl5Eps2bpcnWrRI1oChK8HngAZkca8oUEZrc+//eaHyvk+ld9dj5uecxgYpMRSfuZEojMg0mJxMACxZIpkB3N1x3nQQP9JMjjpDO1OrVedg/JXCsXi1CYl5umCMRuUGfMSNzKFiZUcaZTFOAHZ73Dc4y3zbW2ghwEBiT1ObDwKvW2k7Psl87pXI3GVPY7pqx0cx2zT7ilstcfLEEeWdk4UL5orglc9YysWM7243jZLJWlXxFKQIJTqaODrGfHH64XATPO0+mfkyDV2RSyo9wtIOuPo6FLFokMRUJJXOTJklwS1eX7zabNkmG6he+AD/9KfzP//TpRyuKUiR27oRPf1omS7rtNrl1e/PN7LZd//9toYIYo0/JfWY50EymouOKTNW1iR0IN5NpMDmZ4syZIzVwK1fC/ff3++MqKuC447T/U440N8tNzuLFefrAO+6QFPHvf798asayoIwzmfzEn2TlOmMbY8yRSAndpzzrr3TK6E5zHlf5/nBjlhtjVhtjVu/duzenHfcSKpDI5A5e9FoqB/KlOOmknvDvvXupibbxZrcjMoGWzClKEUjIZKqvl17C/PlZbTt/vlwvB9U95SCiItJJdx9Fpupq+W6kiEwA772X0n7fPti/X75T3/qWZKV+/ONiBFcUJXh0d8NHPypjE7/9LRx9tCzftCm77d99UmqtDzu/b04mzWQqMpFO/3K5wZjJlMB118nV7vOfl3SyfrJkidSddnfnYd+UwLBmjdxfZ8pjypr33oObb4YPfjDDNFvlSblmMiHOpWme91OBd9O1McaEgRHAAef9VODPwMettVvdDay1O53nZuBepCwvBWvtndbaxdbaxePGjevzLxGKRSGcf5HpggvgO9+Bs8/OcoMzzoDXXpOexXaZWe6N9lnYkaNg3jwN/1aUIpDgZHJ7B5mmTE3ixz+G730v//ulDDzhaGefnUwgMzevWSOdUKBHZPIpmXO/evPnS77XAw/I/cOll0o0mKIoweIrXxER+a675LidO1cMsNmKTB2vvEGUELXHzuvTz9dyuSITdZ1MQzSTKYGKCpm2Yu9eyQK59175g/SRJUvkoulOwqKUB9mEfmfNV78qU+/86EeDLqyirk4Or+7usiuXWwXMNcbMMsZUAZcDK5LarECCvQE+AjxlrbXGmJHA/wBftdbGg4iMMWFjzFjndSVwPpB5poJ+YmwUUwAn0/jxkqOR9f972TJRdZ9+Oi4ybYkeJtepE0+EZ5+F3bvzvp+KovRw4ABUVTklb5s3y8J52d/0n3giXHZZYfZNGVjC0U66+5jJBJLL1NUlQhOQtcgEUoRw990SYfCv/9rnXVAUpQA8+qhMPvq5z/Wc/2trYebM7ESmaBSG7niDAyNm9bneWkWmIhPtkuLE6rrM5XKDzskEUiD+wAPSqbnyShli6eO3063m0IH28uLll+UEOXZsPz+opQV+/3tYvjynm/VywT2/tLaWV7mck7F0PbAS2Aj8wVq7wRhzizHmQqfZXcAYY0w98EXgRmf59cAc4CYne2mtMWY8UA2sNMasA9YCO4FfFPL3qLCFcTLlzJIlEt705JNxkektZoqz4v/8H1Hyr7qqOHcRijJIaWwUF5MxSO9g2rRBepOoJBCLEY5198vJ5JbPxDNaJk+W5zQiU00NTJ/es+zii+Hcc+Gxx/q8C4qiFIBHH5XLRLKLdf787ESmd9+Fw6Nv0Dajb6VyUGIikzHmHGPMZmNMvTHmRp/11caY+531LxljZjrLP2CMWWOMed15PtOzzTPOZ3o7FQUj2hkhQgU1tYnOCdfJ9PDDUiftDiYMOi65RDJyvvtdURSefbZPH3PYYTJq75kYSSlxrJUM4lNOycOHrVwp/u5LL83Dh5Uehx0mzw8/XHZOJqy1j1hr51lrZ1trv+Usu9lau8J53WGtvdRaO8dau9Rau81Z/k1r7RBr7ULPY4+1ttVae7y19hhr7ZHW2hustQWNMgxRGCdTzoTDUjLniEwdw8fRylARmY4+WlyAjz8O3/52wmbPPAP/9/8OyB4rSsmwejVcf33vwaiNjU4eE0jvIMs8JqXMccK5++NkcmfpfecdZ8H48dIzTCMyHX44KVOZz58vYxB5mLdHUZQ8sX07zJ4tLlgv8+eLIbY38adhUwvz2EL08P6JTCUR/G2MqQDuAM4FFgBXGGOSf/NrgEZr7RzgdiS8FWAfcIET3Ho18Nuk7a70dir6u6+ZiHRGiBCmpiZxeWUlrF8vwVyf/7znhmIwEgrBZz8LI0bAr3/dp48wRqru3ImRlNJn2zaJUXJnVu8XDz4IY8bkSbEqPc4/H44/XioGoawymcqCkI1iguBkAjjzTBnmfu45OifNAjyxeddeC1dcATfdJHcuZ58NTz7JzTdL9tPBgwO324oSdH73O5l74m9/y9zOdTJhrfQOcshjUsoYJwipK1TTS8P0VFfDxIkekamiQoSmd5OjDNPrm7NmSRXGnoL2nhRFyYVt2+TYTGb+fEkK2bEjdZ2XUbffTJgI5kN9z6wtpeDvpUC9tXabtbYLuA9InnP8IuBu5/UDwDJjjLHWvmqtdc+YG4AaY/rhL+0HsTQiUzgsAwejR/d0/AY1tbVw+eVSPnfoUJ8+4tRTRcn1uVYqJYjrSuu3yNTdLfPunn9+eVl4ciAUEgvtzp3yfpD+GQJJLAYVBEhkWrZMnjdtIjpd7ljcIGKMgZ//XAI5jj4aNmwg8rGreek56fy48TGKoqTilizcc0/mdgcOOCLTrl0yxao6mRTwiEz9685Mn57U4Zw0KcXJ1NEh99PpRCaIV1QrijLAWCvHo1u14MU9hjOWzL36KvMe+yF3spyx553Q5/0opXK5KYD3NNjgLPNt42RzHATGJLX5MPCqtdY7F8KvnVK5m4wpbAJwrCu9yATw9a+LgUcBPvlJkVvvv79Pm7tihJbMlQfPPw8jR8KCvjs3heeek17yxRfnZb9Klfe9Dy50UopUZAoO3d0iMgXmn3LUUeDMlGcOSxKZQDKbvvEN+OMf4b//m/B7O/kk4kDduLHYO6soAeaZZxImNHGPjwce8Mzu5UPcyeSqtupkUiAuMkX6KTJNm+ZxMkFcZGpqkqjUJ5+E+nrpLPp99dyO7LZt/doNRVHyxO7d0n1O52QCEZnee0+iNxOSaaJR+NSnaKkZy63Dv82wYX3fj1ISmfzEn+QK4IxtjDFHIiV0n/Ksv9IpozvNeVzl+8ONWW6MWW2MWb13796cdtxLOpFpwgTpPH/qU/7bDUqWLpU/Sh9L5hYulFm0tGSuPHj+eZkJJTkPIGceekjSKz/wgbzsVynzne/In2J8QZPolFzo6kIsypUBcTKFQlIyB1Qd7iMyebDLzmJt7Yn8e9W3qQt3qcikKC5r1sD73y+KEjL3xDvvwGmnSVnp//xP+k3jmUzJ03spgxtHZOpPJhP0OJnimUqOyLR6NaxdC9dc0zP7nN9Xb+ZMeVYnk6IEA/dY9HMyjRsngxabNsEXviDZgCtXeho89BCsWsUv5/8nI2aO6td+lEwmE+JcmuZ5PxVILoSKtzHGhIERwAHn/VTgz8DHrbVb3Q2stTud52bgXqQsLwVr7Z3W2sXW2sXjnFHdvpBOZPrFL+DFFzUbJQFjxM30wgsSWJUjlZVwwgmJIpO1sGGDTLuq9ePB5PXXpXzA69bet09GfftdKmet5DGdfbbOzoOMSjY0yARhSjDo6hInUygo5XIQL5mrOSKzyLTudcPX2m9iUtc7fHHcb9m00Yo1S1EGO//4hzyvWwfAli3y9rOflUycdCVz0aiIUHEn05AhPWnNyuAmTyLTtGky02z8vD5lCuzZw5uvyXTXb78NX/6yrPKbjLeuTr7D6mRSlGDgHot+TiZjRCx+4AG47z5ZljAg+NZbAPy5+3ymTUvZPCdKKZNpFfD/s3fe0VFVXRTfN53em/QSIPQuRYo0KSIqiIA0RRBFUBEVVBARED6qoIA0pSkionRBupWOCgQB6YhIDTX9fH/sGWaSTCaTZJKZwPmtNevNvHffnfumvrvfOfsEG2NKGmMCAHQGsDJem5WgsTcAdASwWUTEGJMTwBoAQ0XkbvKUMcbPGJPXct8fwKMAkq9mJAOJciwyBQUhVSFp9yy9ejF/cPDgFJWueOghXrjwizsAACAASURBVIm5cYNXCosXZ/ZHr15Ao0YMFVS8h23bKAx268ZKunXq8PfOen6eapFp4UJePm4f387t/iVPHjdEhyluwyoyeU0kEwB07QqMHQvfpo2RKxdFX0csXgz84Nsa0VVr4r1/+2HpykA6y65Ykb7jVRRvY8cOLi1n89aT+kqV6J2/Zo3dJD82Fti4EZg5E5fPMY/ugYBLwOrVjO5OW1cHJaNgTZfzTX0kE2CXMvfgg0BsLCK3/YocOVjf4eJFtkvs2lzJkhrJpCjegvW7aI0yjE/58jyPK18eaNs2nshkqdhy+Fy2u78NKSXDpMtZPJZeBrAeQCiApSJy0Bgz0hhjcRbBXAB5jDHHAAwCMMSy/mUAZQAMs3gv7TfG5AcQCGC9MeYPAPsBnAMwO7VjdUZsIiKTkgh58wLvv89YvpXxNcWkeeghfsAnTQI6dqSnz+zZDGY5exZo3pw2CR9+CLz4YvI9RP79Fxg5MvFJ1/3C3r00krazm0gSEZ4z9+lD7+DvvuOPXYkSrLbz4YcsatWqFeeoAQHMHU4x33wDPPccUxa6dk1FR4qSdnhlJFOWLMBbbwEBAShdmt/L+Ijwqlir1gZ+cz/Fnhp9MckMhpQtBwwYwEvlinK/snMnl3YiU3uflQh5ri66dQxHZCTw7bcAli9nuEiLFsCLLyJT6yYoh8N4+rNWrNQwYYLnjkHxLiwikwlyj8h01/y7YUPA1xd5/tiCkBCm1RcoAFSpkngfpUppJJOieAvHjzO6MHNmx9ut3+VZs4CqVYG//7YLOg8Lg2TLhktXfVMtMr3+Ov/S0hq3OJiKyFoAa+OtG253PxzAUw72GwVgVCLd1nTH2FwmKhox8NW0uOTw0ktUhl57jWlOmTK5vGvdulRSR4wAgoNpYGjNdly1CmjThpoDwAvus2czfH3kyKQN2M+do1XJkSPAF19QBytePGWHmJH5/nugQweWsN25k9EMSX2+jxwBBg7kaxYUBMyZw/XW96hQIZ7nNGjAc+2//qIfU4rF2c2bebn4wQcpVqrKq3gp9GSKgY83RTLZUb58PJNICwcOcJIyYgSAmjVx7LWaGNoN6PROW5Tq8RDNwceOTe/hKornuXKFymzWrHRQjoxEaGgAemVZBp+dO1D9wvcoUuRxbFgdiee29bHlz/n5IbBHbxxGCOSkH6/ENGrk6aNRvAWLW3xqRSZrSszdSKZs2YBatVB27xaUf5ipmjt28Bw5MUqWBL78khNVnd8oimdJrLKclRde4Py1ShVmi0RH868pJARAWBiis+QAbiDV6XKO0mvTAk3GsCBR0YgxfhrtnBz8/YGpU/mtmTQpWbtmzw7UrMlztvXrbQITQHFp1y4Wr/v3X06Q+vQBPv6YEU7XriXe7+nTQOPG9A2aOpVO/vXrp8g6CrduOa8s4wgRCjqzZ/MlGTOG4/j883hVQlzk2rWUmbMtXgy0a8cfkvfeY4GpTp04UU6M/fsp/u3YAUyZwsjM0FBg0SL6ZxUqZGvbsCGjI3x8+HqnmJEj6TOwZg1P9BXFS4kMZ2yxN4tMZ87QuNietZbLP61acRkSwuW+zA2YnzxxIhXkQ4c46VaU+4Vdu7h86in+0R47hsOHgZrC9earJWjTBsC6dfxujB/PaNtOnTC1y2/Yah5G7KIvGOqrKFYskUy+mVMnMuXPz0hx+3PH8LpNUD1qJ6qUZgRq8eI8j06MUqWYNXDmTOJtFEVJH06ccOzHZCVTJls0k/Vc7W4mT1gYIgIZZZHaSKb0QkUmCxIdjVgfLylNnZFo2pRhR1OnJluRWb6clTEcfeEqVaIoUqAABagZM3ix8PffOVm6epV/vFaTToCTqzZtmCL3ww/MBNm+ndsaNkxeNbuzZ/kFL1iQ/Rw86Ly9CLBgAYWzBx8E+vZlOOI77wCvvEKf9JIlWZre6mPkrK/vvweefJJZie3bJ8+jd/Jkeic1bMjIhhEjgGnT+Pq9/LKtXXQ0zbz/+4+ep82bU+fZu5djDgjgxPWZZxxXOXv8ce739tuujy0Ox49zgH36MF9SUbyYyDtUe73Kk8kOa3Uh+99EgCJT9er0UgNspa5DQ8F8i2zZ+OWvWJE5sceOpdeQFcWz7NxJHyVLhYWYA6G4cCQMRW8eZlTtqlVo1/QWOoYvRGSu/IzYtrDtUkUMrLQZvk8nCNJX7ncsIpNPKkUmHx+gSJG4AtGJEg/DH9GoF/tz4jvaYT2/1pQ5RfEsUVH8LjuLZLLHek5nLV6KsDDc8qPIlNpIpvRCRSYr0TGINSoypYhBg6hUWO3wXaRIEdvExxXatWN00549LBtcvDgnTAMH8svbuzcnTsuWUegBgMqVKeoUKMD0Llfso65dA1q35rJVK0YlVanCKh537jjeZ9gwoGdPCjfTpzPM8do1tr90iSLVkCGMEnr44XhlKe04c4bP3bo1RbGOHRnk069f0v7qIrRnGTSI+61bx4gxgOLS0KE8ltmzGeHVpAmPq0AB5v4GBQFbtjhX2eNTsWIqApDmz+cJfo8eKexAUdKPqHCKTL5eLjLdPSEBf4N+/pniu5UsWfjbGRoKKvh79vCHddEifh9d+bFRlHuBnTv5xanD4sVXfglFlWhLTfhBg4Dbt9H8/AK0wyrsKNUF8LOdIx4+7LhsvKLcjWTKlDqRCWDEgn0k0+7ABoiCH8qe2+LS/tYJrZp/K4pnOX2aUYWuzrGyZqWYZB/JFGZywJiMU8hUVRUrGsmUcpo2ZejRRx9RaUnDnMMnngA2bGB2R/HijKSZNo1CzPHjwLhxvChvT/HiFGzatuX+s2ZRkIqNpf/ToUOMgoqI4Dnk1q30Glq3jhXCL11iRNKECTTEHjuWgpe18tfo0bxZjbLjH35QECuFjR7NYnxNmzIKaN06Cj0AU/Nmz2ZqW3Q0A8P69mWufblyzCq7fZsnHDExNI3Llo191axJka1PH+o2L73E/X3jzYU/+IDzyZdfZnTT1auMevL1pUb47LOuK+ypJjaWg23Rgmqjong5VpHJW9PlypThb5K9yLRhA38v7EUmgJPju+1KlrSd9dy4wUoL8+czlU5R7lVEeNWnbVsqr8WKIXxfKOpYT4tfew34/HMEDXsTQCSmXemGhpZdIyJ4vtGli6cGr3g1FpHJL4t7RKYtdnrSnyeyooypjbr7XROZChems4VGMimKZ7F+B5MzzwoJsROZrl3DldhgPPBAxvFXU1XFiopMKccYhhP17cv8tFSZ9CTNww/bTMEBVjbr25e2Cm+84XifvHnpMd2xI8u+7t9P4Wn/fm739aWgEx3NNLH58ykwWff99FPu268fBaKQEEb/HDjAW7duTOlLSl/LlYsTvyZNgEceAWrUAEqXtlk+NGvG5ypd2raPVRCaMYPj9PWl4GTFau69bh3FqHffdTwOX18aodepw4nnL78A1aol9WqnEVu3AqdOqeGwkmG4KzIFeKfIFBjIkxd7kWntWkZ9WiM7rYSEAD/+SK3Xxz6euW9fGroNGkSl/fp1hlU++mi6HIOipBunTrH+uyWKCSEh8P8zFLURjphSZeCbNy/w9NPA5Mm4nL88vj5REydP2jJKY2M1kklJBKvIlDX1hUyKFgX++Yfnpn5+nHCWyfsw6u0Zx4sC2bI53d/XlxdaNZJJUTyL9TuYnGyR8uWBuXMt52phYbgYmCPDpMoBmi5nIyYasb4qMqWYbt0YrjNuXOI5ZWlEjx78E16yxLnIkyUL0+W6daOJ+PXrzBC5fZuRQLdu8dzgxg3HVyit1dQWL2Yk0a+/8oTzww+Bzz5LGDmUGPnyMRLrpZcY5bRpE/DQQ0xr2bgxrsAE8JimTuUYw8M5zpgYRliNGMHUu/XrKU4NG+b8NciTh75Whw97UGAC+ILlyEHFTlEyANHh0QAA3wDv/Z8oX56/UQBPStato5gd/7cpJIS/ewnMYH18cO69Wbh8MwCxEyby7KZXL/7oKEpGJzYW24Ofw9/lWgNvvsl1VgU2JAS5LhxGfZ8d8H2wNtdZTgRiu3YHYLBuHVdbv2MqMikOsYhM/lndE8kUE8NiNgDP3S6EPMyVLhqNliypkUyK4mmOH2cEUnJS3UJCePp17qwAYWE4fztnhjH9BlRkuouJiYaoyJRyMmUCXn2Vs5pChYD+/ROWOUpDcueOd0U+Efz9GaX088+8IvTMMxy6qxl+fn4sLrN7N32XVq2i15JfMj86BQsyVW3LFp48rFjBKniu4uNDwei996iO79/PIARXyJqV4pbH2LyZNXW7d/fwQBTFde56MnlpJBPASe+RI5x/7NzJNNjWrRO2q1CBy9dfB/bts62PjASefCcEeaPOo13LSCrgly8D8+alzwEoShpycuinaHTsM+DIEciyZTxxqFwZt24BW/8LQWDMHRSKPWeLbqpdG9iwAXnHDEKpUvyfBmzRgulVBlrJWEi4+0Qma9TCmTO8yHj8OIB69Xjl4GfXzb81kklRPMuJE4wqdDUgAbBVmPvr93AgKgpnbmgkU4ZERSY38M47VE0ee4zu11OmeHpEDvHxoaATEODpkbiHvHlpcJ4hOHmSZQPLlQPGjPH0aBTFZaIiMobIFB5Og8m5cxlx+dhjCdvVr0+B6fvvmbLbrBkjM995h+JUo0aM2jhWoAHDLCdMSF6JS0XxNk6cQMFJb2CTbwt0rHoMxXOE4a+Vf2H0//xRpAgw7IsQW1uryAQALVrAZApCly6sWnvmDEWmokVTUfRCuaeJuU2RKSCbeyKZAP6mHz3KCNUyVbMwFD2pUsUWSpVi5Pv166kejqIoKeT48eT73lpFphP7wwAAl6NzaCRTRkRFJjdgDM2GFizgrMWaSKooANMon3iC5gLffZekl4CieBMxGURkAoBduxgs2Lkzs1Lj4+ND3ejcOS4PHKDwNGECfb+//JJX22bMAEM1T5/mSkXJaISHA3/+ichuzyIi2hdbu83F0q8NrkZnQ/mH8uLdd4GGDYGJayxn876+DnPJe/emV/i8eVpZTnFO1E2LyJQ19VcyrVELp0/bDIBDQsAf7B07eD6VBMHBXB45kurhKIqSAkSYZm39LrpKvnwMuD17kCJTGFRkypD4xEYDKjK5jz59GLWycaOnR6J4CwMHMq9v8eLk/9IqioeJzgAiU7lyXI4cyTz+pFJoc+RgRNPx4/SW694dmDgReOABFjqYNw+41bgNQyXffZfVQ599Fjh4MO0PRlFSw/nz9FTKkgWoUgUBv2zDq5iCZ4YURXAwi2B06gT89hu9Guu0yWsLC86cOUF3JUsCLVsCc+aoyKQ4J/pWBCLhj8xZUz/Fyp6dv9OTJ/On18/PkqZZvz6N9f74I8k+rOnRd6tUKYqSrvzzD/1+rd9FVzGGxdvXL7WJTBkpXU5VFQs+MdHJN9ZREufxx2kaNHs2z8yU+5svvuDZ+dChLBmtKBmMjCAy5c3Ln92DB4EqVeJm/TgjSxYGLNnz8sssprBoscELY8awUsH27cy72LSJeXUFC7r/IBQlJdy8yfONmzc5+Z4xg1FMr7yC6Bp10HJwFfhVqXBXHGrXjrc4vPYaPSUToW9fVnMFbIKuosQn+lYEgEBHWmWKaNWK1+c6duTnL3Nm2Ew8f/mFOc9OKF2afqSHDrlnPIqiJA/rdy+5IhPAqdMv74cBi4FshXNkKC9AVVUs+MRGA37eO3nIcAQGsuzbxx/TfTZ/fk+PSElPIiP5/t+5A1SvzvCIBg0YYqEoGRCryOQX6N3/E+XL0w+2Tx/XCxo4on59+h6/+ioQMP1RPHv6UW7Yt48+TU88QQ8+Ne9XPE10NMOSrOXfAKbsz5iB81mD0aULsO0CsPa1JPp5+22nm9u1AwoUAC5c0EimtMIY0wrARwB8AcwRkbHxtgcCWACgJoDLAJ4WkZPGmBYAxgIIABAJ4A0R2WzZZyuAQgCspY9bish/aXUMMbfDEeNGkWnJEgcrixZlmapffuEVASf4+zN4XEUmRfEM1u9eSIjzdo4IDgaCH6fI9OXaHEAGchrRdDkLPhINo5FM7qVPH5rFzp/v6ZEo6c3QocBXX/GX9f33KTp++aVGCyoZlruRTF4uMlWuzIqZ3bqlrh9jWD2zQQPguef4cy4CisYLFjDPKDiYt9q1+VhR0hsRYMAACkyffsrSilFRwMaN2B0WjOrV6VG2YIHjSovJwd+f3wVjUnZFWnGOMcYXwCcAWgOoAKCLMSb+K90bwFURKQNgMoBxlvWXALQTkcoAegJYGG+/Z0SkmuWWZgITAMTejkCEG0UmhxjDKwEumn9XqKDpcoriKUJD6a2U4niLMKbLOTTZ9GJUZLLASCadALuVkBDg4YeB8eOBa9c8PRolvVi9Gpg0iVfXjh4Frl6l411GSiRWlHjcjWTy4nQ5gMGCv/4K5MyZ+r4KFADWr6ed2pw5DGICwJyNzz8H6talwHTxItCoETBtGnD5Mh+7YEirKCkiIoKfwQoVmBc6cybw1lvMZ/PxuXsu9/771KB27qTfmDsYPpxZo06y6pSUUwfAMRE5LiKRAJYAaB+vTXsA1iuXywA0M8YYEdknIv9Y1h8EEGSJekp3Yu+kg8gEUGQ6dYoVHJKgQgXg77+ZQaooSvpy6BC/gymOLleRKWPjIzEw/ioyuZ2JE+nhMWKEp0eipAfnztEcuHp1iosAZ7t58nh2XIqSSmIiM0YkU758QNWq7uvP15ee38bQIPkuPXsCX39Nv7V9+2gcMnAgjaHy58fV0jW1ZrbiVv77D3jnHSBqxGhg+XKgTBmgeHGuHDMmTtvbt1l3pHNnoGJF940hKIjZokqaUBjAGbvHZy3rHLYRkWgAYQDin2B0ALBPRCLs1n1mjNlvjBlmjOOpnjGmrzFmtzFm98WLF1N8ELHhFJkyZUpxF67RoAGXv/6aZNOQEBZ71gpzipL+HDqUslS5u1hFpgxWlVtFJvCCqx+iVWRKC6pXB154gd5MBw54ejRKWiLCnJrwcJoIqFeLcg9xN13O37tFprQgXz6gXj2mzzkkVy7gu++Ar75C1ISPMDrrGGQ7fRB3nurOmY2iuIGRI4HVY36Hz/8spRBXrmTk7KhRjGCyY+NG/hUlMPdWvBlH4o8kp40xpiKYQveC3fZnLGl0DS03h3FtIjJLRGqJSK18+fIla+Bx+glPp0imatWYG+1CypxWmFMUz3DxIgO8U5ViHRZGgck3Y51/qsgEnoioyJSGjBrFEL8BAyymHso9yfz59MUYOxYZqvyBorhAbKQl/es+Tat+7DFg717g7NlEGvj4AJ06YUHOgXj35lAMwiRk2rCSZsp79tCzKSrKPYPZsoXeO8r9wZUruLpoDU7OWo+56I2ryI3o8ZOd7rJyJcu/N2qUTmNU3MFZAPZ59UUA/JNYG2OMH4AcAK5YHhcB8C2AHiLyt3UHETlnWd4A8AWYlpd2hEcgHEFpLzL5+7Oy3K5dSTYtW5Y/0Wr+rSjpS2oqy90lLCzDpcoBKjIBsBOZAu7PyUOakycPzRG2bqWZgXLvce4cy1A1bAj07+/p0SiK27Gmy2W0K0nu4rHHuFy9OvE2sbHAhAkMYA3rPgCLfHsC48YBtWoB9erhfLVW9NNJDb/8AjRtysgp5Z4lPJx2Xx+8fg3y4IPI1f1RrI5qhVrYg36xn+CbrYmnYMfG8nPaujUQEJCOg1ZSyy4AwcaYksaYAACdAayM12YlaOwNAB0BbBYRMcbkBLAGwFAR+dna2BjjZ4zJa7nvD+BRAGkbVh+ZTpFMAHNBDx1K8gJuUBBQurSKTIqS3rhFZLp2TUWmjIpVZPJRkSnt6N2bXh0TJnh6JEpa8M47QGQkMG9egrQFRbkXuN9FpvLlOUlZGX/KZ8eaNcDhw8AbbwBD3zZ4LmY2xjVcje7ZV+B1TEChQ5uZ5uQkCslSGAwjR1JHSlAz4scfufz999QflJIuxMYCf/wBTJkCTJ8OnDmT9D5r1wJ7dsei2qTuiPn7JLoHLsWIFj8j9sAh/Fm2I8aPT3xevWsXcOGCpsplNCweSy8DWA8gFMBSETlojBlpjLHI3JgLII8x5hiAQQCGWNa/DKAMgGEW76X9xpj8AAIBrDfG/AFgP4BzAGan5XGY9BSZKlQArlxhTk4ShIRoupyipDehocx0KxzfXS45hIW5p5pLOqOqCigyZUc0fDRdLu3IlInVxkaMsNnsK/cGd+4A33wDdO1KI1ZFuQe530UmYxjNNH06cPMmkDUr158+TY//oCDg+++BYsWAjh2ZyfH4U/4Y8nVbFCkCdHgWGPyRYMLXb9DBuUABhAflRNCEUTR9AvDVV0C/fnGFJR8foEsX4KOPLPUDrP4j6vGXIfj2W/rB26dZ9u/PYLRVq3B3In7nDsWoLFHXgFWrcHYqMDvTDrS7sxr95WMsingKByYDPhWB11+n1ePLL/PrWLky7QCtrFrF9a1bp++xKqlHRNYCWBtv3XC7++EAnnKw3ygAoxLptqY7x5gUFJnypr3xN2A7lz50KMn66BUq0NEgKoq/z4qipD1W0+8UV5YDKDIVLOi2MaUXGnIAu0imQBWZ0pT+/TkTmTTJ0yNR3Mn69Zx1durk6ZEoSppxv4tMAEWmiAiKSVbGjAE++cRW2+Gdd2wTmDFjgG7dGHz09tvAFN/B+KHRB8A//+Da9t9hFnyOa806AJGRiIkB3nwTKFoUWLECuHoV2LaNWbhLlzIrZPUqsVVSOngw/V8AxWXCwnjd4cknqSF+9hkFycOHWa1w82bWhwAYkdSmteDt4K8hISFAjx4YuLsHet/5BNKrF8p/9BJGj7ZVieveHShZEpg5E5g9G3jxRZuIJcIIuIYNgdy5PXPsyv2NT1QEokxg+tj32YtMLjSNigL+/jvJpoqiuAm3xFWoJ1PGxSoy+Wq6XNqSNy/w7LPAwoV0kFUT8HuDpUsZYvDww54eiaKkGbFRKjI99BAjlSZP5s/39evAokVAr16MRImOBvr2tbUvU4Y/9yVK8CJ7ixZAn1Pv4uLPR1A68jB6Yj5y/vkj8NJLWLNacPo08N57FLNy5qRp88SJTH8qWBAY/MTfTAspUgQ4doxPmggiwF9/AZ9+CowezeouSvJZu5YByNZrCa4yYAD/Gt5/H9ixg5+RokWBcuWYClmxIrDrw42QGjUQmb8wlmwriI/Od8J58wDWvLUdZXAUf3z9F8y8eRgw0ODtt219Z8rEtz86mlpjbCzFJoCplgcPUuBSFE/gExWBaL/A9HmyBx6gw70LIpO1hLr6MilK+nDtGnD+vO27l2JUZMq43BWZNJIp7Xn9dV7mrlkTCA4G5szx9IiU1HDnDk1annxS46+VexqNZGJhvTfeYMba9u0UkG7dAl56iduTemm6dgVOnQIefZQCVZbnOuMDvAvMnYsHO5fEeZ/CeGJ6c4Yx2VG1KkWOxv6WVLnnnqOycPhwgue4do3eP2XL0keqXz9GzgQHM9UvvYrSHT3K6K1Zs1JfVO/wYf51fvEFre/Sg6NHgbZteXv/faBVK0YGdenC99/ZNaLdu/nZGDwYGD7c7q/hwAFg+nSYz+bh6+y9MeNYC4RfuoktmdpgfWB7zKr6CYIv78DQtQ2B0mVQuUPZRHMMfHy4qVQppsXNns3XefRoel/06OH+10RRXME3OgKx6SUyGcMwCRdFJl9ffj8VRUl79uzhslKlVHQioiJTRib8dix8IPANuH8nD+lG6dI8e505kzH0ffs6L1ekeDfr1nGWqalyyj2ORjKR3r2BAgU4mZ8+nYXjatVybd/HH2cUys6dTIObPBmYlP19zCoyEj+EP4QLVVrA56cfgTZtEoTNFCgAvFD1V4QhO3YUt/zexEuZ27OHUVOvvcb2M2cCR44Af/4JVKvGjO3Ro52P8c8/gfnzKaQkMB13gXPngPbtGbEzdiy9gypWpDdRcvnvP3oNVazI1+qZZ5gmlpJrM7GxfC1WrUparLp8GahXj2mOEydS89uwgWlp69YBDRoAPXs6FppE+Prny4c40Uc4dYqhcP37A717o/yOzzEp4C20LvQ7Wp+ZjdPvzsKja1+Cb6Af/vyTx+qqh8WLL/Jq8ZtvMsVy8GAgMJ3m+IoSH9/oCMT4p+MH0EWRKUsWoH79uOnOiqKkHd9/zwqnjRqlopPwcF5ByYAiE0TknrnVrFlTUsLq5REigJzpNypF+ysp5PZtkRo1RLJnFzl82NOjUVLC00+L5M0rEhXl6ZEoSQBgt3jB77Snbyn9n3i32ioRQGTnzhTtfy8xbhxfCkBk3rzk7duzp0jx4iI3bvDx0KHsx89P5J9/RGT5chFfX5FGjUQmTRKZOFHkyBEREYmpXEW2BbWUGpUjJdbfX+Stt+72e/asyAMPsO89exI+b2ysyFNPiQQFiZw86XhsX38tEhhoO7aAAJFly1w/ttBQkWLFRLJmFRk2jMezapVIpUrs74UXRO7cca2vn37i8fj7i7zyisi//4qsXi3SsCH7GjKEx+QK16+LNGliOy5ApFkzkWvXHLd/6SW+Bb//nnDbjRt8bkBkxIiE25cs4baZM+1WRkeLNG4ski2byL59IqdOifz3n/Tvz7ZZsohcvsymU6fys2B5y10iOprvO8C/o5s3Xd9XiYv+T6Tuf0JE5JZfNlmY55UU759sxo/nh9/6JXLCmDFsev584m127xZZs8aN41OUe5CDB3nO4IyKFflfmyrOn+eX9pNPUtmR+3D1f8LjP+TuvKX0T2H5olsigJx/dWyK9ldSwalTIvnyiZQvLxIZ6enRKMnhp584Y3vhBU+PRHEBnTyk7n/i7Yrf8S/TkYJxnxEWJpIzJ2+3biVv34gIih5W/v2XPyOdO9s1WriQCo9VEcmbV+S330SMkT87jhBA5E9TWX7N21aGDaOwUaMGxZ0//kj8uU+dEsmUiWJTfKZMETFGpF496iCrV4vUr0+x5YsvnB9TbKzIunUiuXOL5M+f8CMS/JC0lQAAIABJREFUFUU9DBCpVo0izfjx1M9GjhQZPlxk7FiOYeRIkT59+Lxlyojs3x+3r+ho/uQCIt27i1y54nxs167xmHx9+Zy//ioyezaFnMqVKc7Zs3+/iI+PyIABzo+3Z0+OYcIEkWnTRJ5/XiQ4mOsqVRKJiogROX6cit7Ikdzw+edx+jl4kK/5oEFx+790yfkxOcI6eR6l1wpThf5PpO5/QkQkwgTIgkJvpnj/ZLNmDT/8P/2UZNN9+xx+Fe8SG8vvb548rovYinI/0qkT/1fPnXO8/fRp239kqjh8mB0tWpTKjtyHq/8TakIEIOI20yD8gvTlSHesLrLdujFRvF49T49IcYUff6QRRrFidOpVlHscTZezkT07sHgxjZetJehdJSCANysFCtDYu3Bhu0bdutHnLSqKZcOaNQOaNAFEUKlvfazrDUQMrIgiJ3/D6NFMBfPxoT1c5cqJP3exYsCQIfzJ2rLFVqtgyxam7z3xBI8rUyam1zVuTE+ibt2AqVM57hw5mJJXrBjrHQQG0ndp2zb6Pq1bx6xwe/z8mDrXoAGfZ+JE5z5N2bMzA3nGjIQR8r6+XP/AAzyOlSuZnla2rM1k9MQJ4J9/WAnw9Gng33+Br7/m8QFA3bo8hiee4LFt3szUNBFg4EAgVy76MCWGMTRUP36cqWmAoEjOW2hQJwqvPROFrkHL4VdpElPjrXTsmMAoqUIFpk7G96vIkyfx506M/v2ZAvjKK8nfV1HchggCJBISkM7pcgBT5ho0cNq0alUWUVi3jimv8dm1i9ZpAH87ihd381gV5R5hzx56PM6fDwwdmnC7NS21VatUPlFYGJcZMF1OVRUAkbejAajI5DFatuRy2zYVmTIC27fTM6VIEc7OChXy9IiUDIAxphWAjwD4ApgjImPjbQ8EsABATQCXATwtIict24YC6A0gBsBAEVnvSp/uRKL4P5E+dam9nzZt3NeXQ1NMq3qVIwfwww9UfCIigAcfRKvsAHpWAt5dglv/3cDhc9kQFESj76R44w1g3jx6S/36K6vYvfgiDaStApOVrFk5GRs8mHpJVBRw8iSwdStw44atXYECFKH69AGCguyeTOKaFrV7lDeJFdy4wc2ZM/Mjdee24M4dIFu2uCIcohMaHxkAw4cKHm/LSm2jRti2+RpBkSIUobJkAqpVAPp/AjzSUoAIW7vmDYEpYwX9Xwa+mEcPpCmTgR3bgWkfCXIFAYhfvC8sDPjuO+CbbxB4/jy2Gh+EF4tE0OVz8Ll2E9gA3gAadU2fThXOzw/o0MGhyZKrfl5JkT27Xu9QvACr2Vl6moIVK8YfEhd8mYzhpHfFCl4kiP93Zq3SCLAItIpMipKQa9eAv//m/TlzgLfe4oUue9atY0VVqwacYlRkytioyORh8uVj2Ytt23iZWfEurlzhj5uvL2dXbdvypGbLFl4SU5QkMMb4AvgEQAsAZwHsMsasFBH7s+LeAK6KSBljTGcA4wA8bYypAKAzgIoAHgCw0RhT1rJPUn26DYnWSCaPUbkyf2/276eaANxVpoKOH0K1Bx/kusuXuf7SpYR9WASfTABOwFJlriDgYwQHxVKtLGvctgCQGcB0JFwfhwsABlpuLmAAZI+3LrPllhyqAFgWf6UAOGO5WVnveP/elhue5+018IZXLLfEKFcOqFQJPjExyOzvDzzQihcbgoI4i61enSbfrjp3K8q9QgSVXImjNqcxPj48h3ZBZAIYhP7554xaypKFRQlefZXdfPklK0guXcpIDWv0o6IoNvbt47JHD2DBAk6NHn7YFmXcti2wcSO/S6n+G1SRKWMTdcciMmXSl8NjNG5sy7/QSAHPs2kT8yF++w04c4b5C02bshJgyZLMryhQwNOjVDIOdQAcE5HjAGCMWQKgPQD7s+L2AEZY7i8D8LExxljWLxGRCAAnjDHHLP3BhT7dhqbLeZiqVXmzUrEilwcPAlaRaccO5oY995xjAdxytmcAnDgGfL0UiBUgJMSgQwfHbZNcl5y2qd0/DZ7r/Hlg6jSDWGE0V+/n7P6C4/fh78/UxcqVVUBSFEdYRCafoHQub1ihAoV4F2jRgoJSv378+YyJAb75hhnKt24BAwZwvbX8uqIocbF+N0aNYsXWTz4B5s5l5VYAqFmT0c6tW7vhyVRkythYRSZ/jWTyHI0bs970/v3ui59Xks/+/cwn2biRk7QmTWhOcugQsH49UKUKTUDy5/f0SJWMRWHEja04C+DBxNqISLQxJgxAHsv63+Lta3XwSapPt6GRTF5GyZLMbbMaiAD8/QKASZOSPCELBlC0LTBlCvDcKjBG7j6kEIDsBfmT//ZywC/jnccqivdgEZmMJ0SmhQuB69dt0Z6JkCsXUL8+8NNPTBtu3Zq6/Pvvs5u6dYEaNYA1axi8qXqyosRlzx6mwhUtSl/DadO4fswYprsPH85l06bJ7HjfPmDZMqpX1i+eikwZG6vI5BuoL4fHaNyYy23bVGTyBDExwPjxwLBhNCmZMoWXuex9BfRsQ0k5jj448XOPEmuT2HqfRNYnfHJj+gLoCwDFihVLfJROqF4lBtgMFZm8BV9fRtTYX27//Xc6Wrt4Mta9O2/3O0OHOjYuVRQlmVgjmTJ7QGQCgNBQW2SnExYuBK5eZWYrwAzY7t2BN9/kaV7NmkypO3eO9puKotjYu5ffEQB4+WUmfwwbBnTuzHWdOgEXLiSp98blxAngkUeAixdp8FiiBNdfu8ZltmzuGn66oaoKbCKTpml5kEKFWJpn+3bg9dcTbhdheZyzZ9N/bPcaIjwRun6d8Zw3bvDEZPduVgD69FMgd+6E+6nApKScswCK2j0uAuCfRNqcNcb4AcgB4EoS+ybVJwBARGYBmAUAtWrVSsRYxznPdFaRyeuoV4+l3aKimMr1++9xU+oURVHSEQmPgAHgk8lDItOhQy6JTCVK2OawAK3srD4zgG0CvWePikyKYs/168CRI7YLVGXLMr3UHmuUk8tcvcpqKpcv8/GRI7YvaFgY1aoMeO6pqgpUZPIaGjUCli+31aO2EhEBPP88sGiR58Z2L+LvT2U8WzbGT8+dCzz7rIpJSlqwC0CwMaYkgHOgkXfXeG1WAugJ4FcAHQFsFhExxqwE8IUxZhKY1BQMYCcY4ZRUn+4jRtPlvI769YGPPqK4VKECS8BZLyUqiqKkM1E3IxAAwC+9I5lKlmTkuYvm30lRtSpPw/fsAdq3d0uXinJPYBVja9RwU4d//gn06sVydYsX0y386FFb5fWwsAyZKgeoyAQAiA5XkckraNyYQseff9quRoeFAe3aAT/+yBzVAQM8O8Z7hcDA9C2xq9zXWDyWXgbrXPkCmCciB40xIwHsFpGVAOYCWGgx9r4CikawtFsKGnpHA+gvIjEA4KjPNDsIFZm8j/r1ufzlF16ciI3VSCZFUTxGeJhFZMqSzudXvr5A+fJuE5kyZ2bBur173dKdotwzWL8T1mi/FCNCI7TRo2lTsmwZ57t9+jCSyYqKTBkbFZm8BKsv0w8/2CYK06ZRYPryS71CrSgZGBFZC2BtvHXD7e6HA3gqkX1HAxjtSp9phopM3keRIoxJ/+UXmoADKjIpiuIxIm/QkyndRSaA0Zy//uq27mrWBDZscFt3inJPsGcPULiwGwpsr1hBkalrV0Zk583L9cHBjGSycvkyRagMiCPj1GRjjGlljPnLGHPMGDPEwfZAY8xXlu07jDEl7LYNtaz/yxjziKt9upO7IpNOHjxLsWJA7dq2GpAivN+woQpMiqJ4FhWZvJP69YGff2bKXLZscY1GFEVR0pHI6+EAAP+sHhKZTp4Ebt1yS3c1awL//ku7TkVRgCtXeLrhliimDz8ESpUC5s+3CUwARSZrJFNsLPDHHzRNy4CkWmQyxvgC+ARAawAVAHQxxlSI16w3gKsiUgbAZADjLPtWAFMiKgJoBWC6McbXxT7dRkyERjJ5Dd26MeH14EF+sUJDmZ+qKIriSVRk8k7q12dBiLVrgSpV4vr5KYqipCMej2QCgMOH3dJdu3ZAnjz8iR0+/G7hvDQnJgYYNAh49VXWdFCU+MyfDzzxBHDpUvo959KlTCE9c4ZT1VSxZQuwcyfLOcbXHsqWpVgcGUmfprCwDFt13R1nY3UAHBOR4yISCWAJgPg2ce0BzLfcXwagmTHGWNYvEZEIETkB4JilP1f6dBvREZbJg4pMnqdzZ07iFi5kipyfH/CUwwwaRVGU9CNaL0Z4JQ0acHnihKbKKYriUawiU0A2D4pMbvJlKlmS13k7dQI++AB4+223dJuAq1eZOXTnDgM3nn8emDyZGURt27Kal6IAtgCgXr2A774DmjVjNllsLLBtm9v01QSsXQs8/TQTbnbvdsO09MMPgYIFgZ49E24LDqbSeuKELYywdu1UPqFncMfZcmEAZ+wenwUQv37m3TYWA9gwAHks63+Lt29hy/2k+gQAGGP6AugLAMWKFUvRAWTPrJMHryF/fqBVKzrs+/oCLVrEDSNUFEXxBBrJ5J1UqUKX2tu3VWRSFMWjRN2kyBSY3QMiU+nSrBrsJpEJAPLlY2HnyEhGj3z4IRAQ4LbuERUFPP44sH07bWcqV6YN63vvAcWLA337svD0mjX0wVHuX2JigNdeo1Vv1668dehAR5WoKODYMU4X9+6lVaM7mT2bHky//MKvWKrYvRvYuBEYNw4ICkq4vWxZLo8eZdugIJuAnMFwRySTo3rn4mKb5K5PuFJklojUEpFa+fLlczrQxBg5XEUmr6JbN6Y/nDqlqXKKongHKjJ5J/7+QJ06vF+tmmfHoijKfU30LYvIlMPB5DGt8ffnBNWNIpOVXr0YMbJmjXv7HTKEAtOIEby+vGMHU/Peew949lk+399/A/Xq0UVDuT+5cYNi5LRpFJoWLmSU24oVnCoWLMhtERFAx47uTe28dImfw27d3CAwAVRrM2cG+vVzvD04mMsjRygyVa+eYfUJd4z6LAB7zbAIgH8SaXPWGOMHIAdYotrZvkn16T40DcK7aN+eBq7WSxyKoiiexioyqeeP99GkCWcnGdQcU1GUe4OY2x6MZAIY8bBvn9u7bdmSE3mrFw5AU/CCBV3b//RpYMIEWq0ePUotLDiYESIvv0xRCeB0zH4q1rIlI5vatGFm9ObNQI0a7j02xbu5cYPRSgcOANOnAy++aNv2yCO0LLJ+ZgoXBp58khFOAQHArl2MQKpShZlp1iLmSXHxIpA7N68pfvklp6OOMttSxMaNDM/Lnt3x9jx5+OSHDzMs69ln3fTE6Y87zpZ3AQg2xpQ0xgSARt4r47VZCcD69nQEsFlExLK+s6X6XEkAwQB2utin+1CRybvIlImXNd59l2KToiiKp4mJocBkHAXaKh7lzTdZXS5zZk+PRFGU+xiryJQppwdFpuPHaXDkRvz8GMmxZg0n4CNGAIUKAT16UARwhgj3nTWLaXfNmtFnae5cCkcTJ8Z9nvhUqwb8+itFA6sYpdw/zJnDv/dvv40rMFmx/8w88QSj49as4T4NG1Kv+e474LHH+Nl1hgj9wAoXZmTd7dvA559T2Kxc2Q0Hc+4cxaNmzZy3Cw6mEdTNmxnW9BtwQySTxWPpZQDrAfgCmCciB40xIwHsFpGVAOYCWGiMOQZGMHW27HvQGLMUwCEA0QD6i0gMADjqM7VjTRQVmbyPQYM8PQJFURQbMTGaKuetZMpkCzFXFEXxEF4RyRQby1QbRx514eFAYGCKLpb07MlopFatGGBRrx7tU3/9FViyJPGy7t99x2ikmTOBF16wrb95k8KRKx5PxYsDL70EvP8+I6H05/7+ICYGmDqVYlG7dq7tM2YMrzvlymVbFxpKkWjkSKbVOeK//xg0tHYtP9ubNvF59+6lCb1b2LyZy+bNnbcrW5bR2UCGFpncEvcvImtFpKyIlBaR0ZZ1wy0CE0QkXESeEpEyIlJHRI7b7Tvasl85EVnnrM80Q0UmRVEUxRkqMimKoihOiL1DkSlzLg+KTACwfn3CbbduMURj8eIUdV2pEiM69u4F+vQBfvqJc+bwcE7KJ06kvmVPZCQn/BUqAL17x92WNWvyTMT79WP7qVOTbnvpErBypS3LXfEufv6Zwk9SrFwJnDwJvPqq630bE1dgAoCQEJrIz5wJ/PVXwn02bqQmu2kT8PHHHN+CBcD+/ZQG3GYPvHEj3cmrVHHezqqiZskClCvnpidPf9RcAlCRSVEURXGOikyKoiiKE2LvRCAKfsiUxUPTq5AQpuK89VbC8ItDh4ArV1LloP3xx0wnmjmT2eONG3Mi3qYNMHgwbVQjI23tP/2UVb/Gj0/9FKtgQU72P/sMuHbNcZtTpxgtVbQo7V379WMKVFpz+zYr782ezcrzGYUrV4BvvqHZ+t9/p89zLl7MCKEKFYDWrRnllhhTpgAlSvC9TC0jRjDo+a234q7/3//o/ZUrF7BzJ9C/P4Wqbt2AVauYrpfCumJxEaGK1bRp0t6e1gpzNWpk6PNOFZkAFZkURVEU56jIpCiKojhBwiMQgUD3VKFKCb6+wOrVdD9+9dW4hkdWcem//1Lcfb167NZ+jpwnD/1yJk/mpNwq7GzaRH+c5s0pJriDV15hQNacOQm3XbhA8WLBAqB7d2DAALYbNMh1oUkE+PNPHsesWcAPP9D02Rk3b1Jke/ttRsuUKsXjvXUr+ceXXojQ3ypvXlZj++AD4KGHko4uEqHX0aefAsuWMVXSXlRMiu++sxlwjxxJj/qmTYEtWxK23buXlQcHDHDPqVf+/HyPVqwARo3iukWLKDp16kST8PgBRm3auNHw+8gRejIl5ccE2CKZMnCqHOCe6nIZHxWZFEVRFGeoyKQoiqI4IyICkcZDqXJWgoKApUup7syYAbz+OtcfOsRlUu7HKcAYik9Xr1I8iIykCFG2LCNX3FUvo3p1FhP93/+AXr0okgBM2XviCabJ/fQT/aFE+LxTplDwmTyZ2UeJsX8/o7E2bYq7Pk8eGpyPHZswve/qVeDRR2mf88UXHJ81MqhNG+p93la/SIQpjBMmAF27MnIna1ZG8zRuDGzYQLP1+MydC4wbR08se0qX5vonn0z8fRahMPXKK0Dt2kyDy5YNGDiQwmXHjowiKl3a1n7IELaJn2aZGt54g1+DYcOo+Xz1FfDwwxQmk5O6CYBK5KZN/GDFP/DoaIa32VeQ27iRS1dEpgoVOLCnnkrmoLwLVVUAm8ikEwhFURTFESoyKYqiKM7wBpEJ4H9Vy5YM3bh6lblAbohkSooRIyhCLF7MTJ8NGyjSuJOpUykiDRhgKy///POMqvn6a5sBuTGc/2fKRFFq61aKIUWLMvWuSBG2EwHeeYciUq5cDP566CGgQAFG2ixZwn5CQykgZc7M55w5k0bkYWEUKzp0YH/vvAOUKQM88wwFsf79aZb+wAMJj+X2bQofV6/SLitnTo4na1aKK4kJVLGxPPYTJ3icd+4wSCY8nGOyZlvZc/MmNZHFi/k69e/P19IalbZtG6OKatdmFbfhwyniiTDS6b33gLp1qVm2aMGqgqGhjArq2JHrli2z6SpXrgBnznA5fjywbh3bfPWV7bhy5KBWU6cOjb23bmXE0Zw5jCKbPp1t3IWvL9MtAwP5HOXK8T1NtsAE8AOwdi3f5Mcft60/cIDq3eXLfAGsL/CmTcz9K1Uq6b6Dgmwm4RkZEblnbjVr1pQUMWOGCCDyzz8p219RFMXLAat9evx32tO3FP9P9OkjUrBgyvZVFEXJAOj/ROr+J34s0U1O+5VI0b5uZ8MGEUBk40Y+Ll6cj0uVStOnvXNHZO5ckatX0+45PviAhzJ1qki9erw/alTi7bdsESlWjO2st6efFjl5UuS55/j4uecSH/Ps2SLGiNStK/LkkyJ583Kfpk1F9u1zvM+334oULmx7vo8/jrs9Kkrk0UfZb6lSIoGBccdXooTItm0J+z19WqRZs7htfXz4XNmy8W0+dy7uPrt2iWTNyrZZs4q8845IbGzCvv/9V+SFF9hfpkw8vg4duF/PnhxzfKKieGx+fiI1avA1fe+9uMcTFCQybZpITIzj12rLFrYvVEhk0SIex8MPJ94+tcTEiCxcKHLmTAo7iI0VyZ+fBxccLBIRwXXTpvFAfHy47fhx2z4FC4r06OGW8XsaV/8nPP5D7s5biicPH3/Ml+K//1K2v6Ioipejk4dU/k889xzP4hRFUe5R9H8idf8TPxV+So4HlEvRvm7n0iURQGTcOJEbN+SuGpE1q6dHlmoiI0WqV+chZcsmsmRJ0vvcvCmyfbvIihUUWYKCKPAAIsOHOxZd7FmyRCRzZoo/PXqIrFmT9D6xsSJ//CHSurWIvz/FHuv6Z5/lc8+YYVsXHk7R5qefKDwZIzJpkq2/H38UyZFDJEsWkVmz2DYyUiQ6mtt37+bbW7myTTALC2NfxYqJbN5MPSQpDhwQeeUVkWrVOO5XXkla8Fm7lsKU9TXt3Flk2TI+pysxHPv3i5Qrx32zZImrz3gdZ85woG3acDlhgk2tbNuWCiMgsno121+8yMcTJ3p23G7C1f8JTZcD1JNJURRFcY6myymKoihO8ImMQLSvF6TLAcxTK1EC2L3b5uhcsyYdju/cYR6ZMw4cYBrQpk1A8eJpPtzk4O9PD6Tx44GhQ5melhRZstAYHAAee4zpaCNGAA0a0LA7KZ5+mmlhyTkNMAaoXJkG09WqsY85c/i827czJa1fP1vbQMtHp0EDGmz37Enj8ly5+NY9+ijT+Natc5x1VbMmsHw50LYt/aE+/JBG16dO8fnq13dt3BUr0ssKcP3Up3Vr2g6NH0/vpSZNXHsuK1Wr8qP6/vv0aSpZMnn7uw0R5mJ26EBfJEfs3s3lu+/SgGzwYD4eNoxvblgYHx88yDfjzz/5uHLlNB26t6HV5QAVmRRFURTnqMikKIqiOMEnOgLRfkGeHoaNWrWAPXtspt/Wmb8r5t9btrCu/ZIlztt99JHj8mBpTPnyNKN2RWByRMmSwPz5rglMVlJ6CpA7Nz2UTp2i71FoKC19RoxIfJ+sWSmkNW9Oz6lmzSiUbdjg3NanRQuKPTlyAF268O374APXBab4JOeY69dnpcHkCkxWsmalSPXkkynb3y2cOQN88glfNCtXrtBsy1oycM8evjBVq9Kwq1IlvlkjR9KDKVcuoFAhmw+aVWSqVCl9j8XDqMgEqMikKIqiOEdFJkVRFMUJvtERiPHzkkgmgKEtx48DP/5Id+O6dbneFfNva/TT8uWJt4mIoHO1tSa8kigNGtB0esQI4Ngx4IUXkq66FxjIl796dZqNr1/vWlBZo0bUQT77jN7vb73llkO4P/jtNy63bqWbOgBMmgSMGUMhCWAkU4UKdIGvVIkiUpcucfupWNEm7h44wMjCggXT5RC8BRWZABWZFEVRFOeoyKQoiqI4wTc6ArH+XiQy1arF5ddfM/SnUCE+diWSySoy7dxpm2zH5+BBqh8//8wUvNTy/fe2aJHEOHwY2Ls39c/lAbp3Z5U2+8r2SZEtG1/ev/9OXiCMry/QqxcwerStwNl9x9WrTH9LDr/9Rj1AhOFn4eHAp59y26JFXL9nj+27lRhWkSk2liJU5cpJq4r3GPfrxy4uVpFJJxCKoiiKI1RkUhRFSVOMMa2MMX8ZY44ZY4Y42B5ojPnKsn2HMaaE3bahlvV/GWMecbVPd+IX42UiU40aXF6/zsiL/Pn52NVIpnr1eP+77xy32bePy4gI4JdfeF8E+PVXTq6Tw++/09jn3Xedt+venTlh168nr/8MTEAAU+7uC156iSFYzoiMZFqbM86cAQoXBubNc95u/nxg4kTb499+Y8RfnTrA4sXAV18Bly4xX3H7dip+Fy8yStAZFSsCt28DJ08ykuk+S5UDVGQi0dGUee9bqVdRFEVxiopMiqIoaYYxxhfAJwBaA6gAoIsxpkK8Zr0BXBWRMgAmAxhn2bcCgM4AKgJoBWC6McbXxT7dhn9MBGIDvEhkyp3bZuBTsSKQLx/vWyOZRIB//km439WrwIULNP4OCUk8ZW7fPhoF+fnRIBwA1q6lOY81+sPabsQI/o8mhlXImjkT+Pdfx23OnmWq0pUr9IJSMi7R0RR3Bg+2RRvt3w/MmEE/pMQikMLCaMgdHBw3wm7MGJtbOUCh6s4dRiMlxoULFLXefhu4do1i6d69FJm6duV4hg2jQDtrFvexmny7EskE8Ptw8+Z9Z/oNqMhEoqM1VU5RFEVJHBWZFEVR0pI6AI6JyHERiQSwBED7eG3aA5hvub8MQDNjjLGsXyIiESJyAsAxS3+u9Ok2/GIjAG8SmQBbxEXFisy9Cgy0RTKtWAEUKUL/GXsOH+YyJAR44glg2zbg8uWEfe/dy2ipBx+k2zQATJvG5f/+x1S66GigWzeWDfvwQ9u+33wD/PST7fGKFXTxjowEJkzguitXgM2bbW1WruSyenUKFFev8vGpU3wuxTs4epSmU2vXOt4eGkqTqsGD+T5u2MD1n3zC5YkTTNOMz+XLjCjauZOfk9mzuf7UKZbqe+stCk+xsbZoqK1bbZ+TixeBqVOZAgfQS+z2bfa1fDmj6SIiKDI9/TSDT86cYbW5UqU45h07eC5YpYrz16CCRcv+6isu70ORSZUVQEUmRVEUxTkqMimKoqQlhQHY58CcBfBgYm1EJNoYEwYgj2X9b/H2LWy5n1SfbkEECJAIWx16b6F2bXoyVaxIT5j8+W0i0549tpLt+/bZ5kJWP6aQEPo4jRkDDBrENLXGjYGiRfmf+PvvQJ8+LGU2ahQn/+vXAw0b0mx8yRJO4A8dYrrQiBEsPbZ2LQWnggVpTH7xIp//f/+jf8306ZzQDxzI6KVNm1iWbcUKRrB8/jkre1krfi1YwPuuGJDfukWh4coV2zp7r5zk3k/t/vfac4eGMkUtOppCz/LlwKOPctuFC6zANmunP+4xAAATPklEQVQWPzMLFzI9ctgwW3pax44UE5csoXgJAOfPs5TgzJlMXfvuO35GZs3i+z5lCp8/NhYYOxZo355paq++ym1r1wLPPAMMHcp+Vq6kmfenn1IM27SJpt6PPcbnq1uXn80WLSgqde/O9d26MV2uYkUgUyY4JWdO4IEHbEKqNbLpPkKVFUBFJkVRFMU5KjIpiqKkJY5ccePnzCTWJrH1jjI2HObhGGP6AugLAMWKFUt8lIkgAuTNFoGAUl4mMr3wAkuSlS3Lx/ny2dLlDh+m4c+BA5y0DxzI9aGhFMtKlmQ0R8OGFHIWLOD+p07xdvs2o4pKlqR40LMn4O/P6I1HHqE4df06vZ3WrWPUU7NmFJ5atmQEy9y5NoHisccoECxezDr2pUtzoj5sGCOytmwBXnmFUSRPP83UqoAAoFgxCk/vv5/0//To0RS4Mme2rbNPzUrP+/cifn5A3778LHXrBnTowAqEf/xBMScigtvfew8oUICfhd69WZ3tzh2KRlFR/AxNmEBRp2VL7teiBfdr0IDP9eijFLJmzwY6dwaCgig8HTwI5MrF9/qrryhKNW7Mz2/t2vwc1anDsQ4fTuF11Cie5xUpQi8ngH1fucKUUAB46il+/urUce21qFiR6ajFiyfP7f0eQZUVgCKTTh4URVGUxNCLEYqiKGnJWQBF7R4XARDfMMja5qwxxg9ADgBXktg3qT4BACIyC8AsAKhVq1aylQAfHyCzTwQyF/MykSl7dqBTJ9tj+0imw4cpBoWHc7LduTO3h4ZSlLLOjbZvZ5vVqznRXrqUYhJAkal8eYo2hw9TLChUCBgyhNEjACOpcuRgdErr1pyov/su0KgRI0/KlAHKleMNoKh0/jwwfjwjTF58EXjtNYoP7S3ZjuPHUwzo14/eOZ06UTxo3pzCwMKFFDPsI07OnWNkS9euFLK8ifjik6eEr9SOIyDAJuBt2MD3Y/Rovsc9evB9tAqeANd9+CEj4OrVA6pV42doxQoKRIMGUaRZvZpRbFZataK4OWAAharXX+dn7PPP+Tl4+WWO47HHWBUub16O8+uv6evVpQvw5psUMbt0oQ/U1q38fFspVMhWkREA8uRh+mbJknCJihWBH364L1PlAAAics/catasKSmiXz+RfPlStq+iKEoGAMBu8YLfaU/fUvw/0aSJSMOGKdtXURQlA+DJ/wnwwvdxACUBBAD4HUDFeG36A5hpud8ZwFLL/YqW9oGW/Y8D8HWlT0e3FP9PBAWJvPFGyvZNL7p3FyleXCQ6WiQgQOTNN0VCQ0X8/EQGDmSb0qVFnnoq4b6xsSLlyonUrSsyeLBIYKBIZCS3tWolAoj8/DMfR0WJVKgg8vTTCfuwsmED9wE4DkdERIiUKME2+fJx3PG5c0ckRw4em4hI585s//77cdv17s1jPn7c6UukuJHoaJF//3Xe5osv+H4tXszHN2+KZM4s4uPD5YEDjvcbN477NW9uW/f881y3bx8fr1tn+4z17Glrd/Vq3M9i9epsM3Fisg8xUWbPZp9Dh7qvTy/A1f8JNf4GGB6nV6gVRVGUxNB0OUVRlDRDRKIBvAxgPYBQUEA6aIwZaYyxmKVgLoA8xphjAAYBGGLZ9yCApQAOAfgeQH8RiUmszzQ6AEb7eJsnU3yskUwnTjACpHx53rp1Y9rR2bPcFhKScF9jGDn022+MMqlc2RbR9OqrjFCqV4+P/fxoDL5oUcI+rDRvbvPdaZ+IH3tAACObAKZHOfofDgpi+tw33zBSZckSRq6MHWsrdX/oENOfXnrJ9UgUJfX4+jItzhmdO9PPq0sXPs6ShZ+H2FimUybmZ9S7Nz9vI0fa1k2YQA+matX4+OGHaXhvDI3BreTMGfez2LUrl9bPrzuwmoNbx3KfocoKoGkQiqIoinNiYniyqyiKoqQJIrIWwNp464bb3Q8H8FT8/SzbRgMY7Uqfacb69d4vYOTPT++bPXv4uHx5Lt98k6lGAwdycu9IZALouzR0KMWb1q1t6x95hDd7khLcjGE1us8+s4lNjujRgybjvXs7bzNrFlP0ypdnulXVqkzbGziQAka2bPT8UbwLY+iVZM9HH/GzFv8zZU+ePMAvv8RdlyNH3M9lYCAF0Js3E/9MA0D//jS0r1s3+eNPjNq16UXWvLn7+sxAqLICqMikKIqiOEcjmRRFUZTEMIYGxd5Ovnxcbt/OpdUHKSSE0SPffmt77IhcuRh58vnn9GNKLbVrJxQY4uPnR9HBGfXrs8z88eOsGla2LDB4MA2dly6lf9P33zPCSfF+8uVzLjAlB/tIp8TIlInRcO7EGHpH3adouhygIpOiKIriHBWZFEVRlIxO/vxcbt/O+7lz27YNGcKlMXHNmePz6qs0TG7aNO3GmVyMAaZOBT7+mIbiAI+nShWgY0eag7szSkVRFKeosgKoyKQoiqI4R0UmRVEUJaNjjWQ6cIBl3e2pWxdo0gT499+4VdniU7UqK7V5G23bxn2cJQvT7BRFSXdUWQFUZFIURVGcoyKToiiKktGxRjIBNj8me775BrhxI/3GoyjKPYkqK4CKTIqiKIpzVGRSFEVRMjrWSCbAsciUO3fcFDpFUZQUoJ5MgIpMiqIoinNUZFIURVEyOlmyAJkz874jkUlRFMUNqMgEqMikKIqiOCcmRv8nFEVRlIyPNWVORSZFUdIIFZkAFZkURVHSCGNMbmPMD8aYo5ZlrkTa9bS0OWqM6WlZl9kYs8YYc9gYc9AYM9aufS9jzEVjzH7L7fk0PZDoaI1kUhRFUTI++fLR2LtYMU+PRFGUexQVmQCdPCiKoqQdQwBsEpFgAJssj+NgjMkN4D0ADwKoA+A9OzFqgoiUB1AdQANjTGu7Xb8SkWqW25w0PQpNl1MURVHuBUqXBqpXB3x0GqgoStqg4TsA8NFHnh6BoijKvUp7AE0s9+cD2ArgrXhtHgHwg4hcAQBjzA8AWonIlwC2AICIRBpj9gIokg5jTsiSJUDOnB55akVRFEVxGzNm8AK7oihKGqEiE0A1X1EURUkLCojIeQAQkfPGmPwO2hQGcMbu8VnLursYY3ICaAfA/qpAB2NMIwBHALwmIvZ9uJd69dKsa0VRFEVJN/SCiaIoaYyKTIqiKEqqMMZsBFDQwaZ3XO3CwTqx698PwJcAporIccvqVQC+FJEIY0w/MEqqaSLj6wugLwAUUw8KRVEURVEURUkzVGRSFEVRUoWINE9smzHmgjGmkCWKqRCA/xw0OwtbSh3AlLitdo9nATgqIlPsnvOy3fbZAMY5Gd8sSx+oVauWJNZOURRFURRFUZTUoY5viqIoSlqyEkBPy/2eAFY4aLMeQEtjTC6L4XdLyzoYY0YByAHgVfsdLIKVlccAhLp53IqiKIqiKIqiJBMVmRRFUZS0ZCyAFsaYowBaWB7DGFPLGDMHACyG3x8A2GW5jRSRK8aYImDKXQUAe40x+40xz1v6HWiMOWiM+R3AQAC90vOgFEVRFEVRFEVJiKbLKYqiKGmGJa2tmYP1uwE8b/d4HoB58dqchWO/JojIUABD3TpYRVEURVEURVFSRaoimYwxuY0xPxhjjlqWuRJp19PS5qgxpqdlXWZjzBpjzGHL1eixdu17GWMuWq5a21+5VhRFURRFURRFURRFUbyQ1KbLDQGwSUSCAWyyPI6DMSY3gPcAPAigDoD37MSoCSJSHkB1AA2MMa3tdv1KRKpZbnNSOU5FURRFURRFURRFURQlDUmtyNQeLBsNy/JxB20eAfCDiFwRkasAfgDQSkRui8gWABCRSAB7wYpCiqIoiqIoiqIoiqIoSgYjtSJTARE5DwCWZX4HbQoDOGP3+Kxl3V2MMTkBtAOjoax0MMb8YYxZZowpmtgAjDF9jTG7jTG7L168mNLjUBRFURRFURRFURRFUVJBkiKTMWajMeaAg1t7F5/DkWmr2PXvB+BLAFNF5Lhl9SoAJUSkCoCNsEVLJexIZJaI1BKRWvny5XNxSIqiKIqiKIqiKIqiKIo7SbK6nIg0T2ybMeaCMaaQiJw3xhQC8J+DZmcBNLF7XATAVrvHswAcFZEpds952W77bADjkhqnoiiKoiiKoiiKoiiK4jlSmy63EkBPy/2eAFY4aLMeQEtjTC6L4XdLyzoYY0YByAHgVfsdLIKVlccAhKZynIqiKIqiKIqiKIqiKEoaklqRaSyAFsaYowBaWB7DGFPLGDMHAETkCoAPAOyy3EaKyBVjTBEA7wCoAGCvMWa/MeZ5S78DjTEHjTG/AxgIoFcqx6koiqIoiqIoiqIoiqKkIUmmyznDktbWzMH63QCet3s8D8C8eG3OwrFfE0RkKIChqRmboiiKoiiKoiiKoiiKkn6kNpJJURRFURRFURRFURRFUWBEJOlWGQRjzEUAp1K4e14Al9w4nPREx+4ZdOyeQceeMorL/9u7t1C5yjOM4/+XRNOqSLTQ0iaCEYJH2ipS0gMiKjSxYrzwIiIYUPBGqJaCGnLVy2LpCayleIiKaGnqIQgVJQpemVZbidEY3a1S00YjeKQFD/Ttxfo2HeKevYNT17e+mf8Phj1r7Qk8+12z5oEva2YyZ/4rOO2JJpm9DrPXYU9UZk80yex1mL2OwffEVC0yTSIins7Ms2vn+DTMXofZ6zC7amn5+Jm9DrPXYXbV0vLxM3sdZq/D7J8t3y4nSZIkSZKkibnIJEmSJEmSpIm5yPQ/v6kdYAJmr8PsdZhdtbR8/Mxeh9nrMLtqafn4mb0Os9dh9s+Qn8kkSZIkSZKkiXklkyRJkiRJkiY284tMEbE+IvZFxFxE3Fg7z2Ii4oSIeCIi9kbE8xFxbdl/fEQ8FhEvl5/H1c46TkQsi4i/RMTDZXtNROwq2X8bEUfWzriQiFgZEdsj4sUy/2+2MveI+EF5vuyJiHsj4nNDnntE3B4RByNiz8i+BWcdnV+W83d3RJxVL/nY7DeV583uiHggIlaO/G5Lyb4vIr5bJ7WWYk/0y57onz1RNbcdMQXsiX7ZE/1rqSda7YiSZyp6YqYXmSJiGXAzsAE4DbgsIk6rm2pRHwM/zMxTgXXANSXvjcDOzFwL7CzbQ3UtsHdk+8fAz0r2t4GrqqRa2i+ARzLzFOBrdH/D4OceEauA7wNnZ+YZwDJgE8Oe+zZg/SH7xs16A7C23K4Gbukp4zjb+GT2x4AzMvOrwEvAFoBy7m4CTi//5lflNUkDYk9UYU/0yJ7o1TbsiKljT1RhT/SowZ7YRpsdAVPSEzO9yAR8A5jLzL9l5ofAfcDGypnGyswDmfnncv99uhemVXSZ7ywPuxO4pE7CxUXEauB7wK1lO4DzgO3lIYPMHhHHAucAtwFk5oeZ+Q6NzB1YDnw+IpYDRwEHGPDcM/NJ4K1Ddo+b9Ubgruw8BayMiC/3k/STFsqemY9m5sdl8ylgdbm/EbgvMz/IzFeAObrXJA2LPdEje6Iae6IHdsTUsid6ZE9U00xPtNoRMD09MeuLTKuA10a295d9gxcRJwJnAruAL2XmAeiKA/hivWSL+jlwPfCfsv0F4J2Rk2ao8z8JeBO4o1yae2tEHE0Dc8/MfwA/Af5OVwbvAs/QxtxHjZt1a+fwlcAfyv3Wss+qZo+TPdEre6K+aegJO6JNzR4re6JX9kRd09AR0EhPzPoiUyywb/BftxcRxwC/B67LzPdq5zkcEXERcDAznxndvcBDhzj/5cBZwC2ZeSbwLwZ4KetCyvuNNwJrgK8AR9NdFnqoIc79cLTyHCIittJdon7P/K4FHjbI7DOuyeNkT/TOnhiuJp5DdkTTmjxW9kTv7IlhauX501RPzPoi037ghJHt1cA/K2U5LBFxBF0h3JOZ95fdb8xf1ld+HqyVbxHfBi6OiFfpLiM+j+5/IlaWyy5huPPfD+zPzF1leztdSbQw9wuAVzLzzcz8CLgf+BZtzH3UuFk3cQ5HxGbgIuDyzJx/8W8iu9o7TvZEFfZEfc32hB3RvOaOlT1RhT1RV7MdAe31xKwvMv0JWFs+Gf9Iug/O2lE501jlPce3AXsz86cjv9oBbC73NwMP9Z1tKZm5JTNXZ+aJdHN+PDMvB54ALi0PG2r214HXIuLksut84AUamDvdZa3rIuKo8vyZzz74uR9i3Kx3AFeUb4ZYB7w7fynsUETEeuAG4OLM/PfIr3YAmyJiRUSsofvAwT/WyKhF2RM9sSeqsScqsiOmgj3RE3uimmnoiSY7Ahrticyc6RtwId2ntP8V2Fo7zxJZv0N3Cdxu4Nlyu5Duvcg7gZfLz+NrZ13i7zgXeLjcP4nuZJgDfgesqJ1vTOavA0+X2T8IHNfK3IEfAS8Ce4C7gRVDnjtwL937vT+iW6G/atys6S4Tvbmcv8/RfevF0LLP0b1fev6c/fXI47eW7PuADbVn723scbUn+v877Il+s9sT9XLbEVNwsyeq/B32RL/Zm+mJVjtikezN9USUcJIkSZIkSdKnNutvl5MkSZIkSdL/gYtMkiRJkiRJmpiLTJIkSZIkSZqYi0ySJEmSJEmamItMkiRJkiRJmpiLTJIkSZIkSZqYi0ySJEmSJEmamItMkiRJkiRJmth/Ab8AGBPJ0PzLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 1440x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x504 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = 32\n",
    "gyroy = gyroModel.predict(gyro_train[sample].reshape(1,128,3))\n",
    "fig=plt.figure(figsize=(20,7))\n",
    "for channel in range(3):\n",
    "    plt.subplot(1,3,channel+1)\n",
    "    plot(gyro_train[sample,:,channel],'b')\n",
    "    plot(gyroy[0,:,channel],'r')\n",
    "linearAccy = linearAccModel.predict(linearAcc_train[sample].reshape(1,128,3))\n",
    "fig=plt.figure(figsize=(20,7))\n",
    "for channel in range(3):\n",
    "    plt.subplot(1,3,channel+1)\n",
    "    plot(linearAcc_train[sample,:,channel],'b')\n",
    "    plot(linearAccy[0,:,channel],'r')\n",
    "gravityy = gravityModel.predict(gravity_train[sample].reshape(1,128,3))\n",
    "fig=plt.figure(figsize=(20,7))\n",
    "for channel in range(3):\n",
    "    plt.subplot(1,3,channel+1)\n",
    "    plot(gravity_train[sample,:,channel],'b')\n",
    "    plot(gravityy[0,:,channel],'r')\n",
    "gameVecy = gameVecModel.predict(gameVec_train[sample].reshape(1,128,4))\n",
    "fig=plt.figure(figsize=(20,7))\n",
    "for channel in range(4):\n",
    "    plt.subplot(1,4,channel+1)\n",
    "    plot(gameVec_train[sample,:,channel],'b')\n",
    "    plot(gameVecy[0,:,channel],'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
