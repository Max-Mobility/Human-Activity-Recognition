{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "from keras.layers import Input, Conv1D,Activation,MaxPooling1D,Dense,Flatten,UpSampling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.models import Model,load_model\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from matplotlib.pyplot import plot\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sub_model(myinputs,channel=3,sensors=1):\n",
    "    x     = Conv1D(filters =8*sensors, kernel_size=5,strides = 1, padding = 'same')(myinputs)\n",
    "    x     = BatchNormalization()(x) \n",
    "    x     = Activation('relu')(x)\n",
    "    x     = MaxPooling1D(pool_size=3,strides = 2,padding='same')(x)\n",
    "    #  x shape is 64X16 =3072\n",
    "    x     = Conv1D(filters =12*sensors, kernel_size=3,strides = 1, padding = 'same')(x)\n",
    "    x     = BatchNormalization()(x) \n",
    "    x     = Activation('relu')(x)\n",
    "    x     = MaxPooling1D(pool_size=2,strides = 2,padding='same')(x)\n",
    "    #  x shape is 32X24 = 2048\n",
    "    x     = Conv1D(filters =16*sensors, kernel_size=3,strides = 1, padding = 'same')(x)\n",
    "    x     = BatchNormalization()(x) \n",
    "    x     = Activation('relu')(x)\n",
    "    x     = MaxPooling1D(pool_size=2,strides = 2,padding='same')(x)\n",
    "    # x shape is 16*32 = 1536\n",
    "    x     = Conv1D(filters =20*sensors, kernel_size=3,strides = 1, padding = 'same')(x)\n",
    "    x     = BatchNormalization()(x) \n",
    "    x     = Activation('relu')(x)\n",
    "    x     = MaxPooling1D(pool_size=2,strides = 2,padding='same')(x)\n",
    "    # x shape is 8*32 = 1024\n",
    "    x     = Conv1D(filters =24, kernel_size=3,strides = 2, padding = 'same')(x)\n",
    "    # x shape is 4*32 = 512\n",
    "\n",
    "    latent_vector = Flatten()(x)\n",
    "    # decoder x = 4*128\n",
    "    x     = Conv1D(filters =24*sensors, kernel_size=3,strides = 1, padding = 'same')(x)\n",
    "    x     = BatchNormalization()(x) \n",
    "    x     = Activation('relu')(x)\n",
    "    x     = UpSampling1D(2)(x)\n",
    "    # x shape is 8*128\n",
    "    x     = Conv1D(filters =20*sensors, kernel_size=3,strides = 1, padding = 'same')(x)\n",
    "    x     = BatchNormalization()(x) \n",
    "    x     = Activation('relu')(x)\n",
    "    x     = UpSampling1D(2)(x)\n",
    "    # x shape is 16*96\n",
    "    x     = Conv1D(filters =16*sensors, kernel_size=3,strides = 1, padding = 'same')(x)\n",
    "    x     = BatchNormalization()(x) \n",
    "    x     = Activation('relu')(x)\n",
    "    x     = UpSampling1D(2)(x)\n",
    "    # x shape is 32*64\n",
    "    x     = Conv1D(filters =12*sensors, kernel_size=3,strides = 1, padding = 'same')(x)\n",
    "    x     = BatchNormalization()(x) \n",
    "    x     = Activation('relu')(x)\n",
    "    x     = UpSampling1D(2)(x)\n",
    "    # x shape is 64*48\n",
    "    x     = Conv1D(filters =8*sensors, kernel_size=3,strides = 1, padding = 'same')(x)\n",
    "    x     = BatchNormalization()(x) \n",
    "    x     = Activation('relu')(x)\n",
    "    x     = UpSampling1D(2)(x)\n",
    "    # x shape is 128*48\n",
    "    y     = Conv1D(filters =channel, kernel_size=3,strides = 1, padding = 'same')(x)\n",
    "    #y     = Activation('relu')(x)\n",
    "    return latent_vector,y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(name=\"push_detect\",channel=3,sensors=1,inputLength=128):\n",
    "    # input shape 128X26 =3382\n",
    "    with tf.name_scope(name):\n",
    "        myInputs = Input(shape=(inputLength,channel))\n",
    "                \n",
    "        LV,Y = get_sub_model(myInputs,channel=channel,sensors=sensors)\n",
    "        \n",
    "        autoencoder = Model (inputs=myInputs,\n",
    "                             outputs =Y)\n",
    "    \n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputLength=64\n",
    "gyroModel=build_model(sensors=3,inputLength=inputLength)\n",
    "linearAccModel=build_model(sensors=3,inputLength=inputLength)\n",
    "gravityModel=build_model(sensors=3,inputLength=inputLength)\n",
    "gameVecModel=build_model(channel=4,sensors=3,inputLength=inputLength)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_41 (InputLayer)        (None, 64, 3)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_441 (Conv1D)          (None, 64, 24)            384       \n",
      "_________________________________________________________________\n",
      "batch_normalization_145 (Bat (None, 64, 24)            96        \n",
      "_________________________________________________________________\n",
      "activation_369 (Activation)  (None, 64, 24)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_161 (MaxPoolin (None, 32, 24)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_442 (Conv1D)          (None, 32, 36)            2628      \n",
      "_________________________________________________________________\n",
      "batch_normalization_146 (Bat (None, 32, 36)            144       \n",
      "_________________________________________________________________\n",
      "activation_370 (Activation)  (None, 32, 36)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_162 (MaxPoolin (None, 16, 36)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_443 (Conv1D)          (None, 16, 48)            5232      \n",
      "_________________________________________________________________\n",
      "batch_normalization_147 (Bat (None, 16, 48)            192       \n",
      "_________________________________________________________________\n",
      "activation_371 (Activation)  (None, 16, 48)            0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_163 (MaxPoolin (None, 8, 48)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_444 (Conv1D)          (None, 8, 60)             8700      \n",
      "_________________________________________________________________\n",
      "batch_normalization_148 (Bat (None, 8, 60)             240       \n",
      "_________________________________________________________________\n",
      "activation_372 (Activation)  (None, 8, 60)             0         \n",
      "_________________________________________________________________\n",
      "max_pooling1d_164 (MaxPoolin (None, 4, 60)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_445 (Conv1D)          (None, 2, 24)             4344      \n",
      "_________________________________________________________________\n",
      "conv1d_446 (Conv1D)          (None, 2, 72)             5256      \n",
      "_________________________________________________________________\n",
      "batch_normalization_149 (Bat (None, 2, 72)             288       \n",
      "_________________________________________________________________\n",
      "activation_373 (Activation)  (None, 2, 72)             0         \n",
      "_________________________________________________________________\n",
      "up_sampling1d_201 (UpSamplin (None, 4, 72)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_447 (Conv1D)          (None, 4, 60)             13020     \n",
      "_________________________________________________________________\n",
      "batch_normalization_150 (Bat (None, 4, 60)             240       \n",
      "_________________________________________________________________\n",
      "activation_374 (Activation)  (None, 4, 60)             0         \n",
      "_________________________________________________________________\n",
      "up_sampling1d_202 (UpSamplin (None, 8, 60)             0         \n",
      "_________________________________________________________________\n",
      "conv1d_448 (Conv1D)          (None, 8, 48)             8688      \n",
      "_________________________________________________________________\n",
      "batch_normalization_151 (Bat (None, 8, 48)             192       \n",
      "_________________________________________________________________\n",
      "activation_375 (Activation)  (None, 8, 48)             0         \n",
      "_________________________________________________________________\n",
      "up_sampling1d_203 (UpSamplin (None, 16, 48)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_449 (Conv1D)          (None, 16, 36)            5220      \n",
      "_________________________________________________________________\n",
      "batch_normalization_152 (Bat (None, 16, 36)            144       \n",
      "_________________________________________________________________\n",
      "activation_376 (Activation)  (None, 16, 36)            0         \n",
      "_________________________________________________________________\n",
      "up_sampling1d_204 (UpSamplin (None, 32, 36)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_450 (Conv1D)          (None, 32, 24)            2616      \n",
      "_________________________________________________________________\n",
      "batch_normalization_153 (Bat (None, 32, 24)            96        \n",
      "_________________________________________________________________\n",
      "activation_377 (Activation)  (None, 32, 24)            0         \n",
      "_________________________________________________________________\n",
      "up_sampling1d_205 (UpSamplin (None, 64, 24)            0         \n",
      "_________________________________________________________________\n",
      "conv1d_451 (Conv1D)          (None, 64, 3)             219       \n",
      "=================================================================\n",
      "Total params: 57,939\n",
      "Trainable params: 57,123\n",
      "Non-trainable params: 816\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gyroModel.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gyroModel=load_model('gyroModel.h5')\n",
    "linearAccModel=load_model('linearAccModel.h5')\n",
    "gravityModel=load_model('gravityModel.h5')\n",
    "gameVecModel=load_model('gameVecModel.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = keras.optimizers.SGD(lr=0.1, momentum=0.0, decay=0.000, nesterov=False)\n",
    "gyroModel.compile(optimizer=opt,loss='mae',metrics=['accuracy'])\n",
    "opt = keras.optimizers.SGD(lr=.1, momentum=0.0, decay=0.000, nesterov=False)\n",
    "linearAccModel.compile(optimizer=opt,loss='mae',metrics=['accuracy'])\n",
    "opt = keras.optimizers.SGD(lr=.1, momentum=0.0, decay=0.000, nesterov=False)\n",
    "gravityModel.compile(optimizer=opt,loss='mae',metrics=['accuracy'])\n",
    "opt = keras.optimizers.SGD(lr=.1, momentum=0.0, decay=0.000, nesterov=False)\n",
    "gameVecModel.compile(optimizer=opt,loss='mae',metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "data= np.load('watch_norm.npy')\n",
    "np.random.shuffle(data)\n",
    "p=0.85\n",
    "train_num =int(data.shape[0]*p)\n",
    "gyro_train=data[:train_num,:inputLength,0:3]\n",
    "linearAcc_train=data[:train_num,:inputLength,3:6]\n",
    "gravity_train=data[:train_num,:inputLength,6:9]\n",
    "gameVec_train=data[:train_num,:inputLength,9:13]\n",
    "\n",
    "gyro_test =data[train_num:,:inputLength,0:3]\n",
    "linearAcc_test =data[train_num:,:inputLength,3:6]\n",
    "gravity_test =data[train_num:,:inputLength,6:9]\n",
    "gameVec_test =data[train_num:,:inputLength,9:13]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start training round 0\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 1/20\n",
      "8564/8564 [==============================] - 27s 3ms/step - loss: 0.2041 - acc: 0.3522 - val_loss: 0.1907 - val_acc: 0.3348\n",
      "Epoch 2/20\n",
      "8564/8564 [==============================] - 4s 478us/step - loss: 0.0898 - acc: 0.3761 - val_loss: 0.1194 - val_acc: 0.3612\n",
      "Epoch 3/20\n",
      "8564/8564 [==============================] - 4s 495us/step - loss: 0.0795 - acc: 0.3810 - val_loss: 0.1178 - val_acc: 0.3608\n",
      "Epoch 4/20\n",
      "8564/8564 [==============================] - 4s 499us/step - loss: 0.0785 - acc: 0.3953 - val_loss: 0.1085 - val_acc: 0.3095\n",
      "Epoch 5/20\n",
      "8564/8564 [==============================] - 4s 491us/step - loss: 0.0758 - acc: 0.3938 - val_loss: 0.0928 - val_acc: 0.2887\n",
      "Epoch 6/20\n",
      "8564/8564 [==============================] - 4s 486us/step - loss: 0.0725 - acc: 0.4003 - val_loss: 0.0861 - val_acc: 0.3157\n",
      "Epoch 7/20\n",
      "8564/8564 [==============================] - 4s 479us/step - loss: 0.0693 - acc: 0.4104 - val_loss: 0.0780 - val_acc: 0.3096\n",
      "Epoch 8/20\n",
      "8564/8564 [==============================] - 4s 463us/step - loss: 0.0673 - acc: 0.4082 - val_loss: 0.0788 - val_acc: 0.3944\n",
      "Epoch 9/20\n",
      "8564/8564 [==============================] - 4s 511us/step - loss: 0.0670 - acc: 0.4191 - val_loss: 0.0695 - val_acc: 0.3446\n",
      "Epoch 10/20\n",
      "8564/8564 [==============================] - 4s 471us/step - loss: 0.0645 - acc: 0.4244 - val_loss: 0.0651 - val_acc: 0.4023\n",
      "Epoch 11/20\n",
      "8564/8564 [==============================] - 4s 501us/step - loss: 0.0628 - acc: 0.4295 - val_loss: 0.0647 - val_acc: 0.4084\n",
      "Epoch 12/20\n",
      "8564/8564 [==============================] - 4s 489us/step - loss: 0.0621 - acc: 0.4355 - val_loss: 0.0644 - val_acc: 0.3924\n",
      "Epoch 13/20\n",
      "8564/8564 [==============================] - 4s 461us/step - loss: 0.0606 - acc: 0.4338 - val_loss: 0.0627 - val_acc: 0.3574\n",
      "Epoch 14/20\n",
      "8564/8564 [==============================] - 4s 513us/step - loss: 0.0588 - acc: 0.4362 - val_loss: 0.0609 - val_acc: 0.3241\n",
      "Epoch 15/20\n",
      "8564/8564 [==============================] - 4s 471us/step - loss: 0.0588 - acc: 0.4425 - val_loss: 0.0600 - val_acc: 0.3651\n",
      "Epoch 16/20\n",
      "8564/8564 [==============================] - 4s 476us/step - loss: 0.0566 - acc: 0.4380 - val_loss: 0.0560 - val_acc: 0.4564\n",
      "Epoch 17/20\n",
      "8564/8564 [==============================] - 4s 478us/step - loss: 0.0568 - acc: 0.4603 - val_loss: 0.0585 - val_acc: 0.4839\n",
      "Epoch 18/20\n",
      "8564/8564 [==============================] - 4s 469us/step - loss: 0.0552 - acc: 0.4544 - val_loss: 0.0588 - val_acc: 0.3743\n",
      "Epoch 19/20\n",
      "8564/8564 [==============================] - 4s 496us/step - loss: 0.0542 - acc: 0.4658 - val_loss: 0.0575 - val_acc: 0.3909\n",
      "Epoch 20/20\n",
      "8564/8564 [==============================] - 4s 477us/step - loss: 0.0548 - acc: 0.4803 - val_loss: 0.0589 - val_acc: 0.4620\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 1/20\n",
      "8564/8564 [==============================] - 24s 3ms/step - loss: 0.2122 - acc: 0.5092 - val_loss: 0.1920 - val_acc: 0.5101\n",
      "Epoch 2/20\n",
      "8564/8564 [==============================] - 4s 471us/step - loss: 0.0983 - acc: 0.6299 - val_loss: 0.1030 - val_acc: 0.5571\n",
      "Epoch 3/20\n",
      "8564/8564 [==============================] - 4s 477us/step - loss: 0.0725 - acc: 0.6441 - val_loss: 0.0811 - val_acc: 0.6352\n",
      "Epoch 4/20\n",
      "8564/8564 [==============================] - 4s 474us/step - loss: 0.0649 - acc: 0.6982 - val_loss: 0.0782 - val_acc: 0.6994\n",
      "Epoch 5/20\n",
      "8564/8564 [==============================] - 4s 472us/step - loss: 0.0562 - acc: 0.7110 - val_loss: 0.0567 - val_acc: 0.7281\n",
      "Epoch 6/20\n",
      "8564/8564 [==============================] - 4s 440us/step - loss: 0.0523 - acc: 0.7192 - val_loss: 0.0524 - val_acc: 0.7226\n",
      "Epoch 7/20\n",
      "8564/8564 [==============================] - 4s 472us/step - loss: 0.0508 - acc: 0.7295 - val_loss: 0.0506 - val_acc: 0.7378\n",
      "Epoch 8/20\n",
      "8564/8564 [==============================] - 4s 486us/step - loss: 0.0488 - acc: 0.7429 - val_loss: 0.0568 - val_acc: 0.7367\n",
      "Epoch 9/20\n",
      "8564/8564 [==============================] - 4s 484us/step - loss: 0.0473 - acc: 0.7428 - val_loss: 0.0494 - val_acc: 0.7491\n",
      "Epoch 10/20\n",
      "8564/8564 [==============================] - 4s 499us/step - loss: 0.0476 - acc: 0.7541 - val_loss: 0.0486 - val_acc: 0.7453\n",
      "Epoch 11/20\n",
      "8564/8564 [==============================] - 4s 501us/step - loss: 0.0455 - acc: 0.7223 - val_loss: 0.0439 - val_acc: 0.6561\n",
      "Epoch 12/20\n",
      "8564/8564 [==============================] - 4s 484us/step - loss: 0.0446 - acc: 0.7174 - val_loss: 0.0463 - val_acc: 0.7519\n",
      "Epoch 13/20\n",
      "8564/8564 [==============================] - 4s 501us/step - loss: 0.0445 - acc: 0.7280 - val_loss: 0.0472 - val_acc: 0.7574\n",
      "Epoch 14/20\n",
      "8564/8564 [==============================] - 4s 482us/step - loss: 0.0442 - acc: 0.7626 - val_loss: 0.0445 - val_acc: 0.7592\n",
      "Epoch 15/20\n",
      "8564/8564 [==============================] - 4s 478us/step - loss: 0.0429 - acc: 0.7646 - val_loss: 0.0441 - val_acc: 0.7615\n",
      "Epoch 16/20\n",
      "8564/8564 [==============================] - 4s 494us/step - loss: 0.0430 - acc: 0.7681 - val_loss: 0.0401 - val_acc: 0.7596\n",
      "Epoch 17/20\n",
      "8564/8564 [==============================] - 4s 472us/step - loss: 0.0417 - acc: 0.7519 - val_loss: 0.0455 - val_acc: 0.5292\n",
      "Epoch 18/20\n",
      "8564/8564 [==============================] - 4s 485us/step - loss: 0.0423 - acc: 0.7522 - val_loss: 0.0471 - val_acc: 0.5143\n",
      "Epoch 19/20\n",
      "8564/8564 [==============================] - 4s 508us/step - loss: 0.0426 - acc: 0.7268 - val_loss: 0.0429 - val_acc: 0.7596\n",
      "Epoch 20/20\n",
      "8564/8564 [==============================] - 4s 497us/step - loss: 0.0423 - acc: 0.7417 - val_loss: 0.0433 - val_acc: 0.4970\n",
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 1/20\n",
      "8564/8564 [==============================] - 23s 3ms/step - loss: 0.2171 - acc: 0.7809 - val_loss: 0.1929 - val_acc: 0.8597\n",
      "Epoch 2/20\n",
      "8564/8564 [==============================] - 5s 538us/step - loss: 0.1142 - acc: 0.9035 - val_loss: 0.1409 - val_acc: 0.9062\n",
      "Epoch 3/20\n",
      "8564/8564 [==============================] - 4s 517us/step - loss: 0.0963 - acc: 0.9220 - val_loss: 0.1211 - val_acc: 0.9117\n",
      "Epoch 4/20\n",
      "8564/8564 [==============================] - 5s 552us/step - loss: 0.0905 - acc: 0.9352 - val_loss: 0.1114 - val_acc: 0.9375\n",
      "Epoch 5/20\n",
      "8564/8564 [==============================] - 4s 507us/step - loss: 0.0803 - acc: 0.9391 - val_loss: 0.0905 - val_acc: 0.9388\n",
      "Epoch 6/20\n",
      "8564/8564 [==============================] - 4s 517us/step - loss: 0.0797 - acc: 0.9442 - val_loss: 0.1049 - val_acc: 0.9413\n",
      "Epoch 7/20\n",
      "8564/8564 [==============================] - 5s 549us/step - loss: 0.0747 - acc: 0.9445 - val_loss: 0.0926 - val_acc: 0.9358\n",
      "Epoch 8/20\n",
      "8564/8564 [==============================] - 5s 541us/step - loss: 0.0705 - acc: 0.9495 - val_loss: 0.0968 - val_acc: 0.9494\n",
      "Epoch 9/20\n",
      "8564/8564 [==============================] - 4s 522us/step - loss: 0.0674 - acc: 0.9506 - val_loss: 0.0911 - val_acc: 0.9528\n",
      "Epoch 10/20\n",
      "8564/8564 [==============================] - 4s 502us/step - loss: 0.0675 - acc: 0.9475 - val_loss: 0.0813 - val_acc: 0.9353\n",
      "Epoch 11/20\n",
      "8564/8564 [==============================] - 5s 547us/step - loss: 0.0638 - acc: 0.9506 - val_loss: 0.0795 - val_acc: 0.9498\n",
      "Epoch 12/20\n",
      "8564/8564 [==============================] - 5s 528us/step - loss: 0.0642 - acc: 0.9511 - val_loss: 0.0695 - val_acc: 0.9528\n",
      "Epoch 13/20\n",
      "8564/8564 [==============================] - 5s 529us/step - loss: 0.0598 - acc: 0.9538 - val_loss: 0.0674 - val_acc: 0.9542\n",
      "Epoch 14/20\n",
      "8564/8564 [==============================] - 5s 553us/step - loss: 0.0607 - acc: 0.9519 - val_loss: 0.0738 - val_acc: 0.9572\n",
      "Epoch 15/20\n",
      "8564/8564 [==============================] - 5s 540us/step - loss: 0.0609 - acc: 0.9524 - val_loss: 0.0560 - val_acc: 0.9588\n",
      "Epoch 16/20\n",
      "8564/8564 [==============================] - 5s 552us/step - loss: 0.0592 - acc: 0.9570 - val_loss: 0.0575 - val_acc: 0.9585\n",
      "Epoch 17/20\n",
      "8564/8564 [==============================] - 5s 529us/step - loss: 0.0567 - acc: 0.9531 - val_loss: 0.0680 - val_acc: 0.9593\n",
      "Epoch 18/20\n",
      "8564/8564 [==============================] - 5s 534us/step - loss: 0.0545 - acc: 0.9554 - val_loss: 0.0623 - val_acc: 0.9585\n",
      "Epoch 19/20\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 5s 536us/step - loss: 0.0525 - acc: 0.9567 - val_loss: 0.0549 - val_acc: 0.9612\n",
      "Epoch 20/20\n",
      "8564/8564 [==============================] - 5s 535us/step - loss: 0.0538 - acc: 0.9569 - val_loss: 0.0556 - val_acc: 0.9602\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 1/20\n",
      "8564/8564 [==============================] - 25s 3ms/step - loss: 0.2691 - acc: 0.5070 - val_loss: 0.1966 - val_acc: 0.6013\n",
      "Epoch 2/20\n",
      "8564/8564 [==============================] - 5s 545us/step - loss: 0.1260 - acc: 0.7091 - val_loss: 0.1516 - val_acc: 0.7052\n",
      "Epoch 3/20\n",
      "8564/8564 [==============================] - 5s 544us/step - loss: 0.1056 - acc: 0.7622 - val_loss: 0.1366 - val_acc: 0.7338\n",
      "Epoch 4/20\n",
      "8564/8564 [==============================] - 4s 497us/step - loss: 0.0946 - acc: 0.7956 - val_loss: 0.1116 - val_acc: 0.7974\n",
      "Epoch 5/20\n",
      "8564/8564 [==============================] - 5s 528us/step - loss: 0.0875 - acc: 0.8121 - val_loss: 0.0990 - val_acc: 0.8033\n",
      "Epoch 6/20\n",
      "8564/8564 [==============================] - 5s 551us/step - loss: 0.0852 - acc: 0.8183 - val_loss: 0.1078 - val_acc: 0.8097\n",
      "Epoch 7/20\n",
      "8564/8564 [==============================] - 4s 505us/step - loss: 0.0811 - acc: 0.8282 - val_loss: 0.1022 - val_acc: 0.8478\n",
      "Epoch 8/20\n",
      "8564/8564 [==============================] - 4s 514us/step - loss: 0.0789 - acc: 0.8300 - val_loss: 0.0885 - val_acc: 0.8427\n",
      "Epoch 9/20\n",
      "8564/8564 [==============================] - 5s 531us/step - loss: 0.0747 - acc: 0.8366 - val_loss: 0.0864 - val_acc: 0.8539\n",
      "Epoch 10/20\n",
      "8564/8564 [==============================] - 4s 524us/step - loss: 0.0763 - acc: 0.8419 - val_loss: 0.0904 - val_acc: 0.8335\n",
      "Epoch 11/20\n",
      "8564/8564 [==============================] - 4s 516us/step - loss: 0.0747 - acc: 0.8396 - val_loss: 0.0804 - val_acc: 0.8484\n",
      "Epoch 12/20\n",
      "8564/8564 [==============================] - 4s 499us/step - loss: 0.0710 - acc: 0.8497 - val_loss: 0.0687 - val_acc: 0.8723\n",
      "Epoch 13/20\n",
      "8564/8564 [==============================] - 4s 521us/step - loss: 0.0694 - acc: 0.8482 - val_loss: 0.0738 - val_acc: 0.8550\n",
      "Epoch 14/20\n",
      "8564/8564 [==============================] - 4s 513us/step - loss: 0.0680 - acc: 0.8532 - val_loss: 0.0867 - val_acc: 0.8645\n",
      "Epoch 15/20\n",
      "8564/8564 [==============================] - 4s 507us/step - loss: 0.0658 - acc: 0.8552 - val_loss: 0.0667 - val_acc: 0.8470\n",
      "Epoch 16/20\n",
      "8564/8564 [==============================] - 4s 519us/step - loss: 0.0654 - acc: 0.8550 - val_loss: 0.0714 - val_acc: 0.8524\n",
      "Epoch 17/20\n",
      "8564/8564 [==============================] - 4s 500us/step - loss: 0.0645 - acc: 0.8560 - val_loss: 0.0616 - val_acc: 0.8784\n",
      "Epoch 18/20\n",
      "8564/8564 [==============================] - 5s 534us/step - loss: 0.0641 - acc: 0.8580 - val_loss: 0.0628 - val_acc: 0.8820\n",
      "Epoch 19/20\n",
      "8564/8564 [==============================] - 4s 512us/step - loss: 0.0641 - acc: 0.8643 - val_loss: 0.0662 - val_acc: 0.8689\n",
      "Epoch 20/20\n",
      "8564/8564 [==============================] - 5s 533us/step - loss: 0.0653 - acc: 0.8625 - val_loss: 0.0628 - val_acc: 0.8588\n",
      "start training round 1\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 21/40\n",
      "8564/8564 [==============================] - 4s 520us/step - loss: 0.0538 - acc: 0.4699 - val_loss: 0.0533 - val_acc: 0.4527\n",
      "Epoch 22/40\n",
      "8564/8564 [==============================] - 5s 541us/step - loss: 0.0532 - acc: 0.4737 - val_loss: 0.0525 - val_acc: 0.5130\n",
      "Epoch 23/40\n",
      "8564/8564 [==============================] - 4s 512us/step - loss: 0.0521 - acc: 0.4801 - val_loss: 0.0528 - val_acc: 0.4448\n",
      "Epoch 24/40\n",
      "8564/8564 [==============================] - 5s 536us/step - loss: 0.0523 - acc: 0.4762 - val_loss: 0.0518 - val_acc: 0.4312\n",
      "Epoch 25/40\n",
      "8564/8564 [==============================] - 5s 530us/step - loss: 0.0522 - acc: 0.4863 - val_loss: 0.0531 - val_acc: 0.4914\n",
      "Epoch 26/40\n",
      "8564/8564 [==============================] - 5s 547us/step - loss: 0.0513 - acc: 0.4853 - val_loss: 0.0525 - val_acc: 0.3899\n",
      "Epoch 27/40\n",
      "8564/8564 [==============================] - 5s 535us/step - loss: 0.0513 - acc: 0.4747 - val_loss: 0.0518 - val_acc: 0.4889\n",
      "Epoch 28/40\n",
      "8564/8564 [==============================] - 5s 533us/step - loss: 0.0512 - acc: 0.5032 - val_loss: 0.0497 - val_acc: 0.4984\n",
      "Epoch 29/40\n",
      "8564/8564 [==============================] - 5s 539us/step - loss: 0.0504 - acc: 0.5042 - val_loss: 0.0517 - val_acc: 0.4250\n",
      "Epoch 30/40\n",
      "8564/8564 [==============================] - 4s 525us/step - loss: 0.0504 - acc: 0.4838 - val_loss: 0.0579 - val_acc: 0.3797\n",
      "Epoch 31/40\n",
      "8564/8564 [==============================] - 4s 521us/step - loss: 0.0501 - acc: 0.4970 - val_loss: 0.0511 - val_acc: 0.4073\n",
      "Epoch 32/40\n",
      "8564/8564 [==============================] - 5s 533us/step - loss: 0.0499 - acc: 0.4803 - val_loss: 0.0516 - val_acc: 0.4053\n",
      "Epoch 33/40\n",
      "8564/8564 [==============================] - 5s 535us/step - loss: 0.0484 - acc: 0.5066 - val_loss: 0.0543 - val_acc: 0.3864\n",
      "Epoch 34/40\n",
      "8564/8564 [==============================] - 4s 523us/step - loss: 0.0489 - acc: 0.5148 - val_loss: 0.0497 - val_acc: 0.4645\n",
      "Epoch 35/40\n",
      "8564/8564 [==============================] - 5s 536us/step - loss: 0.0479 - acc: 0.5164 - val_loss: 0.0500 - val_acc: 0.5290\n",
      "Epoch 36/40\n",
      "8564/8564 [==============================] - 4s 511us/step - loss: 0.0477 - acc: 0.5199 - val_loss: 0.0472 - val_acc: 0.5534\n",
      "Epoch 37/40\n",
      "8564/8564 [==============================] - 5s 534us/step - loss: 0.0479 - acc: 0.5452 - val_loss: 0.0505 - val_acc: 0.4576\n",
      "Epoch 38/40\n",
      "8564/8564 [==============================] - 4s 523us/step - loss: 0.0483 - acc: 0.5377 - val_loss: 0.0471 - val_acc: 0.4309\n",
      "Epoch 39/40\n",
      "8564/8564 [==============================] - 4s 469us/step - loss: 0.0475 - acc: 0.5268 - val_loss: 0.0488 - val_acc: 0.4116\n",
      "Epoch 40/40\n",
      "8564/8564 [==============================] - 4s 502us/step - loss: 0.0464 - acc: 0.5379 - val_loss: 0.0479 - val_acc: 0.5121\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 21/40\n",
      "8564/8564 [==============================] - 4s 506us/step - loss: 0.0424 - acc: 0.7178 - val_loss: 0.0410 - val_acc: 0.7633\n",
      "Epoch 22/40\n",
      "8564/8564 [==============================] - 5s 546us/step - loss: 0.0413 - acc: 0.7409 - val_loss: 0.0412 - val_acc: 0.7490\n",
      "Epoch 23/40\n",
      "8564/8564 [==============================] - 4s 518us/step - loss: 0.0413 - acc: 0.7520 - val_loss: 0.0428 - val_acc: 0.7359\n",
      "Epoch 24/40\n",
      "8564/8564 [==============================] - 4s 509us/step - loss: 0.0408 - acc: 0.7662 - val_loss: 0.0432 - val_acc: 0.7642\n",
      "Epoch 25/40\n",
      "8564/8564 [==============================] - 4s 517us/step - loss: 0.0403 - acc: 0.7753 - val_loss: 0.0408 - val_acc: 0.7640\n",
      "Epoch 26/40\n",
      "8564/8564 [==============================] - 5s 542us/step - loss: 0.0401 - acc: 0.7781 - val_loss: 0.0396 - val_acc: 0.7719\n",
      "Epoch 27/40\n",
      "8564/8564 [==============================] - 4s 519us/step - loss: 0.0398 - acc: 0.7767 - val_loss: 0.0380 - val_acc: 0.7793\n",
      "Epoch 28/40\n",
      "8564/8564 [==============================] - 4s 522us/step - loss: 0.0389 - acc: 0.7898 - val_loss: 0.0430 - val_acc: 0.7393\n",
      "Epoch 29/40\n",
      "8564/8564 [==============================] - 5s 530us/step - loss: 0.0392 - acc: 0.7872 - val_loss: 0.0386 - val_acc: 0.7868\n",
      "Epoch 30/40\n",
      "8564/8564 [==============================] - 4s 524us/step - loss: 0.0393 - acc: 0.7820 - val_loss: 0.0383 - val_acc: 0.7829\n",
      "Epoch 31/40\n",
      "8564/8564 [==============================] - 4s 507us/step - loss: 0.0389 - acc: 0.7858 - val_loss: 0.0410 - val_acc: 0.7747\n",
      "Epoch 32/40\n",
      "8564/8564 [==============================] - 5s 527us/step - loss: 0.0388 - acc: 0.7846 - val_loss: 0.0392 - val_acc: 0.7890\n",
      "Epoch 33/40\n",
      "8564/8564 [==============================] - 4s 511us/step - loss: 0.0386 - acc: 0.7903 - val_loss: 0.0410 - val_acc: 0.7687\n",
      "Epoch 34/40\n",
      "8564/8564 [==============================] - 5s 546us/step - loss: 0.0389 - acc: 0.7842 - val_loss: 0.0395 - val_acc: 0.7909\n",
      "Epoch 35/40\n",
      "8564/8564 [==============================] - 5s 527us/step - loss: 0.0384 - acc: 0.7931 - val_loss: 0.0380 - val_acc: 0.7944\n",
      "Epoch 36/40\n",
      "8564/8564 [==============================] - 5s 533us/step - loss: 0.0386 - acc: 0.7923 - val_loss: 0.0380 - val_acc: 0.7962\n",
      "Epoch 37/40\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 5s 544us/step - loss: 0.0384 - acc: 0.7963 - val_loss: 0.0395 - val_acc: 0.7947\n",
      "Epoch 38/40\n",
      "8564/8564 [==============================] - 4s 515us/step - loss: 0.0380 - acc: 0.7993 - val_loss: 0.0365 - val_acc: 0.8003\n",
      "Epoch 39/40\n",
      "8564/8564 [==============================] - 4s 513us/step - loss: 0.0378 - acc: 0.7998 - val_loss: 0.0364 - val_acc: 0.8024\n",
      "Epoch 40/40\n",
      "8564/8564 [==============================] - 4s 497us/step - loss: 0.0378 - acc: 0.8010 - val_loss: 0.0372 - val_acc: 0.8017\n",
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 21/40\n",
      "8564/8564 [==============================] - 5s 526us/step - loss: 0.0534 - acc: 0.9563 - val_loss: 0.0541 - val_acc: 0.9542\n",
      "Epoch 22/40\n",
      "8564/8564 [==============================] - 5s 538us/step - loss: 0.0544 - acc: 0.9586 - val_loss: 0.0603 - val_acc: 0.9602\n",
      "Epoch 23/40\n",
      "8564/8564 [==============================] - 5s 534us/step - loss: 0.0529 - acc: 0.9589 - val_loss: 0.0744 - val_acc: 0.9519\n",
      "Epoch 24/40\n",
      "8564/8564 [==============================] - 4s 498us/step - loss: 0.0511 - acc: 0.9579 - val_loss: 0.0566 - val_acc: 0.9578\n",
      "Epoch 25/40\n",
      "8564/8564 [==============================] - 5s 526us/step - loss: 0.0522 - acc: 0.9575 - val_loss: 0.0474 - val_acc: 0.9617\n",
      "Epoch 26/40\n",
      "8564/8564 [==============================] - 4s 523us/step - loss: 0.0507 - acc: 0.9586 - val_loss: 0.0471 - val_acc: 0.9607\n",
      "Epoch 27/40\n",
      "8564/8564 [==============================] - 4s 476us/step - loss: 0.0496 - acc: 0.9561 - val_loss: 0.0455 - val_acc: 0.9614\n",
      "Epoch 28/40\n",
      "8564/8564 [==============================] - 5s 536us/step - loss: 0.0491 - acc: 0.9570 - val_loss: 0.0467 - val_acc: 0.9573\n",
      "Epoch 29/40\n",
      "8564/8564 [==============================] - 4s 504us/step - loss: 0.0476 - acc: 0.9611 - val_loss: 0.0522 - val_acc: 0.9640\n",
      "Epoch 30/40\n",
      "8564/8564 [==============================] - 5s 526us/step - loss: 0.0484 - acc: 0.9593 - val_loss: 0.0496 - val_acc: 0.9637\n",
      "Epoch 31/40\n",
      "8564/8564 [==============================] - 4s 523us/step - loss: 0.0489 - acc: 0.9594 - val_loss: 0.0565 - val_acc: 0.9637\n",
      "Epoch 32/40\n",
      "8564/8564 [==============================] - 4s 495us/step - loss: 0.0476 - acc: 0.9608 - val_loss: 0.0472 - val_acc: 0.9652\n",
      "Epoch 33/40\n",
      "8564/8564 [==============================] - 4s 494us/step - loss: 0.0453 - acc: 0.9605 - val_loss: 0.0441 - val_acc: 0.9635\n",
      "Epoch 34/40\n",
      "8564/8564 [==============================] - 4s 514us/step - loss: 0.0470 - acc: 0.9604 - val_loss: 0.0500 - val_acc: 0.9535\n",
      "Epoch 35/40\n",
      "8564/8564 [==============================] - 4s 520us/step - loss: 0.0452 - acc: 0.9605 - val_loss: 0.0487 - val_acc: 0.9595\n",
      "Epoch 36/40\n",
      "8564/8564 [==============================] - 5s 529us/step - loss: 0.0450 - acc: 0.9626 - val_loss: 0.0438 - val_acc: 0.9643\n",
      "Epoch 37/40\n",
      "8564/8564 [==============================] - 4s 506us/step - loss: 0.0458 - acc: 0.9629 - val_loss: 0.0548 - val_acc: 0.9591\n",
      "Epoch 38/40\n",
      "8564/8564 [==============================] - 4s 510us/step - loss: 0.0446 - acc: 0.9635 - val_loss: 0.0517 - val_acc: 0.9615\n",
      "Epoch 39/40\n",
      "8564/8564 [==============================] - 4s 520us/step - loss: 0.0434 - acc: 0.9643 - val_loss: 0.0414 - val_acc: 0.9648\n",
      "Epoch 40/40\n",
      "8564/8564 [==============================] - 5s 543us/step - loss: 0.0450 - acc: 0.9612 - val_loss: 0.0424 - val_acc: 0.9609\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 21/40\n",
      "8564/8564 [==============================] - 4s 499us/step - loss: 0.0639 - acc: 0.8639 - val_loss: 0.0712 - val_acc: 0.8764\n",
      "Epoch 22/40\n",
      "8564/8564 [==============================] - 4s 510us/step - loss: 0.0624 - acc: 0.8648 - val_loss: 0.0657 - val_acc: 0.8672\n",
      "Epoch 23/40\n",
      "8564/8564 [==============================] - 4s 523us/step - loss: 0.0613 - acc: 0.8687 - val_loss: 0.0542 - val_acc: 0.8894\n",
      "Epoch 24/40\n",
      "8564/8564 [==============================] - 5s 537us/step - loss: 0.0583 - acc: 0.8691 - val_loss: 0.0597 - val_acc: 0.8820\n",
      "Epoch 25/40\n",
      "8564/8564 [==============================] - 5s 535us/step - loss: 0.0608 - acc: 0.8693 - val_loss: 0.0602 - val_acc: 0.8768\n",
      "Epoch 26/40\n",
      "8564/8564 [==============================] - 4s 506us/step - loss: 0.0575 - acc: 0.8725 - val_loss: 0.0557 - val_acc: 0.8813\n",
      "Epoch 27/40\n",
      "8564/8564 [==============================] - 4s 522us/step - loss: 0.0580 - acc: 0.8730 - val_loss: 0.0630 - val_acc: 0.8554\n",
      "Epoch 28/40\n",
      "8564/8564 [==============================] - 5s 531us/step - loss: 0.0583 - acc: 0.8753 - val_loss: 0.0576 - val_acc: 0.8866\n",
      "Epoch 29/40\n",
      "8564/8564 [==============================] - 5s 541us/step - loss: 0.0562 - acc: 0.8796 - val_loss: 0.0518 - val_acc: 0.8898\n",
      "Epoch 30/40\n",
      "8564/8564 [==============================] - 5s 532us/step - loss: 0.0567 - acc: 0.8721 - val_loss: 0.0590 - val_acc: 0.8774\n",
      "Epoch 31/40\n",
      "8564/8564 [==============================] - 4s 515us/step - loss: 0.0546 - acc: 0.8796 - val_loss: 0.0557 - val_acc: 0.8968\n",
      "Epoch 32/40\n",
      "8564/8564 [==============================] - 4s 516us/step - loss: 0.0557 - acc: 0.8794 - val_loss: 0.0538 - val_acc: 0.8763\n",
      "Epoch 33/40\n",
      "8564/8564 [==============================] - 5s 550us/step - loss: 0.0558 - acc: 0.8769 - val_loss: 0.0513 - val_acc: 0.9022\n",
      "Epoch 34/40\n",
      "8564/8564 [==============================] - 4s 499us/step - loss: 0.0547 - acc: 0.8787 - val_loss: 0.0580 - val_acc: 0.8924\n",
      "Epoch 35/40\n",
      "8564/8564 [==============================] - 4s 493us/step - loss: 0.0544 - acc: 0.8811 - val_loss: 0.0541 - val_acc: 0.8818\n",
      "Epoch 36/40\n",
      "8564/8564 [==============================] - 4s 490us/step - loss: 0.0536 - acc: 0.8842 - val_loss: 0.0514 - val_acc: 0.8870\n",
      "Epoch 37/40\n",
      "8564/8564 [==============================] - 5s 542us/step - loss: 0.0522 - acc: 0.8805 - val_loss: 0.0489 - val_acc: 0.9026\n",
      "Epoch 38/40\n",
      "8564/8564 [==============================] - 4s 508us/step - loss: 0.0546 - acc: 0.8805 - val_loss: 0.0524 - val_acc: 0.8653\n",
      "Epoch 39/40\n",
      "8564/8564 [==============================] - 4s 524us/step - loss: 0.0521 - acc: 0.8845 - val_loss: 0.0470 - val_acc: 0.9097\n",
      "Epoch 40/40\n",
      "8564/8564 [==============================] - 5s 539us/step - loss: 0.0523 - acc: 0.8838 - val_loss: 0.0466 - val_acc: 0.8987\n",
      "start training round 2\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 41/60\n",
      "8564/8564 [==============================] - 4s 508us/step - loss: 0.0470 - acc: 0.5542 - val_loss: 0.0476 - val_acc: 0.5439\n",
      "Epoch 42/60\n",
      "8564/8564 [==============================] - 5s 531us/step - loss: 0.0461 - acc: 0.5441 - val_loss: 0.0463 - val_acc: 0.4506\n",
      "Epoch 43/60\n",
      "8564/8564 [==============================] - 4s 517us/step - loss: 0.0457 - acc: 0.5395 - val_loss: 0.0476 - val_acc: 0.5822\n",
      "Epoch 44/60\n",
      "8564/8564 [==============================] - 5s 536us/step - loss: 0.0456 - acc: 0.5620 - val_loss: 0.0463 - val_acc: 0.5236\n",
      "Epoch 45/60\n",
      "8564/8564 [==============================] - 4s 520us/step - loss: 0.0453 - acc: 0.5180 - val_loss: 0.0477 - val_acc: 0.4196\n",
      "Epoch 46/60\n",
      "8564/8564 [==============================] - 4s 504us/step - loss: 0.0467 - acc: 0.5638 - val_loss: 0.0473 - val_acc: 0.5275\n",
      "Epoch 47/60\n",
      "8564/8564 [==============================] - 4s 521us/step - loss: 0.0458 - acc: 0.5565 - val_loss: 0.0458 - val_acc: 0.5517\n",
      "Epoch 48/60\n",
      "8564/8564 [==============================] - 5s 556us/step - loss: 0.0458 - acc: 0.5660 - val_loss: 0.0456 - val_acc: 0.5555\n",
      "Epoch 49/60\n",
      "8564/8564 [==============================] - 4s 515us/step - loss: 0.0452 - acc: 0.5563 - val_loss: 0.0447 - val_acc: 0.5361\n",
      "Epoch 50/60\n",
      "8564/8564 [==============================] - 5s 530us/step - loss: 0.0444 - acc: 0.5586 - val_loss: 0.0441 - val_acc: 0.4962\n",
      "Epoch 51/60\n",
      "8564/8564 [==============================] - 5s 554us/step - loss: 0.0437 - acc: 0.5562 - val_loss: 0.0463 - val_acc: 0.4194\n",
      "Epoch 52/60\n",
      "8564/8564 [==============================] - 5s 537us/step - loss: 0.0444 - acc: 0.5577 - val_loss: 0.0463 - val_acc: 0.5692\n",
      "Epoch 53/60\n",
      "8564/8564 [==============================] - 4s 516us/step - loss: 0.0440 - acc: 0.5458 - val_loss: 0.0439 - val_acc: 0.4413\n",
      "Epoch 54/60\n",
      "8564/8564 [==============================] - 5s 540us/step - loss: 0.0434 - acc: 0.5373 - val_loss: 0.0444 - val_acc: 0.4712\n",
      "Epoch 55/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 5s 547us/step - loss: 0.0435 - acc: 0.5490 - val_loss: 0.0424 - val_acc: 0.5388\n",
      "Epoch 56/60\n",
      "8564/8564 [==============================] - 4s 505us/step - loss: 0.0429 - acc: 0.5290 - val_loss: 0.0439 - val_acc: 0.5824\n",
      "Epoch 57/60\n",
      "8564/8564 [==============================] - 5s 528us/step - loss: 0.0439 - acc: 0.5676 - val_loss: 0.0434 - val_acc: 0.5841\n",
      "Epoch 58/60\n",
      "8564/8564 [==============================] - 5s 539us/step - loss: 0.0435 - acc: 0.5837 - val_loss: 0.0475 - val_acc: 0.4112\n",
      "Epoch 59/60\n",
      "8564/8564 [==============================] - 5s 533us/step - loss: 0.0430 - acc: 0.5790 - val_loss: 0.0428 - val_acc: 0.4778\n",
      "Epoch 60/60\n",
      "8564/8564 [==============================] - 4s 500us/step - loss: 0.0426 - acc: 0.5368 - val_loss: 0.0437 - val_acc: 0.4372\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 41/60\n",
      "8564/8564 [==============================] - 5s 539us/step - loss: 0.0380 - acc: 0.8005 - val_loss: 0.0365 - val_acc: 0.8071\n",
      "Epoch 42/60\n",
      "8564/8564 [==============================] - 4s 518us/step - loss: 0.0373 - acc: 0.8032 - val_loss: 0.0388 - val_acc: 0.8006\n",
      "Epoch 43/60\n",
      "8564/8564 [==============================] - 4s 481us/step - loss: 0.0373 - acc: 0.8051 - val_loss: 0.0383 - val_acc: 0.8020\n",
      "Epoch 44/60\n",
      "8564/8564 [==============================] - 4s 522us/step - loss: 0.0368 - acc: 0.8096 - val_loss: 0.0397 - val_acc: 0.7998\n",
      "Epoch 45/60\n",
      "8564/8564 [==============================] - 4s 518us/step - loss: 0.0368 - acc: 0.8119 - val_loss: 0.0381 - val_acc: 0.8048\n",
      "Epoch 46/60\n",
      "8564/8564 [==============================] - 4s 503us/step - loss: 0.0369 - acc: 0.8065 - val_loss: 0.0379 - val_acc: 0.8068\n",
      "Epoch 47/60\n",
      "8564/8564 [==============================] - 5s 527us/step - loss: 0.0368 - acc: 0.8085 - val_loss: 0.0370 - val_acc: 0.8088\n",
      "Epoch 48/60\n",
      "8564/8564 [==============================] - 4s 508us/step - loss: 0.0364 - acc: 0.8082 - val_loss: 0.0390 - val_acc: 0.8071\n",
      "Epoch 49/60\n",
      "8564/8564 [==============================] - 4s 511us/step - loss: 0.0363 - acc: 0.8154 - val_loss: 0.0365 - val_acc: 0.8117\n",
      "Epoch 50/60\n",
      "8564/8564 [==============================] - 5s 535us/step - loss: 0.0361 - acc: 0.8154 - val_loss: 0.0368 - val_acc: 0.8126\n",
      "Epoch 51/60\n",
      "8564/8564 [==============================] - 5s 534us/step - loss: 0.0359 - acc: 0.8163 - val_loss: 0.0358 - val_acc: 0.8149\n",
      "Epoch 52/60\n",
      "8564/8564 [==============================] - 5s 530us/step - loss: 0.0358 - acc: 0.8173 - val_loss: 0.0355 - val_acc: 0.8131\n",
      "Epoch 53/60\n",
      "8564/8564 [==============================] - 4s 515us/step - loss: 0.0359 - acc: 0.8188 - val_loss: 0.0376 - val_acc: 0.8140\n",
      "Epoch 54/60\n",
      "8564/8564 [==============================] - 4s 521us/step - loss: 0.0359 - acc: 0.8162 - val_loss: 0.0369 - val_acc: 0.8159\n",
      "Epoch 55/60\n",
      "8564/8564 [==============================] - 5s 536us/step - loss: 0.0361 - acc: 0.8138 - val_loss: 0.0380 - val_acc: 0.8100\n",
      "Epoch 56/60\n",
      "8564/8564 [==============================] - 4s 522us/step - loss: 0.0357 - acc: 0.8179 - val_loss: 0.0351 - val_acc: 0.8188\n",
      "Epoch 57/60\n",
      "8564/8564 [==============================] - 5s 532us/step - loss: 0.0357 - acc: 0.8152 - val_loss: 0.0373 - val_acc: 0.8160\n",
      "Epoch 58/60\n",
      "8564/8564 [==============================] - 5s 558us/step - loss: 0.0358 - acc: 0.8188 - val_loss: 0.0346 - val_acc: 0.8215\n",
      "Epoch 59/60\n",
      "8564/8564 [==============================] - 4s 522us/step - loss: 0.0352 - acc: 0.8202 - val_loss: 0.0381 - val_acc: 0.8125\n",
      "Epoch 60/60\n",
      "8564/8564 [==============================] - 5s 526us/step - loss: 0.0355 - acc: 0.8216 - val_loss: 0.0368 - val_acc: 0.8178\n",
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 41/60\n",
      "8564/8564 [==============================] - 4s 495us/step - loss: 0.0455 - acc: 0.9618 - val_loss: 0.0443 - val_acc: 0.9667\n",
      "Epoch 42/60\n",
      "8564/8564 [==============================] - 5s 533us/step - loss: 0.0444 - acc: 0.9625 - val_loss: 0.0454 - val_acc: 0.9686\n",
      "Epoch 43/60\n",
      "8564/8564 [==============================] - 5s 527us/step - loss: 0.0430 - acc: 0.9629 - val_loss: 0.0451 - val_acc: 0.9659\n",
      "Epoch 44/60\n",
      "8564/8564 [==============================] - 5s 526us/step - loss: 0.0444 - acc: 0.9642 - val_loss: 0.0460 - val_acc: 0.9598\n",
      "Epoch 45/60\n",
      "8564/8564 [==============================] - 4s 518us/step - loss: 0.0446 - acc: 0.9612 - val_loss: 0.0431 - val_acc: 0.9680\n",
      "Epoch 46/60\n",
      "8564/8564 [==============================] - 4s 519us/step - loss: 0.0434 - acc: 0.9630 - val_loss: 0.0390 - val_acc: 0.9676\n",
      "Epoch 47/60\n",
      "8564/8564 [==============================] - 5s 531us/step - loss: 0.0438 - acc: 0.9641 - val_loss: 0.0387 - val_acc: 0.9684\n",
      "Epoch 48/60\n",
      "8564/8564 [==============================] - 4s 514us/step - loss: 0.0430 - acc: 0.9650 - val_loss: 0.0430 - val_acc: 0.9635\n",
      "Epoch 49/60\n",
      "8564/8564 [==============================] - 4s 520us/step - loss: 0.0434 - acc: 0.9646 - val_loss: 0.0431 - val_acc: 0.9679\n",
      "Epoch 50/60\n",
      "8564/8564 [==============================] - 5s 526us/step - loss: 0.0420 - acc: 0.9634 - val_loss: 0.0430 - val_acc: 0.9537\n",
      "Epoch 51/60\n",
      "8564/8564 [==============================] - 4s 491us/step - loss: 0.0414 - acc: 0.9665 - val_loss: 0.0434 - val_acc: 0.9672\n",
      "Epoch 52/60\n",
      "8564/8564 [==============================] - 4s 492us/step - loss: 0.0425 - acc: 0.9675 - val_loss: 0.0449 - val_acc: 0.9709\n",
      "Epoch 53/60\n",
      "8564/8564 [==============================] - 5s 542us/step - loss: 0.0410 - acc: 0.9643 - val_loss: 0.0402 - val_acc: 0.9659\n",
      "Epoch 54/60\n",
      "8564/8564 [==============================] - 4s 501us/step - loss: 0.0418 - acc: 0.9637 - val_loss: 0.0449 - val_acc: 0.9669\n",
      "Epoch 55/60\n",
      "8564/8564 [==============================] - 4s 501us/step - loss: 0.0411 - acc: 0.9645 - val_loss: 0.0400 - val_acc: 0.9686\n",
      "Epoch 56/60\n",
      "8564/8564 [==============================] - 5s 538us/step - loss: 0.0410 - acc: 0.9643 - val_loss: 0.0397 - val_acc: 0.9716\n",
      "Epoch 57/60\n",
      "8564/8564 [==============================] - 4s 523us/step - loss: 0.0409 - acc: 0.9673 - val_loss: 0.0415 - val_acc: 0.9629\n",
      "Epoch 58/60\n",
      "8564/8564 [==============================] - 4s 501us/step - loss: 0.0419 - acc: 0.9652 - val_loss: 0.0380 - val_acc: 0.9693\n",
      "Epoch 59/60\n",
      "8564/8564 [==============================] - 4s 517us/step - loss: 0.0407 - acc: 0.9647 - val_loss: 0.0389 - val_acc: 0.9677\n",
      "Epoch 60/60\n",
      "8564/8564 [==============================] - 5s 546us/step - loss: 0.0391 - acc: 0.9670 - val_loss: 0.0437 - val_acc: 0.9672\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 41/60\n",
      "8564/8564 [==============================] - 4s 507us/step - loss: 0.0526 - acc: 0.8818 - val_loss: 0.0580 - val_acc: 0.8863\n",
      "Epoch 42/60\n",
      "8564/8564 [==============================] - 5s 542us/step - loss: 0.0523 - acc: 0.8841 - val_loss: 0.0516 - val_acc: 0.8921\n",
      "Epoch 43/60\n",
      "8564/8564 [==============================] - 5s 528us/step - loss: 0.0531 - acc: 0.8824 - val_loss: 0.0544 - val_acc: 0.8882\n",
      "Epoch 44/60\n",
      "8564/8564 [==============================] - 5s 540us/step - loss: 0.0521 - acc: 0.8864 - val_loss: 0.0586 - val_acc: 0.8997\n",
      "Epoch 45/60\n",
      "8564/8564 [==============================] - 5s 540us/step - loss: 0.0537 - acc: 0.8882 - val_loss: 0.0482 - val_acc: 0.9051\n",
      "Epoch 46/60\n",
      "8564/8564 [==============================] - 5s 529us/step - loss: 0.0520 - acc: 0.8870 - val_loss: 0.0472 - val_acc: 0.9076\n",
      "Epoch 47/60\n",
      "8564/8564 [==============================] - 4s 509us/step - loss: 0.0493 - acc: 0.8917 - val_loss: 0.0489 - val_acc: 0.8827\n",
      "Epoch 48/60\n",
      "8564/8564 [==============================] - 5s 568us/step - loss: 0.0507 - acc: 0.8859 - val_loss: 0.0476 - val_acc: 0.9039\n",
      "Epoch 49/60\n",
      "8564/8564 [==============================] - 5s 548us/step - loss: 0.0522 - acc: 0.8827 - val_loss: 0.0519 - val_acc: 0.9017\n",
      "Epoch 50/60\n",
      "8564/8564 [==============================] - 5s 554us/step - loss: 0.0502 - acc: 0.8869 - val_loss: 0.0489 - val_acc: 0.9005\n",
      "Epoch 51/60\n",
      "8564/8564 [==============================] - 5s 578us/step - loss: 0.0494 - acc: 0.8926 - val_loss: 0.0484 - val_acc: 0.9164\n",
      "Epoch 52/60\n",
      "8564/8564 [==============================] - 4s 522us/step - loss: 0.0497 - acc: 0.8913 - val_loss: 0.0479 - val_acc: 0.9055\n",
      "Epoch 53/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 5s 570us/step - loss: 0.0481 - acc: 0.8910 - val_loss: 0.0459 - val_acc: 0.9112\n",
      "Epoch 54/60\n",
      "8564/8564 [==============================] - 5s 614us/step - loss: 0.0502 - acc: 0.8858 - val_loss: 0.0474 - val_acc: 0.8988\n",
      "Epoch 55/60\n",
      "8564/8564 [==============================] - 5s 581us/step - loss: 0.0492 - acc: 0.8900 - val_loss: 0.0529 - val_acc: 0.8893\n",
      "Epoch 56/60\n",
      "8564/8564 [==============================] - 5s 615us/step - loss: 0.0494 - acc: 0.8908 - val_loss: 0.0440 - val_acc: 0.8965\n",
      "Epoch 57/60\n",
      "8564/8564 [==============================] - 5s 615us/step - loss: 0.0477 - acc: 0.8922 - val_loss: 0.0452 - val_acc: 0.9107\n",
      "Epoch 58/60\n",
      "8564/8564 [==============================] - 5s 585us/step - loss: 0.0478 - acc: 0.8915 - val_loss: 0.0458 - val_acc: 0.9100\n",
      "Epoch 59/60\n",
      "8564/8564 [==============================] - 5s 587us/step - loss: 0.0479 - acc: 0.8954 - val_loss: 0.0448 - val_acc: 0.8947\n",
      "Epoch 60/60\n",
      "8564/8564 [==============================] - 5s 603us/step - loss: 0.0470 - acc: 0.8955 - val_loss: 0.0481 - val_acc: 0.9166\n",
      "start training round 3\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 61/80\n",
      "8564/8564 [==============================] - 5s 613us/step - loss: 0.0427 - acc: 0.5582 - val_loss: 0.0432 - val_acc: 0.4578\n",
      "Epoch 62/80\n",
      "8564/8564 [==============================] - 5s 603us/step - loss: 0.0417 - acc: 0.5469 - val_loss: 0.0431 - val_acc: 0.4661\n",
      "Epoch 63/80\n",
      "8564/8564 [==============================] - 5s 567us/step - loss: 0.0420 - acc: 0.5574 - val_loss: 0.0453 - val_acc: 0.4280\n",
      "Epoch 64/80\n",
      "8564/8564 [==============================] - 5s 577us/step - loss: 0.0426 - acc: 0.5875 - val_loss: 0.0435 - val_acc: 0.5983\n",
      "Epoch 65/80\n",
      "8564/8564 [==============================] - 5s 610us/step - loss: 0.0419 - acc: 0.5570 - val_loss: 0.0442 - val_acc: 0.5867\n",
      "Epoch 66/80\n",
      "8564/8564 [==============================] - 5s 630us/step - loss: 0.0421 - acc: 0.5854 - val_loss: 0.0424 - val_acc: 0.5436\n",
      "Epoch 67/80\n",
      "8564/8564 [==============================] - 5s 595us/step - loss: 0.0409 - acc: 0.5793 - val_loss: 0.0409 - val_acc: 0.5869\n",
      "Epoch 68/80\n",
      "8564/8564 [==============================] - 5s 609us/step - loss: 0.0416 - acc: 0.5737 - val_loss: 0.0430 - val_acc: 0.5682\n",
      "Epoch 69/80\n",
      "8564/8564 [==============================] - 5s 588us/step - loss: 0.0420 - acc: 0.5823 - val_loss: 0.0427 - val_acc: 0.5992\n",
      "Epoch 70/80\n",
      "8564/8564 [==============================] - 5s 603us/step - loss: 0.0418 - acc: 0.5880 - val_loss: 0.0414 - val_acc: 0.5779\n",
      "Epoch 71/80\n",
      "8564/8564 [==============================] - 5s 592us/step - loss: 0.0409 - acc: 0.5667 - val_loss: 0.0417 - val_acc: 0.5943\n",
      "Epoch 72/80\n",
      "8564/8564 [==============================] - 5s 592us/step - loss: 0.0406 - acc: 0.5667 - val_loss: 0.0415 - val_acc: 0.4452\n",
      "Epoch 73/80\n",
      "8564/8564 [==============================] - 5s 603us/step - loss: 0.0410 - acc: 0.5714 - val_loss: 0.0429 - val_acc: 0.6101\n",
      "Epoch 74/80\n",
      "8564/8564 [==============================] - 5s 609us/step - loss: 0.0411 - acc: 0.5917 - val_loss: 0.0417 - val_acc: 0.6056\n",
      "Epoch 75/80\n",
      "8564/8564 [==============================] - 5s 605us/step - loss: 0.0413 - acc: 0.5925 - val_loss: 0.0413 - val_acc: 0.5997\n",
      "Epoch 76/80\n",
      "8564/8564 [==============================] - 5s 572us/step - loss: 0.0402 - acc: 0.5778 - val_loss: 0.0408 - val_acc: 0.5625\n",
      "Epoch 77/80\n",
      "8564/8564 [==============================] - 5s 568us/step - loss: 0.0400 - acc: 0.5773 - val_loss: 0.0416 - val_acc: 0.6069\n",
      "Epoch 78/80\n",
      "8564/8564 [==============================] - 5s 549us/step - loss: 0.0403 - acc: 0.5825 - val_loss: 0.0418 - val_acc: 0.4508\n",
      "Epoch 79/80\n",
      "8564/8564 [==============================] - 4s 512us/step - loss: 0.0402 - acc: 0.5807 - val_loss: 0.0400 - val_acc: 0.6058\n",
      "Epoch 80/80\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0397 - acc: 0.5834 - val_loss: 0.0393 - val_acc: 0.5707\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 61/80\n",
      "8564/8564 [==============================] - 5s 566us/step - loss: 0.0352 - acc: 0.8228 - val_loss: 0.0373 - val_acc: 0.8173\n",
      "Epoch 62/80\n",
      "8564/8564 [==============================] - 5s 603us/step - loss: 0.0354 - acc: 0.8216 - val_loss: 0.0362 - val_acc: 0.8198\n",
      "Epoch 63/80\n",
      "8564/8564 [==============================] - 5s 570us/step - loss: 0.0352 - acc: 0.8214 - val_loss: 0.0353 - val_acc: 0.8230\n",
      "Epoch 64/80\n",
      "8564/8564 [==============================] - 5s 593us/step - loss: 0.0342 - acc: 0.8273 - val_loss: 0.0340 - val_acc: 0.8302\n",
      "Epoch 65/80\n",
      "8564/8564 [==============================] - 5s 611us/step - loss: 0.0345 - acc: 0.8310 - val_loss: 0.0363 - val_acc: 0.8307\n",
      "Epoch 66/80\n",
      "8564/8564 [==============================] - 5s 614us/step - loss: 0.0349 - acc: 0.8327 - val_loss: 0.0374 - val_acc: 0.8298\n",
      "Epoch 67/80\n",
      "8564/8564 [==============================] - 5s 617us/step - loss: 0.0348 - acc: 0.8320 - val_loss: 0.0353 - val_acc: 0.8319\n",
      "Epoch 68/80\n",
      "8564/8564 [==============================] - 5s 571us/step - loss: 0.0340 - acc: 0.8321 - val_loss: 0.0343 - val_acc: 0.8352\n",
      "Epoch 69/80\n",
      "8564/8564 [==============================] - 5s 615us/step - loss: 0.0347 - acc: 0.8331 - val_loss: 0.0339 - val_acc: 0.8339\n",
      "Epoch 70/80\n",
      "8564/8564 [==============================] - 5s 593us/step - loss: 0.0341 - acc: 0.8336 - val_loss: 0.0349 - val_acc: 0.8327\n",
      "Epoch 71/80\n",
      "8564/8564 [==============================] - 5s 561us/step - loss: 0.0339 - acc: 0.8337 - val_loss: 0.0336 - val_acc: 0.8394\n",
      "Epoch 72/80\n",
      "8564/8564 [==============================] - 5s 611us/step - loss: 0.0343 - acc: 0.8239 - val_loss: 0.0355 - val_acc: 0.8350\n",
      "Epoch 73/80\n",
      "8564/8564 [==============================] - 5s 614us/step - loss: 0.0342 - acc: 0.8309 - val_loss: 0.0346 - val_acc: 0.8376\n",
      "Epoch 74/80\n",
      "8564/8564 [==============================] - 5s 600us/step - loss: 0.0341 - acc: 0.8277 - val_loss: 0.0379 - val_acc: 0.8184\n",
      "Epoch 75/80\n",
      "8564/8564 [==============================] - 5s 602us/step - loss: 0.0341 - acc: 0.8271 - val_loss: 0.0345 - val_acc: 0.8363\n",
      "Epoch 76/80\n",
      "8564/8564 [==============================] - 5s 590us/step - loss: 0.0342 - acc: 0.8283 - val_loss: 0.0342 - val_acc: 0.8375\n",
      "Epoch 77/80\n",
      "8564/8564 [==============================] - 5s 575us/step - loss: 0.0336 - acc: 0.8324 - val_loss: 0.0347 - val_acc: 0.8428\n",
      "Epoch 78/80\n",
      "8564/8564 [==============================] - 5s 559us/step - loss: 0.0338 - acc: 0.8325 - val_loss: 0.0327 - val_acc: 0.8428\n",
      "Epoch 79/80\n",
      "8564/8564 [==============================] - 5s 603us/step - loss: 0.0335 - acc: 0.8316 - val_loss: 0.0356 - val_acc: 0.8389\n",
      "Epoch 80/80\n",
      "8564/8564 [==============================] - 5s 613us/step - loss: 0.0336 - acc: 0.8317 - val_loss: 0.0360 - val_acc: 0.8385\n",
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 61/80\n",
      "8564/8564 [==============================] - 5s 590us/step - loss: 0.0401 - acc: 0.9677 - val_loss: 0.0368 - val_acc: 0.9692\n",
      "Epoch 62/80\n",
      "8564/8564 [==============================] - 5s 617us/step - loss: 0.0394 - acc: 0.9682 - val_loss: 0.0359 - val_acc: 0.9715\n",
      "Epoch 63/80\n",
      "8564/8564 [==============================] - 5s 598us/step - loss: 0.0396 - acc: 0.9679 - val_loss: 0.0480 - val_acc: 0.9639\n",
      "Epoch 64/80\n",
      "8564/8564 [==============================] - 5s 611us/step - loss: 0.0392 - acc: 0.9658 - val_loss: 0.0446 - val_acc: 0.9690\n",
      "Epoch 65/80\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0382 - acc: 0.9676 - val_loss: 0.0385 - val_acc: 0.9717\n",
      "Epoch 66/80\n",
      "8564/8564 [==============================] - 4s 502us/step - loss: 0.0403 - acc: 0.9690 - val_loss: 0.0469 - val_acc: 0.9567\n",
      "Epoch 67/80\n",
      "8564/8564 [==============================] - 5s 555us/step - loss: 0.0428 - acc: 0.9641 - val_loss: 0.0403 - val_acc: 0.9710\n",
      "Epoch 68/80\n",
      "8564/8564 [==============================] - 5s 541us/step - loss: 0.0386 - acc: 0.9668 - val_loss: 0.0396 - val_acc: 0.9713\n",
      "Epoch 69/80\n",
      "8564/8564 [==============================] - 5s 576us/step - loss: 0.0396 - acc: 0.9673 - val_loss: 0.0448 - val_acc: 0.9545\n",
      "Epoch 70/80\n",
      "8564/8564 [==============================] - 5s 616us/step - loss: 0.0393 - acc: 0.9672 - val_loss: 0.0415 - val_acc: 0.9697\n",
      "Epoch 71/80\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 5s 623us/step - loss: 0.0393 - acc: 0.9682 - val_loss: 0.0349 - val_acc: 0.9733\n",
      "Epoch 72/80\n",
      "8564/8564 [==============================] - 5s 608us/step - loss: 0.0382 - acc: 0.9690 - val_loss: 0.0357 - val_acc: 0.9664\n",
      "Epoch 73/80\n",
      "8564/8564 [==============================] - 5s 619us/step - loss: 0.0378 - acc: 0.9677 - val_loss: 0.0351 - val_acc: 0.9687\n",
      "Epoch 74/80\n",
      "8564/8564 [==============================] - 5s 599us/step - loss: 0.0381 - acc: 0.9677 - val_loss: 0.0365 - val_acc: 0.9730\n",
      "Epoch 75/80\n",
      "8564/8564 [==============================] - 5s 624us/step - loss: 0.0377 - acc: 0.9681 - val_loss: 0.0422 - val_acc: 0.9687\n",
      "Epoch 76/80\n",
      "8564/8564 [==============================] - 5s 605us/step - loss: 0.0388 - acc: 0.9699 - val_loss: 0.0390 - val_acc: 0.9727\n",
      "Epoch 77/80\n",
      "8564/8564 [==============================] - 5s 586us/step - loss: 0.0369 - acc: 0.9687 - val_loss: 0.0342 - val_acc: 0.9748\n",
      "Epoch 78/80\n",
      "8564/8564 [==============================] - 5s 608us/step - loss: 0.0378 - acc: 0.9701 - val_loss: 0.0440 - val_acc: 0.9575\n",
      "Epoch 79/80\n",
      "8564/8564 [==============================] - 5s 605us/step - loss: 0.0385 - acc: 0.9671 - val_loss: 0.0433 - val_acc: 0.9699\n",
      "Epoch 80/80\n",
      "8564/8564 [==============================] - 5s 598us/step - loss: 0.0367 - acc: 0.9697 - val_loss: 0.0359 - val_acc: 0.9736\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 61/80\n",
      "8564/8564 [==============================] - 5s 607us/step - loss: 0.0473 - acc: 0.8951 - val_loss: 0.0433 - val_acc: 0.9013\n",
      "Epoch 62/80\n",
      "8564/8564 [==============================] - 5s 602us/step - loss: 0.0474 - acc: 0.8934 - val_loss: 0.0457 - val_acc: 0.8852\n",
      "Epoch 63/80\n",
      "8564/8564 [==============================] - 5s 625us/step - loss: 0.0474 - acc: 0.8964 - val_loss: 0.0451 - val_acc: 0.9105\n",
      "Epoch 64/80\n",
      "8564/8564 [==============================] - 5s 605us/step - loss: 0.0460 - acc: 0.8939 - val_loss: 0.0483 - val_acc: 0.9099\n",
      "Epoch 65/80\n",
      "8564/8564 [==============================] - 5s 606us/step - loss: 0.0465 - acc: 0.8981 - val_loss: 0.0446 - val_acc: 0.9102\n",
      "Epoch 66/80\n",
      "8564/8564 [==============================] - 5s 635us/step - loss: 0.0463 - acc: 0.8937 - val_loss: 0.0441 - val_acc: 0.9077\n",
      "Epoch 67/80\n",
      "8564/8564 [==============================] - 5s 608us/step - loss: 0.0459 - acc: 0.9000 - val_loss: 0.0471 - val_acc: 0.9017\n",
      "Epoch 68/80\n",
      "8564/8564 [==============================] - 5s 613us/step - loss: 0.0459 - acc: 0.9015 - val_loss: 0.0468 - val_acc: 0.9096\n",
      "Epoch 69/80\n",
      "8564/8564 [==============================] - 5s 619us/step - loss: 0.0467 - acc: 0.8954 - val_loss: 0.0479 - val_acc: 0.9035\n",
      "Epoch 70/80\n",
      "8564/8564 [==============================] - 5s 570us/step - loss: 0.0451 - acc: 0.8994 - val_loss: 0.0507 - val_acc: 0.9104\n",
      "Epoch 71/80\n",
      "8564/8564 [==============================] - 5s 596us/step - loss: 0.0481 - acc: 0.8926 - val_loss: 0.0430 - val_acc: 0.9065\n",
      "Epoch 72/80\n",
      "8564/8564 [==============================] - 5s 623us/step - loss: 0.0459 - acc: 0.8933 - val_loss: 0.0440 - val_acc: 0.9150\n",
      "Epoch 73/80\n",
      "8564/8564 [==============================] - 4s 525us/step - loss: 0.0456 - acc: 0.8974 - val_loss: 0.0419 - val_acc: 0.9095\n",
      "Epoch 74/80\n",
      "8564/8564 [==============================] - 4s 522us/step - loss: 0.0461 - acc: 0.8999 - val_loss: 0.0428 - val_acc: 0.8976\n",
      "Epoch 75/80\n",
      "8564/8564 [==============================] - 5s 558us/step - loss: 0.0436 - acc: 0.8998 - val_loss: 0.0422 - val_acc: 0.9183\n",
      "Epoch 76/80\n",
      "8564/8564 [==============================] - 5s 565us/step - loss: 0.0456 - acc: 0.8933 - val_loss: 0.0453 - val_acc: 0.9040\n",
      "Epoch 77/80\n",
      "8564/8564 [==============================] - 5s 600us/step - loss: 0.0447 - acc: 0.8975 - val_loss: 0.0518 - val_acc: 0.9138\n",
      "Epoch 78/80\n",
      "8564/8564 [==============================] - 5s 586us/step - loss: 0.0461 - acc: 0.8984 - val_loss: 0.0446 - val_acc: 0.9128\n",
      "Epoch 79/80\n",
      "8564/8564 [==============================] - 5s 597us/step - loss: 0.0445 - acc: 0.8995 - val_loss: 0.0439 - val_acc: 0.8898\n",
      "Epoch 80/80\n",
      "8564/8564 [==============================] - 6s 663us/step - loss: 0.0472 - acc: 0.8951 - val_loss: 0.0471 - val_acc: 0.9100\n",
      "start training round 4\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 81/100\n",
      "8564/8564 [==============================] - 6s 649us/step - loss: 0.0400 - acc: 0.5543 - val_loss: 0.0443 - val_acc: 0.4605\n",
      "Epoch 82/100\n",
      "8564/8564 [==============================] - 5s 598us/step - loss: 0.0399 - acc: 0.5693 - val_loss: 0.0437 - val_acc: 0.4319\n",
      "Epoch 83/100\n",
      "8564/8564 [==============================] - 5s 614us/step - loss: 0.0402 - acc: 0.5803 - val_loss: 0.0402 - val_acc: 0.5746\n",
      "Epoch 84/100\n",
      "8564/8564 [==============================] - 5s 605us/step - loss: 0.0404 - acc: 0.5997 - val_loss: 0.0403 - val_acc: 0.6055\n",
      "Epoch 85/100\n",
      "8564/8564 [==============================] - 5s 595us/step - loss: 0.0403 - acc: 0.6035 - val_loss: 0.0402 - val_acc: 0.4736\n",
      "Epoch 86/100\n",
      "8564/8564 [==============================] - 5s 594us/step - loss: 0.0392 - acc: 0.5886 - val_loss: 0.0395 - val_acc: 0.6059\n",
      "Epoch 87/100\n",
      "8564/8564 [==============================] - 5s 627us/step - loss: 0.0397 - acc: 0.5979 - val_loss: 0.0400 - val_acc: 0.6053\n",
      "Epoch 88/100\n",
      "8564/8564 [==============================] - 5s 608us/step - loss: 0.0391 - acc: 0.5540 - val_loss: 0.0400 - val_acc: 0.4749\n",
      "Epoch 89/100\n",
      "8564/8564 [==============================] - 5s 609us/step - loss: 0.0391 - acc: 0.5677 - val_loss: 0.0391 - val_acc: 0.5690\n",
      "Epoch 90/100\n",
      "8564/8564 [==============================] - 5s 601us/step - loss: 0.0389 - acc: 0.5670 - val_loss: 0.0388 - val_acc: 0.5254\n",
      "Epoch 91/100\n",
      "8564/8564 [==============================] - 5s 598us/step - loss: 0.0394 - acc: 0.5800 - val_loss: 0.0391 - val_acc: 0.6063\n",
      "Epoch 92/100\n",
      "8564/8564 [==============================] - 5s 585us/step - loss: 0.0389 - acc: 0.5540 - val_loss: 0.0390 - val_acc: 0.4858\n",
      "Epoch 93/100\n",
      "8564/8564 [==============================] - 5s 582us/step - loss: 0.0390 - acc: 0.5491 - val_loss: 0.0422 - val_acc: 0.4444\n",
      "Epoch 94/100\n",
      "8564/8564 [==============================] - 5s 619us/step - loss: 0.0387 - acc: 0.5619 - val_loss: 0.0390 - val_acc: 0.4766\n",
      "Epoch 95/100\n",
      "8564/8564 [==============================] - 5s 588us/step - loss: 0.0386 - acc: 0.5680 - val_loss: 0.0395 - val_acc: 0.4880\n",
      "Epoch 96/100\n",
      "8564/8564 [==============================] - 5s 619us/step - loss: 0.0390 - acc: 0.5586 - val_loss: 0.0392 - val_acc: 0.6230\n",
      "Epoch 97/100\n",
      "8564/8564 [==============================] - 5s 616us/step - loss: 0.0384 - acc: 0.5954 - val_loss: 0.0393 - val_acc: 0.6192\n",
      "Epoch 98/100\n",
      "8564/8564 [==============================] - 5s 638us/step - loss: 0.0384 - acc: 0.5923 - val_loss: 0.0390 - val_acc: 0.5929\n",
      "Epoch 99/100\n",
      "8564/8564 [==============================] - 5s 639us/step - loss: 0.0381 - acc: 0.5762 - val_loss: 0.0383 - val_acc: 0.5728\n",
      "Epoch 100/100\n",
      "8564/8564 [==============================] - 6s 691us/step - loss: 0.0383 - acc: 0.6007 - val_loss: 0.0400 - val_acc: 0.6233\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 81/100\n",
      "8564/8564 [==============================] - 5s 544us/step - loss: 0.0335 - acc: 0.8329 - val_loss: 0.0350 - val_acc: 0.8431\n",
      "Epoch 82/100\n",
      "8564/8564 [==============================] - 5s 529us/step - loss: 0.0335 - acc: 0.8368 - val_loss: 0.0336 - val_acc: 0.8440\n",
      "Epoch 83/100\n",
      "8564/8564 [==============================] - 4s 524us/step - loss: 0.0331 - acc: 0.8365 - val_loss: 0.0332 - val_acc: 0.8442\n",
      "Epoch 84/100\n",
      "8564/8564 [==============================] - 4s 500us/step - loss: 0.0332 - acc: 0.8362 - val_loss: 0.0337 - val_acc: 0.8376\n",
      "Epoch 85/100\n",
      "8564/8564 [==============================] - 5s 546us/step - loss: 0.0331 - acc: 0.8346 - val_loss: 0.0326 - val_acc: 0.8426\n",
      "Epoch 86/100\n",
      "8564/8564 [==============================] - 5s 531us/step - loss: 0.0336 - acc: 0.8314 - val_loss: 0.0359 - val_acc: 0.8337\n",
      "Epoch 87/100\n",
      "8564/8564 [==============================] - 5s 530us/step - loss: 0.0333 - acc: 0.8339 - val_loss: 0.0349 - val_acc: 0.8393\n",
      "Epoch 88/100\n",
      "8564/8564 [==============================] - 5s 534us/step - loss: 0.0328 - acc: 0.8350 - val_loss: 0.0356 - val_acc: 0.8426\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 89/100\n",
      "8564/8564 [==============================] - 5s 557us/step - loss: 0.0329 - acc: 0.8367 - val_loss: 0.0330 - val_acc: 0.8428\n",
      "Epoch 90/100\n",
      "8564/8564 [==============================] - 4s 499us/step - loss: 0.0328 - acc: 0.8383 - val_loss: 0.0348 - val_acc: 0.8459\n",
      "Epoch 91/100\n",
      "8564/8564 [==============================] - 5s 559us/step - loss: 0.0328 - acc: 0.8392 - val_loss: 0.0326 - val_acc: 0.8426\n",
      "Epoch 92/100\n",
      "8564/8564 [==============================] - 5s 536us/step - loss: 0.0326 - acc: 0.8387 - val_loss: 0.0335 - val_acc: 0.8469\n",
      "Epoch 93/100\n",
      "8564/8564 [==============================] - 4s 509us/step - loss: 0.0327 - acc: 0.8422 - val_loss: 0.0352 - val_acc: 0.8394\n",
      "Epoch 94/100\n",
      "8564/8564 [==============================] - 5s 533us/step - loss: 0.0328 - acc: 0.8432 - val_loss: 0.0341 - val_acc: 0.8410\n",
      "Epoch 95/100\n",
      "8564/8564 [==============================] - 4s 507us/step - loss: 0.0327 - acc: 0.8428 - val_loss: 0.0342 - val_acc: 0.8406\n",
      "Epoch 96/100\n",
      "8564/8564 [==============================] - 5s 536us/step - loss: 0.0327 - acc: 0.8440 - val_loss: 0.0327 - val_acc: 0.8408\n",
      "Epoch 97/100\n",
      "8564/8564 [==============================] - 5s 566us/step - loss: 0.0325 - acc: 0.8432 - val_loss: 0.0327 - val_acc: 0.8413\n",
      "Epoch 98/100\n",
      "8564/8564 [==============================] - 5s 582us/step - loss: 0.0319 - acc: 0.8443 - val_loss: 0.0336 - val_acc: 0.8405\n",
      "Epoch 99/100\n",
      "8564/8564 [==============================] - 5s 533us/step - loss: 0.0325 - acc: 0.8423 - val_loss: 0.0334 - val_acc: 0.8399\n",
      "Epoch 100/100\n",
      "8564/8564 [==============================] - 4s 516us/step - loss: 0.0325 - acc: 0.8437 - val_loss: 0.0328 - val_acc: 0.8432\n",
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 81/100\n",
      "8564/8564 [==============================] - 5s 542us/step - loss: 0.0368 - acc: 0.9706 - val_loss: 0.0389 - val_acc: 0.9750\n",
      "Epoch 82/100\n",
      "8564/8564 [==============================] - 5s 535us/step - loss: 0.0367 - acc: 0.9710 - val_loss: 0.0369 - val_acc: 0.9743\n",
      "Epoch 83/100\n",
      "8564/8564 [==============================] - 5s 614us/step - loss: 0.0379 - acc: 0.9689 - val_loss: 0.0374 - val_acc: 0.9712\n",
      "Epoch 84/100\n",
      "8564/8564 [==============================] - 5s 633us/step - loss: 0.0369 - acc: 0.9664 - val_loss: 0.0381 - val_acc: 0.9741\n",
      "Epoch 85/100\n",
      "8564/8564 [==============================] - 5s 599us/step - loss: 0.0377 - acc: 0.9676 - val_loss: 0.0370 - val_acc: 0.9728\n",
      "Epoch 86/100\n",
      "8564/8564 [==============================] - 5s 627us/step - loss: 0.0375 - acc: 0.9687 - val_loss: 0.0335 - val_acc: 0.9740\n",
      "Epoch 87/100\n",
      "8564/8564 [==============================] - 5s 598us/step - loss: 0.0383 - acc: 0.9668 - val_loss: 0.0389 - val_acc: 0.9747\n",
      "Epoch 88/100\n",
      "8564/8564 [==============================] - 5s 603us/step - loss: 0.0380 - acc: 0.9695 - val_loss: 0.0328 - val_acc: 0.9705\n",
      "Epoch 89/100\n",
      "8564/8564 [==============================] - 5s 625us/step - loss: 0.0358 - acc: 0.9709 - val_loss: 0.0353 - val_acc: 0.9742\n",
      "Epoch 90/100\n",
      "8564/8564 [==============================] - 5s 607us/step - loss: 0.0370 - acc: 0.9724 - val_loss: 0.0350 - val_acc: 0.9748\n",
      "Epoch 91/100\n",
      "8564/8564 [==============================] - 5s 612us/step - loss: 0.0371 - acc: 0.9694 - val_loss: 0.0295 - val_acc: 0.9758\n",
      "Epoch 92/100\n",
      "8564/8564 [==============================] - 5s 603us/step - loss: 0.0345 - acc: 0.9724 - val_loss: 0.0336 - val_acc: 0.9708\n",
      "Epoch 93/100\n",
      "8564/8564 [==============================] - 5s 588us/step - loss: 0.0370 - acc: 0.9695 - val_loss: 0.0387 - val_acc: 0.9623\n",
      "Epoch 94/100\n",
      "8564/8564 [==============================] - 5s 609us/step - loss: 0.0363 - acc: 0.9703 - val_loss: 0.0314 - val_acc: 0.9751\n",
      "Epoch 95/100\n",
      "8564/8564 [==============================] - 5s 603us/step - loss: 0.0378 - acc: 0.9706 - val_loss: 0.0334 - val_acc: 0.9731\n",
      "Epoch 96/100\n",
      "8564/8564 [==============================] - 5s 609us/step - loss: 0.0354 - acc: 0.9701 - val_loss: 0.0426 - val_acc: 0.9669\n",
      "Epoch 97/100\n",
      "8564/8564 [==============================] - 5s 581us/step - loss: 0.0359 - acc: 0.9713 - val_loss: 0.0354 - val_acc: 0.9684\n",
      "Epoch 98/100\n",
      "8564/8564 [==============================] - 5s 554us/step - loss: 0.0354 - acc: 0.9724 - val_loss: 0.0293 - val_acc: 0.9770\n",
      "Epoch 99/100\n",
      "8564/8564 [==============================] - 5s 571us/step - loss: 0.0350 - acc: 0.9741 - val_loss: 0.0456 - val_acc: 0.9570\n",
      "Epoch 100/100\n",
      "8564/8564 [==============================] - 5s 600us/step - loss: 0.0342 - acc: 0.9721 - val_loss: 0.0361 - val_acc: 0.9782\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 81/100\n",
      "8564/8564 [==============================] - 5s 606us/step - loss: 0.0436 - acc: 0.9000 - val_loss: 0.0470 - val_acc: 0.9196\n",
      "Epoch 82/100\n",
      "8564/8564 [==============================] - 5s 592us/step - loss: 0.0439 - acc: 0.9000 - val_loss: 0.0451 - val_acc: 0.9149\n",
      "Epoch 83/100\n",
      "8564/8564 [==============================] - 5s 586us/step - loss: 0.0444 - acc: 0.8992 - val_loss: 0.0404 - val_acc: 0.9081\n",
      "Epoch 84/100\n",
      "8564/8564 [==============================] - 5s 592us/step - loss: 0.0435 - acc: 0.9013 - val_loss: 0.0398 - val_acc: 0.9150\n",
      "Epoch 85/100\n",
      "8564/8564 [==============================] - 5s 622us/step - loss: 0.0433 - acc: 0.9009 - val_loss: 0.0415 - val_acc: 0.9196\n",
      "Epoch 86/100\n",
      "8564/8564 [==============================] - 5s 603us/step - loss: 0.0448 - acc: 0.9031 - val_loss: 0.0449 - val_acc: 0.8764\n",
      "Epoch 87/100\n",
      "8564/8564 [==============================] - 5s 581us/step - loss: 0.0430 - acc: 0.9012 - val_loss: 0.0485 - val_acc: 0.8907\n",
      "Epoch 88/100\n",
      "8564/8564 [==============================] - 5s 580us/step - loss: 0.0424 - acc: 0.9059 - val_loss: 0.0394 - val_acc: 0.9193\n",
      "Epoch 89/100\n",
      "8564/8564 [==============================] - 5s 633us/step - loss: 0.0425 - acc: 0.9033 - val_loss: 0.0372 - val_acc: 0.9191\n",
      "Epoch 90/100\n",
      "8564/8564 [==============================] - 5s 628us/step - loss: 0.0444 - acc: 0.8985 - val_loss: 0.0385 - val_acc: 0.9172\n",
      "Epoch 91/100\n",
      "8564/8564 [==============================] - 5s 609us/step - loss: 0.0448 - acc: 0.8985 - val_loss: 0.0391 - val_acc: 0.9232\n",
      "Epoch 92/100\n",
      "8564/8564 [==============================] - 5s 601us/step - loss: 0.0425 - acc: 0.9022 - val_loss: 0.0401 - val_acc: 0.9207\n",
      "Epoch 93/100\n",
      "8564/8564 [==============================] - 5s 601us/step - loss: 0.0442 - acc: 0.9003 - val_loss: 0.0433 - val_acc: 0.9117\n",
      "Epoch 94/100\n",
      "8564/8564 [==============================] - 5s 598us/step - loss: 0.0440 - acc: 0.9017 - val_loss: 0.0370 - val_acc: 0.9163\n",
      "Epoch 95/100\n",
      "8564/8564 [==============================] - 5s 609us/step - loss: 0.0424 - acc: 0.9013 - val_loss: 0.0429 - val_acc: 0.9017\n",
      "Epoch 96/100\n",
      "8564/8564 [==============================] - 5s 613us/step - loss: 0.0429 - acc: 0.9010 - val_loss: 0.0429 - val_acc: 0.9013\n",
      "Epoch 97/100\n",
      "8564/8564 [==============================] - 5s 600us/step - loss: 0.0420 - acc: 0.9017 - val_loss: 0.0445 - val_acc: 0.9055\n",
      "Epoch 98/100\n",
      "8564/8564 [==============================] - 5s 623us/step - loss: 0.0426 - acc: 0.9037 - val_loss: 0.0437 - val_acc: 0.9215\n",
      "Epoch 99/100\n",
      "8564/8564 [==============================] - 5s 579us/step - loss: 0.0431 - acc: 0.9041 - val_loss: 0.0430 - val_acc: 0.9067\n",
      "Epoch 100/100\n",
      "8564/8564 [==============================] - 6s 660us/step - loss: 0.0422 - acc: 0.9053 - val_loss: 0.0394 - val_acc: 0.9267\n",
      "start training round 5\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 101/120\n",
      "8564/8564 [==============================] - 5s 630us/step - loss: 0.0384 - acc: 0.5906 - val_loss: 0.0378 - val_acc: 0.5717\n",
      "Epoch 102/120\n",
      "8564/8564 [==============================] - 5s 600us/step - loss: 0.0386 - acc: 0.6133 - val_loss: 0.0381 - val_acc: 0.5970\n",
      "Epoch 103/120\n",
      "8564/8564 [==============================] - 5s 618us/step - loss: 0.0382 - acc: 0.6134 - val_loss: 0.0381 - val_acc: 0.6162\n",
      "Epoch 104/120\n",
      "8564/8564 [==============================] - 5s 629us/step - loss: 0.0382 - acc: 0.6009 - val_loss: 0.0376 - val_acc: 0.5854\n",
      "Epoch 105/120\n",
      "8564/8564 [==============================] - 5s 617us/step - loss: 0.0383 - acc: 0.6061 - val_loss: 0.0386 - val_acc: 0.6271\n",
      "Epoch 106/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 5s 630us/step - loss: 0.0378 - acc: 0.6081 - val_loss: 0.0383 - val_acc: 0.4849\n",
      "Epoch 107/120\n",
      "8564/8564 [==============================] - 5s 631us/step - loss: 0.0375 - acc: 0.5903 - val_loss: 0.0381 - val_acc: 0.4790\n",
      "Epoch 108/120\n",
      "8564/8564 [==============================] - 5s 607us/step - loss: 0.0374 - acc: 0.5821 - val_loss: 0.0370 - val_acc: 0.6149\n",
      "Epoch 109/120\n",
      "8564/8564 [==============================] - 5s 627us/step - loss: 0.0373 - acc: 0.6144 - val_loss: 0.0378 - val_acc: 0.6141\n",
      "Epoch 110/120\n",
      "8564/8564 [==============================] - 5s 613us/step - loss: 0.0377 - acc: 0.6017 - val_loss: 0.0378 - val_acc: 0.4805\n",
      "Epoch 111/120\n",
      "8564/8564 [==============================] - 5s 567us/step - loss: 0.0372 - acc: 0.5901 - val_loss: 0.0376 - val_acc: 0.5740\n",
      "Epoch 112/120\n",
      "8564/8564 [==============================] - 5s 591us/step - loss: 0.0378 - acc: 0.6047 - val_loss: 0.0388 - val_acc: 0.4522\n",
      "Epoch 113/120\n",
      "8564/8564 [==============================] - 5s 600us/step - loss: 0.0370 - acc: 0.5695 - val_loss: 0.0403 - val_acc: 0.4372\n",
      "Epoch 114/120\n",
      "8564/8564 [==============================] - 5s 634us/step - loss: 0.0373 - acc: 0.5877 - val_loss: 0.0382 - val_acc: 0.6153\n",
      "Epoch 115/120\n",
      "8564/8564 [==============================] - 5s 639us/step - loss: 0.0378 - acc: 0.5978 - val_loss: 0.0373 - val_acc: 0.5281\n",
      "Epoch 116/120\n",
      "8564/8564 [==============================] - 5s 594us/step - loss: 0.0369 - acc: 0.5949 - val_loss: 0.0373 - val_acc: 0.5027\n",
      "Epoch 117/120\n",
      "8564/8564 [==============================] - 5s 598us/step - loss: 0.0372 - acc: 0.5646 - val_loss: 0.0377 - val_acc: 0.5696\n",
      "Epoch 118/120\n",
      "8564/8564 [==============================] - 6s 646us/step - loss: 0.0375 - acc: 0.5864 - val_loss: 0.0383 - val_acc: 0.6340\n",
      "Epoch 119/120\n",
      "8564/8564 [==============================] - 5s 631us/step - loss: 0.0370 - acc: 0.5956 - val_loss: 0.0366 - val_acc: 0.6088\n",
      "Epoch 120/120\n",
      "8564/8564 [==============================] - 5s 585us/step - loss: 0.0364 - acc: 0.5987 - val_loss: 0.0392 - val_acc: 0.4472\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 101/120\n",
      "8564/8564 [==============================] - 5s 585us/step - loss: 0.0323 - acc: 0.8427 - val_loss: 0.0326 - val_acc: 0.8411\n",
      "Epoch 102/120\n",
      "8564/8564 [==============================] - 5s 621us/step - loss: 0.0323 - acc: 0.8409 - val_loss: 0.0337 - val_acc: 0.8414\n",
      "Epoch 103/120\n",
      "8564/8564 [==============================] - 5s 576us/step - loss: 0.0326 - acc: 0.8378 - val_loss: 0.0329 - val_acc: 0.8443\n",
      "Epoch 104/120\n",
      "8564/8564 [==============================] - 5s 586us/step - loss: 0.0321 - acc: 0.8405 - val_loss: 0.0345 - val_acc: 0.8397\n",
      "Epoch 105/120\n",
      "8564/8564 [==============================] - 5s 599us/step - loss: 0.0321 - acc: 0.8405 - val_loss: 0.0344 - val_acc: 0.8395\n",
      "Epoch 106/120\n",
      "8564/8564 [==============================] - 5s 597us/step - loss: 0.0321 - acc: 0.8397 - val_loss: 0.0337 - val_acc: 0.8401\n",
      "Epoch 107/120\n",
      "8564/8564 [==============================] - 5s 631us/step - loss: 0.0322 - acc: 0.8398 - val_loss: 0.0337 - val_acc: 0.8491\n",
      "Epoch 108/120\n",
      "8564/8564 [==============================] - 5s 591us/step - loss: 0.0319 - acc: 0.8437 - val_loss: 0.0316 - val_acc: 0.8470\n",
      "Epoch 109/120\n",
      "8564/8564 [==============================] - 5s 618us/step - loss: 0.0315 - acc: 0.8414 - val_loss: 0.0329 - val_acc: 0.8452\n",
      "Epoch 110/120\n",
      "8564/8564 [==============================] - 5s 626us/step - loss: 0.0316 - acc: 0.8412 - val_loss: 0.0337 - val_acc: 0.8448\n",
      "Epoch 111/120\n",
      "8564/8564 [==============================] - 5s 625us/step - loss: 0.0314 - acc: 0.8431 - val_loss: 0.0326 - val_acc: 0.8436\n",
      "Epoch 112/120\n",
      "8564/8564 [==============================] - 5s 581us/step - loss: 0.0318 - acc: 0.8411 - val_loss: 0.0333 - val_acc: 0.8436\n",
      "Epoch 113/120\n",
      "8564/8564 [==============================] - 5s 607us/step - loss: 0.0319 - acc: 0.8413 - val_loss: 0.0325 - val_acc: 0.8444\n",
      "Epoch 114/120\n",
      "8564/8564 [==============================] - 5s 615us/step - loss: 0.0314 - acc: 0.8432 - val_loss: 0.0318 - val_acc: 0.8497\n",
      "Epoch 115/120\n",
      "8564/8564 [==============================] - 5s 608us/step - loss: 0.0318 - acc: 0.8427 - val_loss: 0.0308 - val_acc: 0.8488\n",
      "Epoch 116/120\n",
      "8564/8564 [==============================] - 5s 594us/step - loss: 0.0318 - acc: 0.8407 - val_loss: 0.0330 - val_acc: 0.8446\n",
      "Epoch 117/120\n",
      "8564/8564 [==============================] - 5s 605us/step - loss: 0.0314 - acc: 0.8432 - val_loss: 0.0335 - val_acc: 0.8462\n",
      "Epoch 118/120\n",
      "8564/8564 [==============================] - 5s 591us/step - loss: 0.0313 - acc: 0.8423 - val_loss: 0.0314 - val_acc: 0.8472\n",
      "Epoch 119/120\n",
      "8564/8564 [==============================] - 5s 601us/step - loss: 0.0313 - acc: 0.8425 - val_loss: 0.0320 - val_acc: 0.8469\n",
      "Epoch 120/120\n",
      "8564/8564 [==============================] - 5s 627us/step - loss: 0.0314 - acc: 0.8450 - val_loss: 0.0327 - val_acc: 0.8499\n",
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 101/120\n",
      "8564/8564 [==============================] - 5s 605us/step - loss: 0.0350 - acc: 0.9715 - val_loss: 0.0303 - val_acc: 0.9742\n",
      "Epoch 102/120\n",
      "8564/8564 [==============================] - 5s 606us/step - loss: 0.0359 - acc: 0.9715 - val_loss: 0.0318 - val_acc: 0.9712\n",
      "Epoch 103/120\n",
      "8564/8564 [==============================] - 5s 576us/step - loss: 0.0357 - acc: 0.9707 - val_loss: 0.0382 - val_acc: 0.9732\n",
      "Epoch 104/120\n",
      "8564/8564 [==============================] - 5s 550us/step - loss: 0.0360 - acc: 0.9706 - val_loss: 0.0364 - val_acc: 0.9739\n",
      "Epoch 105/120\n",
      "8564/8564 [==============================] - 5s 604us/step - loss: 0.0341 - acc: 0.9726 - val_loss: 0.0376 - val_acc: 0.9647\n",
      "Epoch 106/120\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0362 - acc: 0.9717 - val_loss: 0.0403 - val_acc: 0.9637\n",
      "Epoch 107/120\n",
      "8564/8564 [==============================] - 5s 601us/step - loss: 0.0357 - acc: 0.9709 - val_loss: 0.0338 - val_acc: 0.9763\n",
      "Epoch 108/120\n",
      "8564/8564 [==============================] - 5s 576us/step - loss: 0.0341 - acc: 0.9735 - val_loss: 0.0369 - val_acc: 0.9767\n",
      "Epoch 109/120\n",
      "8564/8564 [==============================] - 5s 579us/step - loss: 0.0349 - acc: 0.9742 - val_loss: 0.0334 - val_acc: 0.9703\n",
      "Epoch 110/120\n",
      "8564/8564 [==============================] - 5s 595us/step - loss: 0.0347 - acc: 0.9730 - val_loss: 0.0286 - val_acc: 0.9789\n",
      "Epoch 111/120\n",
      "8564/8564 [==============================] - 5s 595us/step - loss: 0.0356 - acc: 0.9742 - val_loss: 0.0347 - val_acc: 0.9773\n",
      "Epoch 112/120\n",
      "8564/8564 [==============================] - 5s 574us/step - loss: 0.0341 - acc: 0.9720 - val_loss: 0.0331 - val_acc: 0.9746\n",
      "Epoch 113/120\n",
      "8564/8564 [==============================] - 5s 589us/step - loss: 0.0343 - acc: 0.9722 - val_loss: 0.0345 - val_acc: 0.9767\n",
      "Epoch 114/120\n",
      "8564/8564 [==============================] - 5s 608us/step - loss: 0.0335 - acc: 0.9732 - val_loss: 0.0414 - val_acc: 0.9750\n",
      "Epoch 115/120\n",
      "8564/8564 [==============================] - 5s 610us/step - loss: 0.0339 - acc: 0.9736 - val_loss: 0.0317 - val_acc: 0.9748\n",
      "Epoch 116/120\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0341 - acc: 0.9751 - val_loss: 0.0306 - val_acc: 0.9774\n",
      "Epoch 117/120\n",
      "8564/8564 [==============================] - 5s 591us/step - loss: 0.0333 - acc: 0.9738 - val_loss: 0.0275 - val_acc: 0.9784\n",
      "Epoch 118/120\n",
      "8564/8564 [==============================] - 5s 607us/step - loss: 0.0345 - acc: 0.9742 - val_loss: 0.0287 - val_acc: 0.9744\n",
      "Epoch 119/120\n",
      "8564/8564 [==============================] - 5s 588us/step - loss: 0.0343 - acc: 0.9730 - val_loss: 0.0415 - val_acc: 0.9720\n",
      "Epoch 120/120\n",
      "8564/8564 [==============================] - 5s 597us/step - loss: 0.0332 - acc: 0.9728 - val_loss: 0.0331 - val_acc: 0.9748\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 101/120\n",
      "8564/8564 [==============================] - 5s 622us/step - loss: 0.0427 - acc: 0.9038 - val_loss: 0.0418 - val_acc: 0.9166\n",
      "Epoch 102/120\n",
      "8564/8564 [==============================] - 5s 624us/step - loss: 0.0419 - acc: 0.9045 - val_loss: 0.0451 - val_acc: 0.9059\n",
      "Epoch 103/120\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 5s 584us/step - loss: 0.0430 - acc: 0.9025 - val_loss: 0.0406 - val_acc: 0.9024\n",
      "Epoch 104/120\n",
      "8564/8564 [==============================] - 5s 592us/step - loss: 0.0408 - acc: 0.9046 - val_loss: 0.0384 - val_acc: 0.9126\n",
      "Epoch 105/120\n",
      "8564/8564 [==============================] - 5s 614us/step - loss: 0.0417 - acc: 0.9059 - val_loss: 0.0414 - val_acc: 0.9113\n",
      "Epoch 106/120\n",
      "8564/8564 [==============================] - 5s 605us/step - loss: 0.0441 - acc: 0.9022 - val_loss: 0.0391 - val_acc: 0.9162\n",
      "Epoch 107/120\n",
      "8564/8564 [==============================] - 5s 594us/step - loss: 0.0415 - acc: 0.9033 - val_loss: 0.0396 - val_acc: 0.9173\n",
      "Epoch 108/120\n",
      "8564/8564 [==============================] - 5s 627us/step - loss: 0.0414 - acc: 0.9043 - val_loss: 0.0479 - val_acc: 0.8750\n",
      "Epoch 109/120\n",
      "8564/8564 [==============================] - 5s 589us/step - loss: 0.0419 - acc: 0.9061 - val_loss: 0.0382 - val_acc: 0.9104\n",
      "Epoch 110/120\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0416 - acc: 0.9057 - val_loss: 0.0365 - val_acc: 0.9215\n",
      "Epoch 111/120\n",
      "8564/8564 [==============================] - 5s 603us/step - loss: 0.0400 - acc: 0.9064 - val_loss: 0.0467 - val_acc: 0.9040\n",
      "Epoch 112/120\n",
      "8564/8564 [==============================] - 5s 604us/step - loss: 0.0416 - acc: 0.9030 - val_loss: 0.0398 - val_acc: 0.8956\n",
      "Epoch 113/120\n",
      "8564/8564 [==============================] - 5s 581us/step - loss: 0.0427 - acc: 0.9023 - val_loss: 0.0390 - val_acc: 0.9084\n",
      "Epoch 114/120\n",
      "8564/8564 [==============================] - 5s 592us/step - loss: 0.0398 - acc: 0.9083 - val_loss: 0.0386 - val_acc: 0.9171\n",
      "Epoch 115/120\n",
      "8564/8564 [==============================] - 5s 603us/step - loss: 0.0406 - acc: 0.9059 - val_loss: 0.0379 - val_acc: 0.9174\n",
      "Epoch 116/120\n",
      "8564/8564 [==============================] - 5s 570us/step - loss: 0.0408 - acc: 0.9074 - val_loss: 0.0397 - val_acc: 0.9128\n",
      "Epoch 117/120\n",
      "8564/8564 [==============================] - 5s 560us/step - loss: 0.0399 - acc: 0.9076 - val_loss: 0.0421 - val_acc: 0.9146\n",
      "Epoch 118/120\n",
      "8564/8564 [==============================] - 5s 639us/step - loss: 0.0404 - acc: 0.9080 - val_loss: 0.0395 - val_acc: 0.9077\n",
      "Epoch 119/120\n",
      "8564/8564 [==============================] - 5s 592us/step - loss: 0.0403 - acc: 0.9066 - val_loss: 0.0402 - val_acc: 0.9191\n",
      "Epoch 120/120\n",
      "8564/8564 [==============================] - 5s 599us/step - loss: 0.0413 - acc: 0.9070 - val_loss: 0.0369 - val_acc: 0.9181\n",
      "start training round 6\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 121/140\n",
      "8564/8564 [==============================] - 5s 618us/step - loss: 0.0370 - acc: 0.5661 - val_loss: 0.0367 - val_acc: 0.4914\n",
      "Epoch 122/140\n",
      "8564/8564 [==============================] - 5s 617us/step - loss: 0.0366 - acc: 0.5800 - val_loss: 0.0371 - val_acc: 0.4786\n",
      "Epoch 123/140\n",
      "8564/8564 [==============================] - 5s 631us/step - loss: 0.0365 - acc: 0.5799 - val_loss: 0.0386 - val_acc: 0.4789\n",
      "Epoch 124/140\n",
      "8564/8564 [==============================] - 5s 603us/step - loss: 0.0364 - acc: 0.5889 - val_loss: 0.0365 - val_acc: 0.6056\n",
      "Epoch 125/140\n",
      "8564/8564 [==============================] - 5s 609us/step - loss: 0.0367 - acc: 0.5700 - val_loss: 0.0365 - val_acc: 0.5691\n",
      "Epoch 126/140\n",
      "8564/8564 [==============================] - 5s 596us/step - loss: 0.0360 - acc: 0.5872 - val_loss: 0.0382 - val_acc: 0.6206\n",
      "Epoch 127/140\n",
      "8564/8564 [==============================] - 5s 590us/step - loss: 0.0361 - acc: 0.6131 - val_loss: 0.0365 - val_acc: 0.6307\n",
      "Epoch 128/140\n",
      "8564/8564 [==============================] - 5s 606us/step - loss: 0.0364 - acc: 0.6052 - val_loss: 0.0377 - val_acc: 0.6373\n",
      "Epoch 129/140\n",
      "8564/8564 [==============================] - 5s 609us/step - loss: 0.0365 - acc: 0.6172 - val_loss: 0.0393 - val_acc: 0.5707\n",
      "Epoch 130/140\n",
      "8564/8564 [==============================] - 5s 584us/step - loss: 0.0363 - acc: 0.5899 - val_loss: 0.0379 - val_acc: 0.5803\n",
      "Epoch 131/140\n",
      "8564/8564 [==============================] - 5s 589us/step - loss: 0.0365 - acc: 0.6156 - val_loss: 0.0358 - val_acc: 0.5907\n",
      "Epoch 132/140\n",
      "8564/8564 [==============================] - 5s 596us/step - loss: 0.0360 - acc: 0.5931 - val_loss: 0.0369 - val_acc: 0.6070\n",
      "Epoch 133/140\n",
      "8564/8564 [==============================] - 5s 610us/step - loss: 0.0364 - acc: 0.5616 - val_loss: 0.0368 - val_acc: 0.6290\n",
      "Epoch 134/140\n",
      "8564/8564 [==============================] - 5s 618us/step - loss: 0.0361 - acc: 0.5697 - val_loss: 0.0356 - val_acc: 0.6329\n",
      "Epoch 135/140\n",
      "8564/8564 [==============================] - 5s 594us/step - loss: 0.0363 - acc: 0.6121 - val_loss: 0.0358 - val_acc: 0.6339\n",
      "Epoch 136/140\n",
      "8564/8564 [==============================] - 5s 595us/step - loss: 0.0360 - acc: 0.5748 - val_loss: 0.0376 - val_acc: 0.6233\n",
      "Epoch 137/140\n",
      "8564/8564 [==============================] - 5s 566us/step - loss: 0.0356 - acc: 0.5847 - val_loss: 0.0354 - val_acc: 0.5722\n",
      "Epoch 138/140\n",
      "8564/8564 [==============================] - 5s 601us/step - loss: 0.0362 - acc: 0.6184 - val_loss: 0.0365 - val_acc: 0.6402\n",
      "Epoch 139/140\n",
      "8564/8564 [==============================] - 5s 563us/step - loss: 0.0358 - acc: 0.6166 - val_loss: 0.0363 - val_acc: 0.6372\n",
      "Epoch 140/140\n",
      "8564/8564 [==============================] - 5s 600us/step - loss: 0.0355 - acc: 0.5970 - val_loss: 0.0356 - val_acc: 0.6329\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 121/140\n",
      "8564/8564 [==============================] - 5s 592us/step - loss: 0.0310 - acc: 0.8439 - val_loss: 0.0320 - val_acc: 0.8477\n",
      "Epoch 122/140\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0311 - acc: 0.8459 - val_loss: 0.0318 - val_acc: 0.8487\n",
      "Epoch 123/140\n",
      "8564/8564 [==============================] - 5s 591us/step - loss: 0.0311 - acc: 0.8444 - val_loss: 0.0313 - val_acc: 0.8487\n",
      "Epoch 124/140\n",
      "8564/8564 [==============================] - 5s 593us/step - loss: 0.0306 - acc: 0.8462 - val_loss: 0.0322 - val_acc: 0.8533\n",
      "Epoch 125/140\n",
      "8564/8564 [==============================] - 5s 602us/step - loss: 0.0309 - acc: 0.8503 - val_loss: 0.0326 - val_acc: 0.8494\n",
      "Epoch 126/140\n",
      "8564/8564 [==============================] - 5s 603us/step - loss: 0.0307 - acc: 0.8509 - val_loss: 0.0307 - val_acc: 0.8503\n",
      "Epoch 127/140\n",
      "8564/8564 [==============================] - 5s 625us/step - loss: 0.0306 - acc: 0.8507 - val_loss: 0.0308 - val_acc: 0.8417\n",
      "Epoch 128/140\n",
      "8564/8564 [==============================] - 5s 584us/step - loss: 0.0310 - acc: 0.8442 - val_loss: 0.0306 - val_acc: 0.8425\n",
      "Epoch 129/140\n",
      "8564/8564 [==============================] - 5s 559us/step - loss: 0.0311 - acc: 0.8440 - val_loss: 0.0297 - val_acc: 0.8434\n",
      "Epoch 130/140\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0305 - acc: 0.8455 - val_loss: 0.0304 - val_acc: 0.8427\n",
      "Epoch 131/140\n",
      "8564/8564 [==============================] - 5s 598us/step - loss: 0.0307 - acc: 0.8519 - val_loss: 0.0321 - val_acc: 0.8485\n",
      "Epoch 132/140\n",
      "8564/8564 [==============================] - 5s 600us/step - loss: 0.0303 - acc: 0.8504 - val_loss: 0.0317 - val_acc: 0.8525\n",
      "Epoch 133/140\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0305 - acc: 0.8479 - val_loss: 0.0315 - val_acc: 0.8478\n",
      "Epoch 134/140\n",
      "8564/8564 [==============================] - 5s 589us/step - loss: 0.0307 - acc: 0.8444 - val_loss: 0.0313 - val_acc: 0.8478\n",
      "Epoch 135/140\n",
      "8564/8564 [==============================] - 5s 584us/step - loss: 0.0305 - acc: 0.8467 - val_loss: 0.0324 - val_acc: 0.8484\n",
      "Epoch 136/140\n",
      "8564/8564 [==============================] - 5s 602us/step - loss: 0.0304 - acc: 0.8462 - val_loss: 0.0320 - val_acc: 0.8496\n",
      "Epoch 137/140\n",
      "8564/8564 [==============================] - 5s 579us/step - loss: 0.0303 - acc: 0.8485 - val_loss: 0.0303 - val_acc: 0.8516\n",
      "Epoch 138/140\n",
      "8564/8564 [==============================] - 5s 580us/step - loss: 0.0303 - acc: 0.8473 - val_loss: 0.0313 - val_acc: 0.8513\n",
      "Epoch 139/140\n",
      "8564/8564 [==============================] - 5s 594us/step - loss: 0.0302 - acc: 0.8476 - val_loss: 0.0304 - val_acc: 0.8513\n",
      "Epoch 140/140\n",
      "8564/8564 [==============================] - 5s 605us/step - loss: 0.0303 - acc: 0.8471 - val_loss: 0.0305 - val_acc: 0.8505\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 121/140\n",
      "8564/8564 [==============================] - 5s 597us/step - loss: 0.0342 - acc: 0.9705 - val_loss: 0.0362 - val_acc: 0.9760\n",
      "Epoch 122/140\n",
      "8564/8564 [==============================] - 5s 574us/step - loss: 0.0338 - acc: 0.9745 - val_loss: 0.0311 - val_acc: 0.9724\n",
      "Epoch 123/140\n",
      "8564/8564 [==============================] - 5s 581us/step - loss: 0.0345 - acc: 0.9722 - val_loss: 0.0346 - val_acc: 0.9749\n",
      "Epoch 124/140\n",
      "8564/8564 [==============================] - 5s 582us/step - loss: 0.0340 - acc: 0.9726 - val_loss: 0.0356 - val_acc: 0.9786\n",
      "Epoch 125/140\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0329 - acc: 0.9738 - val_loss: 0.0303 - val_acc: 0.9724\n",
      "Epoch 126/140\n",
      "8564/8564 [==============================] - 5s 581us/step - loss: 0.0332 - acc: 0.9731 - val_loss: 0.0301 - val_acc: 0.9742\n",
      "Epoch 127/140\n",
      "8564/8564 [==============================] - 5s 593us/step - loss: 0.0346 - acc: 0.9743 - val_loss: 0.0314 - val_acc: 0.9749\n",
      "Epoch 128/140\n",
      "8564/8564 [==============================] - 5s 575us/step - loss: 0.0350 - acc: 0.9743 - val_loss: 0.0293 - val_acc: 0.9762\n",
      "Epoch 129/140\n",
      "8564/8564 [==============================] - 5s 614us/step - loss: 0.0336 - acc: 0.9751 - val_loss: 0.0295 - val_acc: 0.9720\n",
      "Epoch 130/140\n",
      "8564/8564 [==============================] - 5s 581us/step - loss: 0.0330 - acc: 0.9746 - val_loss: 0.0314 - val_acc: 0.9772\n",
      "Epoch 131/140\n",
      "8564/8564 [==============================] - 5s 605us/step - loss: 0.0333 - acc: 0.9744 - val_loss: 0.0359 - val_acc: 0.9757\n",
      "Epoch 132/140\n",
      "8564/8564 [==============================] - 5s 588us/step - loss: 0.0329 - acc: 0.9750 - val_loss: 0.0267 - val_acc: 0.9795\n",
      "Epoch 133/140\n",
      "8564/8564 [==============================] - 5s 587us/step - loss: 0.0338 - acc: 0.9756 - val_loss: 0.0277 - val_acc: 0.9784\n",
      "Epoch 134/140\n",
      "8564/8564 [==============================] - 5s 606us/step - loss: 0.0365 - acc: 0.9727 - val_loss: 0.0289 - val_acc: 0.9779\n",
      "Epoch 135/140\n",
      "8564/8564 [==============================] - 5s 607us/step - loss: 0.0318 - acc: 0.9760 - val_loss: 0.0360 - val_acc: 0.9735\n",
      "Epoch 136/140\n",
      "8564/8564 [==============================] - 5s 582us/step - loss: 0.0323 - acc: 0.9752 - val_loss: 0.0315 - val_acc: 0.9782\n",
      "Epoch 137/140\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0311 - acc: 0.9753 - val_loss: 0.0334 - val_acc: 0.9777\n",
      "Epoch 138/140\n",
      "8564/8564 [==============================] - 5s 608us/step - loss: 0.0357 - acc: 0.9745 - val_loss: 0.0309 - val_acc: 0.9788\n",
      "Epoch 139/140\n",
      "8564/8564 [==============================] - 5s 599us/step - loss: 0.0339 - acc: 0.9754 - val_loss: 0.0330 - val_acc: 0.9778\n",
      "Epoch 140/140\n",
      "8564/8564 [==============================] - 5s 582us/step - loss: 0.0325 - acc: 0.9752 - val_loss: 0.0305 - val_acc: 0.9804\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 121/140\n",
      "8564/8564 [==============================] - 5s 579us/step - loss: 0.0397 - acc: 0.9075 - val_loss: 0.0397 - val_acc: 0.9233\n",
      "Epoch 122/140\n",
      "8564/8564 [==============================] - 5s 603us/step - loss: 0.0397 - acc: 0.9076 - val_loss: 0.0383 - val_acc: 0.9145\n",
      "Epoch 123/140\n",
      "8564/8564 [==============================] - 5s 545us/step - loss: 0.0399 - acc: 0.9079 - val_loss: 0.0381 - val_acc: 0.9280\n",
      "Epoch 124/140\n",
      "8564/8564 [==============================] - 5s 582us/step - loss: 0.0418 - acc: 0.9052 - val_loss: 0.0390 - val_acc: 0.9113\n",
      "Epoch 125/140\n",
      "8564/8564 [==============================] - 5s 579us/step - loss: 0.0407 - acc: 0.9055 - val_loss: 0.0422 - val_acc: 0.8908\n",
      "Epoch 126/140\n",
      "8564/8564 [==============================] - 5s 564us/step - loss: 0.0397 - acc: 0.9099 - val_loss: 0.0351 - val_acc: 0.9152\n",
      "Epoch 127/140\n",
      "8564/8564 [==============================] - 5s 588us/step - loss: 0.0397 - acc: 0.9106 - val_loss: 0.0443 - val_acc: 0.8881\n",
      "Epoch 128/140\n",
      "8564/8564 [==============================] - 5s 612us/step - loss: 0.0404 - acc: 0.9079 - val_loss: 0.0452 - val_acc: 0.8689\n",
      "Epoch 129/140\n",
      "8564/8564 [==============================] - 5s 597us/step - loss: 0.0391 - acc: 0.9091 - val_loss: 0.0408 - val_acc: 0.9010\n",
      "Epoch 130/140\n",
      "8564/8564 [==============================] - 5s 601us/step - loss: 0.0396 - acc: 0.9103 - val_loss: 0.0434 - val_acc: 0.9218\n",
      "Epoch 131/140\n",
      "8564/8564 [==============================] - 5s 573us/step - loss: 0.0396 - acc: 0.9087 - val_loss: 0.0366 - val_acc: 0.9258\n",
      "Epoch 132/140\n",
      "8564/8564 [==============================] - 5s 603us/step - loss: 0.0409 - acc: 0.9051 - val_loss: 0.0396 - val_acc: 0.9121\n",
      "Epoch 133/140\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0391 - acc: 0.9103 - val_loss: 0.0446 - val_acc: 0.8943\n",
      "Epoch 134/140\n",
      "8564/8564 [==============================] - 5s 595us/step - loss: 0.0401 - acc: 0.9070 - val_loss: 0.0412 - val_acc: 0.9013\n",
      "Epoch 135/140\n",
      "8564/8564 [==============================] - 5s 593us/step - loss: 0.0403 - acc: 0.9093 - val_loss: 0.0357 - val_acc: 0.9277\n",
      "Epoch 136/140\n",
      "8564/8564 [==============================] - 5s 546us/step - loss: 0.0380 - acc: 0.9121 - val_loss: 0.0361 - val_acc: 0.9324\n",
      "Epoch 137/140\n",
      "8564/8564 [==============================] - 5s 591us/step - loss: 0.0386 - acc: 0.9134 - val_loss: 0.0454 - val_acc: 0.8883\n",
      "Epoch 138/140\n",
      "8564/8564 [==============================] - 5s 588us/step - loss: 0.0406 - acc: 0.9078 - val_loss: 0.0431 - val_acc: 0.9154\n",
      "Epoch 139/140\n",
      "8564/8564 [==============================] - 5s 591us/step - loss: 0.0391 - acc: 0.9109 - val_loss: 0.0476 - val_acc: 0.8975\n",
      "Epoch 140/140\n",
      "8564/8564 [==============================] - 5s 580us/step - loss: 0.0395 - acc: 0.9102 - val_loss: 0.0371 - val_acc: 0.9164\n",
      "start training round 7\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 141/160\n",
      "8564/8564 [==============================] - 5s 557us/step - loss: 0.0357 - acc: 0.6113 - val_loss: 0.0362 - val_acc: 0.6375\n",
      "Epoch 142/160\n",
      "8564/8564 [==============================] - 5s 568us/step - loss: 0.0356 - acc: 0.5857 - val_loss: 0.0357 - val_acc: 0.5278\n",
      "Epoch 143/160\n",
      "8564/8564 [==============================] - 5s 548us/step - loss: 0.0357 - acc: 0.5713 - val_loss: 0.0375 - val_acc: 0.4777\n",
      "Epoch 144/160\n",
      "8564/8564 [==============================] - 5s 576us/step - loss: 0.0356 - acc: 0.6078 - val_loss: 0.0367 - val_acc: 0.6420\n",
      "Epoch 145/160\n",
      "8564/8564 [==============================] - 5s 580us/step - loss: 0.0358 - acc: 0.6243 - val_loss: 0.0353 - val_acc: 0.5514\n",
      "Epoch 146/160\n",
      "8564/8564 [==============================] - 5s 568us/step - loss: 0.0355 - acc: 0.5689 - val_loss: 0.0359 - val_acc: 0.5082\n",
      "Epoch 147/160\n",
      "8564/8564 [==============================] - 5s 577us/step - loss: 0.0354 - acc: 0.6119 - val_loss: 0.0361 - val_acc: 0.6301\n",
      "Epoch 148/160\n",
      "8564/8564 [==============================] - 5s 554us/step - loss: 0.0360 - acc: 0.6217 - val_loss: 0.0375 - val_acc: 0.6353\n",
      "Epoch 149/160\n",
      "8564/8564 [==============================] - 5s 567us/step - loss: 0.0351 - acc: 0.5990 - val_loss: 0.0355 - val_acc: 0.6115\n",
      "Epoch 150/160\n",
      "8564/8564 [==============================] - 5s 568us/step - loss: 0.0352 - acc: 0.5828 - val_loss: 0.0361 - val_acc: 0.6320\n",
      "Epoch 151/160\n",
      "8564/8564 [==============================] - 5s 587us/step - loss: 0.0352 - acc: 0.5949 - val_loss: 0.0366 - val_acc: 0.6019\n",
      "Epoch 152/160\n",
      "8564/8564 [==============================] - 5s 552us/step - loss: 0.0349 - acc: 0.6084 - val_loss: 0.0352 - val_acc: 0.6401\n",
      "Epoch 153/160\n",
      "8564/8564 [==============================] - 5s 578us/step - loss: 0.0356 - acc: 0.6267 - val_loss: 0.0362 - val_acc: 0.5928\n",
      "Epoch 154/160\n",
      "8564/8564 [==============================] - 5s 565us/step - loss: 0.0352 - acc: 0.5819 - val_loss: 0.0350 - val_acc: 0.6397\n",
      "Epoch 155/160\n",
      "8564/8564 [==============================] - 5s 579us/step - loss: 0.0353 - acc: 0.5725 - val_loss: 0.0352 - val_acc: 0.6208\n",
      "Epoch 156/160\n",
      "8564/8564 [==============================] - 5s 528us/step - loss: 0.0346 - acc: 0.6004 - val_loss: 0.0347 - val_acc: 0.6426\n",
      "Epoch 157/160\n",
      "8564/8564 [==============================] - 5s 543us/step - loss: 0.0346 - acc: 0.6220 - val_loss: 0.0351 - val_acc: 0.6396\n",
      "Epoch 158/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 5s 553us/step - loss: 0.0350 - acc: 0.6111 - val_loss: 0.0352 - val_acc: 0.6383\n",
      "Epoch 159/160\n",
      "8564/8564 [==============================] - 5s 578us/step - loss: 0.0347 - acc: 0.6187 - val_loss: 0.0354 - val_acc: 0.6374\n",
      "Epoch 160/160\n",
      "8564/8564 [==============================] - 5s 579us/step - loss: 0.0346 - acc: 0.6041 - val_loss: 0.0365 - val_acc: 0.6338\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 141/160\n",
      "8564/8564 [==============================] - 5s 559us/step - loss: 0.0303 - acc: 0.8471 - val_loss: 0.0315 - val_acc: 0.8497\n",
      "Epoch 142/160\n",
      "8564/8564 [==============================] - 5s 541us/step - loss: 0.0301 - acc: 0.8503 - val_loss: 0.0311 - val_acc: 0.8504\n",
      "Epoch 143/160\n",
      "8564/8564 [==============================] - 5s 558us/step - loss: 0.0301 - acc: 0.8532 - val_loss: 0.0321 - val_acc: 0.8492\n",
      "Epoch 144/160\n",
      "8564/8564 [==============================] - 5s 566us/step - loss: 0.0302 - acc: 0.8532 - val_loss: 0.0300 - val_acc: 0.8517\n",
      "Epoch 145/160\n",
      "8564/8564 [==============================] - 5s 581us/step - loss: 0.0299 - acc: 0.8535 - val_loss: 0.0324 - val_acc: 0.8486\n",
      "Epoch 146/160\n",
      "8564/8564 [==============================] - 5s 584us/step - loss: 0.0299 - acc: 0.8539 - val_loss: 0.0303 - val_acc: 0.8471\n",
      "Epoch 147/160\n",
      "8564/8564 [==============================] - 5s 574us/step - loss: 0.0296 - acc: 0.8533 - val_loss: 0.0298 - val_acc: 0.8514\n",
      "Epoch 148/160\n",
      "8564/8564 [==============================] - 5s 603us/step - loss: 0.0301 - acc: 0.8541 - val_loss: 0.0314 - val_acc: 0.8501\n",
      "Epoch 149/160\n",
      "8564/8564 [==============================] - 5s 560us/step - loss: 0.0298 - acc: 0.8536 - val_loss: 0.0302 - val_acc: 0.8527\n",
      "Epoch 150/160\n",
      "8564/8564 [==============================] - 5s 538us/step - loss: 0.0297 - acc: 0.8519 - val_loss: 0.0298 - val_acc: 0.8532\n",
      "Epoch 151/160\n",
      "8564/8564 [==============================] - 5s 557us/step - loss: 0.0294 - acc: 0.8523 - val_loss: 0.0306 - val_acc: 0.8536\n",
      "Epoch 152/160\n",
      "8564/8564 [==============================] - 5s 585us/step - loss: 0.0298 - acc: 0.8486 - val_loss: 0.0291 - val_acc: 0.8455\n",
      "Epoch 153/160\n",
      "8564/8564 [==============================] - 5s 549us/step - loss: 0.0297 - acc: 0.8499 - val_loss: 0.0307 - val_acc: 0.8433\n",
      "Epoch 154/160\n",
      "8564/8564 [==============================] - 5s 612us/step - loss: 0.0300 - acc: 0.8471 - val_loss: 0.0297 - val_acc: 0.8442\n",
      "Epoch 155/160\n",
      "8564/8564 [==============================] - 5s 566us/step - loss: 0.0293 - acc: 0.8495 - val_loss: 0.0303 - val_acc: 0.8408\n",
      "Epoch 156/160\n",
      "8564/8564 [==============================] - 5s 559us/step - loss: 0.0297 - acc: 0.8476 - val_loss: 0.0292 - val_acc: 0.8469\n",
      "Epoch 157/160\n",
      "8564/8564 [==============================] - 5s 549us/step - loss: 0.0297 - acc: 0.8488 - val_loss: 0.0292 - val_acc: 0.8477\n",
      "Epoch 158/160\n",
      "8564/8564 [==============================] - 5s 566us/step - loss: 0.0297 - acc: 0.8488 - val_loss: 0.0295 - val_acc: 0.8456\n",
      "Epoch 159/160\n",
      "8564/8564 [==============================] - 5s 564us/step - loss: 0.0298 - acc: 0.8489 - val_loss: 0.0305 - val_acc: 0.8407\n",
      "Epoch 160/160\n",
      "8564/8564 [==============================] - 5s 579us/step - loss: 0.0295 - acc: 0.8497 - val_loss: 0.0306 - val_acc: 0.8446\n",
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 141/160\n",
      "8564/8564 [==============================] - 5s 565us/step - loss: 0.0325 - acc: 0.9756 - val_loss: 0.0285 - val_acc: 0.9804\n",
      "Epoch 142/160\n",
      "8564/8564 [==============================] - 5s 594us/step - loss: 0.0311 - acc: 0.9759 - val_loss: 0.0267 - val_acc: 0.9781\n",
      "Epoch 143/160\n",
      "8564/8564 [==============================] - 5s 535us/step - loss: 0.0312 - acc: 0.9762 - val_loss: 0.0365 - val_acc: 0.9680\n",
      "Epoch 144/160\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0322 - acc: 0.9750 - val_loss: 0.0293 - val_acc: 0.9756\n",
      "Epoch 145/160\n",
      "8564/8564 [==============================] - 5s 573us/step - loss: 0.0315 - acc: 0.9755 - val_loss: 0.0260 - val_acc: 0.9779\n",
      "Epoch 146/160\n",
      "8564/8564 [==============================] - 5s 572us/step - loss: 0.0330 - acc: 0.9746 - val_loss: 0.0305 - val_acc: 0.9797\n",
      "Epoch 147/160\n",
      "8564/8564 [==============================] - 5s 571us/step - loss: 0.0310 - acc: 0.9767 - val_loss: 0.0293 - val_acc: 0.9753\n",
      "Epoch 148/160\n",
      "8564/8564 [==============================] - 5s 578us/step - loss: 0.0313 - acc: 0.9762 - val_loss: 0.0304 - val_acc: 0.9806\n",
      "Epoch 149/160\n",
      "8564/8564 [==============================] - 5s 563us/step - loss: 0.0328 - acc: 0.9765 - val_loss: 0.0324 - val_acc: 0.9778\n",
      "Epoch 150/160\n",
      "8564/8564 [==============================] - 5s 532us/step - loss: 0.0319 - acc: 0.9742 - val_loss: 0.0278 - val_acc: 0.9755\n",
      "Epoch 151/160\n",
      "8564/8564 [==============================] - 4s 522us/step - loss: 0.0314 - acc: 0.9761 - val_loss: 0.0308 - val_acc: 0.9776\n",
      "Epoch 152/160\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0321 - acc: 0.9757 - val_loss: 0.0285 - val_acc: 0.9779\n",
      "Epoch 153/160\n",
      "8564/8564 [==============================] - 5s 582us/step - loss: 0.0337 - acc: 0.9742 - val_loss: 0.0278 - val_acc: 0.9772\n",
      "Epoch 154/160\n",
      "8564/8564 [==============================] - 5s 597us/step - loss: 0.0320 - acc: 0.9754 - val_loss: 0.0318 - val_acc: 0.9718\n",
      "Epoch 155/160\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0324 - acc: 0.9739 - val_loss: 0.0275 - val_acc: 0.9799\n",
      "Epoch 156/160\n",
      "8564/8564 [==============================] - 5s 568us/step - loss: 0.0320 - acc: 0.9753 - val_loss: 0.0374 - val_acc: 0.9715\n",
      "Epoch 157/160\n",
      "8564/8564 [==============================] - 5s 555us/step - loss: 0.0314 - acc: 0.9763 - val_loss: 0.0265 - val_acc: 0.9775\n",
      "Epoch 158/160\n",
      "8564/8564 [==============================] - 5s 606us/step - loss: 0.0320 - acc: 0.9751 - val_loss: 0.0295 - val_acc: 0.9743\n",
      "Epoch 159/160\n",
      "8564/8564 [==============================] - 5s 613us/step - loss: 0.0305 - acc: 0.9785 - val_loss: 0.0278 - val_acc: 0.9789\n",
      "Epoch 160/160\n",
      "8564/8564 [==============================] - 5s 597us/step - loss: 0.0303 - acc: 0.9767 - val_loss: 0.0266 - val_acc: 0.9822\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 141/160\n",
      "8564/8564 [==============================] - 5s 607us/step - loss: 0.0380 - acc: 0.9108 - val_loss: 0.0397 - val_acc: 0.9074\n",
      "Epoch 142/160\n",
      "8564/8564 [==============================] - 5s 618us/step - loss: 0.0376 - acc: 0.9141 - val_loss: 0.0424 - val_acc: 0.9058\n",
      "Epoch 143/160\n",
      "8564/8564 [==============================] - 5s 600us/step - loss: 0.0398 - acc: 0.9061 - val_loss: 0.0381 - val_acc: 0.9161\n",
      "Epoch 144/160\n",
      "8564/8564 [==============================] - 5s 590us/step - loss: 0.0402 - acc: 0.9060 - val_loss: 0.0356 - val_acc: 0.9254\n",
      "Epoch 145/160\n",
      "8564/8564 [==============================] - 5s 600us/step - loss: 0.0395 - acc: 0.9130 - val_loss: 0.0415 - val_acc: 0.9038\n",
      "Epoch 146/160\n",
      "8564/8564 [==============================] - 5s 606us/step - loss: 0.0385 - acc: 0.9117 - val_loss: 0.0455 - val_acc: 0.9196\n",
      "Epoch 147/160\n",
      "8564/8564 [==============================] - 5s 612us/step - loss: 0.0376 - acc: 0.9130 - val_loss: 0.0400 - val_acc: 0.9095\n",
      "Epoch 148/160\n",
      "8564/8564 [==============================] - 5s 595us/step - loss: 0.0388 - acc: 0.9132 - val_loss: 0.0455 - val_acc: 0.9076\n",
      "Epoch 149/160\n",
      "8564/8564 [==============================] - 5s 588us/step - loss: 0.0409 - acc: 0.9092 - val_loss: 0.0336 - val_acc: 0.9357\n",
      "Epoch 150/160\n",
      "8564/8564 [==============================] - 5s 574us/step - loss: 0.0379 - acc: 0.9115 - val_loss: 0.0341 - val_acc: 0.9211\n",
      "Epoch 151/160\n",
      "8564/8564 [==============================] - 5s 605us/step - loss: 0.0383 - acc: 0.9112 - val_loss: 0.0344 - val_acc: 0.9296\n",
      "Epoch 152/160\n",
      "8564/8564 [==============================] - 5s 590us/step - loss: 0.0394 - acc: 0.9136 - val_loss: 0.0435 - val_acc: 0.8871\n",
      "Epoch 153/160\n",
      "8564/8564 [==============================] - 5s 574us/step - loss: 0.0403 - acc: 0.9088 - val_loss: 0.0387 - val_acc: 0.9185\n",
      "Epoch 154/160\n",
      "8564/8564 [==============================] - 5s 585us/step - loss: 0.0388 - acc: 0.9100 - val_loss: 0.0362 - val_acc: 0.9198\n",
      "Epoch 155/160\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 5s 610us/step - loss: 0.0374 - acc: 0.9124 - val_loss: 0.0326 - val_acc: 0.9258\n",
      "Epoch 156/160\n",
      "8564/8564 [==============================] - 5s 587us/step - loss: 0.0393 - acc: 0.9108 - val_loss: 0.0376 - val_acc: 0.9131\n",
      "Epoch 157/160\n",
      "8564/8564 [==============================] - 5s 597us/step - loss: 0.0393 - acc: 0.9111 - val_loss: 0.0432 - val_acc: 0.9154\n",
      "Epoch 158/160\n",
      "8564/8564 [==============================] - 5s 571us/step - loss: 0.0385 - acc: 0.9147 - val_loss: 0.0369 - val_acc: 0.9314\n",
      "Epoch 159/160\n",
      "8564/8564 [==============================] - 5s 576us/step - loss: 0.0377 - acc: 0.9145 - val_loss: 0.0450 - val_acc: 0.8914\n",
      "Epoch 160/160\n",
      "8564/8564 [==============================] - 5s 593us/step - loss: 0.0369 - acc: 0.9159 - val_loss: 0.0335 - val_acc: 0.9342\n",
      "start training round 8\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 161/180\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0351 - acc: 0.5753 - val_loss: 0.0360 - val_acc: 0.6362\n",
      "Epoch 162/180\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0349 - acc: 0.5812 - val_loss: 0.0351 - val_acc: 0.6320\n",
      "Epoch 163/180\n",
      "8564/8564 [==============================] - 5s 564us/step - loss: 0.0349 - acc: 0.6324 - val_loss: 0.0356 - val_acc: 0.5490\n",
      "Epoch 164/180\n",
      "8564/8564 [==============================] - 5s 580us/step - loss: 0.0353 - acc: 0.6195 - val_loss: 0.0359 - val_acc: 0.6393\n",
      "Epoch 165/180\n",
      "8564/8564 [==============================] - 5s 573us/step - loss: 0.0344 - acc: 0.6150 - val_loss: 0.0366 - val_acc: 0.5268\n",
      "Epoch 166/180\n",
      "8564/8564 [==============================] - 5s 529us/step - loss: 0.0347 - acc: 0.6223 - val_loss: 0.0349 - val_acc: 0.6435\n",
      "Epoch 167/180\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0347 - acc: 0.6109 - val_loss: 0.0350 - val_acc: 0.6375\n",
      "Epoch 168/180\n",
      "8564/8564 [==============================] - 5s 600us/step - loss: 0.0346 - acc: 0.5789 - val_loss: 0.0346 - val_acc: 0.6396\n",
      "Epoch 169/180\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0346 - acc: 0.5750 - val_loss: 0.0349 - val_acc: 0.6446\n",
      "Epoch 170/180\n",
      "8564/8564 [==============================] - 5s 574us/step - loss: 0.0346 - acc: 0.6091 - val_loss: 0.0346 - val_acc: 0.6513\n",
      "Epoch 171/180\n",
      "8564/8564 [==============================] - 5s 596us/step - loss: 0.0342 - acc: 0.6252 - val_loss: 0.0342 - val_acc: 0.6173\n",
      "Epoch 172/180\n",
      "8564/8564 [==============================] - 5s 608us/step - loss: 0.0341 - acc: 0.5916 - val_loss: 0.0344 - val_acc: 0.5266\n",
      "Epoch 173/180\n",
      "8564/8564 [==============================] - 5s 585us/step - loss: 0.0342 - acc: 0.5790 - val_loss: 0.0342 - val_acc: 0.5159\n",
      "Epoch 174/180\n",
      "8564/8564 [==============================] - 5s 612us/step - loss: 0.0338 - acc: 0.6142 - val_loss: 0.0354 - val_acc: 0.6425\n",
      "Epoch 175/180\n",
      "8564/8564 [==============================] - 5s 606us/step - loss: 0.0343 - acc: 0.6369 - val_loss: 0.0342 - val_acc: 0.6508\n",
      "Epoch 176/180\n",
      "8564/8564 [==============================] - 5s 604us/step - loss: 0.0337 - acc: 0.5990 - val_loss: 0.0340 - val_acc: 0.6176\n",
      "Epoch 177/180\n",
      "8564/8564 [==============================] - 5s 584us/step - loss: 0.0343 - acc: 0.5813 - val_loss: 0.0360 - val_acc: 0.4806\n",
      "Epoch 178/180\n",
      "8564/8564 [==============================] - 5s 560us/step - loss: 0.0343 - acc: 0.5766 - val_loss: 0.0336 - val_acc: 0.5629\n",
      "Epoch 179/180\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0343 - acc: 0.5776 - val_loss: 0.0350 - val_acc: 0.5154\n",
      "Epoch 180/180\n",
      "8564/8564 [==============================] - 5s 579us/step - loss: 0.0341 - acc: 0.5795 - val_loss: 0.0351 - val_acc: 0.4953\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 161/180\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0293 - acc: 0.8511 - val_loss: 0.0297 - val_acc: 0.8474\n",
      "Epoch 162/180\n",
      "8564/8564 [==============================] - 5s 594us/step - loss: 0.0295 - acc: 0.8547 - val_loss: 0.0302 - val_acc: 0.8541\n",
      "Epoch 163/180\n",
      "8564/8564 [==============================] - 5s 604us/step - loss: 0.0291 - acc: 0.8547 - val_loss: 0.0296 - val_acc: 0.8533\n",
      "Epoch 164/180\n",
      "8564/8564 [==============================] - 5s 590us/step - loss: 0.0296 - acc: 0.8522 - val_loss: 0.0293 - val_acc: 0.8536\n",
      "Epoch 165/180\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0293 - acc: 0.8566 - val_loss: 0.0306 - val_acc: 0.8506\n",
      "Epoch 166/180\n",
      "8564/8564 [==============================] - 5s 569us/step - loss: 0.0291 - acc: 0.8557 - val_loss: 0.0291 - val_acc: 0.8507\n",
      "Epoch 167/180\n",
      "8564/8564 [==============================] - 5s 550us/step - loss: 0.0290 - acc: 0.8565 - val_loss: 0.0305 - val_acc: 0.8533\n",
      "Epoch 168/180\n",
      "8564/8564 [==============================] - 5s 622us/step - loss: 0.0289 - acc: 0.8550 - val_loss: 0.0290 - val_acc: 0.8463\n",
      "Epoch 169/180\n",
      "8564/8564 [==============================] - 5s 592us/step - loss: 0.0291 - acc: 0.8492 - val_loss: 0.0292 - val_acc: 0.8461\n",
      "Epoch 170/180\n",
      "8564/8564 [==============================] - 5s 576us/step - loss: 0.0289 - acc: 0.8550 - val_loss: 0.0296 - val_acc: 0.8536\n",
      "Epoch 171/180\n",
      "8564/8564 [==============================] - 5s 587us/step - loss: 0.0291 - acc: 0.8568 - val_loss: 0.0306 - val_acc: 0.8542\n",
      "Epoch 172/180\n",
      "8564/8564 [==============================] - 5s 598us/step - loss: 0.0289 - acc: 0.8558 - val_loss: 0.0288 - val_acc: 0.8497\n",
      "Epoch 173/180\n",
      "8564/8564 [==============================] - 5s 557us/step - loss: 0.0291 - acc: 0.8570 - val_loss: 0.0289 - val_acc: 0.8546\n",
      "Epoch 174/180\n",
      "8564/8564 [==============================] - 5s 577us/step - loss: 0.0294 - acc: 0.8526 - val_loss: 0.0291 - val_acc: 0.8527\n",
      "Epoch 175/180\n",
      "8564/8564 [==============================] - 5s 574us/step - loss: 0.0294 - acc: 0.8476 - val_loss: 0.0305 - val_acc: 0.8508\n",
      "Epoch 176/180\n",
      "8564/8564 [==============================] - 5s 588us/step - loss: 0.0293 - acc: 0.8491 - val_loss: 0.0289 - val_acc: 0.8564\n",
      "Epoch 177/180\n",
      "8564/8564 [==============================] - 5s 585us/step - loss: 0.0291 - acc: 0.8513 - val_loss: 0.0288 - val_acc: 0.8559\n",
      "Epoch 178/180\n",
      "8564/8564 [==============================] - 5s 580us/step - loss: 0.0287 - acc: 0.8526 - val_loss: 0.0302 - val_acc: 0.8499\n",
      "Epoch 179/180\n",
      "8564/8564 [==============================] - 5s 599us/step - loss: 0.0291 - acc: 0.8490 - val_loss: 0.0285 - val_acc: 0.8566\n",
      "Epoch 180/180\n",
      "8564/8564 [==============================] - 5s 595us/step - loss: 0.0288 - acc: 0.8514 - val_loss: 0.0305 - val_acc: 0.8521\n",
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 161/180\n",
      "8564/8564 [==============================] - 5s 613us/step - loss: 0.0312 - acc: 0.9749 - val_loss: 0.0351 - val_acc: 0.9794\n",
      "Epoch 162/180\n",
      "8564/8564 [==============================] - 5s 545us/step - loss: 0.0337 - acc: 0.9744 - val_loss: 0.0339 - val_acc: 0.9771\n",
      "Epoch 163/180\n",
      "8564/8564 [==============================] - 5s 561us/step - loss: 0.0313 - acc: 0.9752 - val_loss: 0.0407 - val_acc: 0.9631\n",
      "Epoch 164/180\n",
      "8564/8564 [==============================] - 5s 590us/step - loss: 0.0309 - acc: 0.9762 - val_loss: 0.0353 - val_acc: 0.9755\n",
      "Epoch 165/180\n",
      "8564/8564 [==============================] - 5s 546us/step - loss: 0.0318 - acc: 0.9776 - val_loss: 0.0387 - val_acc: 0.9773\n",
      "Epoch 166/180\n",
      "8564/8564 [==============================] - 5s 546us/step - loss: 0.0322 - acc: 0.9778 - val_loss: 0.0360 - val_acc: 0.9781\n",
      "Epoch 167/180\n",
      "8564/8564 [==============================] - 5s 551us/step - loss: 0.0319 - acc: 0.9754 - val_loss: 0.0274 - val_acc: 0.9831\n",
      "Epoch 168/180\n",
      "8564/8564 [==============================] - 5s 544us/step - loss: 0.0304 - acc: 0.9779 - val_loss: 0.0320 - val_acc: 0.9785\n",
      "Epoch 169/180\n",
      "8564/8564 [==============================] - 5s 531us/step - loss: 0.0312 - acc: 0.9765 - val_loss: 0.0290 - val_acc: 0.9828\n",
      "Epoch 170/180\n",
      "8564/8564 [==============================] - 5s 559us/step - loss: 0.0298 - acc: 0.9774 - val_loss: 0.0299 - val_acc: 0.9803\n",
      "Epoch 171/180\n",
      "8564/8564 [==============================] - 5s 541us/step - loss: 0.0318 - acc: 0.9740 - val_loss: 0.0279 - val_acc: 0.9815\n",
      "Epoch 172/180\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 5s 564us/step - loss: 0.0327 - acc: 0.9769 - val_loss: 0.0298 - val_acc: 0.9768\n",
      "Epoch 173/180\n",
      "8564/8564 [==============================] - 5s 565us/step - loss: 0.0313 - acc: 0.9770 - val_loss: 0.0272 - val_acc: 0.9818\n",
      "Epoch 174/180\n",
      "8564/8564 [==============================] - 5s 558us/step - loss: 0.0316 - acc: 0.9772 - val_loss: 0.0264 - val_acc: 0.9811\n",
      "Epoch 175/180\n",
      "8564/8564 [==============================] - 5s 569us/step - loss: 0.0295 - acc: 0.9776 - val_loss: 0.0306 - val_acc: 0.9769\n",
      "Epoch 176/180\n",
      "8564/8564 [==============================] - 5s 560us/step - loss: 0.0303 - acc: 0.9774 - val_loss: 0.0302 - val_acc: 0.9819\n",
      "Epoch 177/180\n",
      "8564/8564 [==============================] - 5s 536us/step - loss: 0.0314 - acc: 0.9771 - val_loss: 0.0290 - val_acc: 0.9820\n",
      "Epoch 178/180\n",
      "8564/8564 [==============================] - 5s 559us/step - loss: 0.0320 - acc: 0.9762 - val_loss: 0.0279 - val_acc: 0.9800\n",
      "Epoch 179/180\n",
      "8564/8564 [==============================] - 5s 551us/step - loss: 0.0321 - acc: 0.9768 - val_loss: 0.0265 - val_acc: 0.9773\n",
      "Epoch 180/180\n",
      "8564/8564 [==============================] - 5s 554us/step - loss: 0.0314 - acc: 0.9761 - val_loss: 0.0253 - val_acc: 0.9806\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 161/180\n",
      "8564/8564 [==============================] - 5s 557us/step - loss: 0.0375 - acc: 0.9131 - val_loss: 0.0415 - val_acc: 0.8980\n",
      "Epoch 162/180\n",
      "8564/8564 [==============================] - 5s 578us/step - loss: 0.0367 - acc: 0.9147 - val_loss: 0.0373 - val_acc: 0.9068\n",
      "Epoch 163/180\n",
      "8564/8564 [==============================] - 5s 593us/step - loss: 0.0364 - acc: 0.9163 - val_loss: 0.0329 - val_acc: 0.9283\n",
      "Epoch 164/180\n",
      "8564/8564 [==============================] - 5s 557us/step - loss: 0.0371 - acc: 0.9155 - val_loss: 0.0343 - val_acc: 0.9213\n",
      "Epoch 165/180\n",
      "8564/8564 [==============================] - 5s 568us/step - loss: 0.0388 - acc: 0.9125 - val_loss: 0.0359 - val_acc: 0.9227\n",
      "Epoch 166/180\n",
      "8564/8564 [==============================] - 5s 553us/step - loss: 0.0381 - acc: 0.9130 - val_loss: 0.0439 - val_acc: 0.9181\n",
      "Epoch 167/180\n",
      "8564/8564 [==============================] - 5s 574us/step - loss: 0.0375 - acc: 0.9145 - val_loss: 0.0346 - val_acc: 0.9291\n",
      "Epoch 168/180\n",
      "8564/8564 [==============================] - 5s 561us/step - loss: 0.0385 - acc: 0.9141 - val_loss: 0.0371 - val_acc: 0.9137\n",
      "Epoch 169/180\n",
      "8564/8564 [==============================] - 5s 589us/step - loss: 0.0379 - acc: 0.9135 - val_loss: 0.0359 - val_acc: 0.9194\n",
      "Epoch 170/180\n",
      "8564/8564 [==============================] - 5s 567us/step - loss: 0.0384 - acc: 0.9157 - val_loss: 0.0389 - val_acc: 0.9253\n",
      "Epoch 171/180\n",
      "8564/8564 [==============================] - 5s 567us/step - loss: 0.0374 - acc: 0.9153 - val_loss: 0.0360 - val_acc: 0.9192\n",
      "Epoch 172/180\n",
      "8564/8564 [==============================] - 5s 557us/step - loss: 0.0371 - acc: 0.9159 - val_loss: 0.0400 - val_acc: 0.9180\n",
      "Epoch 173/180\n",
      "8564/8564 [==============================] - 5s 540us/step - loss: 0.0383 - acc: 0.9173 - val_loss: 0.0363 - val_acc: 0.9343\n",
      "Epoch 174/180\n",
      "8564/8564 [==============================] - 5s 563us/step - loss: 0.0376 - acc: 0.9149 - val_loss: 0.0351 - val_acc: 0.9262\n",
      "Epoch 175/180\n",
      "8564/8564 [==============================] - 5s 556us/step - loss: 0.0379 - acc: 0.9135 - val_loss: 0.0321 - val_acc: 0.9288\n",
      "Epoch 176/180\n",
      "8564/8564 [==============================] - 5s 571us/step - loss: 0.0374 - acc: 0.9144 - val_loss: 0.0336 - val_acc: 0.9290\n",
      "Epoch 177/180\n",
      "8564/8564 [==============================] - 5s 556us/step - loss: 0.0370 - acc: 0.9163 - val_loss: 0.0357 - val_acc: 0.9337\n",
      "Epoch 178/180\n",
      "8564/8564 [==============================] - 5s 535us/step - loss: 0.0372 - acc: 0.9155 - val_loss: 0.0325 - val_acc: 0.9369\n",
      "Epoch 179/180\n",
      "8564/8564 [==============================] - 5s 547us/step - loss: 0.0366 - acc: 0.9157 - val_loss: 0.0436 - val_acc: 0.8946\n",
      "Epoch 180/180\n",
      "8564/8564 [==============================] - 5s 567us/step - loss: 0.0376 - acc: 0.9156 - val_loss: 0.0453 - val_acc: 0.9058\n",
      "start training round 9\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 181/200\n",
      "8564/8564 [==============================] - 5s 557us/step - loss: 0.0339 - acc: 0.5839 - val_loss: 0.0343 - val_acc: 0.5197\n",
      "Epoch 182/200\n",
      "8564/8564 [==============================] - 5s 554us/step - loss: 0.0342 - acc: 0.6273 - val_loss: 0.0344 - val_acc: 0.6356\n",
      "Epoch 183/200\n",
      "8564/8564 [==============================] - 5s 555us/step - loss: 0.0335 - acc: 0.5943 - val_loss: 0.0354 - val_acc: 0.6435\n",
      "Epoch 184/200\n",
      "8564/8564 [==============================] - 5s 570us/step - loss: 0.0340 - acc: 0.5815 - val_loss: 0.0346 - val_acc: 0.6451\n",
      "Epoch 185/200\n",
      "8564/8564 [==============================] - 5s 548us/step - loss: 0.0334 - acc: 0.5990 - val_loss: 0.0352 - val_acc: 0.6460\n",
      "Epoch 186/200\n",
      "8564/8564 [==============================] - 5s 566us/step - loss: 0.0336 - acc: 0.6294 - val_loss: 0.0345 - val_acc: 0.5160\n",
      "Epoch 187/200\n",
      "8564/8564 [==============================] - 5s 557us/step - loss: 0.0334 - acc: 0.6129 - val_loss: 0.0339 - val_acc: 0.6338\n",
      "Epoch 188/200\n",
      "8564/8564 [==============================] - 5s 560us/step - loss: 0.0331 - acc: 0.6243 - val_loss: 0.0336 - val_acc: 0.6340\n",
      "Epoch 189/200\n",
      "8564/8564 [==============================] - 5s 570us/step - loss: 0.0335 - acc: 0.6347 - val_loss: 0.0332 - val_acc: 0.6518\n",
      "Epoch 190/200\n",
      "8564/8564 [==============================] - 5s 555us/step - loss: 0.0335 - acc: 0.6472 - val_loss: 0.0352 - val_acc: 0.5919\n",
      "Epoch 191/200\n",
      "8564/8564 [==============================] - 5s 586us/step - loss: 0.0333 - acc: 0.6298 - val_loss: 0.0338 - val_acc: 0.6538\n",
      "Epoch 192/200\n",
      "8564/8564 [==============================] - 5s 552us/step - loss: 0.0334 - acc: 0.6286 - val_loss: 0.0358 - val_acc: 0.6193\n",
      "Epoch 193/200\n",
      "8564/8564 [==============================] - 5s 564us/step - loss: 0.0336 - acc: 0.6341 - val_loss: 0.0335 - val_acc: 0.6341\n",
      "Epoch 194/200\n",
      "8564/8564 [==============================] - 5s 569us/step - loss: 0.0333 - acc: 0.6015 - val_loss: 0.0342 - val_acc: 0.6502\n",
      "Epoch 195/200\n",
      "8564/8564 [==============================] - 5s 566us/step - loss: 0.0336 - acc: 0.5920 - val_loss: 0.0336 - val_acc: 0.6518\n",
      "Epoch 196/200\n",
      "8564/8564 [==============================] - 5s 578us/step - loss: 0.0330 - acc: 0.6179 - val_loss: 0.0335 - val_acc: 0.6551\n",
      "Epoch 197/200\n",
      "8564/8564 [==============================] - 5s 551us/step - loss: 0.0329 - acc: 0.6074 - val_loss: 0.0341 - val_acc: 0.6466\n",
      "Epoch 198/200\n",
      "8564/8564 [==============================] - 5s 548us/step - loss: 0.0335 - acc: 0.5975 - val_loss: 0.0335 - val_acc: 0.6526\n",
      "Epoch 199/200\n",
      "8564/8564 [==============================] - 5s 556us/step - loss: 0.0332 - acc: 0.6030 - val_loss: 0.0335 - val_acc: 0.6584\n",
      "Epoch 200/200\n",
      "8564/8564 [==============================] - 5s 564us/step - loss: 0.0334 - acc: 0.6063 - val_loss: 0.0340 - val_acc: 0.5061\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 181/200\n",
      "8564/8564 [==============================] - 5s 546us/step - loss: 0.0286 - acc: 0.8528 - val_loss: 0.0296 - val_acc: 0.8539\n",
      "Epoch 182/200\n",
      "8564/8564 [==============================] - 5s 538us/step - loss: 0.0290 - acc: 0.8529 - val_loss: 0.0284 - val_acc: 0.8567\n",
      "Epoch 183/200\n",
      "8564/8564 [==============================] - 5s 550us/step - loss: 0.0288 - acc: 0.8522 - val_loss: 0.0294 - val_acc: 0.8511\n",
      "Epoch 184/200\n",
      "8564/8564 [==============================] - 5s 590us/step - loss: 0.0288 - acc: 0.8517 - val_loss: 0.0293 - val_acc: 0.8524\n",
      "Epoch 185/200\n",
      "8564/8564 [==============================] - 5s 557us/step - loss: 0.0286 - acc: 0.8515 - val_loss: 0.0289 - val_acc: 0.8555\n",
      "Epoch 186/200\n",
      "8564/8564 [==============================] - 5s 559us/step - loss: 0.0285 - acc: 0.8562 - val_loss: 0.0292 - val_acc: 0.8498\n",
      "Epoch 187/200\n",
      "8564/8564 [==============================] - 5s 556us/step - loss: 0.0288 - acc: 0.8543 - val_loss: 0.0287 - val_acc: 0.8477\n",
      "Epoch 188/200\n",
      "8564/8564 [==============================] - 5s 574us/step - loss: 0.0288 - acc: 0.8506 - val_loss: 0.0297 - val_acc: 0.8427\n",
      "Epoch 189/200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 5s 586us/step - loss: 0.0286 - acc: 0.8515 - val_loss: 0.0292 - val_acc: 0.8438\n",
      "Epoch 190/200\n",
      "8564/8564 [==============================] - 5s 564us/step - loss: 0.0287 - acc: 0.8520 - val_loss: 0.0289 - val_acc: 0.8413\n",
      "Epoch 191/200\n",
      "8564/8564 [==============================] - 5s 570us/step - loss: 0.0286 - acc: 0.8515 - val_loss: 0.0294 - val_acc: 0.8440\n",
      "Epoch 192/200\n",
      "8564/8564 [==============================] - 4s 515us/step - loss: 0.0285 - acc: 0.8515 - val_loss: 0.0286 - val_acc: 0.8537\n",
      "Epoch 193/200\n",
      "8564/8564 [==============================] - 5s 555us/step - loss: 0.0283 - acc: 0.8565 - val_loss: 0.0290 - val_acc: 0.8564\n",
      "Epoch 194/200\n",
      "8564/8564 [==============================] - 5s 557us/step - loss: 0.0284 - acc: 0.8580 - val_loss: 0.0283 - val_acc: 0.8558\n",
      "Epoch 195/200\n",
      "8564/8564 [==============================] - 5s 564us/step - loss: 0.0288 - acc: 0.8580 - val_loss: 0.0292 - val_acc: 0.8510\n",
      "Epoch 196/200\n",
      "8564/8564 [==============================] - 5s 547us/step - loss: 0.0286 - acc: 0.8519 - val_loss: 0.0291 - val_acc: 0.8463\n",
      "Epoch 197/200\n",
      "8564/8564 [==============================] - 5s 573us/step - loss: 0.0286 - acc: 0.8516 - val_loss: 0.0279 - val_acc: 0.8490\n",
      "Epoch 198/200\n",
      "8564/8564 [==============================] - 5s 536us/step - loss: 0.0286 - acc: 0.8519 - val_loss: 0.0277 - val_acc: 0.8561\n",
      "Epoch 199/200\n",
      "8564/8564 [==============================] - 5s 558us/step - loss: 0.0280 - acc: 0.8557 - val_loss: 0.0277 - val_acc: 0.8494\n",
      "Epoch 200/200\n",
      "8564/8564 [==============================] - 5s 549us/step - loss: 0.0281 - acc: 0.8547 - val_loss: 0.0291 - val_acc: 0.8441\n",
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 181/200\n",
      "8564/8564 [==============================] - 5s 557us/step - loss: 0.0299 - acc: 0.9768 - val_loss: 0.0283 - val_acc: 0.9785\n",
      "Epoch 182/200\n",
      "8564/8564 [==============================] - 5s 558us/step - loss: 0.0302 - acc: 0.9769 - val_loss: 0.0322 - val_acc: 0.9698\n",
      "Epoch 183/200\n",
      "8564/8564 [==============================] - 5s 544us/step - loss: 0.0299 - acc: 0.9780 - val_loss: 0.0249 - val_acc: 0.9802\n",
      "Epoch 184/200\n",
      "8564/8564 [==============================] - 5s 532us/step - loss: 0.0312 - acc: 0.9767 - val_loss: 0.0282 - val_acc: 0.9777\n",
      "Epoch 185/200\n",
      "8564/8564 [==============================] - 5s 541us/step - loss: 0.0300 - acc: 0.9756 - val_loss: 0.0273 - val_acc: 0.9807\n",
      "Epoch 186/200\n",
      "8564/8564 [==============================] - 5s 567us/step - loss: 0.0317 - acc: 0.9758 - val_loss: 0.0319 - val_acc: 0.9761\n",
      "Epoch 187/200\n",
      "8564/8564 [==============================] - 5s 532us/step - loss: 0.0309 - acc: 0.9776 - val_loss: 0.0340 - val_acc: 0.9782\n",
      "Epoch 188/200\n",
      "8564/8564 [==============================] - 5s 569us/step - loss: 0.0304 - acc: 0.9760 - val_loss: 0.0355 - val_acc: 0.9662\n",
      "Epoch 189/200\n",
      "8564/8564 [==============================] - 5s 588us/step - loss: 0.0309 - acc: 0.9768 - val_loss: 0.0312 - val_acc: 0.9717\n",
      "Epoch 190/200\n",
      "8564/8564 [==============================] - 5s 549us/step - loss: 0.0330 - acc: 0.9776 - val_loss: 0.0271 - val_acc: 0.9830\n",
      "Epoch 191/200\n",
      "8564/8564 [==============================] - 5s 550us/step - loss: 0.0307 - acc: 0.9767 - val_loss: 0.0268 - val_acc: 0.9789\n",
      "Epoch 192/200\n",
      "8564/8564 [==============================] - 5s 566us/step - loss: 0.0293 - acc: 0.9774 - val_loss: 0.0401 - val_acc: 0.9793\n",
      "Epoch 193/200\n",
      "8564/8564 [==============================] - 5s 528us/step - loss: 0.0299 - acc: 0.9764 - val_loss: 0.0243 - val_acc: 0.9808\n",
      "Epoch 194/200\n",
      "8564/8564 [==============================] - 5s 542us/step - loss: 0.0290 - acc: 0.9786 - val_loss: 0.0287 - val_acc: 0.9810\n",
      "Epoch 195/200\n",
      "8564/8564 [==============================] - 5s 546us/step - loss: 0.0288 - acc: 0.9789 - val_loss: 0.0264 - val_acc: 0.9804\n",
      "Epoch 196/200\n",
      "8564/8564 [==============================] - 5s 565us/step - loss: 0.0302 - acc: 0.9782 - val_loss: 0.0313 - val_acc: 0.9817\n",
      "Epoch 197/200\n",
      "8564/8564 [==============================] - 5s 566us/step - loss: 0.0295 - acc: 0.9781 - val_loss: 0.0329 - val_acc: 0.9837\n",
      "Epoch 198/200\n",
      "8564/8564 [==============================] - 5s 540us/step - loss: 0.0299 - acc: 0.9791 - val_loss: 0.0267 - val_acc: 0.9800\n",
      "Epoch 199/200\n",
      "8564/8564 [==============================] - 5s 552us/step - loss: 0.0306 - acc: 0.9764 - val_loss: 0.0307 - val_acc: 0.9792\n",
      "Epoch 200/200\n",
      "8564/8564 [==============================] - 5s 543us/step - loss: 0.0295 - acc: 0.9786 - val_loss: 0.0311 - val_acc: 0.9752\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 181/200\n",
      "8564/8564 [==============================] - 5s 552us/step - loss: 0.0360 - acc: 0.9175 - val_loss: 0.0341 - val_acc: 0.9252\n",
      "Epoch 182/200\n",
      "8564/8564 [==============================] - 5s 544us/step - loss: 0.0369 - acc: 0.9171 - val_loss: 0.0419 - val_acc: 0.9091\n",
      "Epoch 183/200\n",
      "8564/8564 [==============================] - 5s 591us/step - loss: 0.0387 - acc: 0.9120 - val_loss: 0.0324 - val_acc: 0.9294\n",
      "Epoch 184/200\n",
      "8564/8564 [==============================] - 5s 558us/step - loss: 0.0372 - acc: 0.9163 - val_loss: 0.0344 - val_acc: 0.9385\n",
      "Epoch 185/200\n",
      "8564/8564 [==============================] - 5s 556us/step - loss: 0.0362 - acc: 0.9153 - val_loss: 0.0307 - val_acc: 0.9365\n",
      "Epoch 186/200\n",
      "8564/8564 [==============================] - 5s 558us/step - loss: 0.0366 - acc: 0.9182 - val_loss: 0.0316 - val_acc: 0.9234\n",
      "Epoch 187/200\n",
      "8564/8564 [==============================] - 4s 523us/step - loss: 0.0366 - acc: 0.9148 - val_loss: 0.0418 - val_acc: 0.9090\n",
      "Epoch 188/200\n",
      "8564/8564 [==============================] - 5s 528us/step - loss: 0.0372 - acc: 0.9155 - val_loss: 0.0321 - val_acc: 0.9381\n",
      "Epoch 189/200\n",
      "8564/8564 [==============================] - 5s 568us/step - loss: 0.0378 - acc: 0.9121 - val_loss: 0.0360 - val_acc: 0.9212\n",
      "Epoch 190/200\n",
      "8564/8564 [==============================] - 5s 548us/step - loss: 0.0363 - acc: 0.9183 - val_loss: 0.0305 - val_acc: 0.9342\n",
      "Epoch 191/200\n",
      "8564/8564 [==============================] - 5s 548us/step - loss: 0.0365 - acc: 0.9177 - val_loss: 0.0333 - val_acc: 0.9295\n",
      "Epoch 192/200\n",
      "8564/8564 [==============================] - 5s 571us/step - loss: 0.0361 - acc: 0.9157 - val_loss: 0.0328 - val_acc: 0.9257\n",
      "Epoch 193/200\n",
      "8564/8564 [==============================] - 5s 555us/step - loss: 0.0364 - acc: 0.9194 - val_loss: 0.0334 - val_acc: 0.9215\n",
      "Epoch 194/200\n",
      "8564/8564 [==============================] - 5s 552us/step - loss: 0.0367 - acc: 0.9197 - val_loss: 0.0397 - val_acc: 0.9223\n",
      "Epoch 195/200\n",
      "8564/8564 [==============================] - 5s 543us/step - loss: 0.0360 - acc: 0.9175 - val_loss: 0.0344 - val_acc: 0.9192\n",
      "Epoch 196/200\n",
      "8564/8564 [==============================] - 5s 569us/step - loss: 0.0370 - acc: 0.9157 - val_loss: 0.0321 - val_acc: 0.9313\n",
      "Epoch 197/200\n",
      "8564/8564 [==============================] - 5s 535us/step - loss: 0.0355 - acc: 0.9192 - val_loss: 0.0363 - val_acc: 0.9288\n",
      "Epoch 198/200\n",
      "8564/8564 [==============================] - 5s 557us/step - loss: 0.0357 - acc: 0.9158 - val_loss: 0.0396 - val_acc: 0.9230\n",
      "Epoch 199/200\n",
      "8564/8564 [==============================] - 5s 531us/step - loss: 0.0346 - acc: 0.9220 - val_loss: 0.0357 - val_acc: 0.9175\n",
      "Epoch 200/200\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0364 - acc: 0.9159 - val_loss: 0.0301 - val_acc: 0.9277\n",
      "start training round 10\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 201/220\n",
      "8564/8564 [==============================] - 5s 569us/step - loss: 0.0328 - acc: 0.6133 - val_loss: 0.0346 - val_acc: 0.4869\n",
      "Epoch 202/220\n",
      "8564/8564 [==============================] - 5s 547us/step - loss: 0.0331 - acc: 0.6093 - val_loss: 0.0326 - val_acc: 0.6618\n",
      "Epoch 203/220\n",
      "8564/8564 [==============================] - 5s 530us/step - loss: 0.0331 - acc: 0.5958 - val_loss: 0.0332 - val_acc: 0.5367\n",
      "Epoch 204/220\n",
      "8564/8564 [==============================] - 5s 555us/step - loss: 0.0333 - acc: 0.6410 - val_loss: 0.0329 - val_acc: 0.5511\n",
      "Epoch 205/220\n",
      "8564/8564 [==============================] - 5s 566us/step - loss: 0.0330 - acc: 0.5943 - val_loss: 0.0328 - val_acc: 0.6307\n",
      "Epoch 206/220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 5s 557us/step - loss: 0.0329 - acc: 0.6089 - val_loss: 0.0332 - val_acc: 0.6628\n",
      "Epoch 207/220\n",
      "8564/8564 [==============================] - 5s 593us/step - loss: 0.0333 - acc: 0.6428 - val_loss: 0.0328 - val_acc: 0.6358\n",
      "Epoch 208/220\n",
      "8564/8564 [==============================] - 5s 543us/step - loss: 0.0329 - acc: 0.6110 - val_loss: 0.0328 - val_acc: 0.6239\n",
      "Epoch 209/220\n",
      "8564/8564 [==============================] - 5s 568us/step - loss: 0.0330 - acc: 0.5970 - val_loss: 0.0323 - val_acc: 0.6419\n",
      "Epoch 210/220\n",
      "8564/8564 [==============================] - 5s 561us/step - loss: 0.0325 - acc: 0.6024 - val_loss: 0.0322 - val_acc: 0.6431\n",
      "Epoch 211/220\n",
      "8564/8564 [==============================] - 5s 544us/step - loss: 0.0327 - acc: 0.6601 - val_loss: 0.0326 - val_acc: 0.6038\n",
      "Epoch 212/220\n",
      "8564/8564 [==============================] - 5s 568us/step - loss: 0.0326 - acc: 0.6356 - val_loss: 0.0332 - val_acc: 0.6644\n",
      "Epoch 213/220\n",
      "8564/8564 [==============================] - 5s 567us/step - loss: 0.0328 - acc: 0.6498 - val_loss: 0.0330 - val_acc: 0.6616\n",
      "Epoch 214/220\n",
      "8564/8564 [==============================] - 5s 567us/step - loss: 0.0326 - acc: 0.6066 - val_loss: 0.0332 - val_acc: 0.5219\n",
      "Epoch 215/220\n",
      "8564/8564 [==============================] - 5s 544us/step - loss: 0.0331 - acc: 0.6267 - val_loss: 0.0330 - val_acc: 0.6534\n",
      "Epoch 216/220\n",
      "8564/8564 [==============================] - 5s 550us/step - loss: 0.0331 - acc: 0.6483 - val_loss: 0.0340 - val_acc: 0.6660\n",
      "Epoch 217/220\n",
      "8564/8564 [==============================] - 5s 539us/step - loss: 0.0328 - acc: 0.6290 - val_loss: 0.0350 - val_acc: 0.4909\n",
      "Epoch 218/220\n",
      "8564/8564 [==============================] - 5s 550us/step - loss: 0.0327 - acc: 0.6411 - val_loss: 0.0334 - val_acc: 0.6670\n",
      "Epoch 219/220\n",
      "8564/8564 [==============================] - 5s 536us/step - loss: 0.0323 - acc: 0.6264 - val_loss: 0.0330 - val_acc: 0.5362\n",
      "Epoch 220/220\n",
      "8564/8564 [==============================] - 5s 581us/step - loss: 0.0326 - acc: 0.6014 - val_loss: 0.0329 - val_acc: 0.5352\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 201/220\n",
      "8564/8564 [==============================] - 5s 564us/step - loss: 0.0283 - acc: 0.8527 - val_loss: 0.0290 - val_acc: 0.8486\n",
      "Epoch 202/220\n",
      "8564/8564 [==============================] - 5s 526us/step - loss: 0.0283 - acc: 0.8518 - val_loss: 0.0285 - val_acc: 0.8473\n",
      "Epoch 203/220\n",
      "8564/8564 [==============================] - 5s 531us/step - loss: 0.0282 - acc: 0.8516 - val_loss: 0.0279 - val_acc: 0.8491\n",
      "Epoch 204/220\n",
      "8564/8564 [==============================] - 5s 552us/step - loss: 0.0281 - acc: 0.8536 - val_loss: 0.0290 - val_acc: 0.8440\n",
      "Epoch 205/220\n",
      "8564/8564 [==============================] - 5s 591us/step - loss: 0.0283 - acc: 0.8527 - val_loss: 0.0290 - val_acc: 0.8435\n",
      "Epoch 206/220\n",
      "8564/8564 [==============================] - 5s 556us/step - loss: 0.0283 - acc: 0.8518 - val_loss: 0.0287 - val_acc: 0.8470\n",
      "Epoch 207/220\n",
      "8564/8564 [==============================] - 5s 564us/step - loss: 0.0280 - acc: 0.8532 - val_loss: 0.0281 - val_acc: 0.8487\n",
      "Epoch 208/220\n",
      "8564/8564 [==============================] - 5s 566us/step - loss: 0.0281 - acc: 0.8533 - val_loss: 0.0279 - val_acc: 0.8499\n",
      "Epoch 209/220\n",
      "8564/8564 [==============================] - 4s 522us/step - loss: 0.0280 - acc: 0.8535 - val_loss: 0.0289 - val_acc: 0.8450\n",
      "Epoch 210/220\n",
      "8564/8564 [==============================] - 5s 563us/step - loss: 0.0282 - acc: 0.8528 - val_loss: 0.0290 - val_acc: 0.8458\n",
      "Epoch 211/220\n",
      "8564/8564 [==============================] - 5s 533us/step - loss: 0.0278 - acc: 0.8546 - val_loss: 0.0280 - val_acc: 0.8475\n",
      "Epoch 212/220\n",
      "8564/8564 [==============================] - 5s 547us/step - loss: 0.0279 - acc: 0.8543 - val_loss: 0.0271 - val_acc: 0.8521\n",
      "Epoch 213/220\n",
      "8564/8564 [==============================] - 5s 561us/step - loss: 0.0277 - acc: 0.8547 - val_loss: 0.0288 - val_acc: 0.8452\n",
      "Epoch 214/220\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0280 - acc: 0.8533 - val_loss: 0.0277 - val_acc: 0.8502\n",
      "Epoch 215/220\n",
      "8564/8564 [==============================] - 5s 573us/step - loss: 0.0279 - acc: 0.8530 - val_loss: 0.0281 - val_acc: 0.8478\n",
      "Epoch 216/220\n",
      "8564/8564 [==============================] - 5s 566us/step - loss: 0.0276 - acc: 0.8555 - val_loss: 0.0279 - val_acc: 0.8497\n",
      "Epoch 217/220\n",
      "8564/8564 [==============================] - 5s 549us/step - loss: 0.0278 - acc: 0.8538 - val_loss: 0.0289 - val_acc: 0.8465\n",
      "Epoch 218/220\n",
      "8564/8564 [==============================] - 5s 560us/step - loss: 0.0279 - acc: 0.8535 - val_loss: 0.0285 - val_acc: 0.8485\n",
      "Epoch 219/220\n",
      "8564/8564 [==============================] - 5s 532us/step - loss: 0.0276 - acc: 0.8550 - val_loss: 0.0287 - val_acc: 0.8469\n",
      "Epoch 220/220\n",
      "8564/8564 [==============================] - 5s 560us/step - loss: 0.0277 - acc: 0.8545 - val_loss: 0.0274 - val_acc: 0.8550\n",
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 201/220\n",
      "8564/8564 [==============================] - 4s 525us/step - loss: 0.0289 - acc: 0.9788 - val_loss: 0.0317 - val_acc: 0.9778\n",
      "Epoch 202/220\n",
      "8564/8564 [==============================] - 5s 564us/step - loss: 0.0306 - acc: 0.9776 - val_loss: 0.0247 - val_acc: 0.9832\n",
      "Epoch 203/220\n",
      "8564/8564 [==============================] - 5s 567us/step - loss: 0.0298 - acc: 0.9777 - val_loss: 0.0286 - val_acc: 0.9823\n",
      "Epoch 204/220\n",
      "8564/8564 [==============================] - 5s 551us/step - loss: 0.0295 - acc: 0.9768 - val_loss: 0.0339 - val_acc: 0.9604\n",
      "Epoch 205/220\n",
      "8564/8564 [==============================] - 5s 571us/step - loss: 0.0313 - acc: 0.9779 - val_loss: 0.0334 - val_acc: 0.9788\n",
      "Epoch 206/220\n",
      "8564/8564 [==============================] - 5s 563us/step - loss: 0.0288 - acc: 0.9775 - val_loss: 0.0260 - val_acc: 0.9821\n",
      "Epoch 207/220\n",
      "8564/8564 [==============================] - 5s 575us/step - loss: 0.0303 - acc: 0.9779 - val_loss: 0.0332 - val_acc: 0.9778\n",
      "Epoch 208/220\n",
      "8564/8564 [==============================] - 5s 553us/step - loss: 0.0306 - acc: 0.9762 - val_loss: 0.0286 - val_acc: 0.9831\n",
      "Epoch 209/220\n",
      "8564/8564 [==============================] - 5s 529us/step - loss: 0.0299 - acc: 0.9772 - val_loss: 0.0365 - val_acc: 0.9803\n",
      "Epoch 210/220\n",
      "8564/8564 [==============================] - 5s 533us/step - loss: 0.0316 - acc: 0.9772 - val_loss: 0.0286 - val_acc: 0.9781\n",
      "Epoch 211/220\n",
      "8564/8564 [==============================] - 5s 541us/step - loss: 0.0299 - acc: 0.9784 - val_loss: 0.0265 - val_acc: 0.9819\n",
      "Epoch 212/220\n",
      "8564/8564 [==============================] - 5s 587us/step - loss: 0.0296 - acc: 0.9791 - val_loss: 0.0321 - val_acc: 0.9795\n",
      "Epoch 213/220\n",
      "8564/8564 [==============================] - 5s 538us/step - loss: 0.0309 - acc: 0.9779 - val_loss: 0.0263 - val_acc: 0.9827\n",
      "Epoch 214/220\n",
      "8564/8564 [==============================] - 5s 553us/step - loss: 0.0309 - acc: 0.9790 - val_loss: 0.0292 - val_acc: 0.9762\n",
      "Epoch 215/220\n",
      "8564/8564 [==============================] - 5s 553us/step - loss: 0.0290 - acc: 0.9792 - val_loss: 0.0334 - val_acc: 0.9817\n",
      "Epoch 216/220\n",
      "8564/8564 [==============================] - 5s 563us/step - loss: 0.0307 - acc: 0.9773 - val_loss: 0.0393 - val_acc: 0.9830\n",
      "Epoch 217/220\n",
      "8564/8564 [==============================] - 5s 531us/step - loss: 0.0304 - acc: 0.9759 - val_loss: 0.0355 - val_acc: 0.9813\n",
      "Epoch 218/220\n",
      "8564/8564 [==============================] - 5s 532us/step - loss: 0.0299 - acc: 0.9772 - val_loss: 0.0304 - val_acc: 0.9806\n",
      "Epoch 219/220\n",
      "8564/8564 [==============================] - 4s 524us/step - loss: 0.0302 - acc: 0.9794 - val_loss: 0.0262 - val_acc: 0.9825\n",
      "Epoch 220/220\n",
      "8564/8564 [==============================] - 5s 567us/step - loss: 0.0300 - acc: 0.9797 - val_loss: 0.0249 - val_acc: 0.9832\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 201/220\n",
      "8564/8564 [==============================] - 5s 560us/step - loss: 0.0348 - acc: 0.9202 - val_loss: 0.0361 - val_acc: 0.9322\n",
      "Epoch 202/220\n",
      "8564/8564 [==============================] - 5s 566us/step - loss: 0.0355 - acc: 0.9193 - val_loss: 0.0334 - val_acc: 0.9298\n",
      "Epoch 203/220\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 5s 567us/step - loss: 0.0363 - acc: 0.9187 - val_loss: 0.0378 - val_acc: 0.9294\n",
      "Epoch 204/220\n",
      "8564/8564 [==============================] - 5s 552us/step - loss: 0.0363 - acc: 0.9174 - val_loss: 0.0312 - val_acc: 0.9278\n",
      "Epoch 205/220\n",
      "8564/8564 [==============================] - 5s 561us/step - loss: 0.0353 - acc: 0.9185 - val_loss: 0.0345 - val_acc: 0.9325\n",
      "Epoch 206/220\n",
      "8564/8564 [==============================] - 5s 554us/step - loss: 0.0350 - acc: 0.9207 - val_loss: 0.0328 - val_acc: 0.9311\n",
      "Epoch 207/220\n",
      "8564/8564 [==============================] - 5s 576us/step - loss: 0.0354 - acc: 0.9198 - val_loss: 0.0331 - val_acc: 0.9351\n",
      "Epoch 208/220\n",
      "8564/8564 [==============================] - 5s 565us/step - loss: 0.0364 - acc: 0.9183 - val_loss: 0.0384 - val_acc: 0.9034\n",
      "Epoch 209/220\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0359 - acc: 0.9158 - val_loss: 0.0341 - val_acc: 0.9186\n",
      "Epoch 210/220\n",
      "8564/8564 [==============================] - 5s 572us/step - loss: 0.0348 - acc: 0.9195 - val_loss: 0.0317 - val_acc: 0.9357\n",
      "Epoch 211/220\n",
      "8564/8564 [==============================] - 5s 567us/step - loss: 0.0363 - acc: 0.9169 - val_loss: 0.0331 - val_acc: 0.9370\n",
      "Epoch 212/220\n",
      "8564/8564 [==============================] - 5s 550us/step - loss: 0.0354 - acc: 0.9195 - val_loss: 0.0374 - val_acc: 0.9379\n",
      "Epoch 213/220\n",
      "8564/8564 [==============================] - 5s 570us/step - loss: 0.0342 - acc: 0.9233 - val_loss: 0.0346 - val_acc: 0.9210\n",
      "Epoch 214/220\n",
      "8564/8564 [==============================] - 5s 544us/step - loss: 0.0350 - acc: 0.9195 - val_loss: 0.0322 - val_acc: 0.9290\n",
      "Epoch 215/220\n",
      "8564/8564 [==============================] - 5s 546us/step - loss: 0.0345 - acc: 0.9199 - val_loss: 0.0299 - val_acc: 0.9305\n",
      "Epoch 216/220\n",
      "8564/8564 [==============================] - 5s 570us/step - loss: 0.0348 - acc: 0.9219 - val_loss: 0.0342 - val_acc: 0.9260\n",
      "Epoch 217/220\n",
      "8564/8564 [==============================] - 5s 546us/step - loss: 0.0348 - acc: 0.9209 - val_loss: 0.0362 - val_acc: 0.9247\n",
      "Epoch 218/220\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0352 - acc: 0.9174 - val_loss: 0.0322 - val_acc: 0.9215\n",
      "Epoch 219/220\n",
      "8564/8564 [==============================] - 5s 543us/step - loss: 0.0363 - acc: 0.9166 - val_loss: 0.0301 - val_acc: 0.9320\n",
      "Epoch 220/220\n",
      "8564/8564 [==============================] - 5s 546us/step - loss: 0.0361 - acc: 0.9182 - val_loss: 0.0345 - val_acc: 0.9306\n",
      "start training round 11\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 221/240\n",
      "8564/8564 [==============================] - 5s 574us/step - loss: 0.0321 - acc: 0.6150 - val_loss: 0.0324 - val_acc: 0.6377\n",
      "Epoch 222/240\n",
      "8564/8564 [==============================] - 5s 549us/step - loss: 0.0326 - acc: 0.5987 - val_loss: 0.0328 - val_acc: 0.6691\n",
      "Epoch 223/240\n",
      "8564/8564 [==============================] - 5s 560us/step - loss: 0.0327 - acc: 0.6677 - val_loss: 0.0328 - val_acc: 0.6583\n",
      "Epoch 224/240\n",
      "8564/8564 [==============================] - 5s 556us/step - loss: 0.0323 - acc: 0.6265 - val_loss: 0.0334 - val_acc: 0.6693\n",
      "Epoch 225/240\n",
      "8564/8564 [==============================] - 5s 548us/step - loss: 0.0322 - acc: 0.6361 - val_loss: 0.0335 - val_acc: 0.5026\n",
      "Epoch 226/240\n",
      "8564/8564 [==============================] - 5s 571us/step - loss: 0.0324 - acc: 0.5998 - val_loss: 0.0323 - val_acc: 0.5922\n",
      "Epoch 227/240\n",
      "8564/8564 [==============================] - 5s 526us/step - loss: 0.0324 - acc: 0.6327 - val_loss: 0.0323 - val_acc: 0.6677\n",
      "Epoch 228/240\n",
      "8564/8564 [==============================] - 5s 546us/step - loss: 0.0322 - acc: 0.6053 - val_loss: 0.0321 - val_acc: 0.6692\n",
      "Epoch 229/240\n",
      "8564/8564 [==============================] - 4s 522us/step - loss: 0.0324 - acc: 0.6036 - val_loss: 0.0324 - val_acc: 0.6555\n",
      "Epoch 230/240\n",
      "8564/8564 [==============================] - 5s 580us/step - loss: 0.0320 - acc: 0.6070 - val_loss: 0.0325 - val_acc: 0.6663\n",
      "Epoch 231/240\n",
      "8564/8564 [==============================] - 5s 554us/step - loss: 0.0321 - acc: 0.6503 - val_loss: 0.0325 - val_acc: 0.6675\n",
      "Epoch 232/240\n",
      "8564/8564 [==============================] - 5s 542us/step - loss: 0.0321 - acc: 0.6524 - val_loss: 0.0338 - val_acc: 0.6455\n",
      "Epoch 233/240\n",
      "8564/8564 [==============================] - 5s 548us/step - loss: 0.0321 - acc: 0.6554 - val_loss: 0.0319 - val_acc: 0.6636\n",
      "Epoch 234/240\n",
      "8564/8564 [==============================] - 4s 519us/step - loss: 0.0321 - acc: 0.6632 - val_loss: 0.0321 - val_acc: 0.6544\n",
      "Epoch 235/240\n",
      "8564/8564 [==============================] - 4s 516us/step - loss: 0.0318 - acc: 0.6586 - val_loss: 0.0328 - val_acc: 0.6349\n",
      "Epoch 236/240\n",
      "8564/8564 [==============================] - 5s 565us/step - loss: 0.0319 - acc: 0.6632 - val_loss: 0.0321 - val_acc: 0.6460\n",
      "Epoch 237/240\n",
      "8564/8564 [==============================] - 5s 540us/step - loss: 0.0318 - acc: 0.6402 - val_loss: 0.0320 - val_acc: 0.6764\n",
      "Epoch 238/240\n",
      "8564/8564 [==============================] - 5s 555us/step - loss: 0.0317 - acc: 0.6396 - val_loss: 0.0327 - val_acc: 0.5695\n",
      "Epoch 239/240\n",
      "8564/8564 [==============================] - 5s 553us/step - loss: 0.0322 - acc: 0.6569 - val_loss: 0.0326 - val_acc: 0.6243\n",
      "Epoch 240/240\n",
      "8564/8564 [==============================] - 5s 576us/step - loss: 0.0316 - acc: 0.6619 - val_loss: 0.0320 - val_acc: 0.6484\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 221/240\n",
      "8564/8564 [==============================] - 5s 558us/step - loss: 0.0283 - acc: 0.8540 - val_loss: 0.0284 - val_acc: 0.8525\n",
      "Epoch 222/240\n",
      "8564/8564 [==============================] - 5s 567us/step - loss: 0.0275 - acc: 0.8587 - val_loss: 0.0268 - val_acc: 0.8550\n",
      "Epoch 223/240\n",
      "8564/8564 [==============================] - 5s 579us/step - loss: 0.0276 - acc: 0.8562 - val_loss: 0.0282 - val_acc: 0.8491\n",
      "Epoch 224/240\n",
      "8564/8564 [==============================] - 5s 552us/step - loss: 0.0276 - acc: 0.8545 - val_loss: 0.0278 - val_acc: 0.8524\n",
      "Epoch 225/240\n",
      "8564/8564 [==============================] - 5s 564us/step - loss: 0.0277 - acc: 0.8588 - val_loss: 0.0280 - val_acc: 0.8594\n",
      "Epoch 226/240\n",
      "8564/8564 [==============================] - 5s 535us/step - loss: 0.0274 - acc: 0.8612 - val_loss: 0.0276 - val_acc: 0.8577\n",
      "Epoch 227/240\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0273 - acc: 0.8593 - val_loss: 0.0288 - val_acc: 0.8596\n",
      "Epoch 228/240\n",
      "8564/8564 [==============================] - 5s 544us/step - loss: 0.0276 - acc: 0.8617 - val_loss: 0.0269 - val_acc: 0.8600\n",
      "Epoch 229/240\n",
      "8564/8564 [==============================] - 5s 537us/step - loss: 0.0273 - acc: 0.8625 - val_loss: 0.0277 - val_acc: 0.8546\n",
      "Epoch 230/240\n",
      "8564/8564 [==============================] - 5s 565us/step - loss: 0.0275 - acc: 0.8617 - val_loss: 0.0272 - val_acc: 0.8586\n",
      "Epoch 231/240\n",
      "8564/8564 [==============================] - 5s 543us/step - loss: 0.0275 - acc: 0.8568 - val_loss: 0.0278 - val_acc: 0.8493\n",
      "Epoch 232/240\n",
      "8564/8564 [==============================] - 5s 569us/step - loss: 0.0275 - acc: 0.8551 - val_loss: 0.0272 - val_acc: 0.8514\n",
      "Epoch 233/240\n",
      "8564/8564 [==============================] - 5s 568us/step - loss: 0.0269 - acc: 0.8588 - val_loss: 0.0275 - val_acc: 0.8603\n",
      "Epoch 234/240\n",
      "8564/8564 [==============================] - 5s 574us/step - loss: 0.0275 - acc: 0.8564 - val_loss: 0.0272 - val_acc: 0.8565\n",
      "Epoch 235/240\n",
      "8564/8564 [==============================] - 5s 567us/step - loss: 0.0271 - acc: 0.8589 - val_loss: 0.0272 - val_acc: 0.8531\n",
      "Epoch 236/240\n",
      "8564/8564 [==============================] - 5s 550us/step - loss: 0.0271 - acc: 0.8579 - val_loss: 0.0283 - val_acc: 0.8503\n",
      "Epoch 237/240\n",
      "8564/8564 [==============================] - 5s 573us/step - loss: 0.0273 - acc: 0.8561 - val_loss: 0.0273 - val_acc: 0.8529\n",
      "Epoch 238/240\n",
      "8564/8564 [==============================] - 5s 556us/step - loss: 0.0274 - acc: 0.8565 - val_loss: 0.0275 - val_acc: 0.8509\n",
      "Epoch 239/240\n",
      "8564/8564 [==============================] - 5s 552us/step - loss: 0.0272 - acc: 0.8569 - val_loss: 0.0273 - val_acc: 0.8531\n",
      "Epoch 240/240\n",
      "8564/8564 [==============================] - 5s 561us/step - loss: 0.0272 - acc: 0.8562 - val_loss: 0.0267 - val_acc: 0.8543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 221/240\n",
      "8564/8564 [==============================] - 5s 570us/step - loss: 0.0285 - acc: 0.9782 - val_loss: 0.0298 - val_acc: 0.9796\n",
      "Epoch 222/240\n",
      "8564/8564 [==============================] - 5s 561us/step - loss: 0.0317 - acc: 0.9801 - val_loss: 0.0249 - val_acc: 0.9825\n",
      "Epoch 223/240\n",
      "8564/8564 [==============================] - 5s 540us/step - loss: 0.0275 - acc: 0.9795 - val_loss: 0.0262 - val_acc: 0.9830\n",
      "Epoch 224/240\n",
      "8564/8564 [==============================] - 5s 546us/step - loss: 0.0297 - acc: 0.9800 - val_loss: 0.0306 - val_acc: 0.9812\n",
      "Epoch 225/240\n",
      "8564/8564 [==============================] - 5s 588us/step - loss: 0.0297 - acc: 0.9782 - val_loss: 0.0257 - val_acc: 0.9836\n",
      "Epoch 226/240\n",
      "8564/8564 [==============================] - 5s 566us/step - loss: 0.0308 - acc: 0.9763 - val_loss: 0.0323 - val_acc: 0.9848\n",
      "Epoch 227/240\n",
      "8564/8564 [==============================] - 5s 568us/step - loss: 0.0293 - acc: 0.9770 - val_loss: 0.0272 - val_acc: 0.9742\n",
      "Epoch 228/240\n",
      "8564/8564 [==============================] - 5s 579us/step - loss: 0.0297 - acc: 0.9768 - val_loss: 0.0341 - val_acc: 0.9728\n",
      "Epoch 229/240\n",
      "8564/8564 [==============================] - 5s 531us/step - loss: 0.0302 - acc: 0.9767 - val_loss: 0.0383 - val_acc: 0.9646\n",
      "Epoch 230/240\n",
      "8564/8564 [==============================] - 4s 525us/step - loss: 0.0300 - acc: 0.9782 - val_loss: 0.0277 - val_acc: 0.9816\n",
      "Epoch 231/240\n",
      "8564/8564 [==============================] - 5s 546us/step - loss: 0.0311 - acc: 0.9757 - val_loss: 0.0289 - val_acc: 0.9767\n",
      "Epoch 232/240\n",
      "8564/8564 [==============================] - 5s 558us/step - loss: 0.0295 - acc: 0.9778 - val_loss: 0.0301 - val_acc: 0.9775\n",
      "Epoch 233/240\n",
      "8564/8564 [==============================] - 5s 558us/step - loss: 0.0296 - acc: 0.9770 - val_loss: 0.0347 - val_acc: 0.9813\n",
      "Epoch 234/240\n",
      "8564/8564 [==============================] - 5s 526us/step - loss: 0.0285 - acc: 0.9778 - val_loss: 0.0308 - val_acc: 0.9803\n",
      "Epoch 235/240\n",
      "8564/8564 [==============================] - 5s 550us/step - loss: 0.0302 - acc: 0.9808 - val_loss: 0.0268 - val_acc: 0.9838\n",
      "Epoch 236/240\n",
      "8564/8564 [==============================] - 5s 570us/step - loss: 0.0293 - acc: 0.9763 - val_loss: 0.0277 - val_acc: 0.9785\n",
      "Epoch 237/240\n",
      "8564/8564 [==============================] - 5s 554us/step - loss: 0.0294 - acc: 0.9784 - val_loss: 0.0251 - val_acc: 0.9850\n",
      "Epoch 238/240\n",
      "8564/8564 [==============================] - 5s 554us/step - loss: 0.0270 - acc: 0.9795 - val_loss: 0.0267 - val_acc: 0.9777\n",
      "Epoch 239/240\n",
      "8564/8564 [==============================] - 5s 544us/step - loss: 0.0296 - acc: 0.9780 - val_loss: 0.0258 - val_acc: 0.9834\n",
      "Epoch 240/240\n",
      "8564/8564 [==============================] - 5s 572us/step - loss: 0.0286 - acc: 0.9799 - val_loss: 0.0287 - val_acc: 0.9769\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 221/240\n",
      "8564/8564 [==============================] - 5s 561us/step - loss: 0.0346 - acc: 0.9211 - val_loss: 0.0333 - val_acc: 0.9330\n",
      "Epoch 222/240\n",
      "8564/8564 [==============================] - 5s 559us/step - loss: 0.0348 - acc: 0.9220 - val_loss: 0.0340 - val_acc: 0.9391\n",
      "Epoch 223/240\n",
      "8564/8564 [==============================] - 5s 556us/step - loss: 0.0349 - acc: 0.9198 - val_loss: 0.0355 - val_acc: 0.9177\n",
      "Epoch 224/240\n",
      "8564/8564 [==============================] - 5s 573us/step - loss: 0.0356 - acc: 0.9176 - val_loss: 0.0415 - val_acc: 0.9095\n",
      "Epoch 225/240\n",
      "8564/8564 [==============================] - 5s 572us/step - loss: 0.0352 - acc: 0.9188 - val_loss: 0.0347 - val_acc: 0.9334\n",
      "Epoch 226/240\n",
      "8564/8564 [==============================] - 5s 547us/step - loss: 0.0348 - acc: 0.9204 - val_loss: 0.0320 - val_acc: 0.9333\n",
      "Epoch 227/240\n",
      "8564/8564 [==============================] - 5s 571us/step - loss: 0.0344 - acc: 0.9221 - val_loss: 0.0311 - val_acc: 0.9387\n",
      "Epoch 228/240\n",
      "8564/8564 [==============================] - 5s 567us/step - loss: 0.0357 - acc: 0.9182 - val_loss: 0.0333 - val_acc: 0.9281\n",
      "Epoch 229/240\n",
      "8564/8564 [==============================] - 5s 587us/step - loss: 0.0348 - acc: 0.9208 - val_loss: 0.0351 - val_acc: 0.9373\n",
      "Epoch 230/240\n",
      "8564/8564 [==============================] - 5s 573us/step - loss: 0.0352 - acc: 0.9181 - val_loss: 0.0337 - val_acc: 0.9278\n",
      "Epoch 231/240\n",
      "8564/8564 [==============================] - 5s 606us/step - loss: 0.0339 - acc: 0.9197 - val_loss: 0.0346 - val_acc: 0.9181\n",
      "Epoch 232/240\n",
      "8564/8564 [==============================] - 5s 616us/step - loss: 0.0363 - acc: 0.9167 - val_loss: 0.0337 - val_acc: 0.9296\n",
      "Epoch 233/240\n",
      "8564/8564 [==============================] - 5s 605us/step - loss: 0.0348 - acc: 0.9199 - val_loss: 0.0390 - val_acc: 0.9025\n",
      "Epoch 234/240\n",
      "8564/8564 [==============================] - 5s 574us/step - loss: 0.0348 - acc: 0.9213 - val_loss: 0.0346 - val_acc: 0.9318\n",
      "Epoch 235/240\n",
      "8564/8564 [==============================] - 5s 599us/step - loss: 0.0342 - acc: 0.9223 - val_loss: 0.0371 - val_acc: 0.9240\n",
      "Epoch 236/240\n",
      "8564/8564 [==============================] - 5s 556us/step - loss: 0.0351 - acc: 0.9191 - val_loss: 0.0365 - val_acc: 0.9036\n",
      "Epoch 237/240\n",
      "8564/8564 [==============================] - 5s 581us/step - loss: 0.0342 - acc: 0.9209 - val_loss: 0.0368 - val_acc: 0.9131\n",
      "Epoch 238/240\n",
      "8564/8564 [==============================] - 5s 561us/step - loss: 0.0355 - acc: 0.9175 - val_loss: 0.0327 - val_acc: 0.9291\n",
      "Epoch 239/240\n",
      "8564/8564 [==============================] - 5s 557us/step - loss: 0.0339 - acc: 0.9230 - val_loss: 0.0349 - val_acc: 0.9241\n",
      "Epoch 240/240\n",
      "8564/8564 [==============================] - 5s 547us/step - loss: 0.0349 - acc: 0.9203 - val_loss: 0.0337 - val_acc: 0.9381\n",
      "start training round 12\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 241/260\n",
      "8564/8564 [==============================] - 5s 537us/step - loss: 0.0318 - acc: 0.6077 - val_loss: 0.0326 - val_acc: 0.6642\n",
      "Epoch 242/260\n",
      "8564/8564 [==============================] - 5s 588us/step - loss: 0.0322 - acc: 0.6013 - val_loss: 0.0347 - val_acc: 0.6623\n",
      "Epoch 243/260\n",
      "8564/8564 [==============================] - 5s 578us/step - loss: 0.0319 - acc: 0.6336 - val_loss: 0.0321 - val_acc: 0.5990\n",
      "Epoch 244/260\n",
      "8564/8564 [==============================] - 5s 579us/step - loss: 0.0318 - acc: 0.6518 - val_loss: 0.0329 - val_acc: 0.6478\n",
      "Epoch 245/260\n",
      "8564/8564 [==============================] - 5s 589us/step - loss: 0.0320 - acc: 0.6569 - val_loss: 0.0318 - val_acc: 0.6614\n",
      "Epoch 246/260\n",
      "8564/8564 [==============================] - 5s 535us/step - loss: 0.0317 - acc: 0.6633 - val_loss: 0.0328 - val_acc: 0.6493\n",
      "Epoch 247/260\n",
      "8564/8564 [==============================] - 5s 571us/step - loss: 0.0317 - acc: 0.6622 - val_loss: 0.0330 - val_acc: 0.5734\n",
      "Epoch 248/260\n",
      "8564/8564 [==============================] - 5s 593us/step - loss: 0.0311 - acc: 0.6502 - val_loss: 0.0321 - val_acc: 0.6726\n",
      "Epoch 249/260\n",
      "8564/8564 [==============================] - 5s 579us/step - loss: 0.0314 - acc: 0.6152 - val_loss: 0.0316 - val_acc: 0.6755\n",
      "Epoch 250/260\n",
      "8564/8564 [==============================] - 5s 556us/step - loss: 0.0319 - acc: 0.6066 - val_loss: 0.0315 - val_acc: 0.6734\n",
      "Epoch 251/260\n",
      "8564/8564 [==============================] - 5s 564us/step - loss: 0.0318 - acc: 0.6042 - val_loss: 0.0329 - val_acc: 0.6686\n",
      "Epoch 252/260\n",
      "8564/8564 [==============================] - 5s 542us/step - loss: 0.0316 - acc: 0.6134 - val_loss: 0.0328 - val_acc: 0.6675\n",
      "Epoch 253/260\n",
      "8564/8564 [==============================] - 5s 537us/step - loss: 0.0317 - acc: 0.6060 - val_loss: 0.0318 - val_acc: 0.6740\n",
      "Epoch 254/260\n",
      "8564/8564 [==============================] - 5s 572us/step - loss: 0.0314 - acc: 0.6222 - val_loss: 0.0313 - val_acc: 0.6724\n",
      "Epoch 255/260\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0311 - acc: 0.6298 - val_loss: 0.0324 - val_acc: 0.6660\n",
      "Epoch 256/260\n",
      "8564/8564 [==============================] - 5s 542us/step - loss: 0.0316 - acc: 0.6105 - val_loss: 0.0323 - val_acc: 0.6726\n",
      "Epoch 257/260\n",
      "8564/8564 [==============================] - 5s 552us/step - loss: 0.0316 - acc: 0.6090 - val_loss: 0.0325 - val_acc: 0.6696\n",
      "Epoch 258/260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 5s 563us/step - loss: 0.0318 - acc: 0.6071 - val_loss: 0.0325 - val_acc: 0.6703\n",
      "Epoch 259/260\n",
      "8564/8564 [==============================] - 5s 576us/step - loss: 0.0314 - acc: 0.6568 - val_loss: 0.0318 - val_acc: 0.6684\n",
      "Epoch 260/260\n",
      "8564/8564 [==============================] - 5s 545us/step - loss: 0.0312 - acc: 0.6208 - val_loss: 0.0314 - val_acc: 0.6800\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 241/260\n",
      "8564/8564 [==============================] - 5s 541us/step - loss: 0.0271 - acc: 0.8573 - val_loss: 0.0276 - val_acc: 0.8520\n",
      "Epoch 242/260\n",
      "8564/8564 [==============================] - 5s 592us/step - loss: 0.0272 - acc: 0.8571 - val_loss: 0.0270 - val_acc: 0.8541\n",
      "Epoch 243/260\n",
      "8564/8564 [==============================] - 5s 545us/step - loss: 0.0272 - acc: 0.8571 - val_loss: 0.0270 - val_acc: 0.8554\n",
      "Epoch 244/260\n",
      "8564/8564 [==============================] - 5s 546us/step - loss: 0.0272 - acc: 0.8558 - val_loss: 0.0275 - val_acc: 0.8502\n",
      "Epoch 245/260\n",
      "8564/8564 [==============================] - 5s 552us/step - loss: 0.0274 - acc: 0.8554 - val_loss: 0.0282 - val_acc: 0.8532\n",
      "Epoch 246/260\n",
      "8564/8564 [==============================] - 5s 553us/step - loss: 0.0271 - acc: 0.8567 - val_loss: 0.0266 - val_acc: 0.8555\n",
      "Epoch 247/260\n",
      "8564/8564 [==============================] - 5s 539us/step - loss: 0.0271 - acc: 0.8574 - val_loss: 0.0272 - val_acc: 0.8525\n",
      "Epoch 248/260\n",
      "8564/8564 [==============================] - 5s 543us/step - loss: 0.0273 - acc: 0.8561 - val_loss: 0.0270 - val_acc: 0.8548\n",
      "Epoch 249/260\n",
      "8564/8564 [==============================] - 5s 578us/step - loss: 0.0271 - acc: 0.8576 - val_loss: 0.0275 - val_acc: 0.8511\n",
      "Epoch 250/260\n",
      "8564/8564 [==============================] - 5s 549us/step - loss: 0.0271 - acc: 0.8617 - val_loss: 0.0268 - val_acc: 0.8605\n",
      "Epoch 251/260\n",
      "8564/8564 [==============================] - 5s 558us/step - loss: 0.0272 - acc: 0.8570 - val_loss: 0.0276 - val_acc: 0.8528\n",
      "Epoch 252/260\n",
      "8564/8564 [==============================] - 5s 589us/step - loss: 0.0269 - acc: 0.8572 - val_loss: 0.0270 - val_acc: 0.8553\n",
      "Epoch 253/260\n",
      "8564/8564 [==============================] - 5s 575us/step - loss: 0.0270 - acc: 0.8583 - val_loss: 0.0280 - val_acc: 0.8495\n",
      "Epoch 254/260\n",
      "8564/8564 [==============================] - 5s 567us/step - loss: 0.0269 - acc: 0.8592 - val_loss: 0.0271 - val_acc: 0.8522\n",
      "Epoch 255/260\n",
      "8564/8564 [==============================] - 5s 552us/step - loss: 0.0269 - acc: 0.8578 - val_loss: 0.0273 - val_acc: 0.8533\n",
      "Epoch 256/260\n",
      "8564/8564 [==============================] - 5s 564us/step - loss: 0.0267 - acc: 0.8612 - val_loss: 0.0274 - val_acc: 0.8632\n",
      "Epoch 257/260\n",
      "8564/8564 [==============================] - 5s 559us/step - loss: 0.0266 - acc: 0.8638 - val_loss: 0.0271 - val_acc: 0.8641\n",
      "Epoch 258/260\n",
      "8564/8564 [==============================] - 5s 534us/step - loss: 0.0269 - acc: 0.8624 - val_loss: 0.0275 - val_acc: 0.8594\n",
      "Epoch 259/260\n",
      "8564/8564 [==============================] - 5s 578us/step - loss: 0.0269 - acc: 0.8592 - val_loss: 0.0278 - val_acc: 0.8511\n",
      "Epoch 260/260\n",
      "8564/8564 [==============================] - 5s 563us/step - loss: 0.0269 - acc: 0.8582 - val_loss: 0.0266 - val_acc: 0.8569\n",
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 241/260\n",
      "8564/8564 [==============================] - 5s 592us/step - loss: 0.0294 - acc: 0.9785 - val_loss: 0.0286 - val_acc: 0.9851\n",
      "Epoch 242/260\n",
      "8564/8564 [==============================] - 4s 514us/step - loss: 0.0288 - acc: 0.9798 - val_loss: 0.0235 - val_acc: 0.9807\n",
      "Epoch 243/260\n",
      "8564/8564 [==============================] - 5s 531us/step - loss: 0.0291 - acc: 0.9780 - val_loss: 0.0257 - val_acc: 0.9839\n",
      "Epoch 244/260\n",
      "8564/8564 [==============================] - 5s 564us/step - loss: 0.0293 - acc: 0.9768 - val_loss: 0.0280 - val_acc: 0.9791\n",
      "Epoch 245/260\n",
      "8564/8564 [==============================] - 5s 561us/step - loss: 0.0288 - acc: 0.9794 - val_loss: 0.0293 - val_acc: 0.9788\n",
      "Epoch 246/260\n",
      "8564/8564 [==============================] - 5s 541us/step - loss: 0.0290 - acc: 0.9781 - val_loss: 0.0293 - val_acc: 0.9812\n",
      "Epoch 247/260\n",
      "8564/8564 [==============================] - 5s 580us/step - loss: 0.0310 - acc: 0.9801 - val_loss: 0.0281 - val_acc: 0.9817\n",
      "Epoch 248/260\n",
      "8564/8564 [==============================] - 5s 547us/step - loss: 0.0291 - acc: 0.9798 - val_loss: 0.0243 - val_acc: 0.9850\n",
      "Epoch 249/260\n",
      "8564/8564 [==============================] - 5s 559us/step - loss: 0.0279 - acc: 0.9793 - val_loss: 0.0297 - val_acc: 0.9800\n",
      "Epoch 250/260\n",
      "8564/8564 [==============================] - 5s 558us/step - loss: 0.0264 - acc: 0.9806 - val_loss: 0.0304 - val_acc: 0.9820\n",
      "Epoch 251/260\n",
      "8564/8564 [==============================] - 5s 570us/step - loss: 0.0307 - acc: 0.9783 - val_loss: 0.0252 - val_acc: 0.9829\n",
      "Epoch 252/260\n",
      "8564/8564 [==============================] - 5s 565us/step - loss: 0.0278 - acc: 0.9789 - val_loss: 0.0317 - val_acc: 0.9743\n",
      "Epoch 253/260\n",
      "8564/8564 [==============================] - 5s 555us/step - loss: 0.0292 - acc: 0.9782 - val_loss: 0.0257 - val_acc: 0.9821\n",
      "Epoch 254/260\n",
      "8564/8564 [==============================] - 5s 548us/step - loss: 0.0285 - acc: 0.9787 - val_loss: 0.0280 - val_acc: 0.9809\n",
      "Epoch 255/260\n",
      "8564/8564 [==============================] - 5s 555us/step - loss: 0.0300 - acc: 0.9795 - val_loss: 0.0300 - val_acc: 0.9798\n",
      "Epoch 256/260\n",
      "8564/8564 [==============================] - 5s 551us/step - loss: 0.0276 - acc: 0.9803 - val_loss: 0.0313 - val_acc: 0.9738\n",
      "Epoch 257/260\n",
      "8564/8564 [==============================] - 5s 547us/step - loss: 0.0287 - acc: 0.9789 - val_loss: 0.0291 - val_acc: 0.9826\n",
      "Epoch 258/260\n",
      "8564/8564 [==============================] - 5s 555us/step - loss: 0.0291 - acc: 0.9794 - val_loss: 0.0241 - val_acc: 0.9815\n",
      "Epoch 259/260\n",
      "8564/8564 [==============================] - 5s 552us/step - loss: 0.0289 - acc: 0.9798 - val_loss: 0.0240 - val_acc: 0.9823\n",
      "Epoch 260/260\n",
      "8564/8564 [==============================] - 5s 543us/step - loss: 0.0275 - acc: 0.9794 - val_loss: 0.0317 - val_acc: 0.9719\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 241/260\n",
      "8564/8564 [==============================] - 5s 567us/step - loss: 0.0342 - acc: 0.9228 - val_loss: 0.0325 - val_acc: 0.9310\n",
      "Epoch 242/260\n",
      "8564/8564 [==============================] - 5s 553us/step - loss: 0.0349 - acc: 0.9209 - val_loss: 0.0330 - val_acc: 0.9219\n",
      "Epoch 243/260\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0345 - acc: 0.9248 - val_loss: 0.0330 - val_acc: 0.9232\n",
      "Epoch 244/260\n",
      "8564/8564 [==============================] - 5s 539us/step - loss: 0.0360 - acc: 0.9178 - val_loss: 0.0315 - val_acc: 0.9282\n",
      "Epoch 245/260\n",
      "8564/8564 [==============================] - 5s 563us/step - loss: 0.0349 - acc: 0.9214 - val_loss: 0.0321 - val_acc: 0.9311\n",
      "Epoch 246/260\n",
      "8564/8564 [==============================] - 5s 566us/step - loss: 0.0354 - acc: 0.9185 - val_loss: 0.0348 - val_acc: 0.9295\n",
      "Epoch 247/260\n",
      "8564/8564 [==============================] - 5s 589us/step - loss: 0.0340 - acc: 0.9236 - val_loss: 0.0362 - val_acc: 0.9393\n",
      "Epoch 248/260\n",
      "8564/8564 [==============================] - 5s 555us/step - loss: 0.0325 - acc: 0.9253 - val_loss: 0.0311 - val_acc: 0.9314\n",
      "Epoch 249/260\n",
      "8564/8564 [==============================] - 5s 541us/step - loss: 0.0354 - acc: 0.9213 - val_loss: 0.0365 - val_acc: 0.9219\n",
      "Epoch 250/260\n",
      "8564/8564 [==============================] - 5s 566us/step - loss: 0.0354 - acc: 0.9204 - val_loss: 0.0300 - val_acc: 0.9338\n",
      "Epoch 251/260\n",
      "8564/8564 [==============================] - 5s 570us/step - loss: 0.0336 - acc: 0.9222 - val_loss: 0.0320 - val_acc: 0.9247\n",
      "Epoch 252/260\n",
      "8564/8564 [==============================] - 5s 572us/step - loss: 0.0336 - acc: 0.9231 - val_loss: 0.0330 - val_acc: 0.9263\n",
      "Epoch 253/260\n",
      "8564/8564 [==============================] - 5s 557us/step - loss: 0.0340 - acc: 0.9243 - val_loss: 0.0330 - val_acc: 0.9427\n",
      "Epoch 254/260\n",
      "8564/8564 [==============================] - 5s 577us/step - loss: 0.0339 - acc: 0.9230 - val_loss: 0.0306 - val_acc: 0.9262\n",
      "Epoch 255/260\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 5s 588us/step - loss: 0.0333 - acc: 0.9220 - val_loss: 0.0333 - val_acc: 0.9249\n",
      "Epoch 256/260\n",
      "8564/8564 [==============================] - 5s 597us/step - loss: 0.0340 - acc: 0.9221 - val_loss: 0.0343 - val_acc: 0.9277\n",
      "Epoch 257/260\n",
      "8564/8564 [==============================] - 5s 615us/step - loss: 0.0335 - acc: 0.9226 - val_loss: 0.0319 - val_acc: 0.9361\n",
      "Epoch 258/260\n",
      "8564/8564 [==============================] - 5s 570us/step - loss: 0.0344 - acc: 0.9245 - val_loss: 0.0289 - val_acc: 0.9347\n",
      "Epoch 259/260\n",
      "8564/8564 [==============================] - 5s 587us/step - loss: 0.0343 - acc: 0.9210 - val_loss: 0.0309 - val_acc: 0.9296\n",
      "Epoch 260/260\n",
      "8564/8564 [==============================] - 5s 546us/step - loss: 0.0331 - acc: 0.9235 - val_loss: 0.0331 - val_acc: 0.9380\n",
      "start training round 13\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 261/280\n",
      "8564/8564 [==============================] - 5s 561us/step - loss: 0.0308 - acc: 0.6405 - val_loss: 0.0320 - val_acc: 0.6744\n",
      "Epoch 262/280\n",
      "8564/8564 [==============================] - 5s 581us/step - loss: 0.0317 - acc: 0.6055 - val_loss: 0.0330 - val_acc: 0.6694\n",
      "Epoch 263/280\n",
      "8564/8564 [==============================] - 5s 587us/step - loss: 0.0315 - acc: 0.6086 - val_loss: 0.0312 - val_acc: 0.6648\n",
      "Epoch 264/280\n",
      "8564/8564 [==============================] - 5s 578us/step - loss: 0.0316 - acc: 0.6113 - val_loss: 0.0325 - val_acc: 0.6686\n",
      "Epoch 265/280\n",
      "8564/8564 [==============================] - 5s 559us/step - loss: 0.0308 - acc: 0.6459 - val_loss: 0.0316 - val_acc: 0.5650\n",
      "Epoch 266/280\n",
      "8564/8564 [==============================] - 5s 557us/step - loss: 0.0314 - acc: 0.6573 - val_loss: 0.0311 - val_acc: 0.6783\n",
      "Epoch 267/280\n",
      "8564/8564 [==============================] - 5s 554us/step - loss: 0.0313 - acc: 0.6092 - val_loss: 0.0316 - val_acc: 0.6773\n",
      "Epoch 268/280\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0312 - acc: 0.6314 - val_loss: 0.0311 - val_acc: 0.6657\n",
      "Epoch 269/280\n",
      "8564/8564 [==============================] - 5s 560us/step - loss: 0.0311 - acc: 0.6158 - val_loss: 0.0321 - val_acc: 0.6679\n",
      "Epoch 270/280\n",
      "8564/8564 [==============================] - 5s 580us/step - loss: 0.0310 - acc: 0.6258 - val_loss: 0.0310 - val_acc: 0.6682\n",
      "Epoch 271/280\n",
      "8564/8564 [==============================] - 5s 560us/step - loss: 0.0308 - acc: 0.6226 - val_loss: 0.0311 - val_acc: 0.6816\n",
      "Epoch 272/280\n",
      "8564/8564 [==============================] - 5s 550us/step - loss: 0.0310 - acc: 0.6192 - val_loss: 0.0318 - val_acc: 0.6754\n",
      "Epoch 273/280\n",
      "8564/8564 [==============================] - 5s 578us/step - loss: 0.0309 - acc: 0.6218 - val_loss: 0.0315 - val_acc: 0.6823\n",
      "Epoch 274/280\n",
      "8564/8564 [==============================] - 5s 581us/step - loss: 0.0310 - acc: 0.6245 - val_loss: 0.0324 - val_acc: 0.6752\n",
      "Epoch 275/280\n",
      "8564/8564 [==============================] - 5s 564us/step - loss: 0.0312 - acc: 0.6096 - val_loss: 0.0318 - val_acc: 0.6754\n",
      "Epoch 276/280\n",
      "8564/8564 [==============================] - 5s 588us/step - loss: 0.0310 - acc: 0.6186 - val_loss: 0.0313 - val_acc: 0.6782\n",
      "Epoch 277/280\n",
      "8564/8564 [==============================] - 5s 576us/step - loss: 0.0308 - acc: 0.6462 - val_loss: 0.0312 - val_acc: 0.6513\n",
      "Epoch 278/280\n",
      "8564/8564 [==============================] - 5s 560us/step - loss: 0.0313 - acc: 0.6756 - val_loss: 0.0314 - val_acc: 0.6565\n",
      "Epoch 279/280\n",
      "8564/8564 [==============================] - 5s 577us/step - loss: 0.0305 - acc: 0.6417 - val_loss: 0.0310 - val_acc: 0.6094\n",
      "Epoch 280/280\n",
      "8564/8564 [==============================] - 5s 544us/step - loss: 0.0307 - acc: 0.6627 - val_loss: 0.0315 - val_acc: 0.6785\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 261/280\n",
      "8564/8564 [==============================] - 5s 581us/step - loss: 0.0266 - acc: 0.8604 - val_loss: 0.0264 - val_acc: 0.8589\n",
      "Epoch 262/280\n",
      "8564/8564 [==============================] - 5s 576us/step - loss: 0.0268 - acc: 0.8591 - val_loss: 0.0267 - val_acc: 0.8576\n",
      "Epoch 263/280\n",
      "8564/8564 [==============================] - 5s 589us/step - loss: 0.0268 - acc: 0.8583 - val_loss: 0.0268 - val_acc: 0.8555\n",
      "Epoch 264/280\n",
      "8564/8564 [==============================] - 5s 587us/step - loss: 0.0266 - acc: 0.8612 - val_loss: 0.0269 - val_acc: 0.8620\n",
      "Epoch 265/280\n",
      "8564/8564 [==============================] - 5s 584us/step - loss: 0.0267 - acc: 0.8593 - val_loss: 0.0262 - val_acc: 0.8627\n",
      "Epoch 266/280\n",
      "8564/8564 [==============================] - 5s 581us/step - loss: 0.0266 - acc: 0.8583 - val_loss: 0.0277 - val_acc: 0.8578\n",
      "Epoch 267/280\n",
      "8564/8564 [==============================] - 5s 591us/step - loss: 0.0264 - acc: 0.8612 - val_loss: 0.0265 - val_acc: 0.8648\n",
      "Epoch 268/280\n",
      "8564/8564 [==============================] - 5s 586us/step - loss: 0.0266 - acc: 0.8601 - val_loss: 0.0275 - val_acc: 0.8612\n",
      "Epoch 269/280\n",
      "8564/8564 [==============================] - 5s 569us/step - loss: 0.0267 - acc: 0.8593 - val_loss: 0.0265 - val_acc: 0.8658\n",
      "Epoch 270/280\n",
      "8564/8564 [==============================] - 5s 581us/step - loss: 0.0265 - acc: 0.8664 - val_loss: 0.0267 - val_acc: 0.8663\n",
      "Epoch 271/280\n",
      "8564/8564 [==============================] - 5s 547us/step - loss: 0.0265 - acc: 0.8664 - val_loss: 0.0270 - val_acc: 0.8601\n",
      "Epoch 272/280\n",
      "8564/8564 [==============================] - 5s 555us/step - loss: 0.0265 - acc: 0.8615 - val_loss: 0.0261 - val_acc: 0.8661\n",
      "Epoch 273/280\n",
      "8564/8564 [==============================] - 4s 522us/step - loss: 0.0265 - acc: 0.8612 - val_loss: 0.0263 - val_acc: 0.8658\n",
      "Epoch 274/280\n",
      "8564/8564 [==============================] - 5s 560us/step - loss: 0.0265 - acc: 0.8662 - val_loss: 0.0260 - val_acc: 0.8663\n",
      "Epoch 275/280\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0263 - acc: 0.8671 - val_loss: 0.0271 - val_acc: 0.8596\n",
      "Epoch 276/280\n",
      "8564/8564 [==============================] - 5s 554us/step - loss: 0.0267 - acc: 0.8640 - val_loss: 0.0285 - val_acc: 0.8642\n",
      "Epoch 277/280\n",
      "8564/8564 [==============================] - 5s 540us/step - loss: 0.0264 - acc: 0.8658 - val_loss: 0.0267 - val_acc: 0.8569\n",
      "Epoch 278/280\n",
      "8564/8564 [==============================] - 5s 588us/step - loss: 0.0266 - acc: 0.8597 - val_loss: 0.0271 - val_acc: 0.8556\n",
      "Epoch 279/280\n",
      "8564/8564 [==============================] - 5s 596us/step - loss: 0.0266 - acc: 0.8627 - val_loss: 0.0273 - val_acc: 0.8650\n",
      "Epoch 280/280\n",
      "8564/8564 [==============================] - 5s 559us/step - loss: 0.0262 - acc: 0.8677 - val_loss: 0.0265 - val_acc: 0.8636\n",
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 261/280\n",
      "8564/8564 [==============================] - 5s 574us/step - loss: 0.0275 - acc: 0.9784 - val_loss: 0.0343 - val_acc: 0.9768\n",
      "Epoch 262/280\n",
      "8564/8564 [==============================] - 5s 550us/step - loss: 0.0306 - acc: 0.9775 - val_loss: 0.0330 - val_acc: 0.9776\n",
      "Epoch 263/280\n",
      "8564/8564 [==============================] - 5s 546us/step - loss: 0.0304 - acc: 0.9794 - val_loss: 0.0284 - val_acc: 0.9813\n",
      "Epoch 264/280\n",
      "8564/8564 [==============================] - 5s 555us/step - loss: 0.0285 - acc: 0.9787 - val_loss: 0.0327 - val_acc: 0.9792\n",
      "Epoch 265/280\n",
      "8564/8564 [==============================] - 5s 591us/step - loss: 0.0296 - acc: 0.9778 - val_loss: 0.0323 - val_acc: 0.9834\n",
      "Epoch 266/280\n",
      "8564/8564 [==============================] - 5s 556us/step - loss: 0.0288 - acc: 0.9787 - val_loss: 0.0301 - val_acc: 0.9808\n",
      "Epoch 267/280\n",
      "8564/8564 [==============================] - 5s 550us/step - loss: 0.0272 - acc: 0.9789 - val_loss: 0.0266 - val_acc: 0.9837\n",
      "Epoch 268/280\n",
      "8564/8564 [==============================] - 5s 552us/step - loss: 0.0295 - acc: 0.9803 - val_loss: 0.0299 - val_acc: 0.9807\n",
      "Epoch 269/280\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0286 - acc: 0.9775 - val_loss: 0.0299 - val_acc: 0.9806\n",
      "Epoch 270/280\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0275 - acc: 0.9801 - val_loss: 0.0264 - val_acc: 0.9844\n",
      "Epoch 271/280\n",
      "8564/8564 [==============================] - 5s 574us/step - loss: 0.0290 - acc: 0.9790 - val_loss: 0.0250 - val_acc: 0.9783\n",
      "Epoch 272/280\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 5s 555us/step - loss: 0.0273 - acc: 0.9807 - val_loss: 0.0262 - val_acc: 0.9771\n",
      "Epoch 273/280\n",
      "8564/8564 [==============================] - 5s 550us/step - loss: 0.0303 - acc: 0.9795 - val_loss: 0.0384 - val_acc: 0.9765\n",
      "Epoch 274/280\n",
      "8564/8564 [==============================] - 5s 567us/step - loss: 0.0280 - acc: 0.9792 - val_loss: 0.0300 - val_acc: 0.9814\n",
      "Epoch 275/280\n",
      "8564/8564 [==============================] - 5s 567us/step - loss: 0.0292 - acc: 0.9779 - val_loss: 0.0271 - val_acc: 0.9809\n",
      "Epoch 276/280\n",
      "8564/8564 [==============================] - 5s 576us/step - loss: 0.0299 - acc: 0.9776 - val_loss: 0.0297 - val_acc: 0.9808\n",
      "Epoch 277/280\n",
      "8564/8564 [==============================] - 5s 551us/step - loss: 0.0283 - acc: 0.9781 - val_loss: 0.0262 - val_acc: 0.9809\n",
      "Epoch 278/280\n",
      "8564/8564 [==============================] - 5s 567us/step - loss: 0.0279 - acc: 0.9787 - val_loss: 0.0297 - val_acc: 0.9849\n",
      "Epoch 279/280\n",
      "8564/8564 [==============================] - 5s 553us/step - loss: 0.0287 - acc: 0.9801 - val_loss: 0.0283 - val_acc: 0.9812\n",
      "Epoch 280/280\n",
      "8564/8564 [==============================] - 5s 563us/step - loss: 0.0289 - acc: 0.9772 - val_loss: 0.0228 - val_acc: 0.9837\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 261/280\n",
      "8564/8564 [==============================] - 5s 558us/step - loss: 0.0335 - acc: 0.9243 - val_loss: 0.0311 - val_acc: 0.9418\n",
      "Epoch 262/280\n",
      "8564/8564 [==============================] - 5s 559us/step - loss: 0.0344 - acc: 0.9216 - val_loss: 0.0339 - val_acc: 0.9213\n",
      "Epoch 263/280\n",
      "8564/8564 [==============================] - 5s 569us/step - loss: 0.0325 - acc: 0.9254 - val_loss: 0.0291 - val_acc: 0.9236\n",
      "Epoch 264/280\n",
      "8564/8564 [==============================] - 5s 557us/step - loss: 0.0327 - acc: 0.9238 - val_loss: 0.0326 - val_acc: 0.9235\n",
      "Epoch 265/280\n",
      "8564/8564 [==============================] - 5s 570us/step - loss: 0.0337 - acc: 0.9266 - val_loss: 0.0298 - val_acc: 0.9383\n",
      "Epoch 266/280\n",
      "8564/8564 [==============================] - 5s 600us/step - loss: 0.0337 - acc: 0.9230 - val_loss: 0.0352 - val_acc: 0.9111\n",
      "Epoch 267/280\n",
      "8564/8564 [==============================] - 5s 565us/step - loss: 0.0338 - acc: 0.9234 - val_loss: 0.0317 - val_acc: 0.9293\n",
      "Epoch 268/280\n",
      "8564/8564 [==============================] - 5s 532us/step - loss: 0.0342 - acc: 0.9211 - val_loss: 0.0324 - val_acc: 0.9283\n",
      "Epoch 269/280\n",
      "8564/8564 [==============================] - 5s 555us/step - loss: 0.0348 - acc: 0.9200 - val_loss: 0.0361 - val_acc: 0.9073\n",
      "Epoch 270/280\n",
      "8564/8564 [==============================] - 5s 552us/step - loss: 0.0329 - acc: 0.9256 - val_loss: 0.0370 - val_acc: 0.9308\n",
      "Epoch 271/280\n",
      "8564/8564 [==============================] - 5s 561us/step - loss: 0.0334 - acc: 0.9230 - val_loss: 0.0328 - val_acc: 0.9380\n",
      "Epoch 272/280\n",
      "8564/8564 [==============================] - 5s 575us/step - loss: 0.0339 - acc: 0.9229 - val_loss: 0.0281 - val_acc: 0.9425\n",
      "Epoch 273/280\n",
      "8564/8564 [==============================] - 5s 565us/step - loss: 0.0336 - acc: 0.9244 - val_loss: 0.0309 - val_acc: 0.9346\n",
      "Epoch 274/280\n",
      "8564/8564 [==============================] - 5s 577us/step - loss: 0.0323 - acc: 0.9260 - val_loss: 0.0298 - val_acc: 0.9373\n",
      "Epoch 275/280\n",
      "8564/8564 [==============================] - 5s 561us/step - loss: 0.0328 - acc: 0.9251 - val_loss: 0.0292 - val_acc: 0.9393\n",
      "Epoch 276/280\n",
      "8564/8564 [==============================] - 5s 567us/step - loss: 0.0337 - acc: 0.9231 - val_loss: 0.0314 - val_acc: 0.9332\n",
      "Epoch 277/280\n",
      "8564/8564 [==============================] - 5s 590us/step - loss: 0.0318 - acc: 0.9268 - val_loss: 0.0327 - val_acc: 0.9252\n",
      "Epoch 278/280\n",
      "8564/8564 [==============================] - 5s 570us/step - loss: 0.0333 - acc: 0.9227 - val_loss: 0.0286 - val_acc: 0.9372\n",
      "Epoch 279/280\n",
      "8564/8564 [==============================] - 5s 582us/step - loss: 0.0333 - acc: 0.9244 - val_loss: 0.0307 - val_acc: 0.9320\n",
      "Epoch 280/280\n",
      "8564/8564 [==============================] - 5s 579us/step - loss: 0.0340 - acc: 0.9249 - val_loss: 0.0311 - val_acc: 0.9313\n",
      "start training round 14\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 281/300\n",
      "8564/8564 [==============================] - 5s 559us/step - loss: 0.0310 - acc: 0.6151 - val_loss: 0.0309 - val_acc: 0.6775\n",
      "Epoch 282/300\n",
      "8564/8564 [==============================] - 5s 567us/step - loss: 0.0309 - acc: 0.6190 - val_loss: 0.0314 - val_acc: 0.6827\n",
      "Epoch 283/300\n",
      "8564/8564 [==============================] - 5s 556us/step - loss: 0.0308 - acc: 0.6153 - val_loss: 0.0325 - val_acc: 0.6801\n",
      "Epoch 284/300\n",
      "8564/8564 [==============================] - 5s 567us/step - loss: 0.0309 - acc: 0.6175 - val_loss: 0.0319 - val_acc: 0.6799\n",
      "Epoch 285/300\n",
      "8564/8564 [==============================] - 5s 576us/step - loss: 0.0309 - acc: 0.6142 - val_loss: 0.0311 - val_acc: 0.6835\n",
      "Epoch 286/300\n",
      "8564/8564 [==============================] - 5s 567us/step - loss: 0.0305 - acc: 0.6654 - val_loss: 0.0311 - val_acc: 0.6667\n",
      "Epoch 287/300\n",
      "8564/8564 [==============================] - 5s 567us/step - loss: 0.0305 - acc: 0.6803 - val_loss: 0.0324 - val_acc: 0.6416\n",
      "Epoch 288/300\n",
      "8564/8564 [==============================] - 5s 558us/step - loss: 0.0307 - acc: 0.6416 - val_loss: 0.0317 - val_acc: 0.6397\n",
      "Epoch 289/300\n",
      "8564/8564 [==============================] - 5s 550us/step - loss: 0.0309 - acc: 0.6126 - val_loss: 0.0315 - val_acc: 0.6757\n",
      "Epoch 290/300\n",
      "8564/8564 [==============================] - 5s 571us/step - loss: 0.0310 - acc: 0.6138 - val_loss: 0.0310 - val_acc: 0.6856\n",
      "Epoch 291/300\n",
      "8564/8564 [==============================] - 5s 582us/step - loss: 0.0308 - acc: 0.6191 - val_loss: 0.0314 - val_acc: 0.6801\n",
      "Epoch 292/300\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0306 - acc: 0.6271 - val_loss: 0.0316 - val_acc: 0.6836\n",
      "Epoch 293/300\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0304 - acc: 0.6300 - val_loss: 0.0312 - val_acc: 0.6840\n",
      "Epoch 294/300\n",
      "8564/8564 [==============================] - 5s 603us/step - loss: 0.0304 - acc: 0.6678 - val_loss: 0.0309 - val_acc: 0.6910\n",
      "Epoch 295/300\n",
      "8564/8564 [==============================] - 5s 535us/step - loss: 0.0306 - acc: 0.6173 - val_loss: 0.0316 - val_acc: 0.6824\n",
      "Epoch 296/300\n",
      "8564/8564 [==============================] - 5s 566us/step - loss: 0.0306 - acc: 0.6227 - val_loss: 0.0310 - val_acc: 0.6866\n",
      "Epoch 297/300\n",
      "8564/8564 [==============================] - 5s 557us/step - loss: 0.0303 - acc: 0.6575 - val_loss: 0.0311 - val_acc: 0.6795\n",
      "Epoch 298/300\n",
      "8564/8564 [==============================] - 5s 553us/step - loss: 0.0304 - acc: 0.6540 - val_loss: 0.0314 - val_acc: 0.6844\n",
      "Epoch 299/300\n",
      "8564/8564 [==============================] - 5s 573us/step - loss: 0.0302 - acc: 0.6386 - val_loss: 0.0303 - val_acc: 0.6092\n",
      "Epoch 300/300\n",
      "8564/8564 [==============================] - 5s 578us/step - loss: 0.0306 - acc: 0.6219 - val_loss: 0.0313 - val_acc: 0.5406\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 281/300\n",
      "8564/8564 [==============================] - 5s 561us/step - loss: 0.0266 - acc: 0.8671 - val_loss: 0.0262 - val_acc: 0.8647\n",
      "Epoch 282/300\n",
      "8564/8564 [==============================] - 5s 559us/step - loss: 0.0262 - acc: 0.8676 - val_loss: 0.0269 - val_acc: 0.8664\n",
      "Epoch 283/300\n",
      "8564/8564 [==============================] - 5s 534us/step - loss: 0.0264 - acc: 0.8676 - val_loss: 0.0259 - val_acc: 0.8674\n",
      "Epoch 284/300\n",
      "8564/8564 [==============================] - 5s 574us/step - loss: 0.0263 - acc: 0.8680 - val_loss: 0.0265 - val_acc: 0.8661\n",
      "Epoch 285/300\n",
      "8564/8564 [==============================] - 5s 580us/step - loss: 0.0263 - acc: 0.8681 - val_loss: 0.0276 - val_acc: 0.8675\n",
      "Epoch 286/300\n",
      "8564/8564 [==============================] - 5s 565us/step - loss: 0.0264 - acc: 0.8680 - val_loss: 0.0262 - val_acc: 0.8642\n",
      "Epoch 287/300\n",
      "8564/8564 [==============================] - 5s 572us/step - loss: 0.0261 - acc: 0.8663 - val_loss: 0.0269 - val_acc: 0.8625\n",
      "Epoch 288/300\n",
      "8564/8564 [==============================] - 5s 561us/step - loss: 0.0265 - acc: 0.8619 - val_loss: 0.0270 - val_acc: 0.8633\n",
      "Epoch 289/300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 5s 556us/step - loss: 0.0264 - acc: 0.8619 - val_loss: 0.0271 - val_acc: 0.8622\n",
      "Epoch 290/300\n",
      "8564/8564 [==============================] - 5s 548us/step - loss: 0.0263 - acc: 0.8622 - val_loss: 0.0278 - val_acc: 0.8618\n",
      "Epoch 291/300\n",
      "8564/8564 [==============================] - 5s 550us/step - loss: 0.0264 - acc: 0.8615 - val_loss: 0.0266 - val_acc: 0.8642\n",
      "Epoch 292/300\n",
      "8564/8564 [==============================] - 5s 537us/step - loss: 0.0264 - acc: 0.8613 - val_loss: 0.0268 - val_acc: 0.8645\n",
      "Epoch 293/300\n",
      "8564/8564 [==============================] - 5s 571us/step - loss: 0.0264 - acc: 0.8610 - val_loss: 0.0268 - val_acc: 0.8635\n",
      "Epoch 294/300\n",
      "8564/8564 [==============================] - 5s 558us/step - loss: 0.0263 - acc: 0.8616 - val_loss: 0.0275 - val_acc: 0.8617\n",
      "Epoch 295/300\n",
      "8564/8564 [==============================] - 5s 551us/step - loss: 0.0264 - acc: 0.8618 - val_loss: 0.0265 - val_acc: 0.8643\n",
      "Epoch 296/300\n",
      "8564/8564 [==============================] - 5s 590us/step - loss: 0.0262 - acc: 0.8620 - val_loss: 0.0260 - val_acc: 0.8649\n",
      "Epoch 297/300\n",
      "8564/8564 [==============================] - 5s 557us/step - loss: 0.0261 - acc: 0.8633 - val_loss: 0.0270 - val_acc: 0.8630\n",
      "Epoch 298/300\n",
      "8564/8564 [==============================] - 5s 556us/step - loss: 0.0263 - acc: 0.8633 - val_loss: 0.0263 - val_acc: 0.8644\n",
      "Epoch 299/300\n",
      "8564/8564 [==============================] - 5s 555us/step - loss: 0.0262 - acc: 0.8622 - val_loss: 0.0271 - val_acc: 0.8609\n",
      "Epoch 300/300\n",
      "8564/8564 [==============================] - 5s 565us/step - loss: 0.0263 - acc: 0.8613 - val_loss: 0.0255 - val_acc: 0.8687\n",
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 281/300\n",
      "8564/8564 [==============================] - 5s 553us/step - loss: 0.0287 - acc: 0.9792 - val_loss: 0.0291 - val_acc: 0.9823\n",
      "Epoch 282/300\n",
      "8564/8564 [==============================] - 5s 546us/step - loss: 0.0278 - acc: 0.9797 - val_loss: 0.0277 - val_acc: 0.9836\n",
      "Epoch 283/300\n",
      "8564/8564 [==============================] - 5s 532us/step - loss: 0.0280 - acc: 0.9791 - val_loss: 0.0257 - val_acc: 0.9832\n",
      "Epoch 284/300\n",
      "8564/8564 [==============================] - 5s 571us/step - loss: 0.0289 - acc: 0.9784 - val_loss: 0.0240 - val_acc: 0.9847\n",
      "Epoch 285/300\n",
      "8564/8564 [==============================] - 5s 566us/step - loss: 0.0286 - acc: 0.9809 - val_loss: 0.0320 - val_acc: 0.9843\n",
      "Epoch 286/300\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0296 - acc: 0.9773 - val_loss: 0.0267 - val_acc: 0.9813\n",
      "Epoch 287/300\n",
      "8564/8564 [==============================] - 5s 561us/step - loss: 0.0279 - acc: 0.9792 - val_loss: 0.0232 - val_acc: 0.9797\n",
      "Epoch 288/300\n",
      "8564/8564 [==============================] - 5s 554us/step - loss: 0.0296 - acc: 0.9812 - val_loss: 0.0240 - val_acc: 0.9844\n",
      "Epoch 289/300\n",
      "8564/8564 [==============================] - 5s 549us/step - loss: 0.0284 - acc: 0.9786 - val_loss: 0.0316 - val_acc: 0.9810\n",
      "Epoch 290/300\n",
      "8564/8564 [==============================] - 5s 568us/step - loss: 0.0304 - acc: 0.9801 - val_loss: 0.0252 - val_acc: 0.9829\n",
      "Epoch 291/300\n",
      "8564/8564 [==============================] - 5s 564us/step - loss: 0.0277 - acc: 0.9797 - val_loss: 0.0219 - val_acc: 0.9848\n",
      "Epoch 292/300\n",
      "8564/8564 [==============================] - 5s 572us/step - loss: 0.0285 - acc: 0.9804 - val_loss: 0.0282 - val_acc: 0.9807\n",
      "Epoch 293/300\n",
      "8564/8564 [==============================] - 5s 569us/step - loss: 0.0297 - acc: 0.9799 - val_loss: 0.0257 - val_acc: 0.9825\n",
      "Epoch 294/300\n",
      "8564/8564 [==============================] - 5s 536us/step - loss: 0.0283 - acc: 0.9792 - val_loss: 0.0229 - val_acc: 0.9851\n",
      "Epoch 295/300\n",
      "8564/8564 [==============================] - 5s 553us/step - loss: 0.0261 - acc: 0.9809 - val_loss: 0.0279 - val_acc: 0.9804\n",
      "Epoch 296/300\n",
      "8564/8564 [==============================] - 5s 576us/step - loss: 0.0284 - acc: 0.9778 - val_loss: 0.0250 - val_acc: 0.9836\n",
      "Epoch 297/300\n",
      "8564/8564 [==============================] - 5s 564us/step - loss: 0.0278 - acc: 0.9797 - val_loss: 0.0247 - val_acc: 0.9805\n",
      "Epoch 298/300\n",
      "8564/8564 [==============================] - 5s 563us/step - loss: 0.0270 - acc: 0.9797 - val_loss: 0.0256 - val_acc: 0.9833\n",
      "Epoch 299/300\n",
      "8564/8564 [==============================] - 4s 509us/step - loss: 0.0278 - acc: 0.9800 - val_loss: 0.0240 - val_acc: 0.9823\n",
      "Epoch 300/300\n",
      "8564/8564 [==============================] - 5s 559us/step - loss: 0.0281 - acc: 0.9773 - val_loss: 0.0314 - val_acc: 0.9794\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 281/300\n",
      "8564/8564 [==============================] - 5s 558us/step - loss: 0.0336 - acc: 0.9219 - val_loss: 0.0333 - val_acc: 0.9239\n",
      "Epoch 282/300\n",
      "8564/8564 [==============================] - 5s 555us/step - loss: 0.0331 - acc: 0.9265 - val_loss: 0.0338 - val_acc: 0.9374\n",
      "Epoch 283/300\n",
      "8564/8564 [==============================] - 5s 555us/step - loss: 0.0333 - acc: 0.9241 - val_loss: 0.0359 - val_acc: 0.9335\n",
      "Epoch 284/300\n",
      "8564/8564 [==============================] - 5s 540us/step - loss: 0.0339 - acc: 0.9228 - val_loss: 0.0356 - val_acc: 0.9261\n",
      "Epoch 285/300\n",
      "8564/8564 [==============================] - 5s 592us/step - loss: 0.0331 - acc: 0.9243 - val_loss: 0.0317 - val_acc: 0.9261\n",
      "Epoch 286/300\n",
      "8564/8564 [==============================] - 5s 590us/step - loss: 0.0331 - acc: 0.9252 - val_loss: 0.0361 - val_acc: 0.9315\n",
      "Epoch 287/300\n",
      "8564/8564 [==============================] - 5s 537us/step - loss: 0.0337 - acc: 0.9233 - val_loss: 0.0300 - val_acc: 0.9321\n",
      "Epoch 288/300\n",
      "8564/8564 [==============================] - 5s 533us/step - loss: 0.0328 - acc: 0.9258 - val_loss: 0.0308 - val_acc: 0.9362\n",
      "Epoch 289/300\n",
      "8564/8564 [==============================] - 5s 575us/step - loss: 0.0328 - acc: 0.9242 - val_loss: 0.0329 - val_acc: 0.9230\n",
      "Epoch 290/300\n",
      "8564/8564 [==============================] - 5s 555us/step - loss: 0.0341 - acc: 0.9245 - val_loss: 0.0372 - val_acc: 0.9074\n",
      "Epoch 291/300\n",
      "8564/8564 [==============================] - 5s 592us/step - loss: 0.0334 - acc: 0.9268 - val_loss: 0.0332 - val_acc: 0.9344\n",
      "Epoch 292/300\n",
      "8564/8564 [==============================] - 5s 567us/step - loss: 0.0339 - acc: 0.9244 - val_loss: 0.0285 - val_acc: 0.9444\n",
      "Epoch 293/300\n",
      "8564/8564 [==============================] - 5s 557us/step - loss: 0.0332 - acc: 0.9250 - val_loss: 0.0285 - val_acc: 0.9448\n",
      "Epoch 294/300\n",
      "8564/8564 [==============================] - 5s 540us/step - loss: 0.0334 - acc: 0.9243 - val_loss: 0.0316 - val_acc: 0.9353\n",
      "Epoch 295/300\n",
      "8564/8564 [==============================] - 5s 561us/step - loss: 0.0327 - acc: 0.9259 - val_loss: 0.0351 - val_acc: 0.9268\n",
      "Epoch 296/300\n",
      "8564/8564 [==============================] - 5s 551us/step - loss: 0.0332 - acc: 0.9285 - val_loss: 0.0328 - val_acc: 0.9262\n",
      "Epoch 297/300\n",
      "8564/8564 [==============================] - 5s 589us/step - loss: 0.0339 - acc: 0.9238 - val_loss: 0.0364 - val_acc: 0.9384\n",
      "Epoch 298/300\n",
      "8564/8564 [==============================] - 5s 564us/step - loss: 0.0332 - acc: 0.9250 - val_loss: 0.0298 - val_acc: 0.9402\n",
      "Epoch 299/300\n",
      "8564/8564 [==============================] - 5s 538us/step - loss: 0.0337 - acc: 0.9243 - val_loss: 0.0350 - val_acc: 0.9338\n",
      "Epoch 300/300\n",
      "8564/8564 [==============================] - 5s 566us/step - loss: 0.0325 - acc: 0.9266 - val_loss: 0.0349 - val_acc: 0.9388\n",
      "start training round 15\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 301/320\n",
      "8564/8564 [==============================] - 5s 550us/step - loss: 0.0306 - acc: 0.6178 - val_loss: 0.0309 - val_acc: 0.5554\n",
      "Epoch 302/320\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0303 - acc: 0.6279 - val_loss: 0.0303 - val_acc: 0.6090\n",
      "Epoch 303/320\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0303 - acc: 0.6257 - val_loss: 0.0305 - val_acc: 0.5991\n",
      "Epoch 304/320\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0301 - acc: 0.6397 - val_loss: 0.0299 - val_acc: 0.6834\n",
      "Epoch 305/320\n",
      "8564/8564 [==============================] - 5s 572us/step - loss: 0.0302 - acc: 0.6712 - val_loss: 0.0301 - val_acc: 0.6654\n",
      "Epoch 306/320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 5s 549us/step - loss: 0.0299 - acc: 0.6659 - val_loss: 0.0307 - val_acc: 0.6720\n",
      "Epoch 307/320\n",
      "8564/8564 [==============================] - 5s 558us/step - loss: 0.0301 - acc: 0.6480 - val_loss: 0.0310 - val_acc: 0.6871\n",
      "Epoch 308/320\n",
      "8564/8564 [==============================] - 5s 578us/step - loss: 0.0300 - acc: 0.6527 - val_loss: 0.0305 - val_acc: 0.6059\n",
      "Epoch 309/320\n",
      "8564/8564 [==============================] - 5s 589us/step - loss: 0.0304 - acc: 0.6233 - val_loss: 0.0307 - val_acc: 0.5782\n",
      "Epoch 310/320\n",
      "8564/8564 [==============================] - 5s 544us/step - loss: 0.0300 - acc: 0.6403 - val_loss: 0.0305 - val_acc: 0.5582\n",
      "Epoch 311/320\n",
      "8564/8564 [==============================] - 5s 556us/step - loss: 0.0302 - acc: 0.6219 - val_loss: 0.0307 - val_acc: 0.5462\n",
      "Epoch 312/320\n",
      "8564/8564 [==============================] - 5s 554us/step - loss: 0.0299 - acc: 0.6406 - val_loss: 0.0306 - val_acc: 0.5401\n",
      "Epoch 313/320\n",
      "8564/8564 [==============================] - 5s 589us/step - loss: 0.0298 - acc: 0.6412 - val_loss: 0.0298 - val_acc: 0.6529\n",
      "Epoch 314/320\n",
      "8564/8564 [==============================] - 5s 533us/step - loss: 0.0299 - acc: 0.6874 - val_loss: 0.0311 - val_acc: 0.5238\n",
      "Epoch 315/320\n",
      "8564/8564 [==============================] - 4s 510us/step - loss: 0.0299 - acc: 0.6418 - val_loss: 0.0304 - val_acc: 0.5706\n",
      "Epoch 316/320\n",
      "8564/8564 [==============================] - 5s 593us/step - loss: 0.0301 - acc: 0.6292 - val_loss: 0.0310 - val_acc: 0.5218\n",
      "Epoch 317/320\n",
      "8564/8564 [==============================] - 5s 552us/step - loss: 0.0301 - acc: 0.6326 - val_loss: 0.0297 - val_acc: 0.6303\n",
      "Epoch 318/320\n",
      "8564/8564 [==============================] - 5s 569us/step - loss: 0.0300 - acc: 0.6293 - val_loss: 0.0321 - val_acc: 0.5120\n",
      "Epoch 319/320\n",
      "8564/8564 [==============================] - 5s 558us/step - loss: 0.0299 - acc: 0.6618 - val_loss: 0.0307 - val_acc: 0.6483\n",
      "Epoch 320/320\n",
      "8564/8564 [==============================] - 5s 554us/step - loss: 0.0300 - acc: 0.6321 - val_loss: 0.0299 - val_acc: 0.6893\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 301/320\n",
      "8564/8564 [==============================] - 5s 546us/step - loss: 0.0261 - acc: 0.8692 - val_loss: 0.0261 - val_acc: 0.8643\n",
      "Epoch 302/320\n",
      "8564/8564 [==============================] - 5s 573us/step - loss: 0.0260 - acc: 0.8686 - val_loss: 0.0261 - val_acc: 0.8635\n",
      "Epoch 303/320\n",
      "8564/8564 [==============================] - 5s 566us/step - loss: 0.0261 - acc: 0.8630 - val_loss: 0.0265 - val_acc: 0.8699\n",
      "Epoch 304/320\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0262 - acc: 0.8634 - val_loss: 0.0273 - val_acc: 0.8602\n",
      "Epoch 305/320\n",
      "8564/8564 [==============================] - 5s 559us/step - loss: 0.0260 - acc: 0.8629 - val_loss: 0.0266 - val_acc: 0.8626\n",
      "Epoch 306/320\n",
      "8564/8564 [==============================] - 5s 534us/step - loss: 0.0262 - acc: 0.8633 - val_loss: 0.0262 - val_acc: 0.8663\n",
      "Epoch 307/320\n",
      "8564/8564 [==============================] - 5s 559us/step - loss: 0.0260 - acc: 0.8633 - val_loss: 0.0269 - val_acc: 0.8631\n",
      "Epoch 308/320\n",
      "8564/8564 [==============================] - 5s 579us/step - loss: 0.0259 - acc: 0.8649 - val_loss: 0.0257 - val_acc: 0.8672\n",
      "Epoch 309/320\n",
      "8564/8564 [==============================] - 5s 563us/step - loss: 0.0261 - acc: 0.8630 - val_loss: 0.0258 - val_acc: 0.8688\n",
      "Epoch 310/320\n",
      "8564/8564 [==============================] - 5s 571us/step - loss: 0.0260 - acc: 0.8638 - val_loss: 0.0272 - val_acc: 0.8621\n",
      "Epoch 311/320\n",
      "8564/8564 [==============================] - 5s 576us/step - loss: 0.0261 - acc: 0.8626 - val_loss: 0.0254 - val_acc: 0.8684\n",
      "Epoch 312/320\n",
      "8564/8564 [==============================] - 5s 555us/step - loss: 0.0257 - acc: 0.8666 - val_loss: 0.0281 - val_acc: 0.8587\n",
      "Epoch 313/320\n",
      "8564/8564 [==============================] - 5s 570us/step - loss: 0.0259 - acc: 0.8641 - val_loss: 0.0264 - val_acc: 0.8638\n",
      "Epoch 314/320\n",
      "8564/8564 [==============================] - 5s 570us/step - loss: 0.0255 - acc: 0.8673 - val_loss: 0.0262 - val_acc: 0.8658\n",
      "Epoch 315/320\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0260 - acc: 0.8670 - val_loss: 0.0269 - val_acc: 0.8624\n",
      "Epoch 316/320\n",
      "8564/8564 [==============================] - 5s 560us/step - loss: 0.0258 - acc: 0.8641 - val_loss: 0.0260 - val_acc: 0.8654\n",
      "Epoch 317/320\n",
      "8564/8564 [==============================] - 5s 579us/step - loss: 0.0260 - acc: 0.8639 - val_loss: 0.0263 - val_acc: 0.8669\n",
      "Epoch 318/320\n",
      "8564/8564 [==============================] - 5s 570us/step - loss: 0.0259 - acc: 0.8665 - val_loss: 0.0267 - val_acc: 0.8697\n",
      "Epoch 319/320\n",
      "8564/8564 [==============================] - 5s 554us/step - loss: 0.0259 - acc: 0.8714 - val_loss: 0.0261 - val_acc: 0.8710\n",
      "Epoch 320/320\n",
      "8564/8564 [==============================] - 5s 573us/step - loss: 0.0260 - acc: 0.8715 - val_loss: 0.0251 - val_acc: 0.8712\n",
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 301/320\n",
      "8564/8564 [==============================] - 5s 584us/step - loss: 0.0292 - acc: 0.9809 - val_loss: 0.0255 - val_acc: 0.9834\n",
      "Epoch 302/320\n",
      "8564/8564 [==============================] - 5s 551us/step - loss: 0.0276 - acc: 0.9798 - val_loss: 0.0283 - val_acc: 0.9855\n",
      "Epoch 303/320\n",
      "8564/8564 [==============================] - 5s 595us/step - loss: 0.0276 - acc: 0.9813 - val_loss: 0.0307 - val_acc: 0.9806\n",
      "Epoch 304/320\n",
      "8564/8564 [==============================] - 5s 551us/step - loss: 0.0286 - acc: 0.9805 - val_loss: 0.0264 - val_acc: 0.9841\n",
      "Epoch 305/320\n",
      "8564/8564 [==============================] - 5s 575us/step - loss: 0.0284 - acc: 0.9800 - val_loss: 0.0274 - val_acc: 0.9828\n",
      "Epoch 306/320\n",
      "8564/8564 [==============================] - 5s 570us/step - loss: 0.0291 - acc: 0.9778 - val_loss: 0.0343 - val_acc: 0.9829\n",
      "Epoch 307/320\n",
      "8564/8564 [==============================] - 5s 556us/step - loss: 0.0271 - acc: 0.9812 - val_loss: 0.0268 - val_acc: 0.9848\n",
      "Epoch 308/320\n",
      "8564/8564 [==============================] - 5s 563us/step - loss: 0.0262 - acc: 0.9805 - val_loss: 0.0223 - val_acc: 0.9813\n",
      "Epoch 309/320\n",
      "8564/8564 [==============================] - 5s 564us/step - loss: 0.0262 - acc: 0.9817 - val_loss: 0.0262 - val_acc: 0.9851\n",
      "Epoch 310/320\n",
      "8564/8564 [==============================] - 4s 516us/step - loss: 0.0275 - acc: 0.9803 - val_loss: 0.0285 - val_acc: 0.9787\n",
      "Epoch 311/320\n",
      "8564/8564 [==============================] - 5s 556us/step - loss: 0.0284 - acc: 0.9800 - val_loss: 0.0282 - val_acc: 0.9806\n",
      "Epoch 312/320\n",
      "8564/8564 [==============================] - 5s 551us/step - loss: 0.0275 - acc: 0.9813 - val_loss: 0.0320 - val_acc: 0.9836\n",
      "Epoch 313/320\n",
      "8564/8564 [==============================] - 5s 567us/step - loss: 0.0280 - acc: 0.9787 - val_loss: 0.0250 - val_acc: 0.9840\n",
      "Epoch 314/320\n",
      "8564/8564 [==============================] - 5s 560us/step - loss: 0.0277 - acc: 0.9804 - val_loss: 0.0327 - val_acc: 0.9802\n",
      "Epoch 315/320\n",
      "8564/8564 [==============================] - 5s 548us/step - loss: 0.0299 - acc: 0.9806 - val_loss: 0.0237 - val_acc: 0.9839\n",
      "Epoch 316/320\n",
      "8564/8564 [==============================] - 5s 580us/step - loss: 0.0285 - acc: 0.9798 - val_loss: 0.0284 - val_acc: 0.9841\n",
      "Epoch 317/320\n",
      "8564/8564 [==============================] - 5s 553us/step - loss: 0.0284 - acc: 0.9800 - val_loss: 0.0225 - val_acc: 0.9829\n",
      "Epoch 318/320\n",
      "8564/8564 [==============================] - 5s 569us/step - loss: 0.0278 - acc: 0.9803 - val_loss: 0.0274 - val_acc: 0.9824\n",
      "Epoch 319/320\n",
      "8564/8564 [==============================] - 5s 556us/step - loss: 0.0291 - acc: 0.9803 - val_loss: 0.0307 - val_acc: 0.9860\n",
      "Epoch 320/320\n",
      "8564/8564 [==============================] - 5s 549us/step - loss: 0.0277 - acc: 0.9796 - val_loss: 0.0286 - val_acc: 0.9792\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 301/320\n",
      "8564/8564 [==============================] - 5s 557us/step - loss: 0.0330 - acc: 0.9265 - val_loss: 0.0344 - val_acc: 0.9186\n",
      "Epoch 302/320\n",
      "8564/8564 [==============================] - 5s 570us/step - loss: 0.0328 - acc: 0.9239 - val_loss: 0.0284 - val_acc: 0.9382\n",
      "Epoch 303/320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 5s 544us/step - loss: 0.0321 - acc: 0.9302 - val_loss: 0.0320 - val_acc: 0.9358\n",
      "Epoch 304/320\n",
      "8564/8564 [==============================] - 5s 554us/step - loss: 0.0319 - acc: 0.9289 - val_loss: 0.0341 - val_acc: 0.9297\n",
      "Epoch 305/320\n",
      "8564/8564 [==============================] - 5s 561us/step - loss: 0.0324 - acc: 0.9282 - val_loss: 0.0274 - val_acc: 0.9445\n",
      "Epoch 306/320\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0324 - acc: 0.9298 - val_loss: 0.0297 - val_acc: 0.9410\n",
      "Epoch 307/320\n",
      "8564/8564 [==============================] - 5s 600us/step - loss: 0.0321 - acc: 0.9253 - val_loss: 0.0246 - val_acc: 0.9439\n",
      "Epoch 308/320\n",
      "8564/8564 [==============================] - 5s 558us/step - loss: 0.0319 - acc: 0.9278 - val_loss: 0.0363 - val_acc: 0.9128\n",
      "Epoch 309/320\n",
      "8564/8564 [==============================] - 5s 577us/step - loss: 0.0323 - acc: 0.9273 - val_loss: 0.0327 - val_acc: 0.9365\n",
      "Epoch 310/320\n",
      "8564/8564 [==============================] - 5s 564us/step - loss: 0.0333 - acc: 0.9259 - val_loss: 0.0326 - val_acc: 0.9293\n",
      "Epoch 311/320\n",
      "8564/8564 [==============================] - 5s 561us/step - loss: 0.0321 - acc: 0.9282 - val_loss: 0.0298 - val_acc: 0.9420\n",
      "Epoch 312/320\n",
      "8564/8564 [==============================] - 5s 573us/step - loss: 0.0333 - acc: 0.9257 - val_loss: 0.0309 - val_acc: 0.9269\n",
      "Epoch 313/320\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0316 - acc: 0.9287 - val_loss: 0.0330 - val_acc: 0.9282\n",
      "Epoch 314/320\n",
      "8564/8564 [==============================] - 5s 576us/step - loss: 0.0331 - acc: 0.9245 - val_loss: 0.0343 - val_acc: 0.9210\n",
      "Epoch 315/320\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0328 - acc: 0.9279 - val_loss: 0.0299 - val_acc: 0.9349\n",
      "Epoch 316/320\n",
      "8564/8564 [==============================] - 5s 568us/step - loss: 0.0330 - acc: 0.9246 - val_loss: 0.0301 - val_acc: 0.9349\n",
      "Epoch 317/320\n",
      "8564/8564 [==============================] - 5s 552us/step - loss: 0.0318 - acc: 0.9267 - val_loss: 0.0297 - val_acc: 0.9275\n",
      "Epoch 318/320\n",
      "8564/8564 [==============================] - 5s 571us/step - loss: 0.0320 - acc: 0.9266 - val_loss: 0.0327 - val_acc: 0.9267\n",
      "Epoch 319/320\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0322 - acc: 0.9294 - val_loss: 0.0343 - val_acc: 0.9359\n",
      "Epoch 320/320\n",
      "8564/8564 [==============================] - 5s 545us/step - loss: 0.0329 - acc: 0.9268 - val_loss: 0.0319 - val_acc: 0.9316\n",
      "start training round 16\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 321/340\n",
      "8564/8564 [==============================] - 5s 535us/step - loss: 0.0296 - acc: 0.6616 - val_loss: 0.0301 - val_acc: 0.6890\n",
      "Epoch 322/340\n",
      "8564/8564 [==============================] - 5s 584us/step - loss: 0.0293 - acc: 0.6848 - val_loss: 0.0299 - val_acc: 0.6346\n",
      "Epoch 323/340\n",
      "8564/8564 [==============================] - 5s 563us/step - loss: 0.0301 - acc: 0.6379 - val_loss: 0.0308 - val_acc: 0.5345\n",
      "Epoch 324/340\n",
      "8564/8564 [==============================] - 5s 559us/step - loss: 0.0298 - acc: 0.6339 - val_loss: 0.0295 - val_acc: 0.6275\n",
      "Epoch 325/340\n",
      "8564/8564 [==============================] - 5s 531us/step - loss: 0.0297 - acc: 0.6412 - val_loss: 0.0319 - val_acc: 0.5091\n",
      "Epoch 326/340\n",
      "8564/8564 [==============================] - 5s 569us/step - loss: 0.0297 - acc: 0.6480 - val_loss: 0.0300 - val_acc: 0.6967\n",
      "Epoch 327/340\n",
      "8564/8564 [==============================] - 5s 593us/step - loss: 0.0297 - acc: 0.6823 - val_loss: 0.0308 - val_acc: 0.6972\n",
      "Epoch 328/340\n",
      "8564/8564 [==============================] - 5s 570us/step - loss: 0.0297 - acc: 0.6587 - val_loss: 0.0298 - val_acc: 0.6943\n",
      "Epoch 329/340\n",
      "8564/8564 [==============================] - 5s 561us/step - loss: 0.0296 - acc: 0.6816 - val_loss: 0.0292 - val_acc: 0.6940\n",
      "Epoch 330/340\n",
      "8564/8564 [==============================] - 5s 576us/step - loss: 0.0296 - acc: 0.6399 - val_loss: 0.0298 - val_acc: 0.6908\n",
      "Epoch 331/340\n",
      "8564/8564 [==============================] - 5s 593us/step - loss: 0.0297 - acc: 0.6332 - val_loss: 0.0315 - val_acc: 0.6921\n",
      "Epoch 332/340\n",
      "8564/8564 [==============================] - 5s 580us/step - loss: 0.0293 - acc: 0.6540 - val_loss: 0.0298 - val_acc: 0.6978\n",
      "Epoch 333/340\n",
      "8564/8564 [==============================] - 5s 585us/step - loss: 0.0295 - acc: 0.6373 - val_loss: 0.0303 - val_acc: 0.6899\n",
      "Epoch 334/340\n",
      "8564/8564 [==============================] - 5s 576us/step - loss: 0.0297 - acc: 0.6369 - val_loss: 0.0302 - val_acc: 0.6944\n",
      "Epoch 335/340\n",
      "8564/8564 [==============================] - 5s 558us/step - loss: 0.0295 - acc: 0.6498 - val_loss: 0.0296 - val_acc: 0.6968\n",
      "Epoch 336/340\n",
      "8564/8564 [==============================] - 5s 563us/step - loss: 0.0296 - acc: 0.6948 - val_loss: 0.0302 - val_acc: 0.6644\n",
      "Epoch 337/340\n",
      "8564/8564 [==============================] - 5s 561us/step - loss: 0.0298 - acc: 0.6955 - val_loss: 0.0296 - val_acc: 0.6975\n",
      "Epoch 338/340\n",
      "8564/8564 [==============================] - 5s 561us/step - loss: 0.0296 - acc: 0.6923 - val_loss: 0.0304 - val_acc: 0.6778\n",
      "Epoch 339/340\n",
      "8564/8564 [==============================] - 5s 558us/step - loss: 0.0297 - acc: 0.6869 - val_loss: 0.0307 - val_acc: 0.6942\n",
      "Epoch 340/340\n",
      "8564/8564 [==============================] - 5s 573us/step - loss: 0.0299 - acc: 0.6260 - val_loss: 0.0303 - val_acc: 0.6966\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 321/340\n",
      "8564/8564 [==============================] - 5s 575us/step - loss: 0.0255 - acc: 0.8718 - val_loss: 0.0260 - val_acc: 0.8652\n",
      "Epoch 322/340\n",
      "8564/8564 [==============================] - 5s 579us/step - loss: 0.0257 - acc: 0.8693 - val_loss: 0.0256 - val_acc: 0.8712\n",
      "Epoch 323/340\n",
      "8564/8564 [==============================] - 5s 553us/step - loss: 0.0257 - acc: 0.8673 - val_loss: 0.0259 - val_acc: 0.8677\n",
      "Epoch 324/340\n",
      "8564/8564 [==============================] - 5s 582us/step - loss: 0.0259 - acc: 0.8643 - val_loss: 0.0257 - val_acc: 0.8698\n",
      "Epoch 325/340\n",
      "8564/8564 [==============================] - 5s 542us/step - loss: 0.0255 - acc: 0.8663 - val_loss: 0.0263 - val_acc: 0.8720\n",
      "Epoch 326/340\n",
      "8564/8564 [==============================] - 5s 563us/step - loss: 0.0258 - acc: 0.8707 - val_loss: 0.0258 - val_acc: 0.8643\n",
      "Epoch 327/340\n",
      "8564/8564 [==============================] - 5s 570us/step - loss: 0.0255 - acc: 0.8700 - val_loss: 0.0268 - val_acc: 0.8718\n",
      "Epoch 328/340\n",
      "8564/8564 [==============================] - 5s 572us/step - loss: 0.0257 - acc: 0.8717 - val_loss: 0.0261 - val_acc: 0.8707\n",
      "Epoch 329/340\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0256 - acc: 0.8668 - val_loss: 0.0266 - val_acc: 0.8669\n",
      "Epoch 330/340\n",
      "8564/8564 [==============================] - 5s 542us/step - loss: 0.0256 - acc: 0.8665 - val_loss: 0.0268 - val_acc: 0.8636\n",
      "Epoch 331/340\n",
      "8564/8564 [==============================] - 5s 566us/step - loss: 0.0257 - acc: 0.8660 - val_loss: 0.0257 - val_acc: 0.8668\n",
      "Epoch 332/340\n",
      "8564/8564 [==============================] - 5s 580us/step - loss: 0.0257 - acc: 0.8650 - val_loss: 0.0257 - val_acc: 0.8699\n",
      "Epoch 333/340\n",
      "8564/8564 [==============================] - 5s 581us/step - loss: 0.0256 - acc: 0.8669 - val_loss: 0.0256 - val_acc: 0.8691\n",
      "Epoch 334/340\n",
      "8564/8564 [==============================] - 5s 576us/step - loss: 0.0258 - acc: 0.8648 - val_loss: 0.0252 - val_acc: 0.8716\n",
      "Epoch 335/340\n",
      "8564/8564 [==============================] - 5s 592us/step - loss: 0.0256 - acc: 0.8659 - val_loss: 0.0263 - val_acc: 0.8663\n",
      "Epoch 336/340\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0254 - acc: 0.8714 - val_loss: 0.0258 - val_acc: 0.8691\n",
      "Epoch 337/340\n",
      "8564/8564 [==============================] - 5s 579us/step - loss: 0.0255 - acc: 0.8695 - val_loss: 0.0253 - val_acc: 0.8709\n",
      "Epoch 338/340\n",
      "8564/8564 [==============================] - 5s 559us/step - loss: 0.0254 - acc: 0.8675 - val_loss: 0.0252 - val_acc: 0.8665\n",
      "Epoch 339/340\n",
      "8564/8564 [==============================] - 5s 557us/step - loss: 0.0255 - acc: 0.8676 - val_loss: 0.0259 - val_acc: 0.8642\n",
      "Epoch 340/340\n",
      "8564/8564 [==============================] - 5s 535us/step - loss: 0.0255 - acc: 0.8669 - val_loss: 0.0256 - val_acc: 0.8652\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 321/340\n",
      "8564/8564 [==============================] - 5s 551us/step - loss: 0.0291 - acc: 0.9798 - val_loss: 0.0261 - val_acc: 0.9831\n",
      "Epoch 322/340\n",
      "8564/8564 [==============================] - 5s 557us/step - loss: 0.0278 - acc: 0.9803 - val_loss: 0.0267 - val_acc: 0.9827\n",
      "Epoch 323/340\n",
      "8564/8564 [==============================] - 5s 566us/step - loss: 0.0271 - acc: 0.9789 - val_loss: 0.0214 - val_acc: 0.9847\n",
      "Epoch 324/340\n",
      "8564/8564 [==============================] - 5s 595us/step - loss: 0.0271 - acc: 0.9802 - val_loss: 0.0260 - val_acc: 0.9853\n",
      "Epoch 325/340\n",
      "8564/8564 [==============================] - 5s 568us/step - loss: 0.0280 - acc: 0.9806 - val_loss: 0.0261 - val_acc: 0.9815\n",
      "Epoch 326/340\n",
      "8564/8564 [==============================] - 5s 582us/step - loss: 0.0285 - acc: 0.9806 - val_loss: 0.0259 - val_acc: 0.9809\n",
      "Epoch 327/340\n",
      "8564/8564 [==============================] - 5s 564us/step - loss: 0.0282 - acc: 0.9804 - val_loss: 0.0247 - val_acc: 0.9841\n",
      "Epoch 328/340\n",
      "8564/8564 [==============================] - 5s 551us/step - loss: 0.0281 - acc: 0.9801 - val_loss: 0.0244 - val_acc: 0.9873\n",
      "Epoch 329/340\n",
      "8564/8564 [==============================] - 5s 622us/step - loss: 0.0273 - acc: 0.9816 - val_loss: 0.0250 - val_acc: 0.9796\n",
      "Epoch 330/340\n",
      "8564/8564 [==============================] - 5s 579us/step - loss: 0.0302 - acc: 0.9783 - val_loss: 0.0251 - val_acc: 0.9836\n",
      "Epoch 331/340\n",
      "8564/8564 [==============================] - 5s 596us/step - loss: 0.0271 - acc: 0.9810 - val_loss: 0.0283 - val_acc: 0.9837\n",
      "Epoch 332/340\n",
      "8564/8564 [==============================] - 5s 582us/step - loss: 0.0270 - acc: 0.9799 - val_loss: 0.0286 - val_acc: 0.9805\n",
      "Epoch 333/340\n",
      "8564/8564 [==============================] - 5s 592us/step - loss: 0.0261 - acc: 0.9814 - val_loss: 0.0239 - val_acc: 0.9850\n",
      "Epoch 334/340\n",
      "8564/8564 [==============================] - 5s 578us/step - loss: 0.0272 - acc: 0.9821 - val_loss: 0.0274 - val_acc: 0.9835\n",
      "Epoch 335/340\n",
      "8564/8564 [==============================] - 5s 568us/step - loss: 0.0279 - acc: 0.9808 - val_loss: 0.0253 - val_acc: 0.9846\n",
      "Epoch 336/340\n",
      "8564/8564 [==============================] - 5s 605us/step - loss: 0.0270 - acc: 0.9799 - val_loss: 0.0259 - val_acc: 0.9801\n",
      "Epoch 337/340\n",
      "8564/8564 [==============================] - 5s 558us/step - loss: 0.0293 - acc: 0.9791 - val_loss: 0.0292 - val_acc: 0.9815\n",
      "Epoch 338/340\n",
      "8564/8564 [==============================] - 5s 557us/step - loss: 0.0280 - acc: 0.9819 - val_loss: 0.0225 - val_acc: 0.9868\n",
      "Epoch 339/340\n",
      "8564/8564 [==============================] - 5s 564us/step - loss: 0.0279 - acc: 0.9815 - val_loss: 0.0311 - val_acc: 0.9806\n",
      "Epoch 340/340\n",
      "8564/8564 [==============================] - 5s 586us/step - loss: 0.0275 - acc: 0.9789 - val_loss: 0.0255 - val_acc: 0.9811\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 321/340\n",
      "8564/8564 [==============================] - 5s 572us/step - loss: 0.0318 - acc: 0.9289 - val_loss: 0.0322 - val_acc: 0.9411\n",
      "Epoch 322/340\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0331 - acc: 0.9261 - val_loss: 0.0306 - val_acc: 0.9414\n",
      "Epoch 323/340\n",
      "8564/8564 [==============================] - 5s 569us/step - loss: 0.0318 - acc: 0.9296 - val_loss: 0.0318 - val_acc: 0.9447\n",
      "Epoch 324/340\n",
      "8564/8564 [==============================] - 5s 586us/step - loss: 0.0336 - acc: 0.9280 - val_loss: 0.0345 - val_acc: 0.9191\n",
      "Epoch 325/340\n",
      "8564/8564 [==============================] - 5s 600us/step - loss: 0.0325 - acc: 0.9271 - val_loss: 0.0310 - val_acc: 0.9313\n",
      "Epoch 326/340\n",
      "8564/8564 [==============================] - 5s 579us/step - loss: 0.0327 - acc: 0.9277 - val_loss: 0.0311 - val_acc: 0.9441\n",
      "Epoch 327/340\n",
      "8564/8564 [==============================] - 5s 581us/step - loss: 0.0329 - acc: 0.9260 - val_loss: 0.0318 - val_acc: 0.9374\n",
      "Epoch 328/340\n",
      "8564/8564 [==============================] - 5s 556us/step - loss: 0.0331 - acc: 0.9269 - val_loss: 0.0281 - val_acc: 0.9409\n",
      "Epoch 329/340\n",
      "8564/8564 [==============================] - 5s 584us/step - loss: 0.0325 - acc: 0.9257 - val_loss: 0.0373 - val_acc: 0.9086\n",
      "Epoch 330/340\n",
      "8564/8564 [==============================] - 5s 602us/step - loss: 0.0324 - acc: 0.9281 - val_loss: 0.0344 - val_acc: 0.9336\n",
      "Epoch 331/340\n",
      "8564/8564 [==============================] - 5s 555us/step - loss: 0.0316 - acc: 0.9283 - val_loss: 0.0266 - val_acc: 0.9340\n",
      "Epoch 332/340\n",
      "8564/8564 [==============================] - 5s 585us/step - loss: 0.0320 - acc: 0.9269 - val_loss: 0.0340 - val_acc: 0.9297\n",
      "Epoch 333/340\n",
      "8564/8564 [==============================] - 5s 561us/step - loss: 0.0333 - acc: 0.9258 - val_loss: 0.0357 - val_acc: 0.9207\n",
      "Epoch 334/340\n",
      "8564/8564 [==============================] - 5s 541us/step - loss: 0.0318 - acc: 0.9272 - val_loss: 0.0277 - val_acc: 0.9441\n",
      "Epoch 335/340\n",
      "8564/8564 [==============================] - 5s 586us/step - loss: 0.0328 - acc: 0.9283 - val_loss: 0.0286 - val_acc: 0.9375\n",
      "Epoch 336/340\n",
      "8564/8564 [==============================] - 5s 584us/step - loss: 0.0310 - acc: 0.9306 - val_loss: 0.0343 - val_acc: 0.9273\n",
      "Epoch 337/340\n",
      "8564/8564 [==============================] - 5s 578us/step - loss: 0.0319 - acc: 0.9305 - val_loss: 0.0320 - val_acc: 0.9299\n",
      "Epoch 338/340\n",
      "8564/8564 [==============================] - 5s 585us/step - loss: 0.0316 - acc: 0.9291 - val_loss: 0.0311 - val_acc: 0.9272\n",
      "Epoch 339/340\n",
      "8564/8564 [==============================] - 5s 557us/step - loss: 0.0322 - acc: 0.9289 - val_loss: 0.0277 - val_acc: 0.9415\n",
      "Epoch 340/340\n",
      "8564/8564 [==============================] - 5s 573us/step - loss: 0.0316 - acc: 0.9303 - val_loss: 0.0308 - val_acc: 0.9275\n",
      "start training round 17\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 341/360\n",
      "8564/8564 [==============================] - 5s 542us/step - loss: 0.0295 - acc: 0.6396 - val_loss: 0.0300 - val_acc: 0.6991\n",
      "Epoch 342/360\n",
      "8564/8564 [==============================] - 5s 585us/step - loss: 0.0294 - acc: 0.6407 - val_loss: 0.0309 - val_acc: 0.6911\n",
      "Epoch 343/360\n",
      "8564/8564 [==============================] - 5s 574us/step - loss: 0.0294 - acc: 0.6756 - val_loss: 0.0294 - val_acc: 0.7049\n",
      "Epoch 344/360\n",
      "8564/8564 [==============================] - 5s 581us/step - loss: 0.0294 - acc: 0.6417 - val_loss: 0.0299 - val_acc: 0.6958\n",
      "Epoch 345/360\n",
      "8564/8564 [==============================] - 5s 585us/step - loss: 0.0294 - acc: 0.6800 - val_loss: 0.0305 - val_acc: 0.6410\n",
      "Epoch 346/360\n",
      "8564/8564 [==============================] - 5s 568us/step - loss: 0.0293 - acc: 0.6776 - val_loss: 0.0314 - val_acc: 0.6076\n",
      "Epoch 347/360\n",
      "8564/8564 [==============================] - 5s 591us/step - loss: 0.0298 - acc: 0.7009 - val_loss: 0.0297 - val_acc: 0.7033\n",
      "Epoch 348/360\n",
      "8564/8564 [==============================] - 5s 573us/step - loss: 0.0292 - acc: 0.6465 - val_loss: 0.0302 - val_acc: 0.6981\n",
      "Epoch 349/360\n",
      "8564/8564 [==============================] - 5s 597us/step - loss: 0.0293 - acc: 0.6444 - val_loss: 0.0307 - val_acc: 0.6949\n",
      "Epoch 350/360\n",
      "8564/8564 [==============================] - 5s 588us/step - loss: 0.0291 - acc: 0.6864 - val_loss: 0.0304 - val_acc: 0.6932\n",
      "Epoch 351/360\n",
      "8564/8564 [==============================] - 5s 545us/step - loss: 0.0294 - acc: 0.6805 - val_loss: 0.0296 - val_acc: 0.6995\n",
      "Epoch 352/360\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0290 - acc: 0.6596 - val_loss: 0.0300 - val_acc: 0.6974\n",
      "Epoch 353/360\n",
      "8564/8564 [==============================] - 5s 596us/step - loss: 0.0294 - acc: 0.6937 - val_loss: 0.0304 - val_acc: 0.6981\n",
      "Epoch 354/360\n",
      "8564/8564 [==============================] - 5s 579us/step - loss: 0.0292 - acc: 0.7032 - val_loss: 0.0294 - val_acc: 0.6406\n",
      "Epoch 355/360\n",
      "8564/8564 [==============================] - 5s 568us/step - loss: 0.0292 - acc: 0.6713 - val_loss: 0.0296 - val_acc: 0.7054\n",
      "Epoch 356/360\n",
      "8564/8564 [==============================] - 5s 561us/step - loss: 0.0293 - acc: 0.6992 - val_loss: 0.0298 - val_acc: 0.7082\n",
      "Epoch 357/360\n",
      "8564/8564 [==============================] - 5s 591us/step - loss: 0.0291 - acc: 0.6828 - val_loss: 0.0303 - val_acc: 0.5485\n",
      "Epoch 358/360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 5s 591us/step - loss: 0.0294 - acc: 0.6367 - val_loss: 0.0293 - val_acc: 0.5831\n",
      "Epoch 359/360\n",
      "8564/8564 [==============================] - 5s 589us/step - loss: 0.0292 - acc: 0.6315 - val_loss: 0.0302 - val_acc: 0.5404\n",
      "Epoch 360/360\n",
      "8564/8564 [==============================] - 5s 570us/step - loss: 0.0292 - acc: 0.6381 - val_loss: 0.0306 - val_acc: 0.5244\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 341/360\n",
      "8564/8564 [==============================] - 5s 573us/step - loss: 0.0256 - acc: 0.8668 - val_loss: 0.0256 - val_acc: 0.8648\n",
      "Epoch 342/360\n",
      "8564/8564 [==============================] - 5s 572us/step - loss: 0.0253 - acc: 0.8691 - val_loss: 0.0256 - val_acc: 0.8724\n",
      "Epoch 343/360\n",
      "8564/8564 [==============================] - 5s 580us/step - loss: 0.0257 - acc: 0.8682 - val_loss: 0.0251 - val_acc: 0.8704\n",
      "Epoch 344/360\n",
      "8564/8564 [==============================] - 5s 570us/step - loss: 0.0254 - acc: 0.8730 - val_loss: 0.0260 - val_acc: 0.8710\n",
      "Epoch 345/360\n",
      "8564/8564 [==============================] - 5s 598us/step - loss: 0.0254 - acc: 0.8728 - val_loss: 0.0261 - val_acc: 0.8725\n",
      "Epoch 346/360\n",
      "8564/8564 [==============================] - 5s 566us/step - loss: 0.0254 - acc: 0.8738 - val_loss: 0.0256 - val_acc: 0.8722\n",
      "Epoch 347/360\n",
      "8564/8564 [==============================] - 5s 586us/step - loss: 0.0253 - acc: 0.8727 - val_loss: 0.0258 - val_acc: 0.8726\n",
      "Epoch 348/360\n",
      "8564/8564 [==============================] - 5s 527us/step - loss: 0.0252 - acc: 0.8728 - val_loss: 0.0260 - val_acc: 0.8738\n",
      "Epoch 349/360\n",
      "8564/8564 [==============================] - 5s 569us/step - loss: 0.0255 - acc: 0.8737 - val_loss: 0.0257 - val_acc: 0.8741\n",
      "Epoch 350/360\n",
      "8564/8564 [==============================] - 5s 566us/step - loss: 0.0254 - acc: 0.8738 - val_loss: 0.0256 - val_acc: 0.8727\n",
      "Epoch 351/360\n",
      "8564/8564 [==============================] - 5s 592us/step - loss: 0.0253 - acc: 0.8720 - val_loss: 0.0252 - val_acc: 0.8651\n",
      "Epoch 352/360\n",
      "8564/8564 [==============================] - 5s 551us/step - loss: 0.0251 - acc: 0.8709 - val_loss: 0.0261 - val_acc: 0.8735\n",
      "Epoch 353/360\n",
      "8564/8564 [==============================] - 5s 594us/step - loss: 0.0255 - acc: 0.8741 - val_loss: 0.0266 - val_acc: 0.8717\n",
      "Epoch 354/360\n",
      "8564/8564 [==============================] - 5s 577us/step - loss: 0.0254 - acc: 0.8735 - val_loss: 0.0264 - val_acc: 0.8717\n",
      "Epoch 355/360\n",
      "8564/8564 [==============================] - 5s 578us/step - loss: 0.0254 - acc: 0.8747 - val_loss: 0.0260 - val_acc: 0.8745\n",
      "Epoch 356/360\n",
      "8564/8564 [==============================] - 5s 572us/step - loss: 0.0253 - acc: 0.8747 - val_loss: 0.0256 - val_acc: 0.8742\n",
      "Epoch 357/360\n",
      "8564/8564 [==============================] - 5s 560us/step - loss: 0.0254 - acc: 0.8753 - val_loss: 0.0259 - val_acc: 0.8739\n",
      "Epoch 358/360\n",
      "8564/8564 [==============================] - 5s 574us/step - loss: 0.0250 - acc: 0.8723 - val_loss: 0.0252 - val_acc: 0.8753\n",
      "Epoch 359/360\n",
      "8564/8564 [==============================] - 5s 578us/step - loss: 0.0253 - acc: 0.8681 - val_loss: 0.0271 - val_acc: 0.8715\n",
      "Epoch 360/360\n",
      "8564/8564 [==============================] - 5s 581us/step - loss: 0.0253 - acc: 0.8706 - val_loss: 0.0262 - val_acc: 0.8661\n",
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 341/360\n",
      "8564/8564 [==============================] - 5s 588us/step - loss: 0.0274 - acc: 0.9794 - val_loss: 0.0238 - val_acc: 0.9828\n",
      "Epoch 342/360\n",
      "8564/8564 [==============================] - 5s 555us/step - loss: 0.0278 - acc: 0.9802 - val_loss: 0.0253 - val_acc: 0.9867\n",
      "Epoch 343/360\n",
      "8564/8564 [==============================] - 5s 558us/step - loss: 0.0277 - acc: 0.9823 - val_loss: 0.0276 - val_acc: 0.9758\n",
      "Epoch 344/360\n",
      "8564/8564 [==============================] - 5s 570us/step - loss: 0.0275 - acc: 0.9806 - val_loss: 0.0247 - val_acc: 0.9862\n",
      "Epoch 345/360\n",
      "8564/8564 [==============================] - 5s 549us/step - loss: 0.0266 - acc: 0.9823 - val_loss: 0.0238 - val_acc: 0.9846\n",
      "Epoch 346/360\n",
      "8564/8564 [==============================] - 5s 576us/step - loss: 0.0279 - acc: 0.9820 - val_loss: 0.0260 - val_acc: 0.9858\n",
      "Epoch 347/360\n",
      "8564/8564 [==============================] - 5s 555us/step - loss: 0.0291 - acc: 0.9810 - val_loss: 0.0252 - val_acc: 0.9864\n",
      "Epoch 348/360\n",
      "8564/8564 [==============================] - 5s 556us/step - loss: 0.0273 - acc: 0.9807 - val_loss: 0.0229 - val_acc: 0.9801\n",
      "Epoch 349/360\n",
      "8564/8564 [==============================] - 5s 577us/step - loss: 0.0287 - acc: 0.9802 - val_loss: 0.0268 - val_acc: 0.9817\n",
      "Epoch 350/360\n",
      "8564/8564 [==============================] - 5s 544us/step - loss: 0.0279 - acc: 0.9801 - val_loss: 0.0262 - val_acc: 0.9841\n",
      "Epoch 351/360\n",
      "8564/8564 [==============================] - 5s 577us/step - loss: 0.0266 - acc: 0.9813 - val_loss: 0.0242 - val_acc: 0.9813\n",
      "Epoch 352/360\n",
      "8564/8564 [==============================] - 5s 565us/step - loss: 0.0278 - acc: 0.9804 - val_loss: 0.0213 - val_acc: 0.9864\n",
      "Epoch 353/360\n",
      "8564/8564 [==============================] - 5s 570us/step - loss: 0.0273 - acc: 0.9801 - val_loss: 0.0287 - val_acc: 0.9821\n",
      "Epoch 354/360\n",
      "8564/8564 [==============================] - 5s 579us/step - loss: 0.0275 - acc: 0.9809 - val_loss: 0.0235 - val_acc: 0.9815\n",
      "Epoch 355/360\n",
      "8564/8564 [==============================] - 5s 595us/step - loss: 0.0269 - acc: 0.9787 - val_loss: 0.0250 - val_acc: 0.9819\n",
      "Epoch 356/360\n",
      "8564/8564 [==============================] - 5s 571us/step - loss: 0.0257 - acc: 0.9796 - val_loss: 0.0259 - val_acc: 0.9820\n",
      "Epoch 357/360\n",
      "8564/8564 [==============================] - 5s 599us/step - loss: 0.0262 - acc: 0.9806 - val_loss: 0.0271 - val_acc: 0.9825\n",
      "Epoch 358/360\n",
      "8564/8564 [==============================] - 5s 579us/step - loss: 0.0286 - acc: 0.9803 - val_loss: 0.0323 - val_acc: 0.9848\n",
      "Epoch 359/360\n",
      "8564/8564 [==============================] - 5s 584us/step - loss: 0.0286 - acc: 0.9803 - val_loss: 0.0297 - val_acc: 0.9874\n",
      "Epoch 360/360\n",
      "8564/8564 [==============================] - 5s 557us/step - loss: 0.0257 - acc: 0.9805 - val_loss: 0.0241 - val_acc: 0.9852\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 341/360\n",
      "8564/8564 [==============================] - 5s 600us/step - loss: 0.0318 - acc: 0.9295 - val_loss: 0.0270 - val_acc: 0.9414\n",
      "Epoch 342/360\n",
      "8564/8564 [==============================] - 5s 575us/step - loss: 0.0329 - acc: 0.9272 - val_loss: 0.0291 - val_acc: 0.9268\n",
      "Epoch 343/360\n",
      "8564/8564 [==============================] - 5s 573us/step - loss: 0.0299 - acc: 0.9299 - val_loss: 0.0267 - val_acc: 0.9384\n",
      "Epoch 344/360\n",
      "8564/8564 [==============================] - 5s 570us/step - loss: 0.0322 - acc: 0.9289 - val_loss: 0.0262 - val_acc: 0.9445\n",
      "Epoch 345/360\n",
      "8564/8564 [==============================] - 5s 589us/step - loss: 0.0322 - acc: 0.9287 - val_loss: 0.0375 - val_acc: 0.9196\n",
      "Epoch 346/360\n",
      "8564/8564 [==============================] - 5s 587us/step - loss: 0.0320 - acc: 0.9304 - val_loss: 0.0343 - val_acc: 0.9297\n",
      "Epoch 347/360\n",
      "8564/8564 [==============================] - 5s 614us/step - loss: 0.0324 - acc: 0.9291 - val_loss: 0.0268 - val_acc: 0.9353\n",
      "Epoch 348/360\n",
      "8564/8564 [==============================] - 5s 574us/step - loss: 0.0330 - acc: 0.9258 - val_loss: 0.0293 - val_acc: 0.9384\n",
      "Epoch 349/360\n",
      "8564/8564 [==============================] - 5s 582us/step - loss: 0.0320 - acc: 0.9271 - val_loss: 0.0293 - val_acc: 0.9435\n",
      "Epoch 350/360\n",
      "8564/8564 [==============================] - 5s 576us/step - loss: 0.0310 - acc: 0.9307 - val_loss: 0.0306 - val_acc: 0.9341\n",
      "Epoch 351/360\n",
      "8564/8564 [==============================] - 5s 582us/step - loss: 0.0317 - acc: 0.9290 - val_loss: 0.0302 - val_acc: 0.9387\n",
      "Epoch 352/360\n",
      "8564/8564 [==============================] - 5s 600us/step - loss: 0.0305 - acc: 0.9306 - val_loss: 0.0332 - val_acc: 0.9332\n",
      "Epoch 353/360\n",
      "8564/8564 [==============================] - 5s 590us/step - loss: 0.0309 - acc: 0.9296 - val_loss: 0.0256 - val_acc: 0.9433\n",
      "Epoch 354/360\n",
      "8564/8564 [==============================] - 5s 584us/step - loss: 0.0328 - acc: 0.9266 - val_loss: 0.0284 - val_acc: 0.9360\n",
      "Epoch 355/360\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 5s 581us/step - loss: 0.0312 - acc: 0.9297 - val_loss: 0.0282 - val_acc: 0.9354\n",
      "Epoch 356/360\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0315 - acc: 0.9284 - val_loss: 0.0310 - val_acc: 0.9383\n",
      "Epoch 357/360\n",
      "8564/8564 [==============================] - 5s 592us/step - loss: 0.0315 - acc: 0.9305 - val_loss: 0.0282 - val_acc: 0.9323\n",
      "Epoch 358/360\n",
      "8564/8564 [==============================] - 5s 575us/step - loss: 0.0335 - acc: 0.9281 - val_loss: 0.0324 - val_acc: 0.9429\n",
      "Epoch 359/360\n",
      "8564/8564 [==============================] - 5s 585us/step - loss: 0.0313 - acc: 0.9304 - val_loss: 0.0258 - val_acc: 0.9418\n",
      "Epoch 360/360\n",
      "8564/8564 [==============================] - 5s 593us/step - loss: 0.0326 - acc: 0.9271 - val_loss: 0.0292 - val_acc: 0.9342\n",
      "start training round 18\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 361/380\n",
      "8564/8564 [==============================] - 5s 565us/step - loss: 0.0291 - acc: 0.6415 - val_loss: 0.0301 - val_acc: 0.5431\n",
      "Epoch 362/380\n",
      "8564/8564 [==============================] - 5s 567us/step - loss: 0.0295 - acc: 0.6301 - val_loss: 0.0301 - val_acc: 0.5303\n",
      "Epoch 363/380\n",
      "8564/8564 [==============================] - 5s 567us/step - loss: 0.0290 - acc: 0.6923 - val_loss: 0.0297 - val_acc: 0.7097\n",
      "Epoch 364/380\n",
      "8564/8564 [==============================] - 5s 563us/step - loss: 0.0291 - acc: 0.7047 - val_loss: 0.0293 - val_acc: 0.7039\n",
      "Epoch 365/380\n",
      "8564/8564 [==============================] - 5s 603us/step - loss: 0.0289 - acc: 0.6627 - val_loss: 0.0294 - val_acc: 0.5888\n",
      "Epoch 366/380\n",
      "8564/8564 [==============================] - 5s 550us/step - loss: 0.0289 - acc: 0.6502 - val_loss: 0.0294 - val_acc: 0.5620\n",
      "Epoch 367/380\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0289 - acc: 0.6447 - val_loss: 0.0305 - val_acc: 0.5210\n",
      "Epoch 368/380\n",
      "8564/8564 [==============================] - 5s 570us/step - loss: 0.0293 - acc: 0.6639 - val_loss: 0.0295 - val_acc: 0.7079\n",
      "Epoch 369/380\n",
      "8564/8564 [==============================] - 5s 599us/step - loss: 0.0288 - acc: 0.6593 - val_loss: 0.0299 - val_acc: 0.7094\n",
      "Epoch 370/380\n",
      "8564/8564 [==============================] - 5s 587us/step - loss: 0.0289 - acc: 0.6716 - val_loss: 0.0296 - val_acc: 0.7003\n",
      "Epoch 371/380\n",
      "8564/8564 [==============================] - 5s 605us/step - loss: 0.0291 - acc: 0.6801 - val_loss: 0.0294 - val_acc: 0.7084\n",
      "Epoch 372/380\n",
      "8564/8564 [==============================] - 5s 565us/step - loss: 0.0290 - acc: 0.6969 - val_loss: 0.0295 - val_acc: 0.7112\n",
      "Epoch 373/380\n",
      "8564/8564 [==============================] - 5s 575us/step - loss: 0.0286 - acc: 0.6758 - val_loss: 0.0291 - val_acc: 0.6970\n",
      "Epoch 374/380\n",
      "8564/8564 [==============================] - 5s 586us/step - loss: 0.0288 - acc: 0.6606 - val_loss: 0.0284 - val_acc: 0.6770\n",
      "Epoch 375/380\n",
      "8564/8564 [==============================] - 5s 587us/step - loss: 0.0290 - acc: 0.6437 - val_loss: 0.0296 - val_acc: 0.5559\n",
      "Epoch 376/380\n",
      "8564/8564 [==============================] - 5s 531us/step - loss: 0.0293 - acc: 0.6273 - val_loss: 0.0298 - val_acc: 0.5447\n",
      "Epoch 377/380\n",
      "8564/8564 [==============================] - 5s 567us/step - loss: 0.0292 - acc: 0.6353 - val_loss: 0.0294 - val_acc: 0.5473\n",
      "Epoch 378/380\n",
      "8564/8564 [==============================] - 5s 584us/step - loss: 0.0288 - acc: 0.6394 - val_loss: 0.0299 - val_acc: 0.5493\n",
      "Epoch 379/380\n",
      "8564/8564 [==============================] - 5s 564us/step - loss: 0.0287 - acc: 0.6893 - val_loss: 0.0289 - val_acc: 0.7105\n",
      "Epoch 380/380\n",
      "8564/8564 [==============================] - 5s 594us/step - loss: 0.0287 - acc: 0.7061 - val_loss: 0.0295 - val_acc: 0.7128\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 361/380\n",
      "8564/8564 [==============================] - 5s 564us/step - loss: 0.0253 - acc: 0.8682 - val_loss: 0.0253 - val_acc: 0.8706\n",
      "Epoch 362/380\n",
      "8564/8564 [==============================] - 5s 577us/step - loss: 0.0250 - acc: 0.8702 - val_loss: 0.0262 - val_acc: 0.8754\n",
      "Epoch 363/380\n",
      "8564/8564 [==============================] - 5s 593us/step - loss: 0.0252 - acc: 0.8751 - val_loss: 0.0251 - val_acc: 0.8750\n",
      "Epoch 364/380\n",
      "8564/8564 [==============================] - 5s 575us/step - loss: 0.0253 - acc: 0.8688 - val_loss: 0.0252 - val_acc: 0.8681\n",
      "Epoch 365/380\n",
      "8564/8564 [==============================] - 5s 582us/step - loss: 0.0253 - acc: 0.8686 - val_loss: 0.0252 - val_acc: 0.8727\n",
      "Epoch 366/380\n",
      "8564/8564 [==============================] - 5s 569us/step - loss: 0.0254 - acc: 0.8673 - val_loss: 0.0253 - val_acc: 0.8690\n",
      "Epoch 367/380\n",
      "8564/8564 [==============================] - 5s 594us/step - loss: 0.0253 - acc: 0.8687 - val_loss: 0.0258 - val_acc: 0.8651\n",
      "Epoch 368/380\n",
      "8564/8564 [==============================] - 5s 564us/step - loss: 0.0251 - acc: 0.8689 - val_loss: 0.0249 - val_acc: 0.8693\n",
      "Epoch 369/380\n",
      "8564/8564 [==============================] - 5s 584us/step - loss: 0.0251 - acc: 0.8688 - val_loss: 0.0256 - val_acc: 0.8650\n",
      "Epoch 370/380\n",
      "8564/8564 [==============================] - 5s 582us/step - loss: 0.0251 - acc: 0.8710 - val_loss: 0.0265 - val_acc: 0.8615\n",
      "Epoch 371/380\n",
      "8564/8564 [==============================] - 5s 571us/step - loss: 0.0252 - acc: 0.8688 - val_loss: 0.0260 - val_acc: 0.8612\n",
      "Epoch 372/380\n",
      "8564/8564 [==============================] - 5s 565us/step - loss: 0.0254 - acc: 0.8681 - val_loss: 0.0253 - val_acc: 0.8679\n",
      "Epoch 373/380\n",
      "8564/8564 [==============================] - 5s 587us/step - loss: 0.0253 - acc: 0.8675 - val_loss: 0.0253 - val_acc: 0.8710\n",
      "Epoch 374/380\n",
      "8564/8564 [==============================] - 5s 568us/step - loss: 0.0251 - acc: 0.8690 - val_loss: 0.0251 - val_acc: 0.8692\n",
      "Epoch 375/380\n",
      "8564/8564 [==============================] - 5s 580us/step - loss: 0.0248 - acc: 0.8747 - val_loss: 0.0254 - val_acc: 0.8760\n",
      "Epoch 376/380\n",
      "8564/8564 [==============================] - 5s 540us/step - loss: 0.0250 - acc: 0.8734 - val_loss: 0.0252 - val_acc: 0.8734\n",
      "Epoch 377/380\n",
      "8564/8564 [==============================] - 5s 634us/step - loss: 0.0250 - acc: 0.8720 - val_loss: 0.0245 - val_acc: 0.8716\n",
      "Epoch 378/380\n",
      "8564/8564 [==============================] - 5s 587us/step - loss: 0.0249 - acc: 0.8732 - val_loss: 0.0263 - val_acc: 0.8572\n",
      "Epoch 379/380\n",
      "8564/8564 [==============================] - 5s 555us/step - loss: 0.0251 - acc: 0.8714 - val_loss: 0.0253 - val_acc: 0.8760\n",
      "Epoch 380/380\n",
      "8564/8564 [==============================] - 5s 596us/step - loss: 0.0251 - acc: 0.8764 - val_loss: 0.0257 - val_acc: 0.8763\n",
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 361/380\n",
      "8564/8564 [==============================] - 5s 571us/step - loss: 0.0270 - acc: 0.9828 - val_loss: 0.0290 - val_acc: 0.9848\n",
      "Epoch 362/380\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0278 - acc: 0.9809 - val_loss: 0.0246 - val_acc: 0.9848\n",
      "Epoch 363/380\n",
      "8564/8564 [==============================] - 5s 584us/step - loss: 0.0261 - acc: 0.9819 - val_loss: 0.0254 - val_acc: 0.9822\n",
      "Epoch 364/380\n",
      "8564/8564 [==============================] - 5s 598us/step - loss: 0.0274 - acc: 0.9800 - val_loss: 0.0241 - val_acc: 0.9858\n",
      "Epoch 365/380\n",
      "8564/8564 [==============================] - 5s 590us/step - loss: 0.0252 - acc: 0.9824 - val_loss: 0.0316 - val_acc: 0.9814\n",
      "Epoch 366/380\n",
      "8564/8564 [==============================] - 5s 590us/step - loss: 0.0256 - acc: 0.9814 - val_loss: 0.0238 - val_acc: 0.9866\n",
      "Epoch 367/380\n",
      "8564/8564 [==============================] - 5s 577us/step - loss: 0.0266 - acc: 0.9801 - val_loss: 0.0232 - val_acc: 0.9847\n",
      "Epoch 368/380\n",
      "8564/8564 [==============================] - 5s 574us/step - loss: 0.0264 - acc: 0.9798 - val_loss: 0.0278 - val_acc: 0.9849\n",
      "Epoch 369/380\n",
      "8564/8564 [==============================] - 5s 544us/step - loss: 0.0278 - acc: 0.9805 - val_loss: 0.0240 - val_acc: 0.9841\n",
      "Epoch 370/380\n",
      "8564/8564 [==============================] - 5s 547us/step - loss: 0.0268 - acc: 0.9801 - val_loss: 0.0287 - val_acc: 0.9796\n",
      "Epoch 371/380\n",
      "8564/8564 [==============================] - 5s 584us/step - loss: 0.0272 - acc: 0.9799 - val_loss: 0.0275 - val_acc: 0.9869\n",
      "Epoch 372/380\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 5s 566us/step - loss: 0.0268 - acc: 0.9818 - val_loss: 0.0314 - val_acc: 0.9871\n",
      "Epoch 373/380\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0277 - acc: 0.9824 - val_loss: 0.0204 - val_acc: 0.9841\n",
      "Epoch 374/380\n",
      "8564/8564 [==============================] - 5s 584us/step - loss: 0.0267 - acc: 0.9810 - val_loss: 0.0212 - val_acc: 0.9858\n",
      "Epoch 375/380\n",
      "8564/8564 [==============================] - 5s 596us/step - loss: 0.0265 - acc: 0.9809 - val_loss: 0.0230 - val_acc: 0.9827\n",
      "Epoch 376/380\n",
      "8564/8564 [==============================] - 5s 594us/step - loss: 0.0250 - acc: 0.9821 - val_loss: 0.0258 - val_acc: 0.9832\n",
      "Epoch 377/380\n",
      "8564/8564 [==============================] - 5s 552us/step - loss: 0.0257 - acc: 0.9823 - val_loss: 0.0272 - val_acc: 0.9868\n",
      "Epoch 378/380\n",
      "8564/8564 [==============================] - 5s 569us/step - loss: 0.0268 - acc: 0.9828 - val_loss: 0.0293 - val_acc: 0.9814\n",
      "Epoch 379/380\n",
      "8564/8564 [==============================] - 5s 551us/step - loss: 0.0279 - acc: 0.9823 - val_loss: 0.0294 - val_acc: 0.9864\n",
      "Epoch 380/380\n",
      "8564/8564 [==============================] - 5s 578us/step - loss: 0.0268 - acc: 0.9802 - val_loss: 0.0237 - val_acc: 0.9818\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 361/380\n",
      "8564/8564 [==============================] - 5s 606us/step - loss: 0.0328 - acc: 0.9257 - val_loss: 0.0325 - val_acc: 0.9316\n",
      "Epoch 362/380\n",
      "8564/8564 [==============================] - 5s 560us/step - loss: 0.0317 - acc: 0.9302 - val_loss: 0.0304 - val_acc: 0.9443\n",
      "Epoch 363/380\n",
      "8564/8564 [==============================] - 5s 594us/step - loss: 0.0306 - acc: 0.9332 - val_loss: 0.0310 - val_acc: 0.9415\n",
      "Epoch 364/380\n",
      "8564/8564 [==============================] - 5s 590us/step - loss: 0.0335 - acc: 0.9270 - val_loss: 0.0283 - val_acc: 0.9390\n",
      "Epoch 365/380\n",
      "8564/8564 [==============================] - 5s 595us/step - loss: 0.0308 - acc: 0.9310 - val_loss: 0.0336 - val_acc: 0.9277\n",
      "Epoch 366/380\n",
      "8564/8564 [==============================] - 5s 574us/step - loss: 0.0320 - acc: 0.9275 - val_loss: 0.0266 - val_acc: 0.9371\n",
      "Epoch 367/380\n",
      "8564/8564 [==============================] - 5s 587us/step - loss: 0.0319 - acc: 0.9270 - val_loss: 0.0275 - val_acc: 0.9413\n",
      "Epoch 368/380\n",
      "8564/8564 [==============================] - 5s 574us/step - loss: 0.0320 - acc: 0.9297 - val_loss: 0.0274 - val_acc: 0.9458\n",
      "Epoch 369/380\n",
      "8564/8564 [==============================] - 5s 577us/step - loss: 0.0311 - acc: 0.9315 - val_loss: 0.0292 - val_acc: 0.9401\n",
      "Epoch 370/380\n",
      "8564/8564 [==============================] - 5s 582us/step - loss: 0.0306 - acc: 0.9330 - val_loss: 0.0322 - val_acc: 0.9203\n",
      "Epoch 371/380\n",
      "8564/8564 [==============================] - 5s 581us/step - loss: 0.0319 - acc: 0.9314 - val_loss: 0.0270 - val_acc: 0.9433\n",
      "Epoch 372/380\n",
      "8564/8564 [==============================] - 5s 591us/step - loss: 0.0313 - acc: 0.9294 - val_loss: 0.0296 - val_acc: 0.9424\n",
      "Epoch 373/380\n",
      "8564/8564 [==============================] - 5s 587us/step - loss: 0.0326 - acc: 0.9280 - val_loss: 0.0376 - val_acc: 0.9101\n",
      "Epoch 374/380\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0315 - acc: 0.9287 - val_loss: 0.0300 - val_acc: 0.9312\n",
      "Epoch 375/380\n",
      "8564/8564 [==============================] - 5s 567us/step - loss: 0.0312 - acc: 0.9314 - val_loss: 0.0296 - val_acc: 0.9255\n",
      "Epoch 376/380\n",
      "8564/8564 [==============================] - 5s 564us/step - loss: 0.0323 - acc: 0.9263 - val_loss: 0.0273 - val_acc: 0.9406\n",
      "Epoch 377/380\n",
      "8564/8564 [==============================] - 5s 576us/step - loss: 0.0311 - acc: 0.9300 - val_loss: 0.0280 - val_acc: 0.9441\n",
      "Epoch 378/380\n",
      "8564/8564 [==============================] - 5s 560us/step - loss: 0.0297 - acc: 0.9323 - val_loss: 0.0268 - val_acc: 0.9472\n",
      "Epoch 379/380\n",
      "8564/8564 [==============================] - 5s 588us/step - loss: 0.0308 - acc: 0.9321 - val_loss: 0.0266 - val_acc: 0.9432\n",
      "Epoch 380/380\n",
      "8564/8564 [==============================] - 5s 585us/step - loss: 0.0312 - acc: 0.9298 - val_loss: 0.0336 - val_acc: 0.9236\n",
      "start training round 19\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 381/400\n",
      "8564/8564 [==============================] - 5s 577us/step - loss: 0.0283 - acc: 0.7032 - val_loss: 0.0286 - val_acc: 0.6463\n",
      "Epoch 382/400\n",
      "8564/8564 [==============================] - 5s 555us/step - loss: 0.0287 - acc: 0.6542 - val_loss: 0.0301 - val_acc: 0.5328\n",
      "Epoch 383/400\n",
      "8564/8564 [==============================] - 5s 575us/step - loss: 0.0288 - acc: 0.6412 - val_loss: 0.0289 - val_acc: 0.5881\n",
      "Epoch 384/400\n",
      "8564/8564 [==============================] - 5s 530us/step - loss: 0.0287 - acc: 0.6483 - val_loss: 0.0290 - val_acc: 0.7111\n",
      "Epoch 385/400\n",
      "8564/8564 [==============================] - 5s 582us/step - loss: 0.0282 - acc: 0.6785 - val_loss: 0.0289 - val_acc: 0.6659\n",
      "Epoch 386/400\n",
      "8564/8564 [==============================] - 5s 611us/step - loss: 0.0285 - acc: 0.7011 - val_loss: 0.0288 - val_acc: 0.7081\n",
      "Epoch 387/400\n",
      "8564/8564 [==============================] - 5s 561us/step - loss: 0.0287 - acc: 0.7037 - val_loss: 0.0295 - val_acc: 0.7047\n",
      "Epoch 388/400\n",
      "8564/8564 [==============================] - 5s 560us/step - loss: 0.0285 - acc: 0.6976 - val_loss: 0.0288 - val_acc: 0.7020\n",
      "Epoch 389/400\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0286 - acc: 0.6565 - val_loss: 0.0290 - val_acc: 0.5709\n",
      "Epoch 390/400\n",
      "8564/8564 [==============================] - 5s 599us/step - loss: 0.0289 - acc: 0.6448 - val_loss: 0.0290 - val_acc: 0.5803\n",
      "Epoch 391/400\n",
      "8564/8564 [==============================] - 5s 594us/step - loss: 0.0287 - acc: 0.6460 - val_loss: 0.0283 - val_acc: 0.6774\n",
      "Epoch 392/400\n",
      "8564/8564 [==============================] - 5s 582us/step - loss: 0.0290 - acc: 0.7039 - val_loss: 0.0303 - val_acc: 0.7144\n",
      "Epoch 393/400\n",
      "8564/8564 [==============================] - 5s 548us/step - loss: 0.0285 - acc: 0.7064 - val_loss: 0.0300 - val_acc: 0.7046\n",
      "Epoch 394/400\n",
      "8564/8564 [==============================] - 5s 578us/step - loss: 0.0288 - acc: 0.6418 - val_loss: 0.0290 - val_acc: 0.7069\n",
      "Epoch 395/400\n",
      "8564/8564 [==============================] - 5s 575us/step - loss: 0.0285 - acc: 0.6489 - val_loss: 0.0289 - val_acc: 0.7053\n",
      "Epoch 396/400\n",
      "8564/8564 [==============================] - 5s 571us/step - loss: 0.0284 - acc: 0.6645 - val_loss: 0.0289 - val_acc: 0.6933\n",
      "Epoch 397/400\n",
      "8564/8564 [==============================] - 5s 547us/step - loss: 0.0285 - acc: 0.7030 - val_loss: 0.0293 - val_acc: 0.7090\n",
      "Epoch 398/400\n",
      "8564/8564 [==============================] - 5s 596us/step - loss: 0.0285 - acc: 0.6531 - val_loss: 0.0304 - val_acc: 0.7051\n",
      "Epoch 399/400\n",
      "8564/8564 [==============================] - 5s 585us/step - loss: 0.0283 - acc: 0.6899 - val_loss: 0.0299 - val_acc: 0.6879\n",
      "Epoch 400/400\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0289 - acc: 0.7130 - val_loss: 0.0294 - val_acc: 0.6994\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 381/400\n",
      "8564/8564 [==============================] - 5s 601us/step - loss: 0.0253 - acc: 0.8727 - val_loss: 0.0248 - val_acc: 0.8698\n",
      "Epoch 382/400\n",
      "8564/8564 [==============================] - 5s 548us/step - loss: 0.0249 - acc: 0.8708 - val_loss: 0.0253 - val_acc: 0.8669\n",
      "Epoch 383/400\n",
      "8564/8564 [==============================] - 5s 592us/step - loss: 0.0251 - acc: 0.8690 - val_loss: 0.0255 - val_acc: 0.8684\n",
      "Epoch 384/400\n",
      "8564/8564 [==============================] - 5s 571us/step - loss: 0.0250 - acc: 0.8757 - val_loss: 0.0254 - val_acc: 0.8759\n",
      "Epoch 385/400\n",
      "8564/8564 [==============================] - 5s 593us/step - loss: 0.0250 - acc: 0.8760 - val_loss: 0.0256 - val_acc: 0.8766\n",
      "Epoch 386/400\n",
      "8564/8564 [==============================] - 5s 573us/step - loss: 0.0249 - acc: 0.8769 - val_loss: 0.0256 - val_acc: 0.8761\n",
      "Epoch 387/400\n",
      "8564/8564 [==============================] - 5s 571us/step - loss: 0.0251 - acc: 0.8771 - val_loss: 0.0254 - val_acc: 0.8749\n",
      "Epoch 388/400\n",
      "8564/8564 [==============================] - 5s 577us/step - loss: 0.0249 - acc: 0.8766 - val_loss: 0.0256 - val_acc: 0.8744\n",
      "Epoch 389/400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 5s 556us/step - loss: 0.0250 - acc: 0.8711 - val_loss: 0.0254 - val_acc: 0.8646\n",
      "Epoch 390/400\n",
      "8564/8564 [==============================] - 5s 572us/step - loss: 0.0251 - acc: 0.8694 - val_loss: 0.0248 - val_acc: 0.8702\n",
      "Epoch 391/400\n",
      "8564/8564 [==============================] - 5s 566us/step - loss: 0.0250 - acc: 0.8686 - val_loss: 0.0249 - val_acc: 0.8700\n",
      "Epoch 392/400\n",
      "8564/8564 [==============================] - 5s 578us/step - loss: 0.0251 - acc: 0.8702 - val_loss: 0.0253 - val_acc: 0.8664\n",
      "Epoch 393/400\n",
      "8564/8564 [==============================] - 5s 578us/step - loss: 0.0249 - acc: 0.8715 - val_loss: 0.0255 - val_acc: 0.8648\n",
      "Epoch 394/400\n",
      "8564/8564 [==============================] - 5s 563us/step - loss: 0.0250 - acc: 0.8760 - val_loss: 0.0255 - val_acc: 0.8765\n",
      "Epoch 395/400\n",
      "8564/8564 [==============================] - 5s 587us/step - loss: 0.0251 - acc: 0.8772 - val_loss: 0.0254 - val_acc: 0.8768\n",
      "Epoch 396/400\n",
      "8564/8564 [==============================] - 5s 557us/step - loss: 0.0250 - acc: 0.8764 - val_loss: 0.0253 - val_acc: 0.8741\n",
      "Epoch 397/400\n",
      "8564/8564 [==============================] - 5s 545us/step - loss: 0.0251 - acc: 0.8776 - val_loss: 0.0259 - val_acc: 0.8768\n",
      "Epoch 398/400\n",
      "8564/8564 [==============================] - 5s 542us/step - loss: 0.0250 - acc: 0.8748 - val_loss: 0.0254 - val_acc: 0.8675\n",
      "Epoch 399/400\n",
      "8564/8564 [==============================] - 5s 572us/step - loss: 0.0249 - acc: 0.8706 - val_loss: 0.0259 - val_acc: 0.8616\n",
      "Epoch 400/400\n",
      "8564/8564 [==============================] - 5s 571us/step - loss: 0.0255 - acc: 0.8684 - val_loss: 0.0255 - val_acc: 0.8762\n",
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 381/400\n",
      "8564/8564 [==============================] - 5s 598us/step - loss: 0.0258 - acc: 0.9815 - val_loss: 0.0216 - val_acc: 0.9854\n",
      "Epoch 382/400\n",
      "8564/8564 [==============================] - 5s 559us/step - loss: 0.0262 - acc: 0.9810 - val_loss: 0.0255 - val_acc: 0.9838\n",
      "Epoch 383/400\n",
      "8564/8564 [==============================] - 5s 563us/step - loss: 0.0267 - acc: 0.9810 - val_loss: 0.0293 - val_acc: 0.9796\n",
      "Epoch 384/400\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0270 - acc: 0.9815 - val_loss: 0.0289 - val_acc: 0.9857\n",
      "Epoch 385/400\n",
      "8564/8564 [==============================] - 5s 574us/step - loss: 0.0269 - acc: 0.9821 - val_loss: 0.0275 - val_acc: 0.9844\n",
      "Epoch 386/400\n",
      "8564/8564 [==============================] - 5s 560us/step - loss: 0.0268 - acc: 0.9795 - val_loss: 0.0279 - val_acc: 0.9812\n",
      "Epoch 387/400\n",
      "8564/8564 [==============================] - 5s 589us/step - loss: 0.0275 - acc: 0.9812 - val_loss: 0.0253 - val_acc: 0.9814\n",
      "Epoch 388/400\n",
      "8564/8564 [==============================] - 5s 574us/step - loss: 0.0293 - acc: 0.9801 - val_loss: 0.0268 - val_acc: 0.9873\n",
      "Epoch 389/400\n",
      "8564/8564 [==============================] - 5s 576us/step - loss: 0.0256 - acc: 0.9811 - val_loss: 0.0276 - val_acc: 0.9858\n",
      "Epoch 390/400\n",
      "8564/8564 [==============================] - 5s 559us/step - loss: 0.0257 - acc: 0.9810 - val_loss: 0.0235 - val_acc: 0.9846\n",
      "Epoch 391/400\n",
      "8564/8564 [==============================] - 4s 519us/step - loss: 0.0262 - acc: 0.9822 - val_loss: 0.0273 - val_acc: 0.9833\n",
      "Epoch 392/400\n",
      "8564/8564 [==============================] - 5s 585us/step - loss: 0.0256 - acc: 0.9820 - val_loss: 0.0288 - val_acc: 0.9837\n",
      "Epoch 393/400\n",
      "8564/8564 [==============================] - 5s 585us/step - loss: 0.0273 - acc: 0.9823 - val_loss: 0.0255 - val_acc: 0.9823\n",
      "Epoch 394/400\n",
      "8564/8564 [==============================] - 5s 587us/step - loss: 0.0281 - acc: 0.9803 - val_loss: 0.0291 - val_acc: 0.9873\n",
      "Epoch 395/400\n",
      "8564/8564 [==============================] - 5s 561us/step - loss: 0.0276 - acc: 0.9827 - val_loss: 0.0244 - val_acc: 0.9826\n",
      "Epoch 396/400\n",
      "8564/8564 [==============================] - 5s 572us/step - loss: 0.0257 - acc: 0.9814 - val_loss: 0.0292 - val_acc: 0.9792\n",
      "Epoch 397/400\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0278 - acc: 0.9800 - val_loss: 0.0204 - val_acc: 0.9859\n",
      "Epoch 398/400\n",
      "8564/8564 [==============================] - 5s 582us/step - loss: 0.0250 - acc: 0.9825 - val_loss: 0.0289 - val_acc: 0.9817\n",
      "Epoch 399/400\n",
      "8564/8564 [==============================] - 5s 552us/step - loss: 0.0261 - acc: 0.9826 - val_loss: 0.0245 - val_acc: 0.9843\n",
      "Epoch 400/400\n",
      "8564/8564 [==============================] - 5s 570us/step - loss: 0.0292 - acc: 0.9814 - val_loss: 0.0306 - val_acc: 0.9837\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 381/400\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0309 - acc: 0.9305 - val_loss: 0.0292 - val_acc: 0.9429\n",
      "Epoch 382/400\n",
      "8564/8564 [==============================] - 5s 598us/step - loss: 0.0321 - acc: 0.9286 - val_loss: 0.0409 - val_acc: 0.9188\n",
      "Epoch 383/400\n",
      "8564/8564 [==============================] - 5s 588us/step - loss: 0.0321 - acc: 0.9271 - val_loss: 0.0264 - val_acc: 0.9372\n",
      "Epoch 384/400\n",
      "8564/8564 [==============================] - 5s 568us/step - loss: 0.0318 - acc: 0.9271 - val_loss: 0.0296 - val_acc: 0.9207\n",
      "Epoch 385/400\n",
      "8564/8564 [==============================] - 5s 604us/step - loss: 0.0315 - acc: 0.9294 - val_loss: 0.0322 - val_acc: 0.9367\n",
      "Epoch 386/400\n",
      "8564/8564 [==============================] - 5s 598us/step - loss: 0.0315 - acc: 0.9296 - val_loss: 0.0303 - val_acc: 0.9443\n",
      "Epoch 387/400\n",
      "8564/8564 [==============================] - 5s 571us/step - loss: 0.0310 - acc: 0.9302 - val_loss: 0.0289 - val_acc: 0.9440\n",
      "Epoch 388/400\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0307 - acc: 0.9315 - val_loss: 0.0259 - val_acc: 0.9392\n",
      "Epoch 389/400\n",
      "8564/8564 [==============================] - 5s 564us/step - loss: 0.0305 - acc: 0.9322 - val_loss: 0.0273 - val_acc: 0.9373\n",
      "Epoch 390/400\n",
      "8564/8564 [==============================] - 5s 584us/step - loss: 0.0302 - acc: 0.9322 - val_loss: 0.0277 - val_acc: 0.9383\n",
      "Epoch 391/400\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0312 - acc: 0.9304 - val_loss: 0.0306 - val_acc: 0.9393\n",
      "Epoch 392/400\n",
      "8564/8564 [==============================] - 5s 555us/step - loss: 0.0307 - acc: 0.9309 - val_loss: 0.0291 - val_acc: 0.9370\n",
      "Epoch 393/400\n",
      "8564/8564 [==============================] - 5s 591us/step - loss: 0.0299 - acc: 0.9338 - val_loss: 0.0285 - val_acc: 0.9441\n",
      "Epoch 394/400\n",
      "8564/8564 [==============================] - 5s 588us/step - loss: 0.0315 - acc: 0.9312 - val_loss: 0.0275 - val_acc: 0.9247\n",
      "Epoch 395/400\n",
      "8564/8564 [==============================] - 5s 591us/step - loss: 0.0317 - acc: 0.9299 - val_loss: 0.0324 - val_acc: 0.9324\n",
      "Epoch 396/400\n",
      "8564/8564 [==============================] - 5s 596us/step - loss: 0.0308 - acc: 0.9312 - val_loss: 0.0254 - val_acc: 0.9439\n",
      "Epoch 397/400\n",
      "8564/8564 [==============================] - 5s 586us/step - loss: 0.0291 - acc: 0.9348 - val_loss: 0.0297 - val_acc: 0.9345\n",
      "Epoch 398/400\n",
      "8564/8564 [==============================] - 5s 593us/step - loss: 0.0305 - acc: 0.9309 - val_loss: 0.0279 - val_acc: 0.9422\n",
      "Epoch 399/400\n",
      "8564/8564 [==============================] - 5s 578us/step - loss: 0.0301 - acc: 0.9313 - val_loss: 0.0248 - val_acc: 0.9488\n",
      "Epoch 400/400\n",
      "8564/8564 [==============================] - 5s 564us/step - loss: 0.0309 - acc: 0.9337 - val_loss: 0.0283 - val_acc: 0.9333\n",
      "start training round 20\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 401/420\n",
      "8564/8564 [==============================] - 5s 576us/step - loss: 0.0284 - acc: 0.6666 - val_loss: 0.0303 - val_acc: 0.6947\n",
      "Epoch 402/420\n",
      "8564/8564 [==============================] - 5s 581us/step - loss: 0.0287 - acc: 0.6450 - val_loss: 0.0287 - val_acc: 0.7125\n",
      "Epoch 403/420\n",
      "8564/8564 [==============================] - 5s 580us/step - loss: 0.0287 - acc: 0.6417 - val_loss: 0.0284 - val_acc: 0.7116\n",
      "Epoch 404/420\n",
      "8564/8564 [==============================] - 5s 584us/step - loss: 0.0286 - acc: 0.6438 - val_loss: 0.0285 - val_acc: 0.7113\n",
      "Epoch 405/420\n",
      "8564/8564 [==============================] - 5s 568us/step - loss: 0.0283 - acc: 0.6611 - val_loss: 0.0297 - val_acc: 0.5606\n",
      "Epoch 406/420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 5s 592us/step - loss: 0.0286 - acc: 0.6681 - val_loss: 0.0303 - val_acc: 0.7110\n",
      "Epoch 407/420\n",
      "8564/8564 [==============================] - 5s 587us/step - loss: 0.0283 - acc: 0.6627 - val_loss: 0.0292 - val_acc: 0.5515\n",
      "Epoch 408/420\n",
      "8564/8564 [==============================] - 5s 594us/step - loss: 0.0281 - acc: 0.6944 - val_loss: 0.0282 - val_acc: 0.7080\n",
      "Epoch 409/420\n",
      "8564/8564 [==============================] - 5s 569us/step - loss: 0.0283 - acc: 0.6497 - val_loss: 0.0279 - val_acc: 0.6914\n",
      "Epoch 410/420\n",
      "8564/8564 [==============================] - 5s 563us/step - loss: 0.0283 - acc: 0.6834 - val_loss: 0.0295 - val_acc: 0.6941\n",
      "Epoch 411/420\n",
      "8564/8564 [==============================] - 5s 591us/step - loss: 0.0283 - acc: 0.6581 - val_loss: 0.0284 - val_acc: 0.5845\n",
      "Epoch 412/420\n",
      "8564/8564 [==============================] - 5s 560us/step - loss: 0.0287 - acc: 0.6357 - val_loss: 0.0295 - val_acc: 0.5418\n",
      "Epoch 413/420\n",
      "8564/8564 [==============================] - 5s 565us/step - loss: 0.0281 - acc: 0.6633 - val_loss: 0.0285 - val_acc: 0.6143\n",
      "Epoch 414/420\n",
      "8564/8564 [==============================] - 5s 575us/step - loss: 0.0282 - acc: 0.6488 - val_loss: 0.0287 - val_acc: 0.5917\n",
      "Epoch 415/420\n",
      "8564/8564 [==============================] - 5s 557us/step - loss: 0.0283 - acc: 0.6535 - val_loss: 0.0283 - val_acc: 0.6116\n",
      "Epoch 416/420\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0283 - acc: 0.6369 - val_loss: 0.0281 - val_acc: 0.6078\n",
      "Epoch 417/420\n",
      "8564/8564 [==============================] - 5s 584us/step - loss: 0.0279 - acc: 0.6665 - val_loss: 0.0284 - val_acc: 0.6094\n",
      "Epoch 418/420\n",
      "8564/8564 [==============================] - 5s 590us/step - loss: 0.0283 - acc: 0.7047 - val_loss: 0.0287 - val_acc: 0.7138\n",
      "Epoch 419/420\n",
      "8564/8564 [==============================] - 5s 573us/step - loss: 0.0285 - acc: 0.7123 - val_loss: 0.0305 - val_acc: 0.7182\n",
      "Epoch 420/420\n",
      "8564/8564 [==============================] - 5s 580us/step - loss: 0.0284 - acc: 0.7164 - val_loss: 0.0279 - val_acc: 0.6788\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 401/420\n",
      "8564/8564 [==============================] - 5s 581us/step - loss: 0.0250 - acc: 0.8778 - val_loss: 0.0261 - val_acc: 0.8765\n",
      "Epoch 402/420\n",
      "8564/8564 [==============================] - 5s 567us/step - loss: 0.0252 - acc: 0.8768 - val_loss: 0.0252 - val_acc: 0.8765\n",
      "Epoch 403/420\n",
      "8564/8564 [==============================] - 5s 551us/step - loss: 0.0249 - acc: 0.8780 - val_loss: 0.0250 - val_acc: 0.8781\n",
      "Epoch 404/420\n",
      "8564/8564 [==============================] - 5s 542us/step - loss: 0.0249 - acc: 0.8782 - val_loss: 0.0249 - val_acc: 0.8777\n",
      "Epoch 405/420\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0247 - acc: 0.8766 - val_loss: 0.0249 - val_acc: 0.8695\n",
      "Epoch 406/420\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0249 - acc: 0.8701 - val_loss: 0.0247 - val_acc: 0.8722\n",
      "Epoch 407/420\n",
      "8564/8564 [==============================] - 5s 535us/step - loss: 0.0249 - acc: 0.8766 - val_loss: 0.0252 - val_acc: 0.8782\n",
      "Epoch 408/420\n",
      "8564/8564 [==============================] - 5s 579us/step - loss: 0.0249 - acc: 0.8742 - val_loss: 0.0259 - val_acc: 0.8610\n",
      "Epoch 409/420\n",
      "8564/8564 [==============================] - 5s 564us/step - loss: 0.0250 - acc: 0.8702 - val_loss: 0.0249 - val_acc: 0.8703\n",
      "Epoch 410/420\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0249 - acc: 0.8713 - val_loss: 0.0249 - val_acc: 0.8707\n",
      "Epoch 411/420\n",
      "8564/8564 [==============================] - 5s 582us/step - loss: 0.0250 - acc: 0.8701 - val_loss: 0.0248 - val_acc: 0.8704\n",
      "Epoch 412/420\n",
      "8564/8564 [==============================] - 5s 587us/step - loss: 0.0248 - acc: 0.8718 - val_loss: 0.0247 - val_acc: 0.8705\n",
      "Epoch 413/420\n",
      "8564/8564 [==============================] - 5s 588us/step - loss: 0.0249 - acc: 0.8701 - val_loss: 0.0247 - val_acc: 0.8718\n",
      "Epoch 414/420\n",
      "8564/8564 [==============================] - 5s 584us/step - loss: 0.0249 - acc: 0.8747 - val_loss: 0.0249 - val_acc: 0.8784\n",
      "Epoch 415/420\n",
      "8564/8564 [==============================] - 5s 587us/step - loss: 0.0245 - acc: 0.8784 - val_loss: 0.0247 - val_acc: 0.8786\n",
      "Epoch 416/420\n",
      "8564/8564 [==============================] - 5s 558us/step - loss: 0.0248 - acc: 0.8760 - val_loss: 0.0256 - val_acc: 0.8613\n",
      "Epoch 417/420\n",
      "8564/8564 [==============================] - 5s 591us/step - loss: 0.0248 - acc: 0.8706 - val_loss: 0.0243 - val_acc: 0.8757\n",
      "Epoch 418/420\n",
      "8564/8564 [==============================] - 5s 561us/step - loss: 0.0250 - acc: 0.8713 - val_loss: 0.0248 - val_acc: 0.8709\n",
      "Epoch 419/420\n",
      "8564/8564 [==============================] - 5s 573us/step - loss: 0.0248 - acc: 0.8712 - val_loss: 0.0256 - val_acc: 0.8668\n",
      "Epoch 420/420\n",
      "8564/8564 [==============================] - 5s 547us/step - loss: 0.0248 - acc: 0.8720 - val_loss: 0.0247 - val_acc: 0.8708\n",
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 401/420\n",
      "8564/8564 [==============================] - 5s 573us/step - loss: 0.0257 - acc: 0.9818 - val_loss: 0.0281 - val_acc: 0.9875\n",
      "Epoch 402/420\n",
      "8564/8564 [==============================] - 5s 579us/step - loss: 0.0283 - acc: 0.9807 - val_loss: 0.0214 - val_acc: 0.9847\n",
      "Epoch 403/420\n",
      "8564/8564 [==============================] - 5s 586us/step - loss: 0.0255 - acc: 0.9827 - val_loss: 0.0212 - val_acc: 0.9879\n",
      "Epoch 404/420\n",
      "8564/8564 [==============================] - 5s 602us/step - loss: 0.0271 - acc: 0.9812 - val_loss: 0.0266 - val_acc: 0.9861\n",
      "Epoch 405/420\n",
      "8564/8564 [==============================] - 5s 569us/step - loss: 0.0275 - acc: 0.9815 - val_loss: 0.0235 - val_acc: 0.9853\n",
      "Epoch 406/420\n",
      "8564/8564 [==============================] - 5s 579us/step - loss: 0.0263 - acc: 0.9824 - val_loss: 0.0246 - val_acc: 0.9848\n",
      "Epoch 407/420\n",
      "8564/8564 [==============================] - 5s 547us/step - loss: 0.0267 - acc: 0.9818 - val_loss: 0.0279 - val_acc: 0.9862\n",
      "Epoch 408/420\n",
      "8564/8564 [==============================] - 5s 564us/step - loss: 0.0269 - acc: 0.9816 - val_loss: 0.0283 - val_acc: 0.9842\n",
      "Epoch 409/420\n",
      "8564/8564 [==============================] - 5s 578us/step - loss: 0.0254 - acc: 0.9832 - val_loss: 0.0239 - val_acc: 0.9812\n",
      "Epoch 410/420\n",
      "8564/8564 [==============================] - 5s 594us/step - loss: 0.0267 - acc: 0.9804 - val_loss: 0.0307 - val_acc: 0.9858\n",
      "Epoch 411/420\n",
      "8564/8564 [==============================] - 5s 569us/step - loss: 0.0286 - acc: 0.9820 - val_loss: 0.0297 - val_acc: 0.9822\n",
      "Epoch 412/420\n",
      "8564/8564 [==============================] - 5s 601us/step - loss: 0.0272 - acc: 0.9806 - val_loss: 0.0249 - val_acc: 0.9812\n",
      "Epoch 413/420\n",
      "8564/8564 [==============================] - 5s 576us/step - loss: 0.0255 - acc: 0.9807 - val_loss: 0.0286 - val_acc: 0.9824\n",
      "Epoch 414/420\n",
      "8564/8564 [==============================] - 5s 574us/step - loss: 0.0272 - acc: 0.9814 - val_loss: 0.0281 - val_acc: 0.9843\n",
      "Epoch 415/420\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0251 - acc: 0.9822 - val_loss: 0.0242 - val_acc: 0.9853\n",
      "Epoch 416/420\n",
      "8564/8564 [==============================] - 5s 573us/step - loss: 0.0258 - acc: 0.9826 - val_loss: 0.0281 - val_acc: 0.9797\n",
      "Epoch 417/420\n",
      "8564/8564 [==============================] - 5s 563us/step - loss: 0.0257 - acc: 0.9831 - val_loss: 0.0316 - val_acc: 0.9772\n",
      "Epoch 418/420\n",
      "8564/8564 [==============================] - 5s 582us/step - loss: 0.0267 - acc: 0.9802 - val_loss: 0.0288 - val_acc: 0.9822\n",
      "Epoch 419/420\n",
      "8564/8564 [==============================] - 5s 568us/step - loss: 0.0283 - acc: 0.9807 - val_loss: 0.0247 - val_acc: 0.9773\n",
      "Epoch 420/420\n",
      "8564/8564 [==============================] - 5s 585us/step - loss: 0.0259 - acc: 0.9817 - val_loss: 0.0263 - val_acc: 0.9817\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 401/420\n",
      "8564/8564 [==============================] - 5s 548us/step - loss: 0.0319 - acc: 0.9299 - val_loss: 0.0302 - val_acc: 0.9230\n",
      "Epoch 402/420\n",
      "8564/8564 [==============================] - 5s 571us/step - loss: 0.0310 - acc: 0.9319 - val_loss: 0.0323 - val_acc: 0.9223\n",
      "Epoch 403/420\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 5s 596us/step - loss: 0.0296 - acc: 0.9337 - val_loss: 0.0267 - val_acc: 0.9359\n",
      "Epoch 404/420\n",
      "8564/8564 [==============================] - 5s 594us/step - loss: 0.0315 - acc: 0.9297 - val_loss: 0.0273 - val_acc: 0.9463\n",
      "Epoch 405/420\n",
      "8564/8564 [==============================] - 5s 597us/step - loss: 0.0302 - acc: 0.9324 - val_loss: 0.0290 - val_acc: 0.9466\n",
      "Epoch 406/420\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0310 - acc: 0.9320 - val_loss: 0.0290 - val_acc: 0.9482\n",
      "Epoch 407/420\n",
      "8564/8564 [==============================] - 5s 573us/step - loss: 0.0327 - acc: 0.9299 - val_loss: 0.0280 - val_acc: 0.9393\n",
      "Epoch 408/420\n",
      "8564/8564 [==============================] - 5s 574us/step - loss: 0.0313 - acc: 0.9308 - val_loss: 0.0253 - val_acc: 0.9385\n",
      "Epoch 409/420\n",
      "8564/8564 [==============================] - 5s 586us/step - loss: 0.0305 - acc: 0.9306 - val_loss: 0.0297 - val_acc: 0.9473\n",
      "Epoch 410/420\n",
      "8564/8564 [==============================] - 5s 604us/step - loss: 0.0302 - acc: 0.9335 - val_loss: 0.0295 - val_acc: 0.9250\n",
      "Epoch 411/420\n",
      "8564/8564 [==============================] - 5s 573us/step - loss: 0.0305 - acc: 0.9309 - val_loss: 0.0292 - val_acc: 0.9224\n",
      "Epoch 412/420\n",
      "8564/8564 [==============================] - 5s 565us/step - loss: 0.0301 - acc: 0.9343 - val_loss: 0.0303 - val_acc: 0.9359\n",
      "Epoch 413/420\n",
      "8564/8564 [==============================] - 5s 587us/step - loss: 0.0302 - acc: 0.9317 - val_loss: 0.0258 - val_acc: 0.9424\n",
      "Epoch 414/420\n",
      "8564/8564 [==============================] - 5s 588us/step - loss: 0.0305 - acc: 0.9304 - val_loss: 0.0246 - val_acc: 0.9423\n",
      "Epoch 415/420\n",
      "8564/8564 [==============================] - 5s 590us/step - loss: 0.0310 - acc: 0.9315 - val_loss: 0.0288 - val_acc: 0.9321\n",
      "Epoch 416/420\n",
      "8564/8564 [==============================] - 5s 607us/step - loss: 0.0319 - acc: 0.9276 - val_loss: 0.0246 - val_acc: 0.9451\n",
      "Epoch 417/420\n",
      "8564/8564 [==============================] - 5s 597us/step - loss: 0.0308 - acc: 0.9311 - val_loss: 0.0260 - val_acc: 0.9387\n",
      "Epoch 418/420\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0320 - acc: 0.9288 - val_loss: 0.0273 - val_acc: 0.9421\n",
      "Epoch 419/420\n",
      "8564/8564 [==============================] - 5s 591us/step - loss: 0.0288 - acc: 0.9367 - val_loss: 0.0281 - val_acc: 0.9343\n",
      "Epoch 420/420\n",
      "8564/8564 [==============================] - 5s 590us/step - loss: 0.0311 - acc: 0.9314 - val_loss: 0.0288 - val_acc: 0.9278\n",
      "start training round 21\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 421/440\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0282 - acc: 0.7055 - val_loss: 0.0292 - val_acc: 0.7202\n",
      "Epoch 422/440\n",
      "8564/8564 [==============================] - 5s 592us/step - loss: 0.0284 - acc: 0.7178 - val_loss: 0.0283 - val_acc: 0.7019\n",
      "Epoch 423/440\n",
      "8564/8564 [==============================] - 5s 582us/step - loss: 0.0281 - acc: 0.6607 - val_loss: 0.0285 - val_acc: 0.5726\n",
      "Epoch 424/440\n",
      "8564/8564 [==============================] - 5s 578us/step - loss: 0.0284 - acc: 0.6428 - val_loss: 0.0281 - val_acc: 0.7181\n",
      "Epoch 425/440\n",
      "8564/8564 [==============================] - 5s 584us/step - loss: 0.0283 - acc: 0.7148 - val_loss: 0.0283 - val_acc: 0.7122\n",
      "Epoch 426/440\n",
      "8564/8564 [==============================] - 5s 578us/step - loss: 0.0282 - acc: 0.6875 - val_loss: 0.0277 - val_acc: 0.7182\n",
      "Epoch 427/440\n",
      "8564/8564 [==============================] - 5s 609us/step - loss: 0.0280 - acc: 0.6509 - val_loss: 0.0286 - val_acc: 0.5573\n",
      "Epoch 428/440\n",
      "8564/8564 [==============================] - 5s 580us/step - loss: 0.0283 - acc: 0.6532 - val_loss: 0.0289 - val_acc: 0.7203\n",
      "Epoch 429/440\n",
      "8564/8564 [==============================] - 5s 578us/step - loss: 0.0278 - acc: 0.7132 - val_loss: 0.0286 - val_acc: 0.7118\n",
      "Epoch 430/440\n",
      "8564/8564 [==============================] - 5s 572us/step - loss: 0.0281 - acc: 0.6755 - val_loss: 0.0293 - val_acc: 0.7166\n",
      "Epoch 431/440\n",
      "8564/8564 [==============================] - 5s 568us/step - loss: 0.0279 - acc: 0.6922 - val_loss: 0.0292 - val_acc: 0.5445\n",
      "Epoch 432/440\n",
      "8564/8564 [==============================] - 5s 591us/step - loss: 0.0280 - acc: 0.6426 - val_loss: 0.0289 - val_acc: 0.5483\n",
      "Epoch 433/440\n",
      "8564/8564 [==============================] - 5s 573us/step - loss: 0.0278 - acc: 0.6572 - val_loss: 0.0282 - val_acc: 0.6838\n",
      "Epoch 434/440\n",
      "8564/8564 [==============================] - 5s 538us/step - loss: 0.0283 - acc: 0.6612 - val_loss: 0.0280 - val_acc: 0.7199\n",
      "Epoch 435/440\n",
      "8564/8564 [==============================] - 5s 555us/step - loss: 0.0278 - acc: 0.7000 - val_loss: 0.0276 - val_acc: 0.7182\n",
      "Epoch 436/440\n",
      "8564/8564 [==============================] - 5s 567us/step - loss: 0.0278 - acc: 0.7196 - val_loss: 0.0276 - val_acc: 0.7173\n",
      "Epoch 437/440\n",
      "8564/8564 [==============================] - 5s 581us/step - loss: 0.0278 - acc: 0.7050 - val_loss: 0.0277 - val_acc: 0.6836\n",
      "Epoch 438/440\n",
      "8564/8564 [==============================] - 5s 575us/step - loss: 0.0278 - acc: 0.7065 - val_loss: 0.0286 - val_acc: 0.7182\n",
      "Epoch 439/440\n",
      "8564/8564 [==============================] - 5s 579us/step - loss: 0.0278 - acc: 0.6789 - val_loss: 0.0294 - val_acc: 0.5398\n",
      "Epoch 440/440\n",
      "8564/8564 [==============================] - 5s 574us/step - loss: 0.0280 - acc: 0.6541 - val_loss: 0.0287 - val_acc: 0.5629\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 421/440\n",
      "8564/8564 [==============================] - 5s 578us/step - loss: 0.0247 - acc: 0.8710 - val_loss: 0.0248 - val_acc: 0.8716\n",
      "Epoch 422/440\n",
      "8564/8564 [==============================] - 5s 575us/step - loss: 0.0248 - acc: 0.8707 - val_loss: 0.0251 - val_acc: 0.8690\n",
      "Epoch 423/440\n",
      "8564/8564 [==============================] - 5s 561us/step - loss: 0.0248 - acc: 0.8724 - val_loss: 0.0254 - val_acc: 0.8675\n",
      "Epoch 424/440\n",
      "8564/8564 [==============================] - 5s 565us/step - loss: 0.0248 - acc: 0.8710 - val_loss: 0.0249 - val_acc: 0.8726\n",
      "Epoch 425/440\n",
      "8564/8564 [==============================] - 5s 572us/step - loss: 0.0247 - acc: 0.8719 - val_loss: 0.0255 - val_acc: 0.8671\n",
      "Epoch 426/440\n",
      "8564/8564 [==============================] - 5s 572us/step - loss: 0.0248 - acc: 0.8711 - val_loss: 0.0248 - val_acc: 0.8721\n",
      "Epoch 427/440\n",
      "8564/8564 [==============================] - 5s 566us/step - loss: 0.0248 - acc: 0.8705 - val_loss: 0.0247 - val_acc: 0.8743\n",
      "Epoch 428/440\n",
      "8564/8564 [==============================] - 5s 580us/step - loss: 0.0251 - acc: 0.8758 - val_loss: 0.0260 - val_acc: 0.8751\n",
      "Epoch 429/440\n",
      "8564/8564 [==============================] - 5s 582us/step - loss: 0.0246 - acc: 0.8745 - val_loss: 0.0250 - val_acc: 0.8760\n",
      "Epoch 430/440\n",
      "8564/8564 [==============================] - 5s 566us/step - loss: 0.0247 - acc: 0.8717 - val_loss: 0.0251 - val_acc: 0.8737\n",
      "Epoch 431/440\n",
      "8564/8564 [==============================] - 5s 578us/step - loss: 0.0247 - acc: 0.8715 - val_loss: 0.0249 - val_acc: 0.8762\n",
      "Epoch 432/440\n",
      "8564/8564 [==============================] - 5s 574us/step - loss: 0.0247 - acc: 0.8726 - val_loss: 0.0251 - val_acc: 0.8746\n",
      "Epoch 433/440\n",
      "8564/8564 [==============================] - 5s 575us/step - loss: 0.0246 - acc: 0.8735 - val_loss: 0.0252 - val_acc: 0.8734\n",
      "Epoch 434/440\n",
      "8564/8564 [==============================] - 5s 574us/step - loss: 0.0245 - acc: 0.8748 - val_loss: 0.0249 - val_acc: 0.8746\n",
      "Epoch 435/440\n",
      "8564/8564 [==============================] - 5s 563us/step - loss: 0.0247 - acc: 0.8793 - val_loss: 0.0265 - val_acc: 0.8781\n",
      "Epoch 436/440\n",
      "8564/8564 [==============================] - 5s 569us/step - loss: 0.0246 - acc: 0.8790 - val_loss: 0.0248 - val_acc: 0.8796\n",
      "Epoch 437/440\n",
      "8564/8564 [==============================] - 5s 553us/step - loss: 0.0247 - acc: 0.8796 - val_loss: 0.0258 - val_acc: 0.8793\n",
      "Epoch 438/440\n",
      "8564/8564 [==============================] - 5s 589us/step - loss: 0.0248 - acc: 0.8797 - val_loss: 0.0256 - val_acc: 0.8739\n",
      "Epoch 439/440\n",
      "8564/8564 [==============================] - 5s 563us/step - loss: 0.0250 - acc: 0.8719 - val_loss: 0.0254 - val_acc: 0.8781\n",
      "Epoch 440/440\n",
      "8564/8564 [==============================] - 5s 577us/step - loss: 0.0247 - acc: 0.8794 - val_loss: 0.0245 - val_acc: 0.8795\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 421/440\n",
      "8564/8564 [==============================] - 5s 578us/step - loss: 0.0281 - acc: 0.9820 - val_loss: 0.0298 - val_acc: 0.9841\n",
      "Epoch 422/440\n",
      "8564/8564 [==============================] - 5s 570us/step - loss: 0.0275 - acc: 0.9815 - val_loss: 0.0281 - val_acc: 0.9851\n",
      "Epoch 423/440\n",
      "8564/8564 [==============================] - 5s 592us/step - loss: 0.0273 - acc: 0.9827 - val_loss: 0.0222 - val_acc: 0.9860\n",
      "Epoch 424/440\n",
      "8564/8564 [==============================] - 5s 568us/step - loss: 0.0270 - acc: 0.9801 - val_loss: 0.0263 - val_acc: 0.9772\n",
      "Epoch 425/440\n",
      "8564/8564 [==============================] - 5s 566us/step - loss: 0.0270 - acc: 0.9811 - val_loss: 0.0219 - val_acc: 0.9828\n",
      "Epoch 426/440\n",
      "8564/8564 [==============================] - 5s 565us/step - loss: 0.0255 - acc: 0.9833 - val_loss: 0.0345 - val_acc: 0.9708\n",
      "Epoch 427/440\n",
      "8564/8564 [==============================] - 5s 578us/step - loss: 0.0255 - acc: 0.9831 - val_loss: 0.0297 - val_acc: 0.9793\n",
      "Epoch 428/440\n",
      "8564/8564 [==============================] - 5s 544us/step - loss: 0.0268 - acc: 0.9805 - val_loss: 0.0293 - val_acc: 0.9843\n",
      "Epoch 429/440\n",
      "8564/8564 [==============================] - 5s 555us/step - loss: 0.0273 - acc: 0.9807 - val_loss: 0.0277 - val_acc: 0.9744\n",
      "Epoch 430/440\n",
      "8564/8564 [==============================] - 5s 579us/step - loss: 0.0251 - acc: 0.9825 - val_loss: 0.0343 - val_acc: 0.9847\n",
      "Epoch 431/440\n",
      "8564/8564 [==============================] - 5s 584us/step - loss: 0.0268 - acc: 0.9813 - val_loss: 0.0263 - val_acc: 0.9828\n",
      "Epoch 432/440\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0288 - acc: 0.9816 - val_loss: 0.0353 - val_acc: 0.9785\n",
      "Epoch 433/440\n",
      "8564/8564 [==============================] - 5s 573us/step - loss: 0.0262 - acc: 0.9819 - val_loss: 0.0263 - val_acc: 0.9812\n",
      "Epoch 434/440\n",
      "8564/8564 [==============================] - 5s 610us/step - loss: 0.0274 - acc: 0.9800 - val_loss: 0.0215 - val_acc: 0.9867\n",
      "Epoch 435/440\n",
      "8564/8564 [==============================] - 5s 611us/step - loss: 0.0257 - acc: 0.9814 - val_loss: 0.0239 - val_acc: 0.9854\n",
      "Epoch 436/440\n",
      "8564/8564 [==============================] - 5s 586us/step - loss: 0.0258 - acc: 0.9831 - val_loss: 0.0245 - val_acc: 0.9861\n",
      "Epoch 437/440\n",
      "8564/8564 [==============================] - 5s 574us/step - loss: 0.0267 - acc: 0.9802 - val_loss: 0.0220 - val_acc: 0.9845\n",
      "Epoch 438/440\n",
      "8564/8564 [==============================] - 5s 565us/step - loss: 0.0275 - acc: 0.9822 - val_loss: 0.0244 - val_acc: 0.9868\n",
      "Epoch 439/440\n",
      "8564/8564 [==============================] - 5s 558us/step - loss: 0.0259 - acc: 0.9837 - val_loss: 0.0277 - val_acc: 0.9827\n",
      "Epoch 440/440\n",
      "8564/8564 [==============================] - 5s 628us/step - loss: 0.0270 - acc: 0.9799 - val_loss: 0.0276 - val_acc: 0.9843\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 421/440\n",
      "8564/8564 [==============================] - 5s 587us/step - loss: 0.0301 - acc: 0.9330 - val_loss: 0.0257 - val_acc: 0.9487\n",
      "Epoch 422/440\n",
      "8564/8564 [==============================] - 5s 592us/step - loss: 0.0311 - acc: 0.9305 - val_loss: 0.0307 - val_acc: 0.9372\n",
      "Epoch 423/440\n",
      "8564/8564 [==============================] - 5s 597us/step - loss: 0.0298 - acc: 0.9332 - val_loss: 0.0276 - val_acc: 0.9452\n",
      "Epoch 424/440\n",
      "8564/8564 [==============================] - 5s 594us/step - loss: 0.0311 - acc: 0.9304 - val_loss: 0.0282 - val_acc: 0.9398\n",
      "Epoch 425/440\n",
      "8564/8564 [==============================] - 5s 586us/step - loss: 0.0307 - acc: 0.9323 - val_loss: 0.0308 - val_acc: 0.9491\n",
      "Epoch 426/440\n",
      "8564/8564 [==============================] - 5s 569us/step - loss: 0.0306 - acc: 0.9313 - val_loss: 0.0367 - val_acc: 0.9345\n",
      "Epoch 427/440\n",
      "8564/8564 [==============================] - 5s 582us/step - loss: 0.0304 - acc: 0.9322 - val_loss: 0.0302 - val_acc: 0.9459\n",
      "Epoch 428/440\n",
      "8564/8564 [==============================] - 5s 587us/step - loss: 0.0300 - acc: 0.9341 - val_loss: 0.0290 - val_acc: 0.9387\n",
      "Epoch 429/440\n",
      "8564/8564 [==============================] - 5s 570us/step - loss: 0.0303 - acc: 0.9344 - val_loss: 0.0312 - val_acc: 0.9348\n",
      "Epoch 430/440\n",
      "8564/8564 [==============================] - 5s 571us/step - loss: 0.0297 - acc: 0.9334 - val_loss: 0.0309 - val_acc: 0.9371\n",
      "Epoch 431/440\n",
      "8564/8564 [==============================] - 5s 572us/step - loss: 0.0283 - acc: 0.9358 - val_loss: 0.0287 - val_acc: 0.9448\n",
      "Epoch 432/440\n",
      "8564/8564 [==============================] - 5s 587us/step - loss: 0.0306 - acc: 0.9323 - val_loss: 0.0357 - val_acc: 0.9357\n",
      "Epoch 433/440\n",
      "8564/8564 [==============================] - 5s 593us/step - loss: 0.0318 - acc: 0.9298 - val_loss: 0.0299 - val_acc: 0.9420\n",
      "Epoch 434/440\n",
      "8564/8564 [==============================] - 5s 586us/step - loss: 0.0294 - acc: 0.9352 - val_loss: 0.0302 - val_acc: 0.9345\n",
      "Epoch 435/440\n",
      "8564/8564 [==============================] - 5s 578us/step - loss: 0.0300 - acc: 0.9336 - val_loss: 0.0347 - val_acc: 0.9114\n",
      "Epoch 436/440\n",
      "8564/8564 [==============================] - 5s 594us/step - loss: 0.0307 - acc: 0.9331 - val_loss: 0.0269 - val_acc: 0.9455\n",
      "Epoch 437/440\n",
      "8564/8564 [==============================] - 5s 576us/step - loss: 0.0318 - acc: 0.9304 - val_loss: 0.0302 - val_acc: 0.9438\n",
      "Epoch 438/440\n",
      "8564/8564 [==============================] - 5s 569us/step - loss: 0.0309 - acc: 0.9326 - val_loss: 0.0270 - val_acc: 0.9447\n",
      "Epoch 439/440\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0294 - acc: 0.9339 - val_loss: 0.0307 - val_acc: 0.9420\n",
      "Epoch 440/440\n",
      "8564/8564 [==============================] - 5s 585us/step - loss: 0.0314 - acc: 0.9320 - val_loss: 0.0298 - val_acc: 0.9313\n",
      "start training round 22\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 441/460\n",
      "8564/8564 [==============================] - 5s 566us/step - loss: 0.0282 - acc: 0.6476 - val_loss: 0.0292 - val_acc: 0.5364\n",
      "Epoch 442/460\n",
      "8564/8564 [==============================] - 5s 540us/step - loss: 0.0277 - acc: 0.7106 - val_loss: 0.0283 - val_acc: 0.7224\n",
      "Epoch 443/460\n",
      "8564/8564 [==============================] - 5s 579us/step - loss: 0.0281 - acc: 0.7107 - val_loss: 0.0282 - val_acc: 0.7212\n",
      "Epoch 444/460\n",
      "8564/8564 [==============================] - 5s 587us/step - loss: 0.0278 - acc: 0.7206 - val_loss: 0.0275 - val_acc: 0.7120\n",
      "Epoch 445/460\n",
      "8564/8564 [==============================] - 5s 589us/step - loss: 0.0277 - acc: 0.7214 - val_loss: 0.0290 - val_acc: 0.7169\n",
      "Epoch 446/460\n",
      "8564/8564 [==============================] - 5s 560us/step - loss: 0.0279 - acc: 0.7011 - val_loss: 0.0307 - val_acc: 0.7196\n",
      "Epoch 447/460\n",
      "8564/8564 [==============================] - 5s 586us/step - loss: 0.0282 - acc: 0.7143 - val_loss: 0.0291 - val_acc: 0.7235\n",
      "Epoch 448/460\n",
      "8564/8564 [==============================] - 5s 580us/step - loss: 0.0277 - acc: 0.7205 - val_loss: 0.0281 - val_acc: 0.7126\n",
      "Epoch 449/460\n",
      "8564/8564 [==============================] - 5s 586us/step - loss: 0.0278 - acc: 0.6941 - val_loss: 0.0276 - val_acc: 0.7204\n",
      "Epoch 450/460\n",
      "8564/8564 [==============================] - 5s 581us/step - loss: 0.0274 - acc: 0.7139 - val_loss: 0.0272 - val_acc: 0.7227\n",
      "Epoch 451/460\n",
      "8564/8564 [==============================] - 5s 592us/step - loss: 0.0274 - acc: 0.6828 - val_loss: 0.0276 - val_acc: 0.7200\n",
      "Epoch 452/460\n",
      "8564/8564 [==============================] - 5s 569us/step - loss: 0.0277 - acc: 0.6584 - val_loss: 0.0280 - val_acc: 0.7207\n",
      "Epoch 453/460\n",
      "8564/8564 [==============================] - 5s 574us/step - loss: 0.0277 - acc: 0.6545 - val_loss: 0.0284 - val_acc: 0.7171\n",
      "Epoch 454/460\n",
      "8564/8564 [==============================] - 5s 578us/step - loss: 0.0277 - acc: 0.6915 - val_loss: 0.0292 - val_acc: 0.6435\n",
      "Epoch 455/460\n",
      "8564/8564 [==============================] - 5s 577us/step - loss: 0.0276 - acc: 0.6896 - val_loss: 0.0282 - val_acc: 0.5913\n",
      "Epoch 456/460\n",
      "8564/8564 [==============================] - 5s 575us/step - loss: 0.0277 - acc: 0.6919 - val_loss: 0.0291 - val_acc: 0.7174\n",
      "Epoch 457/460\n",
      "8564/8564 [==============================] - 5s 594us/step - loss: 0.0277 - acc: 0.7191 - val_loss: 0.0282 - val_acc: 0.7233\n",
      "Epoch 458/460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 5s 550us/step - loss: 0.0278 - acc: 0.7029 - val_loss: 0.0279 - val_acc: 0.7128\n",
      "Epoch 459/460\n",
      "8564/8564 [==============================] - 5s 615us/step - loss: 0.0277 - acc: 0.6959 - val_loss: 0.0290 - val_acc: 0.7184\n",
      "Epoch 460/460\n",
      "8564/8564 [==============================] - 5s 586us/step - loss: 0.0275 - acc: 0.6643 - val_loss: 0.0276 - val_acc: 0.7236\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 441/460\n",
      "8564/8564 [==============================] - 5s 575us/step - loss: 0.0247 - acc: 0.8797 - val_loss: 0.0240 - val_acc: 0.8801\n",
      "Epoch 442/460\n",
      "8564/8564 [==============================] - 5s 567us/step - loss: 0.0245 - acc: 0.8768 - val_loss: 0.0254 - val_acc: 0.8653\n",
      "Epoch 443/460\n",
      "8564/8564 [==============================] - 5s 559us/step - loss: 0.0246 - acc: 0.8720 - val_loss: 0.0250 - val_acc: 0.8683\n",
      "Epoch 444/460\n",
      "8564/8564 [==============================] - 5s 590us/step - loss: 0.0246 - acc: 0.8723 - val_loss: 0.0246 - val_acc: 0.8749\n",
      "Epoch 445/460\n",
      "8564/8564 [==============================] - 5s 571us/step - loss: 0.0247 - acc: 0.8715 - val_loss: 0.0246 - val_acc: 0.8736\n",
      "Epoch 446/460\n",
      "8564/8564 [==============================] - 5s 599us/step - loss: 0.0246 - acc: 0.8723 - val_loss: 0.0255 - val_acc: 0.8653\n",
      "Epoch 447/460\n",
      "8564/8564 [==============================] - 5s 581us/step - loss: 0.0246 - acc: 0.8727 - val_loss: 0.0246 - val_acc: 0.8733\n",
      "Epoch 448/460\n",
      "8564/8564 [==============================] - 5s 561us/step - loss: 0.0245 - acc: 0.8774 - val_loss: 0.0240 - val_acc: 0.8804\n",
      "Epoch 449/460\n",
      "8564/8564 [==============================] - 5s 588us/step - loss: 0.0246 - acc: 0.8760 - val_loss: 0.0240 - val_acc: 0.8797\n",
      "Epoch 450/460\n",
      "8564/8564 [==============================] - 5s 576us/step - loss: 0.0246 - acc: 0.8730 - val_loss: 0.0245 - val_acc: 0.8784\n",
      "Epoch 451/460\n",
      "8564/8564 [==============================] - 5s 579us/step - loss: 0.0244 - acc: 0.8742 - val_loss: 0.0245 - val_acc: 0.8737\n",
      "Epoch 452/460\n",
      "8564/8564 [==============================] - 5s 585us/step - loss: 0.0246 - acc: 0.8731 - val_loss: 0.0250 - val_acc: 0.8715\n",
      "Epoch 453/460\n",
      "8564/8564 [==============================] - 5s 581us/step - loss: 0.0246 - acc: 0.8725 - val_loss: 0.0243 - val_acc: 0.8737\n",
      "Epoch 454/460\n",
      "8564/8564 [==============================] - 5s 563us/step - loss: 0.0245 - acc: 0.8796 - val_loss: 0.0250 - val_acc: 0.8802\n",
      "Epoch 455/460\n",
      "8564/8564 [==============================] - 5s 534us/step - loss: 0.0247 - acc: 0.8797 - val_loss: 0.0248 - val_acc: 0.8804\n",
      "Epoch 456/460\n",
      "8564/8564 [==============================] - 5s 588us/step - loss: 0.0244 - acc: 0.8758 - val_loss: 0.0247 - val_acc: 0.8745\n",
      "Epoch 457/460\n",
      "8564/8564 [==============================] - 5s 573us/step - loss: 0.0246 - acc: 0.8726 - val_loss: 0.0252 - val_acc: 0.8715\n",
      "Epoch 458/460\n",
      "8564/8564 [==============================] - 5s 578us/step - loss: 0.0245 - acc: 0.8723 - val_loss: 0.0252 - val_acc: 0.8775\n",
      "Epoch 459/460\n",
      "8564/8564 [==============================] - 5s 560us/step - loss: 0.0245 - acc: 0.8735 - val_loss: 0.0247 - val_acc: 0.8761\n",
      "Epoch 460/460\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0243 - acc: 0.8742 - val_loss: 0.0247 - val_acc: 0.8783\n",
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 441/460\n",
      "8564/8564 [==============================] - 5s 593us/step - loss: 0.0261 - acc: 0.9829 - val_loss: 0.0285 - val_acc: 0.9829\n",
      "Epoch 442/460\n",
      "8564/8564 [==============================] - 5s 595us/step - loss: 0.0250 - acc: 0.9827 - val_loss: 0.0276 - val_acc: 0.9816\n",
      "Epoch 443/460\n",
      "8564/8564 [==============================] - 5s 569us/step - loss: 0.0274 - acc: 0.9796 - val_loss: 0.0276 - val_acc: 0.9837\n",
      "Epoch 444/460\n",
      "8564/8564 [==============================] - 5s 581us/step - loss: 0.0266 - acc: 0.9818 - val_loss: 0.0219 - val_acc: 0.9854\n",
      "Epoch 445/460\n",
      "8564/8564 [==============================] - 5s 579us/step - loss: 0.0266 - acc: 0.9828 - val_loss: 0.0264 - val_acc: 0.9874\n",
      "Epoch 446/460\n",
      "8564/8564 [==============================] - 5s 558us/step - loss: 0.0262 - acc: 0.9829 - val_loss: 0.0258 - val_acc: 0.9843\n",
      "Epoch 447/460\n",
      "8564/8564 [==============================] - 5s 561us/step - loss: 0.0270 - acc: 0.9824 - val_loss: 0.0300 - val_acc: 0.9790\n",
      "Epoch 448/460\n",
      "8564/8564 [==============================] - 5s 582us/step - loss: 0.0260 - acc: 0.9827 - val_loss: 0.0265 - val_acc: 0.9880\n",
      "Epoch 449/460\n",
      "8564/8564 [==============================] - 5s 589us/step - loss: 0.0270 - acc: 0.9837 - val_loss: 0.0226 - val_acc: 0.9848\n",
      "Epoch 450/460\n",
      "8564/8564 [==============================] - 5s 576us/step - loss: 0.0275 - acc: 0.9826 - val_loss: 0.0222 - val_acc: 0.9858\n",
      "Epoch 451/460\n",
      "8564/8564 [==============================] - 5s 587us/step - loss: 0.0255 - acc: 0.9819 - val_loss: 0.0289 - val_acc: 0.9842\n",
      "Epoch 452/460\n",
      "8564/8564 [==============================] - 5s 592us/step - loss: 0.0256 - acc: 0.9828 - val_loss: 0.0265 - val_acc: 0.9846\n",
      "Epoch 453/460\n",
      "8564/8564 [==============================] - 5s 577us/step - loss: 0.0248 - acc: 0.9834 - val_loss: 0.0218 - val_acc: 0.9870\n",
      "Epoch 454/460\n",
      "8564/8564 [==============================] - 5s 596us/step - loss: 0.0261 - acc: 0.9823 - val_loss: 0.0304 - val_acc: 0.9825\n",
      "Epoch 455/460\n",
      "8564/8564 [==============================] - 5s 590us/step - loss: 0.0259 - acc: 0.9817 - val_loss: 0.0215 - val_acc: 0.9819\n",
      "Epoch 456/460\n",
      "8564/8564 [==============================] - 5s 563us/step - loss: 0.0267 - acc: 0.9821 - val_loss: 0.0273 - val_acc: 0.9852\n",
      "Epoch 457/460\n",
      "8564/8564 [==============================] - 5s 576us/step - loss: 0.0250 - acc: 0.9825 - val_loss: 0.0220 - val_acc: 0.9865\n",
      "Epoch 458/460\n",
      "8564/8564 [==============================] - 5s 546us/step - loss: 0.0241 - acc: 0.9834 - val_loss: 0.0213 - val_acc: 0.9876\n",
      "Epoch 459/460\n",
      "8564/8564 [==============================] - 5s 558us/step - loss: 0.0248 - acc: 0.9837 - val_loss: 0.0297 - val_acc: 0.9855\n",
      "Epoch 460/460\n",
      "8564/8564 [==============================] - 5s 571us/step - loss: 0.0253 - acc: 0.9824 - val_loss: 0.0228 - val_acc: 0.9857\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 441/460\n",
      "8564/8564 [==============================] - 5s 586us/step - loss: 0.0315 - acc: 0.9313 - val_loss: 0.0321 - val_acc: 0.9308\n",
      "Epoch 442/460\n",
      "8564/8564 [==============================] - 5s 589us/step - loss: 0.0315 - acc: 0.9305 - val_loss: 0.0286 - val_acc: 0.9343\n",
      "Epoch 443/460\n",
      "8564/8564 [==============================] - 5s 577us/step - loss: 0.0302 - acc: 0.9324 - val_loss: 0.0271 - val_acc: 0.9393\n",
      "Epoch 444/460\n",
      "8564/8564 [==============================] - 5s 584us/step - loss: 0.0289 - acc: 0.9346 - val_loss: 0.0324 - val_acc: 0.9437\n",
      "Epoch 445/460\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0288 - acc: 0.9362 - val_loss: 0.0273 - val_acc: 0.9424\n",
      "Epoch 446/460\n",
      "8564/8564 [==============================] - 5s 633us/step - loss: 0.0299 - acc: 0.9322 - val_loss: 0.0308 - val_acc: 0.9302\n",
      "Epoch 447/460\n",
      "8564/8564 [==============================] - 5s 574us/step - loss: 0.0305 - acc: 0.9337 - val_loss: 0.0271 - val_acc: 0.9409\n",
      "Epoch 448/460\n",
      "8564/8564 [==============================] - 5s 572us/step - loss: 0.0292 - acc: 0.9345 - val_loss: 0.0351 - val_acc: 0.9325\n",
      "Epoch 449/460\n",
      "8564/8564 [==============================] - 5s 539us/step - loss: 0.0307 - acc: 0.9321 - val_loss: 0.0289 - val_acc: 0.9385\n",
      "Epoch 450/460\n",
      "8564/8564 [==============================] - 5s 573us/step - loss: 0.0302 - acc: 0.9321 - val_loss: 0.0281 - val_acc: 0.9381\n",
      "Epoch 451/460\n",
      "8564/8564 [==============================] - 5s 588us/step - loss: 0.0297 - acc: 0.9340 - val_loss: 0.0333 - val_acc: 0.9387\n",
      "Epoch 452/460\n",
      "8564/8564 [==============================] - 5s 567us/step - loss: 0.0309 - acc: 0.9326 - val_loss: 0.0293 - val_acc: 0.9453\n",
      "Epoch 453/460\n",
      "8564/8564 [==============================] - 5s 586us/step - loss: 0.0297 - acc: 0.9319 - val_loss: 0.0278 - val_acc: 0.9461\n",
      "Epoch 454/460\n",
      "8564/8564 [==============================] - 5s 582us/step - loss: 0.0301 - acc: 0.9328 - val_loss: 0.0245 - val_acc: 0.9514\n",
      "Epoch 455/460\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 5s 584us/step - loss: 0.0290 - acc: 0.9344 - val_loss: 0.0296 - val_acc: 0.9377\n",
      "Epoch 456/460\n",
      "8564/8564 [==============================] - 5s 577us/step - loss: 0.0304 - acc: 0.9321 - val_loss: 0.0268 - val_acc: 0.9423\n",
      "Epoch 457/460\n",
      "8564/8564 [==============================] - 5s 589us/step - loss: 0.0296 - acc: 0.9337 - val_loss: 0.0276 - val_acc: 0.9375\n",
      "Epoch 458/460\n",
      "8564/8564 [==============================] - 5s 588us/step - loss: 0.0302 - acc: 0.9361 - val_loss: 0.0324 - val_acc: 0.9474\n",
      "Epoch 459/460\n",
      "8564/8564 [==============================] - 5s 585us/step - loss: 0.0307 - acc: 0.9329 - val_loss: 0.0260 - val_acc: 0.9451\n",
      "Epoch 460/460\n",
      "8564/8564 [==============================] - 5s 579us/step - loss: 0.0295 - acc: 0.9341 - val_loss: 0.0270 - val_acc: 0.9493\n",
      "start training round 23\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 461/480\n",
      "8564/8564 [==============================] - 5s 580us/step - loss: 0.0275 - acc: 0.6565 - val_loss: 0.0299 - val_acc: 0.7122\n",
      "Epoch 462/480\n",
      "8564/8564 [==============================] - 5s 581us/step - loss: 0.0275 - acc: 0.6638 - val_loss: 0.0271 - val_acc: 0.7221\n",
      "Epoch 463/480\n",
      "8564/8564 [==============================] - 5s 596us/step - loss: 0.0277 - acc: 0.6669 - val_loss: 0.0280 - val_acc: 0.7089\n",
      "Epoch 464/480\n",
      "8564/8564 [==============================] - 5s 578us/step - loss: 0.0273 - acc: 0.6819 - val_loss: 0.0274 - val_acc: 0.7134\n",
      "Epoch 465/480\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0275 - acc: 0.7135 - val_loss: 0.0280 - val_acc: 0.6768\n",
      "Epoch 466/480\n",
      "8564/8564 [==============================] - 5s 558us/step - loss: 0.0278 - acc: 0.7211 - val_loss: 0.0302 - val_acc: 0.6104\n",
      "Epoch 467/480\n",
      "8564/8564 [==============================] - 5s 553us/step - loss: 0.0278 - acc: 0.7229 - val_loss: 0.0287 - val_acc: 0.7197\n",
      "Epoch 468/480\n",
      "8564/8564 [==============================] - 5s 575us/step - loss: 0.0281 - acc: 0.7217 - val_loss: 0.0288 - val_acc: 0.7204\n",
      "Epoch 469/480\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0275 - acc: 0.7107 - val_loss: 0.0282 - val_acc: 0.7131\n",
      "Epoch 470/480\n",
      "8564/8564 [==============================] - 5s 579us/step - loss: 0.0278 - acc: 0.6486 - val_loss: 0.0270 - val_acc: 0.7257\n",
      "Epoch 471/480\n",
      "8564/8564 [==============================] - 5s 576us/step - loss: 0.0273 - acc: 0.6671 - val_loss: 0.0279 - val_acc: 0.7200\n",
      "Epoch 472/480\n",
      "8564/8564 [==============================] - 5s 594us/step - loss: 0.0275 - acc: 0.7267 - val_loss: 0.0280 - val_acc: 0.6723\n",
      "Epoch 473/480\n",
      "8564/8564 [==============================] - 5s 588us/step - loss: 0.0273 - acc: 0.6975 - val_loss: 0.0279 - val_acc: 0.6715\n",
      "Epoch 474/480\n",
      "8564/8564 [==============================] - 5s 594us/step - loss: 0.0277 - acc: 0.7036 - val_loss: 0.0292 - val_acc: 0.7201\n",
      "Epoch 475/480\n",
      "8564/8564 [==============================] - 5s 594us/step - loss: 0.0277 - acc: 0.7239 - val_loss: 0.0288 - val_acc: 0.7236\n",
      "Epoch 476/480\n",
      "8564/8564 [==============================] - 5s 567us/step - loss: 0.0276 - acc: 0.7251 - val_loss: 0.0274 - val_acc: 0.7232\n",
      "Epoch 477/480\n",
      "8564/8564 [==============================] - 5s 568us/step - loss: 0.0273 - acc: 0.6893 - val_loss: 0.0279 - val_acc: 0.6584\n",
      "Epoch 478/480\n",
      "8564/8564 [==============================] - 5s 570us/step - loss: 0.0275 - acc: 0.7217 - val_loss: 0.0281 - val_acc: 0.7270\n",
      "Epoch 479/480\n",
      "8564/8564 [==============================] - 5s 591us/step - loss: 0.0277 - acc: 0.7240 - val_loss: 0.0274 - val_acc: 0.7235\n",
      "Epoch 480/480\n",
      "8564/8564 [==============================] - 5s 568us/step - loss: 0.0272 - acc: 0.6765 - val_loss: 0.0276 - val_acc: 0.7233\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 461/480\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0246 - acc: 0.8731 - val_loss: 0.0239 - val_acc: 0.8805\n",
      "Epoch 462/480\n",
      "8564/8564 [==============================] - 5s 540us/step - loss: 0.0245 - acc: 0.8802 - val_loss: 0.0243 - val_acc: 0.8772\n",
      "Epoch 463/480\n",
      "8564/8564 [==============================] - 5s 538us/step - loss: 0.0243 - acc: 0.8805 - val_loss: 0.0245 - val_acc: 0.8812\n",
      "Epoch 464/480\n",
      "8564/8564 [==============================] - 5s 553us/step - loss: 0.0244 - acc: 0.8813 - val_loss: 0.0249 - val_acc: 0.8809\n",
      "Epoch 465/480\n",
      "8564/8564 [==============================] - 5s 551us/step - loss: 0.0244 - acc: 0.8806 - val_loss: 0.0245 - val_acc: 0.8813\n",
      "Epoch 466/480\n",
      "8564/8564 [==============================] - 5s 581us/step - loss: 0.0244 - acc: 0.8755 - val_loss: 0.0258 - val_acc: 0.8598\n",
      "Epoch 467/480\n",
      "8564/8564 [==============================] - 5s 568us/step - loss: 0.0247 - acc: 0.8714 - val_loss: 0.0241 - val_acc: 0.8755\n",
      "Epoch 468/480\n",
      "8564/8564 [==============================] - 5s 587us/step - loss: 0.0244 - acc: 0.8736 - val_loss: 0.0246 - val_acc: 0.8719\n",
      "Epoch 469/480\n",
      "8564/8564 [==============================] - 5s 594us/step - loss: 0.0244 - acc: 0.8733 - val_loss: 0.0258 - val_acc: 0.8616\n",
      "Epoch 470/480\n",
      "8564/8564 [==============================] - 5s 582us/step - loss: 0.0246 - acc: 0.8720 - val_loss: 0.0251 - val_acc: 0.8676\n",
      "Epoch 471/480\n",
      "8564/8564 [==============================] - 5s 599us/step - loss: 0.0242 - acc: 0.8770 - val_loss: 0.0252 - val_acc: 0.8816\n",
      "Epoch 472/480\n",
      "8564/8564 [==============================] - 5s 591us/step - loss: 0.0241 - acc: 0.8784 - val_loss: 0.0241 - val_acc: 0.8786\n",
      "Epoch 473/480\n",
      "8564/8564 [==============================] - 5s 566us/step - loss: 0.0244 - acc: 0.8745 - val_loss: 0.0248 - val_acc: 0.8780\n",
      "Epoch 474/480\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0241 - acc: 0.8777 - val_loss: 0.0246 - val_acc: 0.8819\n",
      "Epoch 475/480\n",
      "8564/8564 [==============================] - 5s 585us/step - loss: 0.0241 - acc: 0.8781 - val_loss: 0.0242 - val_acc: 0.8753\n",
      "Epoch 476/480\n",
      "8564/8564 [==============================] - 5s 601us/step - loss: 0.0244 - acc: 0.8736 - val_loss: 0.0251 - val_acc: 0.8667\n",
      "Epoch 477/480\n",
      "8564/8564 [==============================] - 5s 554us/step - loss: 0.0245 - acc: 0.8729 - val_loss: 0.0249 - val_acc: 0.8688\n",
      "Epoch 478/480\n",
      "8564/8564 [==============================] - 5s 564us/step - loss: 0.0244 - acc: 0.8736 - val_loss: 0.0252 - val_acc: 0.8642\n",
      "Epoch 479/480\n",
      "8564/8564 [==============================] - 5s 599us/step - loss: 0.0244 - acc: 0.8744 - val_loss: 0.0248 - val_acc: 0.8687\n",
      "Epoch 480/480\n",
      "8564/8564 [==============================] - 5s 593us/step - loss: 0.0245 - acc: 0.8729 - val_loss: 0.0254 - val_acc: 0.8652\n",
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 461/480\n",
      "8564/8564 [==============================] - 5s 587us/step - loss: 0.0246 - acc: 0.9829 - val_loss: 0.0189 - val_acc: 0.9876\n",
      "Epoch 462/480\n",
      "8564/8564 [==============================] - 5s 613us/step - loss: 0.0275 - acc: 0.9817 - val_loss: 0.0279 - val_acc: 0.9878\n",
      "Epoch 463/480\n",
      "8564/8564 [==============================] - 5s 621us/step - loss: 0.0272 - acc: 0.9820 - val_loss: 0.0233 - val_acc: 0.9813\n",
      "Epoch 464/480\n",
      "8564/8564 [==============================] - 5s 625us/step - loss: 0.0271 - acc: 0.9809 - val_loss: 0.0245 - val_acc: 0.9870\n",
      "Epoch 465/480\n",
      "8564/8564 [==============================] - 5s 626us/step - loss: 0.0269 - acc: 0.9813 - val_loss: 0.0242 - val_acc: 0.9852\n",
      "Epoch 466/480\n",
      "8564/8564 [==============================] - 5s 625us/step - loss: 0.0263 - acc: 0.9828 - val_loss: 0.0209 - val_acc: 0.9883\n",
      "Epoch 467/480\n",
      "8564/8564 [==============================] - 5s 614us/step - loss: 0.0260 - acc: 0.9830 - val_loss: 0.0212 - val_acc: 0.9868\n",
      "Epoch 468/480\n",
      "8564/8564 [==============================] - 5s 629us/step - loss: 0.0247 - acc: 0.9841 - val_loss: 0.0230 - val_acc: 0.9864\n",
      "Epoch 469/480\n",
      "8564/8564 [==============================] - 5s 591us/step - loss: 0.0276 - acc: 0.9825 - val_loss: 0.0211 - val_acc: 0.9827\n",
      "Epoch 470/480\n",
      "8564/8564 [==============================] - 5s 632us/step - loss: 0.0250 - acc: 0.9816 - val_loss: 0.0235 - val_acc: 0.9853\n",
      "Epoch 471/480\n",
      "8564/8564 [==============================] - 5s 625us/step - loss: 0.0260 - acc: 0.9818 - val_loss: 0.0275 - val_acc: 0.9855\n",
      "Epoch 472/480\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 5s 619us/step - loss: 0.0263 - acc: 0.9830 - val_loss: 0.0263 - val_acc: 0.9827\n",
      "Epoch 473/480\n",
      "8564/8564 [==============================] - 5s 609us/step - loss: 0.0267 - acc: 0.9821 - val_loss: 0.0267 - val_acc: 0.9822\n",
      "Epoch 474/480\n",
      "8564/8564 [==============================] - 5s 624us/step - loss: 0.0251 - acc: 0.9824 - val_loss: 0.0233 - val_acc: 0.9823\n",
      "Epoch 475/480\n",
      "8564/8564 [==============================] - 5s 580us/step - loss: 0.0268 - acc: 0.9823 - val_loss: 0.0229 - val_acc: 0.9867\n",
      "Epoch 476/480\n",
      "8564/8564 [==============================] - 5s 617us/step - loss: 0.0253 - acc: 0.9822 - val_loss: 0.0214 - val_acc: 0.9852\n",
      "Epoch 477/480\n",
      "8564/8564 [==============================] - 5s 624us/step - loss: 0.0249 - acc: 0.9830 - val_loss: 0.0272 - val_acc: 0.9817\n",
      "Epoch 478/480\n",
      "8564/8564 [==============================] - 5s 550us/step - loss: 0.0259 - acc: 0.9823 - val_loss: 0.0252 - val_acc: 0.9843\n",
      "Epoch 479/480\n",
      "8564/8564 [==============================] - 5s 571us/step - loss: 0.0256 - acc: 0.9823 - val_loss: 0.0224 - val_acc: 0.9864\n",
      "Epoch 480/480\n",
      "8564/8564 [==============================] - 5s 610us/step - loss: 0.0249 - acc: 0.9823 - val_loss: 0.0253 - val_acc: 0.9803\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 461/480\n",
      "8564/8564 [==============================] - 5s 602us/step - loss: 0.0290 - acc: 0.9355 - val_loss: 0.0279 - val_acc: 0.9289\n",
      "Epoch 462/480\n",
      "8564/8564 [==============================] - 5s 585us/step - loss: 0.0311 - acc: 0.9309 - val_loss: 0.0273 - val_acc: 0.9410\n",
      "Epoch 463/480\n",
      "8564/8564 [==============================] - 5s 602us/step - loss: 0.0297 - acc: 0.9349 - val_loss: 0.0298 - val_acc: 0.9293\n",
      "Epoch 464/480\n",
      "8564/8564 [==============================] - 5s 618us/step - loss: 0.0294 - acc: 0.9365 - val_loss: 0.0246 - val_acc: 0.9409\n",
      "Epoch 465/480\n",
      "8564/8564 [==============================] - 5s 622us/step - loss: 0.0297 - acc: 0.9361 - val_loss: 0.0246 - val_acc: 0.9366\n",
      "Epoch 466/480\n",
      "8564/8564 [==============================] - 5s 590us/step - loss: 0.0293 - acc: 0.9366 - val_loss: 0.0278 - val_acc: 0.9433\n",
      "Epoch 467/480\n",
      "8564/8564 [==============================] - 5s 623us/step - loss: 0.0310 - acc: 0.9332 - val_loss: 0.0245 - val_acc: 0.9469\n",
      "Epoch 468/480\n",
      "8564/8564 [==============================] - 5s 591us/step - loss: 0.0297 - acc: 0.9337 - val_loss: 0.0279 - val_acc: 0.9406\n",
      "Epoch 469/480\n",
      "8564/8564 [==============================] - 5s 586us/step - loss: 0.0294 - acc: 0.9347 - val_loss: 0.0243 - val_acc: 0.9415\n",
      "Epoch 470/480\n",
      "8564/8564 [==============================] - 5s 594us/step - loss: 0.0307 - acc: 0.9329 - val_loss: 0.0268 - val_acc: 0.9420\n",
      "Epoch 471/480\n",
      "8564/8564 [==============================] - 5s 600us/step - loss: 0.0302 - acc: 0.9312 - val_loss: 0.0300 - val_acc: 0.9413\n",
      "Epoch 472/480\n",
      "8564/8564 [==============================] - 5s 600us/step - loss: 0.0297 - acc: 0.9347 - val_loss: 0.0264 - val_acc: 0.9444\n",
      "Epoch 473/480\n",
      "8564/8564 [==============================] - 5s 609us/step - loss: 0.0293 - acc: 0.9328 - val_loss: 0.0341 - val_acc: 0.9246\n",
      "Epoch 474/480\n",
      "8564/8564 [==============================] - 5s 598us/step - loss: 0.0294 - acc: 0.9327 - val_loss: 0.0240 - val_acc: 0.9504\n",
      "Epoch 475/480\n",
      "8564/8564 [==============================] - 5s 622us/step - loss: 0.0298 - acc: 0.9361 - val_loss: 0.0291 - val_acc: 0.9440\n",
      "Epoch 476/480\n",
      "8564/8564 [==============================] - 5s 595us/step - loss: 0.0298 - acc: 0.9337 - val_loss: 0.0295 - val_acc: 0.9382\n",
      "Epoch 477/480\n",
      "8564/8564 [==============================] - 5s 588us/step - loss: 0.0282 - acc: 0.9379 - val_loss: 0.0284 - val_acc: 0.9322\n",
      "Epoch 478/480\n",
      "8564/8564 [==============================] - 5s 630us/step - loss: 0.0300 - acc: 0.9341 - val_loss: 0.0232 - val_acc: 0.9463\n",
      "Epoch 479/480\n",
      "8564/8564 [==============================] - 5s 628us/step - loss: 0.0296 - acc: 0.9352 - val_loss: 0.0273 - val_acc: 0.9315\n",
      "Epoch 480/480\n",
      "8564/8564 [==============================] - 5s 623us/step - loss: 0.0290 - acc: 0.9359 - val_loss: 0.0259 - val_acc: 0.9450\n",
      "start training round 24\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 481/500\n",
      "8564/8564 [==============================] - 5s 612us/step - loss: 0.0275 - acc: 0.6522 - val_loss: 0.0276 - val_acc: 0.7190\n",
      "Epoch 482/500\n",
      "8564/8564 [==============================] - 5s 626us/step - loss: 0.0276 - acc: 0.6495 - val_loss: 0.0282 - val_acc: 0.7200\n",
      "Epoch 483/500\n",
      "8564/8564 [==============================] - 5s 632us/step - loss: 0.0276 - acc: 0.6522 - val_loss: 0.0287 - val_acc: 0.7150\n",
      "Epoch 484/500\n",
      "8564/8564 [==============================] - 5s 599us/step - loss: 0.0275 - acc: 0.6466 - val_loss: 0.0282 - val_acc: 0.7185\n",
      "Epoch 485/500\n",
      "8564/8564 [==============================] - 5s 609us/step - loss: 0.0274 - acc: 0.6617 - val_loss: 0.0273 - val_acc: 0.7254\n",
      "Epoch 486/500\n",
      "8564/8564 [==============================] - 5s 613us/step - loss: 0.0274 - acc: 0.6601 - val_loss: 0.0273 - val_acc: 0.7260\n",
      "Epoch 487/500\n",
      "8564/8564 [==============================] - 5s 637us/step - loss: 0.0271 - acc: 0.6649 - val_loss: 0.0282 - val_acc: 0.7168\n",
      "Epoch 488/500\n",
      "8564/8564 [==============================] - 5s 599us/step - loss: 0.0275 - acc: 0.6501 - val_loss: 0.0284 - val_acc: 0.7185\n",
      "Epoch 489/500\n",
      "8564/8564 [==============================] - 5s 611us/step - loss: 0.0273 - acc: 0.6619 - val_loss: 0.0270 - val_acc: 0.7269\n",
      "Epoch 490/500\n",
      "8564/8564 [==============================] - 5s 617us/step - loss: 0.0271 - acc: 0.6653 - val_loss: 0.0273 - val_acc: 0.7242\n",
      "Epoch 491/500\n",
      "8564/8564 [==============================] - 5s 622us/step - loss: 0.0272 - acc: 0.6585 - val_loss: 0.0278 - val_acc: 0.7191\n",
      "Epoch 492/500\n",
      "8564/8564 [==============================] - 5s 617us/step - loss: 0.0276 - acc: 0.6450 - val_loss: 0.0275 - val_acc: 0.7232\n",
      "Epoch 493/500\n",
      "8564/8564 [==============================] - 4s 480us/step - loss: 0.0271 - acc: 0.6627 - val_loss: 0.0277 - val_acc: 0.7192\n",
      "Epoch 494/500\n",
      "8564/8564 [==============================] - 4s 474us/step - loss: 0.0273 - acc: 0.6577 - val_loss: 0.0279 - val_acc: 0.7229\n",
      "Epoch 495/500\n",
      "8564/8564 [==============================] - 4s 507us/step - loss: 0.0271 - acc: 0.6874 - val_loss: 0.0280 - val_acc: 0.5979\n",
      "Epoch 496/500\n",
      "8564/8564 [==============================] - 4s 476us/step - loss: 0.0272 - acc: 0.7116 - val_loss: 0.0278 - val_acc: 0.7274\n",
      "Epoch 497/500\n",
      "8564/8564 [==============================] - 4s 476us/step - loss: 0.0273 - acc: 0.7219 - val_loss: 0.0275 - val_acc: 0.7287\n",
      "Epoch 498/500\n",
      "8564/8564 [==============================] - 4s 469us/step - loss: 0.0274 - acc: 0.7302 - val_loss: 0.0277 - val_acc: 0.7280\n",
      "Epoch 499/500\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0271 - acc: 0.7114 - val_loss: 0.0287 - val_acc: 0.7108\n",
      "Epoch 500/500\n",
      "8564/8564 [==============================] - 4s 488us/step - loss: 0.0272 - acc: 0.6575 - val_loss: 0.0269 - val_acc: 0.7269\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 481/500\n",
      "8564/8564 [==============================] - 4s 468us/step - loss: 0.0244 - acc: 0.8739 - val_loss: 0.0252 - val_acc: 0.8661\n",
      "Epoch 482/500\n",
      "8564/8564 [==============================] - 4s 466us/step - loss: 0.0243 - acc: 0.8749 - val_loss: 0.0255 - val_acc: 0.8628\n",
      "Epoch 483/500\n",
      "8564/8564 [==============================] - 4s 467us/step - loss: 0.0243 - acc: 0.8749 - val_loss: 0.0239 - val_acc: 0.8776\n",
      "Epoch 484/500\n",
      "8564/8564 [==============================] - 4s 467us/step - loss: 0.0242 - acc: 0.8770 - val_loss: 0.0243 - val_acc: 0.8823\n",
      "Epoch 485/500\n",
      "8564/8564 [==============================] - 4s 483us/step - loss: 0.0243 - acc: 0.8812 - val_loss: 0.0243 - val_acc: 0.8793\n",
      "Epoch 486/500\n",
      "8564/8564 [==============================] - 4s 491us/step - loss: 0.0242 - acc: 0.8802 - val_loss: 0.0244 - val_acc: 0.8815\n",
      "Epoch 487/500\n",
      "8564/8564 [==============================] - 4s 482us/step - loss: 0.0241 - acc: 0.8817 - val_loss: 0.0244 - val_acc: 0.8827\n",
      "Epoch 488/500\n",
      "8564/8564 [==============================] - 4s 468us/step - loss: 0.0243 - acc: 0.8800 - val_loss: 0.0245 - val_acc: 0.8724\n",
      "Epoch 489/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 4s 485us/step - loss: 0.0244 - acc: 0.8740 - val_loss: 0.0243 - val_acc: 0.8715\n",
      "Epoch 490/500\n",
      "8564/8564 [==============================] - 4s 468us/step - loss: 0.0244 - acc: 0.8733 - val_loss: 0.0250 - val_acc: 0.8648\n",
      "Epoch 491/500\n",
      "8564/8564 [==============================] - 4s 492us/step - loss: 0.0244 - acc: 0.8728 - val_loss: 0.0237 - val_acc: 0.8797\n",
      "Epoch 492/500\n",
      "8564/8564 [==============================] - 5s 607us/step - loss: 0.0243 - acc: 0.8749 - val_loss: 0.0246 - val_acc: 0.8725\n",
      "Epoch 493/500\n",
      "8564/8564 [==============================] - 5s 595us/step - loss: 0.0243 - acc: 0.8737 - val_loss: 0.0242 - val_acc: 0.8762\n",
      "Epoch 494/500\n",
      "8564/8564 [==============================] - 5s 610us/step - loss: 0.0242 - acc: 0.8743 - val_loss: 0.0241 - val_acc: 0.8779\n",
      "Epoch 495/500\n",
      "8564/8564 [==============================] - 5s 604us/step - loss: 0.0244 - acc: 0.8736 - val_loss: 0.0254 - val_acc: 0.8678\n",
      "Epoch 496/500\n",
      "8564/8564 [==============================] - 5s 604us/step - loss: 0.0243 - acc: 0.8742 - val_loss: 0.0250 - val_acc: 0.8691\n",
      "Epoch 497/500\n",
      "8564/8564 [==============================] - 5s 610us/step - loss: 0.0245 - acc: 0.8730 - val_loss: 0.0248 - val_acc: 0.8690\n",
      "Epoch 498/500\n",
      "8564/8564 [==============================] - 5s 632us/step - loss: 0.0243 - acc: 0.8730 - val_loss: 0.0240 - val_acc: 0.8783\n",
      "Epoch 499/500\n",
      "8564/8564 [==============================] - 5s 608us/step - loss: 0.0244 - acc: 0.8732 - val_loss: 0.0245 - val_acc: 0.8727\n",
      "Epoch 500/500\n",
      "8564/8564 [==============================] - 5s 621us/step - loss: 0.0242 - acc: 0.8752 - val_loss: 0.0246 - val_acc: 0.8730\n",
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 481/500\n",
      "8564/8564 [==============================] - 5s 609us/step - loss: 0.0267 - acc: 0.9822 - val_loss: 0.0222 - val_acc: 0.9866\n",
      "Epoch 482/500\n",
      "8564/8564 [==============================] - 5s 606us/step - loss: 0.0259 - acc: 0.9810 - val_loss: 0.0230 - val_acc: 0.9864\n",
      "Epoch 483/500\n",
      "8564/8564 [==============================] - 5s 600us/step - loss: 0.0273 - acc: 0.9817 - val_loss: 0.0202 - val_acc: 0.9872\n",
      "Epoch 484/500\n",
      "8564/8564 [==============================] - 5s 600us/step - loss: 0.0254 - acc: 0.9819 - val_loss: 0.0240 - val_acc: 0.9857\n",
      "Epoch 485/500\n",
      "8564/8564 [==============================] - 5s 584us/step - loss: 0.0261 - acc: 0.9823 - val_loss: 0.0240 - val_acc: 0.9861\n",
      "Epoch 486/500\n",
      "8564/8564 [==============================] - 5s 582us/step - loss: 0.0258 - acc: 0.9828 - val_loss: 0.0249 - val_acc: 0.9857\n",
      "Epoch 487/500\n",
      "8564/8564 [==============================] - 5s 630us/step - loss: 0.0250 - acc: 0.9822 - val_loss: 0.0246 - val_acc: 0.9838\n",
      "Epoch 488/500\n",
      "8564/8564 [==============================] - 5s 600us/step - loss: 0.0258 - acc: 0.9812 - val_loss: 0.0213 - val_acc: 0.9858\n",
      "Epoch 489/500\n",
      "8564/8564 [==============================] - 5s 619us/step - loss: 0.0245 - acc: 0.9828 - val_loss: 0.0217 - val_acc: 0.9863\n",
      "Epoch 490/500\n",
      "8564/8564 [==============================] - 5s 600us/step - loss: 0.0240 - acc: 0.9831 - val_loss: 0.0285 - val_acc: 0.9766\n",
      "Epoch 491/500\n",
      "8564/8564 [==============================] - 5s 610us/step - loss: 0.0267 - acc: 0.9809 - val_loss: 0.0272 - val_acc: 0.9874\n",
      "Epoch 492/500\n",
      "8564/8564 [==============================] - 5s 572us/step - loss: 0.0253 - acc: 0.9807 - val_loss: 0.0258 - val_acc: 0.9879\n",
      "Epoch 493/500\n",
      "8564/8564 [==============================] - 5s 593us/step - loss: 0.0249 - acc: 0.9829 - val_loss: 0.0212 - val_acc: 0.9875\n",
      "Epoch 494/500\n",
      "8564/8564 [==============================] - 5s 600us/step - loss: 0.0253 - acc: 0.9835 - val_loss: 0.0255 - val_acc: 0.9828\n",
      "Epoch 495/500\n",
      "8564/8564 [==============================] - 5s 615us/step - loss: 0.0240 - acc: 0.9832 - val_loss: 0.0247 - val_acc: 0.9847\n",
      "Epoch 496/500\n",
      "8564/8564 [==============================] - 5s 582us/step - loss: 0.0270 - acc: 0.9802 - val_loss: 0.0225 - val_acc: 0.9817\n",
      "Epoch 497/500\n",
      "8564/8564 [==============================] - 5s 631us/step - loss: 0.0251 - acc: 0.9818 - val_loss: 0.0212 - val_acc: 0.9863\n",
      "Epoch 498/500\n",
      "8564/8564 [==============================] - 5s 627us/step - loss: 0.0253 - acc: 0.9840 - val_loss: 0.0240 - val_acc: 0.9858\n",
      "Epoch 499/500\n",
      "8564/8564 [==============================] - 5s 616us/step - loss: 0.0271 - acc: 0.9803 - val_loss: 0.0250 - val_acc: 0.9848\n",
      "Epoch 500/500\n",
      "8564/8564 [==============================] - 5s 612us/step - loss: 0.0264 - acc: 0.9815 - val_loss: 0.0199 - val_acc: 0.9854\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 481/500\n",
      "8564/8564 [==============================] - 5s 605us/step - loss: 0.0299 - acc: 0.9337 - val_loss: 0.0322 - val_acc: 0.9303\n",
      "Epoch 482/500\n",
      "8564/8564 [==============================] - 5s 634us/step - loss: 0.0304 - acc: 0.9321 - val_loss: 0.0278 - val_acc: 0.9381\n",
      "Epoch 483/500\n",
      "8564/8564 [==============================] - 5s 581us/step - loss: 0.0285 - acc: 0.9346 - val_loss: 0.0292 - val_acc: 0.9470\n",
      "Epoch 484/500\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0295 - acc: 0.9361 - val_loss: 0.0292 - val_acc: 0.9415\n",
      "Epoch 485/500\n",
      "8564/8564 [==============================] - 5s 616us/step - loss: 0.0302 - acc: 0.9354 - val_loss: 0.0254 - val_acc: 0.9492\n",
      "Epoch 486/500\n",
      "8564/8564 [==============================] - 5s 597us/step - loss: 0.0300 - acc: 0.9339 - val_loss: 0.0294 - val_acc: 0.9347\n",
      "Epoch 487/500\n",
      "8564/8564 [==============================] - 5s 586us/step - loss: 0.0298 - acc: 0.9352 - val_loss: 0.0242 - val_acc: 0.9488\n",
      "Epoch 488/500\n",
      "8564/8564 [==============================] - 5s 606us/step - loss: 0.0290 - acc: 0.9349 - val_loss: 0.0251 - val_acc: 0.9378\n",
      "Epoch 489/500\n",
      "8564/8564 [==============================] - 5s 605us/step - loss: 0.0284 - acc: 0.9378 - val_loss: 0.0342 - val_acc: 0.9072\n",
      "Epoch 490/500\n",
      "8564/8564 [==============================] - 5s 610us/step - loss: 0.0290 - acc: 0.9360 - val_loss: 0.0282 - val_acc: 0.9389\n",
      "Epoch 491/500\n",
      "8564/8564 [==============================] - 5s 617us/step - loss: 0.0300 - acc: 0.9330 - val_loss: 0.0272 - val_acc: 0.9490\n",
      "Epoch 492/500\n",
      "8564/8564 [==============================] - 5s 569us/step - loss: 0.0300 - acc: 0.9350 - val_loss: 0.0276 - val_acc: 0.9502\n",
      "Epoch 493/500\n",
      "8564/8564 [==============================] - 5s 598us/step - loss: 0.0307 - acc: 0.9335 - val_loss: 0.0315 - val_acc: 0.9291\n",
      "Epoch 494/500\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0300 - acc: 0.9348 - val_loss: 0.0240 - val_acc: 0.9499\n",
      "Epoch 495/500\n",
      "8564/8564 [==============================] - 5s 588us/step - loss: 0.0291 - acc: 0.9370 - val_loss: 0.0237 - val_acc: 0.9506\n",
      "Epoch 496/500\n",
      "8564/8564 [==============================] - 5s 571us/step - loss: 0.0280 - acc: 0.9361 - val_loss: 0.0278 - val_acc: 0.9475\n",
      "Epoch 497/500\n",
      "8564/8564 [==============================] - 5s 590us/step - loss: 0.0287 - acc: 0.9358 - val_loss: 0.0260 - val_acc: 0.9512\n",
      "Epoch 498/500\n",
      "8564/8564 [==============================] - 5s 572us/step - loss: 0.0296 - acc: 0.9328 - val_loss: 0.0376 - val_acc: 0.9078\n",
      "Epoch 499/500\n",
      "8564/8564 [==============================] - 5s 558us/step - loss: 0.0296 - acc: 0.9331 - val_loss: 0.0264 - val_acc: 0.9489\n",
      "Epoch 500/500\n",
      "8564/8564 [==============================] - 5s 582us/step - loss: 0.0289 - acc: 0.9367 - val_loss: 0.0270 - val_acc: 0.9395\n",
      "start training round 25\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 501/520\n",
      "8564/8564 [==============================] - 5s 596us/step - loss: 0.0272 - acc: 0.7231 - val_loss: 0.0270 - val_acc: 0.7254\n",
      "Epoch 502/520\n",
      "8564/8564 [==============================] - 5s 612us/step - loss: 0.0273 - acc: 0.7264 - val_loss: 0.0274 - val_acc: 0.7262\n",
      "Epoch 503/520\n",
      "8564/8564 [==============================] - 5s 570us/step - loss: 0.0270 - acc: 0.7320 - val_loss: 0.0269 - val_acc: 0.7286\n",
      "Epoch 504/520\n",
      "8564/8564 [==============================] - 5s 554us/step - loss: 0.0270 - acc: 0.7190 - val_loss: 0.0287 - val_acc: 0.7207\n",
      "Epoch 505/520\n",
      "8564/8564 [==============================] - 5s 566us/step - loss: 0.0271 - acc: 0.7170 - val_loss: 0.0271 - val_acc: 0.6086\n",
      "Epoch 506/520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 5s 568us/step - loss: 0.0270 - acc: 0.7173 - val_loss: 0.0267 - val_acc: 0.7238\n",
      "Epoch 507/520\n",
      "8564/8564 [==============================] - 5s 600us/step - loss: 0.0269 - acc: 0.6616 - val_loss: 0.0291 - val_acc: 0.5287\n",
      "Epoch 508/520\n",
      "8564/8564 [==============================] - 5s 557us/step - loss: 0.0269 - acc: 0.6590 - val_loss: 0.0272 - val_acc: 0.5923\n",
      "Epoch 509/520\n",
      "8564/8564 [==============================] - 5s 567us/step - loss: 0.0268 - acc: 0.6684 - val_loss: 0.0276 - val_acc: 0.7302\n",
      "Epoch 510/520\n",
      "8564/8564 [==============================] - 5s 573us/step - loss: 0.0269 - acc: 0.7057 - val_loss: 0.0275 - val_acc: 0.7230\n",
      "Epoch 511/520\n",
      "8564/8564 [==============================] - 5s 611us/step - loss: 0.0269 - acc: 0.6815 - val_loss: 0.0283 - val_acc: 0.5461\n",
      "Epoch 512/520\n",
      "8564/8564 [==============================] - 5s 557us/step - loss: 0.0271 - acc: 0.6563 - val_loss: 0.0267 - val_acc: 0.7044\n",
      "Epoch 513/520\n",
      "8564/8564 [==============================] - 5s 607us/step - loss: 0.0269 - acc: 0.7288 - val_loss: 0.0271 - val_acc: 0.6930\n",
      "Epoch 514/520\n",
      "8564/8564 [==============================] - 5s 579us/step - loss: 0.0272 - acc: 0.6658 - val_loss: 0.0268 - val_acc: 0.7194\n",
      "Epoch 515/520\n",
      "8564/8564 [==============================] - 5s 580us/step - loss: 0.0269 - acc: 0.7259 - val_loss: 0.0269 - val_acc: 0.7270\n",
      "Epoch 516/520\n",
      "8564/8564 [==============================] - 5s 596us/step - loss: 0.0270 - acc: 0.6630 - val_loss: 0.0273 - val_acc: 0.7208\n",
      "Epoch 517/520\n",
      "8564/8564 [==============================] - 5s 571us/step - loss: 0.0271 - acc: 0.6514 - val_loss: 0.0286 - val_acc: 0.7184\n",
      "Epoch 518/520\n",
      "8564/8564 [==============================] - 5s 582us/step - loss: 0.0268 - acc: 0.6805 - val_loss: 0.0278 - val_acc: 0.6038\n",
      "Epoch 519/520\n",
      "8564/8564 [==============================] - 5s 585us/step - loss: 0.0267 - acc: 0.6976 - val_loss: 0.0287 - val_acc: 0.6707\n",
      "Epoch 520/520\n",
      "8564/8564 [==============================] - 5s 575us/step - loss: 0.0273 - acc: 0.7115 - val_loss: 0.0279 - val_acc: 0.5610\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 501/520\n",
      "8564/8564 [==============================] - 5s 543us/step - loss: 0.0242 - acc: 0.8754 - val_loss: 0.0240 - val_acc: 0.8825\n",
      "Epoch 502/520\n",
      "8564/8564 [==============================] - 5s 529us/step - loss: 0.0241 - acc: 0.8821 - val_loss: 0.0252 - val_acc: 0.8827\n",
      "Epoch 503/520\n",
      "8564/8564 [==============================] - 5s 581us/step - loss: 0.0243 - acc: 0.8823 - val_loss: 0.0243 - val_acc: 0.8810\n",
      "Epoch 504/520\n",
      "8564/8564 [==============================] - 5s 564us/step - loss: 0.0240 - acc: 0.8819 - val_loss: 0.0249 - val_acc: 0.8818\n",
      "Epoch 505/520\n",
      "8564/8564 [==============================] - 5s 572us/step - loss: 0.0242 - acc: 0.8785 - val_loss: 0.0242 - val_acc: 0.8760\n",
      "Epoch 506/520\n",
      "8564/8564 [==============================] - 5s 569us/step - loss: 0.0241 - acc: 0.8800 - val_loss: 0.0240 - val_acc: 0.8835\n",
      "Epoch 507/520\n",
      "8564/8564 [==============================] - 5s 573us/step - loss: 0.0244 - acc: 0.8736 - val_loss: 0.0243 - val_acc: 0.8780\n",
      "Epoch 508/520\n",
      "8564/8564 [==============================] - 5s 576us/step - loss: 0.0242 - acc: 0.8747 - val_loss: 0.0247 - val_acc: 0.8736\n",
      "Epoch 509/520\n",
      "8564/8564 [==============================] - 5s 557us/step - loss: 0.0244 - acc: 0.8741 - val_loss: 0.0243 - val_acc: 0.8729\n",
      "Epoch 510/520\n",
      "8564/8564 [==============================] - 5s 563us/step - loss: 0.0241 - acc: 0.8761 - val_loss: 0.0246 - val_acc: 0.8712\n",
      "Epoch 511/520\n",
      "8564/8564 [==============================] - 5s 592us/step - loss: 0.0243 - acc: 0.8741 - val_loss: 0.0242 - val_acc: 0.8752\n",
      "Epoch 512/520\n",
      "8564/8564 [==============================] - 5s 554us/step - loss: 0.0242 - acc: 0.8749 - val_loss: 0.0240 - val_acc: 0.8762\n",
      "Epoch 513/520\n",
      "8564/8564 [==============================] - 5s 575us/step - loss: 0.0242 - acc: 0.8750 - val_loss: 0.0244 - val_acc: 0.8760\n",
      "Epoch 514/520\n",
      "8564/8564 [==============================] - 5s 566us/step - loss: 0.0240 - acc: 0.8764 - val_loss: 0.0247 - val_acc: 0.8688\n",
      "Epoch 515/520\n",
      "8564/8564 [==============================] - 5s 579us/step - loss: 0.0242 - acc: 0.8751 - val_loss: 0.0244 - val_acc: 0.8743\n",
      "Epoch 516/520\n",
      "8564/8564 [==============================] - 5s 594us/step - loss: 0.0241 - acc: 0.8751 - val_loss: 0.0239 - val_acc: 0.8766\n",
      "Epoch 517/520\n",
      "8564/8564 [==============================] - 5s 605us/step - loss: 0.0242 - acc: 0.8737 - val_loss: 0.0243 - val_acc: 0.8736\n",
      "Epoch 518/520\n",
      "8564/8564 [==============================] - 5s 586us/step - loss: 0.0241 - acc: 0.8756 - val_loss: 0.0243 - val_acc: 0.8756\n",
      "Epoch 519/520\n",
      "8564/8564 [==============================] - 5s 564us/step - loss: 0.0241 - acc: 0.8748 - val_loss: 0.0241 - val_acc: 0.8780\n",
      "Epoch 520/520\n",
      "8564/8564 [==============================] - 5s 579us/step - loss: 0.0242 - acc: 0.8743 - val_loss: 0.0244 - val_acc: 0.8741\n",
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 501/520\n",
      "8564/8564 [==============================] - 5s 566us/step - loss: 0.0258 - acc: 0.9827 - val_loss: 0.0217 - val_acc: 0.9864\n",
      "Epoch 502/520\n",
      "8564/8564 [==============================] - 5s 558us/step - loss: 0.0256 - acc: 0.9815 - val_loss: 0.0236 - val_acc: 0.9873\n",
      "Epoch 503/520\n",
      "8564/8564 [==============================] - 5s 570us/step - loss: 0.0251 - acc: 0.9825 - val_loss: 0.0260 - val_acc: 0.9829\n",
      "Epoch 504/520\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0245 - acc: 0.9841 - val_loss: 0.0215 - val_acc: 0.9844\n",
      "Epoch 505/520\n",
      "8564/8564 [==============================] - 5s 563us/step - loss: 0.0254 - acc: 0.9812 - val_loss: 0.0213 - val_acc: 0.9871\n",
      "Epoch 506/520\n",
      "8564/8564 [==============================] - 5s 566us/step - loss: 0.0265 - acc: 0.9837 - val_loss: 0.0257 - val_acc: 0.9866\n",
      "Epoch 507/520\n",
      "8564/8564 [==============================] - 5s 555us/step - loss: 0.0255 - acc: 0.9827 - val_loss: 0.0225 - val_acc: 0.9867\n",
      "Epoch 508/520\n",
      "8564/8564 [==============================] - 5s 584us/step - loss: 0.0261 - acc: 0.9817 - val_loss: 0.0249 - val_acc: 0.9878\n",
      "Epoch 509/520\n",
      "8564/8564 [==============================] - 5s 572us/step - loss: 0.0248 - acc: 0.9832 - val_loss: 0.0201 - val_acc: 0.9873\n",
      "Epoch 510/520\n",
      "8564/8564 [==============================] - 5s 556us/step - loss: 0.0264 - acc: 0.9841 - val_loss: 0.0237 - val_acc: 0.9817\n",
      "Epoch 511/520\n",
      "8564/8564 [==============================] - 5s 582us/step - loss: 0.0242 - acc: 0.9830 - val_loss: 0.0264 - val_acc: 0.9886\n",
      "Epoch 512/520\n",
      "8564/8564 [==============================] - 5s 551us/step - loss: 0.0261 - acc: 0.9812 - val_loss: 0.0210 - val_acc: 0.9867\n",
      "Epoch 513/520\n",
      "8564/8564 [==============================] - 5s 554us/step - loss: 0.0256 - acc: 0.9827 - val_loss: 0.0246 - val_acc: 0.9821\n",
      "Epoch 514/520\n",
      "8564/8564 [==============================] - 5s 555us/step - loss: 0.0247 - acc: 0.9826 - val_loss: 0.0265 - val_acc: 0.9863\n",
      "Epoch 515/520\n",
      "8564/8564 [==============================] - 5s 582us/step - loss: 0.0253 - acc: 0.9824 - val_loss: 0.0214 - val_acc: 0.9867\n",
      "Epoch 516/520\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0244 - acc: 0.9821 - val_loss: 0.0208 - val_acc: 0.9864\n",
      "Epoch 517/520\n",
      "8564/8564 [==============================] - 5s 565us/step - loss: 0.0254 - acc: 0.9817 - val_loss: 0.0251 - val_acc: 0.9815\n",
      "Epoch 518/520\n",
      "8564/8564 [==============================] - 5s 567us/step - loss: 0.0265 - acc: 0.9826 - val_loss: 0.0323 - val_acc: 0.9798\n",
      "Epoch 519/520\n",
      "8564/8564 [==============================] - 5s 532us/step - loss: 0.0252 - acc: 0.9823 - val_loss: 0.0295 - val_acc: 0.9878\n",
      "Epoch 520/520\n",
      "8564/8564 [==============================] - 5s 575us/step - loss: 0.0241 - acc: 0.9838 - val_loss: 0.0248 - val_acc: 0.9840\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 501/520\n",
      "8564/8564 [==============================] - 5s 554us/step - loss: 0.0304 - acc: 0.9339 - val_loss: 0.0279 - val_acc: 0.9489\n",
      "Epoch 502/520\n",
      "8564/8564 [==============================] - 5s 540us/step - loss: 0.0291 - acc: 0.9347 - val_loss: 0.0264 - val_acc: 0.9274\n",
      "Epoch 503/520\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0299 - acc: 0.9345 - val_loss: 0.0278 - val_acc: 0.9400\n",
      "Epoch 504/520\n",
      "8564/8564 [==============================] - 5s 572us/step - loss: 0.0301 - acc: 0.9339 - val_loss: 0.0270 - val_acc: 0.9392\n",
      "Epoch 505/520\n",
      "8564/8564 [==============================] - 5s 574us/step - loss: 0.0298 - acc: 0.9336 - val_loss: 0.0265 - val_acc: 0.9411\n",
      "Epoch 506/520\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0295 - acc: 0.9353 - val_loss: 0.0285 - val_acc: 0.9286\n",
      "Epoch 507/520\n",
      "8564/8564 [==============================] - 5s 577us/step - loss: 0.0285 - acc: 0.9367 - val_loss: 0.0249 - val_acc: 0.9426\n",
      "Epoch 508/520\n",
      "8564/8564 [==============================] - 5s 566us/step - loss: 0.0297 - acc: 0.9351 - val_loss: 0.0289 - val_acc: 0.9337\n",
      "Epoch 509/520\n",
      "8564/8564 [==============================] - 5s 571us/step - loss: 0.0302 - acc: 0.9356 - val_loss: 0.0258 - val_acc: 0.9411\n",
      "Epoch 510/520\n",
      "8564/8564 [==============================] - 5s 565us/step - loss: 0.0292 - acc: 0.9354 - val_loss: 0.0263 - val_acc: 0.9440\n",
      "Epoch 511/520\n",
      "8564/8564 [==============================] - 5s 552us/step - loss: 0.0303 - acc: 0.9361 - val_loss: 0.0256 - val_acc: 0.9493\n",
      "Epoch 512/520\n",
      "8564/8564 [==============================] - 5s 569us/step - loss: 0.0290 - acc: 0.9374 - val_loss: 0.0246 - val_acc: 0.9508\n",
      "Epoch 513/520\n",
      "8564/8564 [==============================] - 5s 559us/step - loss: 0.0288 - acc: 0.9374 - val_loss: 0.0263 - val_acc: 0.9442\n",
      "Epoch 514/520\n",
      "8564/8564 [==============================] - 5s 603us/step - loss: 0.0293 - acc: 0.9362 - val_loss: 0.0219 - val_acc: 0.9486\n",
      "Epoch 515/520\n",
      "8564/8564 [==============================] - 5s 565us/step - loss: 0.0295 - acc: 0.9344 - val_loss: 0.0241 - val_acc: 0.9474\n",
      "Epoch 516/520\n",
      "8564/8564 [==============================] - 5s 576us/step - loss: 0.0289 - acc: 0.9364 - val_loss: 0.0253 - val_acc: 0.9497\n",
      "Epoch 517/520\n",
      "8564/8564 [==============================] - 5s 574us/step - loss: 0.0292 - acc: 0.9357 - val_loss: 0.0291 - val_acc: 0.9413\n",
      "Epoch 518/520\n",
      "8564/8564 [==============================] - 5s 559us/step - loss: 0.0289 - acc: 0.9359 - val_loss: 0.0255 - val_acc: 0.9508\n",
      "Epoch 519/520\n",
      "8564/8564 [==============================] - 5s 547us/step - loss: 0.0296 - acc: 0.9346 - val_loss: 0.0247 - val_acc: 0.9510\n",
      "Epoch 520/520\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0288 - acc: 0.9345 - val_loss: 0.0258 - val_acc: 0.9431\n",
      "start training round 26\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 521/540\n",
      "8564/8564 [==============================] - 5s 592us/step - loss: 0.0269 - acc: 0.7054 - val_loss: 0.0268 - val_acc: 0.7242\n",
      "Epoch 522/540\n",
      "8564/8564 [==============================] - 5s 571us/step - loss: 0.0268 - acc: 0.6675 - val_loss: 0.0269 - val_acc: 0.7261\n",
      "Epoch 523/540\n",
      "8564/8564 [==============================] - 5s 591us/step - loss: 0.0271 - acc: 0.6554 - val_loss: 0.0276 - val_acc: 0.7235\n",
      "Epoch 524/540\n",
      "8564/8564 [==============================] - 5s 569us/step - loss: 0.0268 - acc: 0.6684 - val_loss: 0.0273 - val_acc: 0.7226\n",
      "Epoch 525/540\n",
      "8564/8564 [==============================] - 5s 550us/step - loss: 0.0268 - acc: 0.6657 - val_loss: 0.0274 - val_acc: 0.7027\n",
      "Epoch 526/540\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0270 - acc: 0.6927 - val_loss: 0.0270 - val_acc: 0.7304\n",
      "Epoch 527/540\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0271 - acc: 0.7313 - val_loss: 0.0273 - val_acc: 0.7309\n",
      "Epoch 528/540\n",
      "8564/8564 [==============================] - 5s 565us/step - loss: 0.0269 - acc: 0.7311 - val_loss: 0.0277 - val_acc: 0.7170\n",
      "Epoch 529/540\n",
      "8564/8564 [==============================] - 5s 589us/step - loss: 0.0270 - acc: 0.7264 - val_loss: 0.0266 - val_acc: 0.7263\n",
      "Epoch 530/540\n",
      "8564/8564 [==============================] - 5s 563us/step - loss: 0.0269 - acc: 0.7270 - val_loss: 0.0281 - val_acc: 0.7202\n",
      "Epoch 531/540\n",
      "8564/8564 [==============================] - 5s 568us/step - loss: 0.0268 - acc: 0.7171 - val_loss: 0.0270 - val_acc: 0.6529\n",
      "Epoch 532/540\n",
      "8564/8564 [==============================] - 5s 542us/step - loss: 0.0267 - acc: 0.7106 - val_loss: 0.0281 - val_acc: 0.7172\n",
      "Epoch 533/540\n",
      "8564/8564 [==============================] - 5s 553us/step - loss: 0.0270 - acc: 0.7275 - val_loss: 0.0279 - val_acc: 0.7167\n",
      "Epoch 534/540\n",
      "8564/8564 [==============================] - 5s 575us/step - loss: 0.0270 - acc: 0.7316 - val_loss: 0.0266 - val_acc: 0.7320\n",
      "Epoch 535/540\n",
      "8564/8564 [==============================] - 5s 581us/step - loss: 0.0268 - acc: 0.7238 - val_loss: 0.0284 - val_acc: 0.6164\n",
      "Epoch 536/540\n",
      "8564/8564 [==============================] - 5s 576us/step - loss: 0.0272 - acc: 0.7203 - val_loss: 0.0282 - val_acc: 0.7224\n",
      "Epoch 537/540\n",
      "8564/8564 [==============================] - 5s 577us/step - loss: 0.0268 - acc: 0.6873 - val_loss: 0.0267 - val_acc: 0.7277\n",
      "Epoch 538/540\n",
      "8564/8564 [==============================] - 5s 569us/step - loss: 0.0267 - acc: 0.6835 - val_loss: 0.0276 - val_acc: 0.7103\n",
      "Epoch 539/540\n",
      "8564/8564 [==============================] - 5s 551us/step - loss: 0.0269 - acc: 0.7240 - val_loss: 0.0268 - val_acc: 0.7325\n",
      "Epoch 540/540\n",
      "8564/8564 [==============================] - 5s 555us/step - loss: 0.0266 - acc: 0.6851 - val_loss: 0.0277 - val_acc: 0.6012\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 521/540\n",
      "8564/8564 [==============================] - 5s 581us/step - loss: 0.0242 - acc: 0.8751 - val_loss: 0.0240 - val_acc: 0.8787\n",
      "Epoch 522/540\n",
      "8564/8564 [==============================] - 5s 599us/step - loss: 0.0240 - acc: 0.8825 - val_loss: 0.0256 - val_acc: 0.8827\n",
      "Epoch 523/540\n",
      "8564/8564 [==============================] - 5s 553us/step - loss: 0.0242 - acc: 0.8825 - val_loss: 0.0236 - val_acc: 0.8827\n",
      "Epoch 524/540\n",
      "8564/8564 [==============================] - 5s 572us/step - loss: 0.0241 - acc: 0.8760 - val_loss: 0.0250 - val_acc: 0.8686\n",
      "Epoch 525/540\n",
      "8564/8564 [==============================] - 5s 588us/step - loss: 0.0245 - acc: 0.8735 - val_loss: 0.0239 - val_acc: 0.8745\n",
      "Epoch 526/540\n",
      "8564/8564 [==============================] - 5s 602us/step - loss: 0.0241 - acc: 0.8752 - val_loss: 0.0243 - val_acc: 0.8723\n",
      "Epoch 527/540\n",
      "8564/8564 [==============================] - 5s 598us/step - loss: 0.0240 - acc: 0.8747 - val_loss: 0.0252 - val_acc: 0.8642\n",
      "Epoch 528/540\n",
      "8564/8564 [==============================] - 5s 573us/step - loss: 0.0243 - acc: 0.8751 - val_loss: 0.0244 - val_acc: 0.8724\n",
      "Epoch 529/540\n",
      "8564/8564 [==============================] - 5s 599us/step - loss: 0.0243 - acc: 0.8756 - val_loss: 0.0240 - val_acc: 0.8788\n",
      "Epoch 530/540\n",
      "8564/8564 [==============================] - 5s 595us/step - loss: 0.0240 - acc: 0.8827 - val_loss: 0.0241 - val_acc: 0.8834\n",
      "Epoch 531/540\n",
      "8564/8564 [==============================] - 5s 582us/step - loss: 0.0242 - acc: 0.8831 - val_loss: 0.0242 - val_acc: 0.8836\n",
      "Epoch 532/540\n",
      "8564/8564 [==============================] - 5s 565us/step - loss: 0.0241 - acc: 0.8834 - val_loss: 0.0238 - val_acc: 0.8831\n",
      "Epoch 533/540\n",
      "8564/8564 [==============================] - 5s 543us/step - loss: 0.0240 - acc: 0.8833 - val_loss: 0.0248 - val_acc: 0.8816\n",
      "Epoch 534/540\n",
      "8564/8564 [==============================] - 5s 589us/step - loss: 0.0241 - acc: 0.8830 - val_loss: 0.0247 - val_acc: 0.8830\n",
      "Epoch 535/540\n",
      "8564/8564 [==============================] - 5s 576us/step - loss: 0.0241 - acc: 0.8828 - val_loss: 0.0244 - val_acc: 0.8828\n",
      "Epoch 536/540\n",
      "8564/8564 [==============================] - 5s 561us/step - loss: 0.0239 - acc: 0.8811 - val_loss: 0.0234 - val_acc: 0.8817\n",
      "Epoch 537/540\n",
      "8564/8564 [==============================] - 5s 599us/step - loss: 0.0244 - acc: 0.8746 - val_loss: 0.0237 - val_acc: 0.8829\n",
      "Epoch 538/540\n",
      "8564/8564 [==============================] - 5s 576us/step - loss: 0.0242 - acc: 0.8747 - val_loss: 0.0248 - val_acc: 0.8760\n",
      "Epoch 539/540\n",
      "8564/8564 [==============================] - 5s 563us/step - loss: 0.0242 - acc: 0.8747 - val_loss: 0.0241 - val_acc: 0.8788\n",
      "Epoch 540/540\n",
      "8564/8564 [==============================] - 5s 578us/step - loss: 0.0237 - acc: 0.8786 - val_loss: 0.0238 - val_acc: 0.8840\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 521/540\n",
      "8564/8564 [==============================] - 5s 574us/step - loss: 0.0251 - acc: 0.9826 - val_loss: 0.0213 - val_acc: 0.9801\n",
      "Epoch 522/540\n",
      "8564/8564 [==============================] - 5s 572us/step - loss: 0.0255 - acc: 0.9842 - val_loss: 0.0205 - val_acc: 0.9881\n",
      "Epoch 523/540\n",
      "8564/8564 [==============================] - 5s 570us/step - loss: 0.0247 - acc: 0.9840 - val_loss: 0.0202 - val_acc: 0.9872\n",
      "Epoch 524/540\n",
      "8564/8564 [==============================] - 5s 550us/step - loss: 0.0252 - acc: 0.9838 - val_loss: 0.0236 - val_acc: 0.9866\n",
      "Epoch 525/540\n",
      "8564/8564 [==============================] - 5s 556us/step - loss: 0.0246 - acc: 0.9835 - val_loss: 0.0218 - val_acc: 0.9878\n",
      "Epoch 526/540\n",
      "8564/8564 [==============================] - 5s 544us/step - loss: 0.0251 - acc: 0.9821 - val_loss: 0.0186 - val_acc: 0.9858\n",
      "Epoch 527/540\n",
      "8564/8564 [==============================] - 5s 566us/step - loss: 0.0236 - acc: 0.9839 - val_loss: 0.0217 - val_acc: 0.9857\n",
      "Epoch 528/540\n",
      "8564/8564 [==============================] - 5s 556us/step - loss: 0.0253 - acc: 0.9823 - val_loss: 0.0288 - val_acc: 0.9816\n",
      "Epoch 529/540\n",
      "8564/8564 [==============================] - 5s 580us/step - loss: 0.0244 - acc: 0.9832 - val_loss: 0.0229 - val_acc: 0.9876\n",
      "Epoch 530/540\n",
      "8564/8564 [==============================] - 5s 592us/step - loss: 0.0249 - acc: 0.9827 - val_loss: 0.0287 - val_acc: 0.9880\n",
      "Epoch 531/540\n",
      "8564/8564 [==============================] - 5s 595us/step - loss: 0.0253 - acc: 0.9833 - val_loss: 0.0187 - val_acc: 0.9867\n",
      "Epoch 532/540\n",
      "8564/8564 [==============================] - 5s 579us/step - loss: 0.0244 - acc: 0.9834 - val_loss: 0.0242 - val_acc: 0.9874\n",
      "Epoch 533/540\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0251 - acc: 0.9825 - val_loss: 0.0263 - val_acc: 0.9818\n",
      "Epoch 534/540\n",
      "8564/8564 [==============================] - 5s 594us/step - loss: 0.0261 - acc: 0.9812 - val_loss: 0.0231 - val_acc: 0.9865\n",
      "Epoch 535/540\n",
      "8564/8564 [==============================] - 5s 606us/step - loss: 0.0272 - acc: 0.9827 - val_loss: 0.0253 - val_acc: 0.9827\n",
      "Epoch 536/540\n",
      "8564/8564 [==============================] - 5s 577us/step - loss: 0.0251 - acc: 0.9842 - val_loss: 0.0229 - val_acc: 0.9841\n",
      "Epoch 537/540\n",
      "8564/8564 [==============================] - 5s 602us/step - loss: 0.0255 - acc: 0.9826 - val_loss: 0.0263 - val_acc: 0.9889\n",
      "Epoch 538/540\n",
      "8564/8564 [==============================] - 5s 554us/step - loss: 0.0254 - acc: 0.9824 - val_loss: 0.0230 - val_acc: 0.9857\n",
      "Epoch 539/540\n",
      "8564/8564 [==============================] - 5s 544us/step - loss: 0.0240 - acc: 0.9829 - val_loss: 0.0216 - val_acc: 0.9871\n",
      "Epoch 540/540\n",
      "8564/8564 [==============================] - 5s 564us/step - loss: 0.0260 - acc: 0.9826 - val_loss: 0.0223 - val_acc: 0.9838\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 521/540\n",
      "8564/8564 [==============================] - 5s 581us/step - loss: 0.0297 - acc: 0.9344 - val_loss: 0.0303 - val_acc: 0.9516\n",
      "Epoch 522/540\n",
      "8564/8564 [==============================] - 5s 542us/step - loss: 0.0288 - acc: 0.9381 - val_loss: 0.0262 - val_acc: 0.9472\n",
      "Epoch 523/540\n",
      "8564/8564 [==============================] - 5s 556us/step - loss: 0.0303 - acc: 0.9359 - val_loss: 0.0265 - val_acc: 0.9491\n",
      "Epoch 524/540\n",
      "8564/8564 [==============================] - 5s 559us/step - loss: 0.0282 - acc: 0.9391 - val_loss: 0.0301 - val_acc: 0.9412\n",
      "Epoch 525/540\n",
      "8564/8564 [==============================] - 5s 588us/step - loss: 0.0287 - acc: 0.9357 - val_loss: 0.0245 - val_acc: 0.9452\n",
      "Epoch 526/540\n",
      "8564/8564 [==============================] - 5s 572us/step - loss: 0.0292 - acc: 0.9351 - val_loss: 0.0274 - val_acc: 0.9457\n",
      "Epoch 527/540\n",
      "8564/8564 [==============================] - 5s 598us/step - loss: 0.0277 - acc: 0.9394 - val_loss: 0.0250 - val_acc: 0.9466\n",
      "Epoch 528/540\n",
      "8564/8564 [==============================] - 5s 572us/step - loss: 0.0289 - acc: 0.9356 - val_loss: 0.0242 - val_acc: 0.9439\n",
      "Epoch 529/540\n",
      "8564/8564 [==============================] - 5s 578us/step - loss: 0.0293 - acc: 0.9364 - val_loss: 0.0265 - val_acc: 0.9508\n",
      "Epoch 530/540\n",
      "8564/8564 [==============================] - 5s 559us/step - loss: 0.0288 - acc: 0.9368 - val_loss: 0.0230 - val_acc: 0.9474\n",
      "Epoch 531/540\n",
      "8564/8564 [==============================] - 5s 578us/step - loss: 0.0279 - acc: 0.9408 - val_loss: 0.0290 - val_acc: 0.9403\n",
      "Epoch 532/540\n",
      "8564/8564 [==============================] - 5s 559us/step - loss: 0.0290 - acc: 0.9369 - val_loss: 0.0270 - val_acc: 0.9501\n",
      "Epoch 533/540\n",
      "8564/8564 [==============================] - 5s 576us/step - loss: 0.0295 - acc: 0.9354 - val_loss: 0.0289 - val_acc: 0.9244\n",
      "Epoch 534/540\n",
      "8564/8564 [==============================] - 5s 574us/step - loss: 0.0286 - acc: 0.9366 - val_loss: 0.0243 - val_acc: 0.9511\n",
      "Epoch 535/540\n",
      "8564/8564 [==============================] - 5s 569us/step - loss: 0.0287 - acc: 0.9364 - val_loss: 0.0263 - val_acc: 0.9397\n",
      "Epoch 536/540\n",
      "8564/8564 [==============================] - 5s 566us/step - loss: 0.0291 - acc: 0.9359 - val_loss: 0.0297 - val_acc: 0.9413\n",
      "Epoch 537/540\n",
      "8564/8564 [==============================] - 5s 561us/step - loss: 0.0279 - acc: 0.9399 - val_loss: 0.0255 - val_acc: 0.9444\n",
      "Epoch 538/540\n",
      "8564/8564 [==============================] - 5s 573us/step - loss: 0.0289 - acc: 0.9362 - val_loss: 0.0282 - val_acc: 0.9415\n",
      "Epoch 539/540\n",
      "8564/8564 [==============================] - 5s 572us/step - loss: 0.0285 - acc: 0.9381 - val_loss: 0.0269 - val_acc: 0.9511\n",
      "Epoch 540/540\n",
      "8564/8564 [==============================] - 5s 559us/step - loss: 0.0287 - acc: 0.9368 - val_loss: 0.0270 - val_acc: 0.9408\n",
      "start training round 27\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 541/560\n",
      "8564/8564 [==============================] - 5s 590us/step - loss: 0.0269 - acc: 0.7075 - val_loss: 0.0274 - val_acc: 0.7326\n",
      "Epoch 542/560\n",
      "8564/8564 [==============================] - 5s 576us/step - loss: 0.0266 - acc: 0.7104 - val_loss: 0.0280 - val_acc: 0.7314\n",
      "Epoch 543/560\n",
      "8564/8564 [==============================] - 5s 545us/step - loss: 0.0266 - acc: 0.6947 - val_loss: 0.0282 - val_acc: 0.7237\n",
      "Epoch 544/560\n",
      "8564/8564 [==============================] - 5s 563us/step - loss: 0.0265 - acc: 0.6766 - val_loss: 0.0269 - val_acc: 0.7212\n",
      "Epoch 545/560\n",
      "8564/8564 [==============================] - 5s 602us/step - loss: 0.0263 - acc: 0.6717 - val_loss: 0.0266 - val_acc: 0.7302\n",
      "Epoch 546/560\n",
      "8564/8564 [==============================] - 5s 575us/step - loss: 0.0269 - acc: 0.7326 - val_loss: 0.0290 - val_acc: 0.6010\n",
      "Epoch 547/560\n",
      "8564/8564 [==============================] - 5s 568us/step - loss: 0.0266 - acc: 0.7274 - val_loss: 0.0270 - val_acc: 0.7331\n",
      "Epoch 548/560\n",
      "8564/8564 [==============================] - 5s 569us/step - loss: 0.0264 - acc: 0.6850 - val_loss: 0.0270 - val_acc: 0.7284\n",
      "Epoch 549/560\n",
      "8564/8564 [==============================] - 5s 560us/step - loss: 0.0266 - acc: 0.6568 - val_loss: 0.0268 - val_acc: 0.7289\n",
      "Epoch 550/560\n",
      "8564/8564 [==============================] - 5s 546us/step - loss: 0.0267 - acc: 0.6599 - val_loss: 0.0276 - val_acc: 0.7248\n",
      "Epoch 551/560\n",
      "8564/8564 [==============================] - 5s 566us/step - loss: 0.0265 - acc: 0.6656 - val_loss: 0.0276 - val_acc: 0.7215\n",
      "Epoch 552/560\n",
      "8564/8564 [==============================] - 5s 568us/step - loss: 0.0267 - acc: 0.6571 - val_loss: 0.0269 - val_acc: 0.7257\n",
      "Epoch 553/560\n",
      "8564/8564 [==============================] - 5s 568us/step - loss: 0.0265 - acc: 0.6536 - val_loss: 0.0272 - val_acc: 0.7264\n",
      "Epoch 554/560\n",
      "8564/8564 [==============================] - 5s 564us/step - loss: 0.0265 - acc: 0.6544 - val_loss: 0.0274 - val_acc: 0.7266\n",
      "Epoch 555/560\n",
      "8564/8564 [==============================] - 5s 576us/step - loss: 0.0267 - acc: 0.6512 - val_loss: 0.0260 - val_acc: 0.7306\n",
      "Epoch 556/560\n",
      "8564/8564 [==============================] - 5s 579us/step - loss: 0.0266 - acc: 0.7309 - val_loss: 0.0264 - val_acc: 0.7331\n",
      "Epoch 557/560\n",
      "8564/8564 [==============================] - 5s 562us/step - loss: 0.0265 - acc: 0.6691 - val_loss: 0.0265 - val_acc: 0.7305\n",
      "Epoch 558/560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 5s 563us/step - loss: 0.0268 - acc: 0.7280 - val_loss: 0.0276 - val_acc: 0.7311\n",
      "Epoch 559/560\n",
      "8564/8564 [==============================] - 5s 578us/step - loss: 0.0262 - acc: 0.6821 - val_loss: 0.0262 - val_acc: 0.7336\n",
      "Epoch 560/560\n",
      "8564/8564 [==============================] - 5s 547us/step - loss: 0.0265 - acc: 0.6610 - val_loss: 0.0273 - val_acc: 0.7208\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 541/560\n",
      "8564/8564 [==============================] - 5s 580us/step - loss: 0.0241 - acc: 0.8767 - val_loss: 0.0239 - val_acc: 0.8802\n",
      "Epoch 542/560\n",
      "8564/8564 [==============================] - 5s 541us/step - loss: 0.0243 - acc: 0.8745 - val_loss: 0.0245 - val_acc: 0.8767\n",
      "Epoch 543/560\n",
      "8564/8564 [==============================] - 5s 583us/step - loss: 0.0241 - acc: 0.8750 - val_loss: 0.0249 - val_acc: 0.8750\n",
      "Epoch 544/560\n",
      "8564/8564 [==============================] - 5s 578us/step - loss: 0.0241 - acc: 0.8748 - val_loss: 0.0242 - val_acc: 0.8796\n",
      "Epoch 545/560\n",
      "8564/8564 [==============================] - 5s 548us/step - loss: 0.0240 - acc: 0.8769 - val_loss: 0.0242 - val_acc: 0.8792\n",
      "Epoch 546/560\n",
      "8564/8564 [==============================] - 5s 568us/step - loss: 0.0242 - acc: 0.8760 - val_loss: 0.0237 - val_acc: 0.8808\n",
      "Epoch 547/560\n",
      "8564/8564 [==============================] - 5s 573us/step - loss: 0.0237 - acc: 0.8834 - val_loss: 0.0243 - val_acc: 0.8834\n",
      "Epoch 548/560\n",
      "8564/8564 [==============================] - 5s 569us/step - loss: 0.0239 - acc: 0.8835 - val_loss: 0.0249 - val_acc: 0.8841\n",
      "Epoch 549/560\n",
      "8564/8564 [==============================] - 5s 569us/step - loss: 0.0239 - acc: 0.8832 - val_loss: 0.0237 - val_acc: 0.8806\n",
      "Epoch 550/560\n",
      "8564/8564 [==============================] - 5s 580us/step - loss: 0.0237 - acc: 0.8839 - val_loss: 0.0241 - val_acc: 0.8839\n",
      "Epoch 551/560\n",
      "8564/8564 [==============================] - 5s 570us/step - loss: 0.0240 - acc: 0.8841 - val_loss: 0.0244 - val_acc: 0.8842\n",
      "Epoch 552/560\n",
      "8564/8564 [==============================] - 5s 596us/step - loss: 0.0239 - acc: 0.8832 - val_loss: 0.0255 - val_acc: 0.8839\n",
      "Epoch 553/560\n",
      "8564/8564 [==============================] - 5s 585us/step - loss: 0.0241 - acc: 0.8835 - val_loss: 0.0248 - val_acc: 0.8822\n",
      "Epoch 554/560\n",
      "8564/8564 [==============================] - 5s 580us/step - loss: 0.0239 - acc: 0.8839 - val_loss: 0.0248 - val_acc: 0.8833\n",
      "Epoch 555/560\n",
      "8564/8564 [==============================] - 5s 561us/step - loss: 0.0240 - acc: 0.8837 - val_loss: 0.0250 - val_acc: 0.8823\n",
      "Epoch 556/560\n",
      "8564/8564 [==============================] - 5s 559us/step - loss: 0.0240 - acc: 0.8839 - val_loss: 0.0248 - val_acc: 0.8829\n",
      "Epoch 557/560\n",
      "8564/8564 [==============================] - 5s 546us/step - loss: 0.0240 - acc: 0.8838 - val_loss: 0.0242 - val_acc: 0.8830\n",
      "Epoch 558/560\n",
      "8564/8564 [==============================] - 5s 534us/step - loss: 0.0240 - acc: 0.8831 - val_loss: 0.0244 - val_acc: 0.8831\n",
      "Epoch 559/560\n",
      "8564/8564 [==============================] - 5s 581us/step - loss: 0.0238 - acc: 0.8821 - val_loss: 0.0249 - val_acc: 0.8744\n",
      "Epoch 560/560\n",
      "8564/8564 [==============================] - 5s 558us/step - loss: 0.0240 - acc: 0.8762 - val_loss: 0.0236 - val_acc: 0.8834\n",
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 541/560\n",
      "8564/8564 [==============================] - 5s 577us/step - loss: 0.0255 - acc: 0.9820 - val_loss: 0.0325 - val_acc: 0.9801\n",
      "Epoch 542/560\n",
      "8564/8564 [==============================] - 5s 539us/step - loss: 0.0241 - acc: 0.9836 - val_loss: 0.0245 - val_acc: 0.9845\n",
      "Epoch 543/560\n",
      "8564/8564 [==============================] - 5s 584us/step - loss: 0.0249 - acc: 0.9828 - val_loss: 0.0259 - val_acc: 0.9854\n",
      "Epoch 544/560\n",
      "8564/8564 [==============================] - 5s 549us/step - loss: 0.0257 - acc: 0.9842 - val_loss: 0.0220 - val_acc: 0.9819\n",
      "Epoch 545/560\n",
      "8564/8564 [==============================] - 5s 579us/step - loss: 0.0251 - acc: 0.9836 - val_loss: 0.0236 - val_acc: 0.9844\n",
      "Epoch 546/560\n",
      "8564/8564 [==============================] - 5s 568us/step - loss: 0.0248 - acc: 0.9818 - val_loss: 0.0269 - val_acc: 0.9848\n",
      "Epoch 547/560\n",
      "8564/8564 [==============================] - 5s 568us/step - loss: 0.0246 - acc: 0.9844 - val_loss: 0.0235 - val_acc: 0.9840\n",
      "Epoch 548/560\n",
      "8564/8564 [==============================] - 5s 557us/step - loss: 0.0268 - acc: 0.9814 - val_loss: 0.0236 - val_acc: 0.9842\n",
      "Epoch 549/560\n",
      "8564/8564 [==============================] - 5s 568us/step - loss: 0.0237 - acc: 0.9850 - val_loss: 0.0221 - val_acc: 0.9823\n",
      "Epoch 550/560\n",
      "8564/8564 [==============================] - 5s 561us/step - loss: 0.0252 - acc: 0.9840 - val_loss: 0.0228 - val_acc: 0.9868\n",
      "Epoch 551/560\n",
      "8564/8564 [==============================] - 5s 547us/step - loss: 0.0252 - acc: 0.9843 - val_loss: 0.0227 - val_acc: 0.9871\n",
      "Epoch 552/560\n",
      "8564/8564 [==============================] - 5s 555us/step - loss: 0.0262 - acc: 0.9837 - val_loss: 0.0232 - val_acc: 0.9868\n",
      "Epoch 553/560\n",
      "8564/8564 [==============================] - 5s 556us/step - loss: 0.0253 - acc: 0.9833 - val_loss: 0.0202 - val_acc: 0.9878\n",
      "Epoch 554/560\n",
      "8564/8564 [==============================] - 5s 566us/step - loss: 0.0250 - acc: 0.9836 - val_loss: 0.0232 - val_acc: 0.9867\n",
      "Epoch 555/560\n",
      "8564/8564 [==============================] - 5s 564us/step - loss: 0.0265 - acc: 0.9827 - val_loss: 0.0253 - val_acc: 0.9895\n",
      "Epoch 556/560\n",
      "8564/8564 [==============================] - 5s 581us/step - loss: 0.0267 - acc: 0.9794 - val_loss: 0.0226 - val_acc: 0.9871\n",
      "Epoch 557/560\n",
      "8564/8564 [==============================] - 5s 586us/step - loss: 0.0259 - acc: 0.9823 - val_loss: 0.0223 - val_acc: 0.9855\n",
      "Epoch 558/560\n",
      "8564/8564 [==============================] - 5s 558us/step - loss: 0.0259 - acc: 0.9809 - val_loss: 0.0290 - val_acc: 0.9857\n",
      "Epoch 559/560\n",
      "8564/8564 [==============================] - 4s 440us/step - loss: 0.0240 - acc: 0.9826 - val_loss: 0.0258 - val_acc: 0.9878\n",
      "Epoch 560/560\n",
      "8564/8564 [==============================] - 4s 433us/step - loss: 0.0254 - acc: 0.9840 - val_loss: 0.0269 - val_acc: 0.9854\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 541/560\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0286 - acc: 0.9367 - val_loss: 0.0272 - val_acc: 0.9487\n",
      "Epoch 542/560\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0290 - acc: 0.9358 - val_loss: 0.0264 - val_acc: 0.9347\n",
      "Epoch 543/560\n",
      "8564/8564 [==============================] - 4s 434us/step - loss: 0.0284 - acc: 0.9367 - val_loss: 0.0260 - val_acc: 0.9518\n",
      "Epoch 544/560\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0283 - acc: 0.9381 - val_loss: 0.0265 - val_acc: 0.9438\n",
      "Epoch 545/560\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0293 - acc: 0.9343 - val_loss: 0.0235 - val_acc: 0.9507\n",
      "Epoch 546/560\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0292 - acc: 0.9365 - val_loss: 0.0256 - val_acc: 0.9521\n",
      "Epoch 547/560\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0281 - acc: 0.9390 - val_loss: 0.0308 - val_acc: 0.9357\n",
      "Epoch 548/560\n",
      "8564/8564 [==============================] - 4s 440us/step - loss: 0.0280 - acc: 0.9406 - val_loss: 0.0230 - val_acc: 0.9509\n",
      "Epoch 549/560\n",
      "8564/8564 [==============================] - 4s 442us/step - loss: 0.0275 - acc: 0.9393 - val_loss: 0.0251 - val_acc: 0.9437\n",
      "Epoch 550/560\n",
      "8564/8564 [==============================] - 4s 440us/step - loss: 0.0281 - acc: 0.9382 - val_loss: 0.0283 - val_acc: 0.9423\n",
      "Epoch 551/560\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0301 - acc: 0.9353 - val_loss: 0.0248 - val_acc: 0.9447\n",
      "Epoch 552/560\n",
      "8564/8564 [==============================] - 4s 435us/step - loss: 0.0291 - acc: 0.9363 - val_loss: 0.0251 - val_acc: 0.9485\n",
      "Epoch 553/560\n",
      "8564/8564 [==============================] - 4s 437us/step - loss: 0.0294 - acc: 0.9371 - val_loss: 0.0290 - val_acc: 0.9369\n",
      "Epoch 554/560\n",
      "8564/8564 [==============================] - 4s 442us/step - loss: 0.0287 - acc: 0.9366 - val_loss: 0.0264 - val_acc: 0.9403\n",
      "Epoch 555/560\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 4s 440us/step - loss: 0.0288 - acc: 0.9358 - val_loss: 0.0286 - val_acc: 0.9353\n",
      "Epoch 556/560\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0291 - acc: 0.9371 - val_loss: 0.0286 - val_acc: 0.9406\n",
      "Epoch 557/560\n",
      "8564/8564 [==============================] - 4s 442us/step - loss: 0.0282 - acc: 0.9394 - val_loss: 0.0333 - val_acc: 0.9501\n",
      "Epoch 558/560\n",
      "8564/8564 [==============================] - 4s 435us/step - loss: 0.0272 - acc: 0.9392 - val_loss: 0.0267 - val_acc: 0.9373\n",
      "Epoch 559/560\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0293 - acc: 0.9387 - val_loss: 0.0306 - val_acc: 0.9451\n",
      "Epoch 560/560\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0287 - acc: 0.9353 - val_loss: 0.0273 - val_acc: 0.9461\n",
      "start training round 28\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 561/580\n",
      "8564/8564 [==============================] - 4s 429us/step - loss: 0.0264 - acc: 0.6728 - val_loss: 0.0263 - val_acc: 0.7289\n",
      "Epoch 562/580\n",
      "8564/8564 [==============================] - 4s 435us/step - loss: 0.0265 - acc: 0.7267 - val_loss: 0.0270 - val_acc: 0.7356\n",
      "Epoch 563/580\n",
      "8564/8564 [==============================] - 4s 429us/step - loss: 0.0268 - acc: 0.7342 - val_loss: 0.0266 - val_acc: 0.7335\n",
      "Epoch 564/580\n",
      "8564/8564 [==============================] - 4s 433us/step - loss: 0.0268 - acc: 0.7283 - val_loss: 0.0273 - val_acc: 0.7344\n",
      "Epoch 565/580\n",
      "8564/8564 [==============================] - 4s 435us/step - loss: 0.0262 - acc: 0.7271 - val_loss: 0.0267 - val_acc: 0.7294\n",
      "Epoch 566/580\n",
      "8564/8564 [==============================] - 4s 437us/step - loss: 0.0268 - acc: 0.7308 - val_loss: 0.0264 - val_acc: 0.7339\n",
      "Epoch 567/580\n",
      "8564/8564 [==============================] - 4s 440us/step - loss: 0.0264 - acc: 0.6656 - val_loss: 0.0263 - val_acc: 0.7336\n",
      "Epoch 568/580\n",
      "8564/8564 [==============================] - 4s 436us/step - loss: 0.0265 - acc: 0.7318 - val_loss: 0.0272 - val_acc: 0.7304\n",
      "Epoch 569/580\n",
      "8564/8564 [==============================] - 4s 437us/step - loss: 0.0266 - acc: 0.7329 - val_loss: 0.0265 - val_acc: 0.7339\n",
      "Epoch 570/580\n",
      "8564/8564 [==============================] - 4s 440us/step - loss: 0.0264 - acc: 0.7082 - val_loss: 0.0272 - val_acc: 0.7258\n",
      "Epoch 571/580\n",
      "8564/8564 [==============================] - 4s 424us/step - loss: 0.0263 - acc: 0.6610 - val_loss: 0.0261 - val_acc: 0.7304\n",
      "Epoch 572/580\n",
      "8564/8564 [==============================] - 4s 430us/step - loss: 0.0264 - acc: 0.7249 - val_loss: 0.0261 - val_acc: 0.7352\n",
      "Epoch 573/580\n",
      "8564/8564 [==============================] - 4s 430us/step - loss: 0.0266 - acc: 0.7334 - val_loss: 0.0260 - val_acc: 0.7344\n",
      "Epoch 574/580\n",
      "8564/8564 [==============================] - 4s 433us/step - loss: 0.0264 - acc: 0.7348 - val_loss: 0.0268 - val_acc: 0.7360\n",
      "Epoch 575/580\n",
      "8564/8564 [==============================] - 4s 434us/step - loss: 0.0267 - acc: 0.7218 - val_loss: 0.0274 - val_acc: 0.5688\n",
      "Epoch 576/580\n",
      "8564/8564 [==============================] - 4s 434us/step - loss: 0.0265 - acc: 0.6558 - val_loss: 0.0265 - val_acc: 0.6010\n",
      "Epoch 577/580\n",
      "8564/8564 [==============================] - 4s 431us/step - loss: 0.0265 - acc: 0.6515 - val_loss: 0.0273 - val_acc: 0.5617\n",
      "Epoch 578/580\n",
      "8564/8564 [==============================] - 4s 434us/step - loss: 0.0263 - acc: 0.6678 - val_loss: 0.0275 - val_acc: 0.7305\n",
      "Epoch 579/580\n",
      "8564/8564 [==============================] - 4s 430us/step - loss: 0.0263 - acc: 0.6867 - val_loss: 0.0266 - val_acc: 0.7277\n",
      "Epoch 580/580\n",
      "8564/8564 [==============================] - 4s 430us/step - loss: 0.0264 - acc: 0.7013 - val_loss: 0.0268 - val_acc: 0.7251\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 561/580\n",
      "8564/8564 [==============================] - 4s 431us/step - loss: 0.0237 - acc: 0.8778 - val_loss: 0.0242 - val_acc: 0.8803\n",
      "Epoch 562/580\n",
      "8564/8564 [==============================] - 4s 435us/step - loss: 0.0241 - acc: 0.8751 - val_loss: 0.0238 - val_acc: 0.8806\n",
      "Epoch 563/580\n",
      "8564/8564 [==============================] - 4s 429us/step - loss: 0.0241 - acc: 0.8760 - val_loss: 0.0234 - val_acc: 0.8819\n",
      "Epoch 564/580\n",
      "8564/8564 [==============================] - 4s 431us/step - loss: 0.0239 - acc: 0.8772 - val_loss: 0.0238 - val_acc: 0.8802\n",
      "Epoch 565/580\n",
      "8564/8564 [==============================] - 4s 434us/step - loss: 0.0239 - acc: 0.8766 - val_loss: 0.0237 - val_acc: 0.8804\n",
      "Epoch 566/580\n",
      "8564/8564 [==============================] - 4s 432us/step - loss: 0.0239 - acc: 0.8767 - val_loss: 0.0241 - val_acc: 0.8795\n",
      "Epoch 567/580\n",
      "8564/8564 [==============================] - 4s 429us/step - loss: 0.0245 - acc: 0.8797 - val_loss: 0.0239 - val_acc: 0.8847\n",
      "Epoch 568/580\n",
      "8564/8564 [==============================] - 4s 430us/step - loss: 0.0237 - acc: 0.8837 - val_loss: 0.0244 - val_acc: 0.8806\n",
      "Epoch 569/580\n",
      "8564/8564 [==============================] - 4s 434us/step - loss: 0.0239 - acc: 0.8760 - val_loss: 0.0244 - val_acc: 0.8773\n",
      "Epoch 570/580\n",
      "8564/8564 [==============================] - 4s 432us/step - loss: 0.0239 - acc: 0.8831 - val_loss: 0.0235 - val_acc: 0.8849\n",
      "Epoch 571/580\n",
      "8564/8564 [==============================] - 4s 433us/step - loss: 0.0238 - acc: 0.8845 - val_loss: 0.0234 - val_acc: 0.8850\n",
      "Epoch 572/580\n",
      "8564/8564 [==============================] - 4s 436us/step - loss: 0.0237 - acc: 0.8831 - val_loss: 0.0243 - val_acc: 0.8845\n",
      "Epoch 573/580\n",
      "8564/8564 [==============================] - 4s 432us/step - loss: 0.0239 - acc: 0.8845 - val_loss: 0.0246 - val_acc: 0.8853\n",
      "Epoch 574/580\n",
      "8564/8564 [==============================] - 4s 433us/step - loss: 0.0240 - acc: 0.8840 - val_loss: 0.0237 - val_acc: 0.8841\n",
      "Epoch 575/580\n",
      "8564/8564 [==============================] - 4s 431us/step - loss: 0.0238 - acc: 0.8843 - val_loss: 0.0247 - val_acc: 0.8843\n",
      "Epoch 576/580\n",
      "8564/8564 [==============================] - 4s 431us/step - loss: 0.0240 - acc: 0.8841 - val_loss: 0.0238 - val_acc: 0.8854\n",
      "Epoch 577/580\n",
      "8564/8564 [==============================] - 4s 426us/step - loss: 0.0239 - acc: 0.8850 - val_loss: 0.0241 - val_acc: 0.8844\n",
      "Epoch 578/580\n",
      "8564/8564 [==============================] - 4s 436us/step - loss: 0.0237 - acc: 0.8812 - val_loss: 0.0239 - val_acc: 0.8787\n",
      "Epoch 579/580\n",
      "8564/8564 [==============================] - 4s 444us/step - loss: 0.0237 - acc: 0.8767 - val_loss: 0.0251 - val_acc: 0.8763\n",
      "Epoch 580/580\n",
      "8564/8564 [==============================] - 4s 444us/step - loss: 0.0239 - acc: 0.8766 - val_loss: 0.0232 - val_acc: 0.8823\n",
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 561/580\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0242 - acc: 0.9837 - val_loss: 0.0299 - val_acc: 0.9844\n",
      "Epoch 562/580\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0247 - acc: 0.9836 - val_loss: 0.0227 - val_acc: 0.9827\n",
      "Epoch 563/580\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0250 - acc: 0.9826 - val_loss: 0.0246 - val_acc: 0.9893\n",
      "Epoch 564/580\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0246 - acc: 0.9831 - val_loss: 0.0252 - val_acc: 0.9864\n",
      "Epoch 565/580\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0257 - acc: 0.9840 - val_loss: 0.0245 - val_acc: 0.9841\n",
      "Epoch 566/580\n",
      "8564/8564 [==============================] - 4s 442us/step - loss: 0.0246 - acc: 0.9810 - val_loss: 0.0260 - val_acc: 0.9864\n",
      "Epoch 567/580\n",
      "8564/8564 [==============================] - 4s 437us/step - loss: 0.0251 - acc: 0.9830 - val_loss: 0.0265 - val_acc: 0.9877\n",
      "Epoch 568/580\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0269 - acc: 0.9826 - val_loss: 0.0227 - val_acc: 0.9851\n",
      "Epoch 569/580\n",
      "8564/8564 [==============================] - 4s 433us/step - loss: 0.0254 - acc: 0.9800 - val_loss: 0.0209 - val_acc: 0.9862\n",
      "Epoch 570/580\n",
      "8564/8564 [==============================] - 4s 435us/step - loss: 0.0253 - acc: 0.9831 - val_loss: 0.0196 - val_acc: 0.9842\n",
      "Epoch 571/580\n",
      "8564/8564 [==============================] - 4s 440us/step - loss: 0.0244 - acc: 0.9830 - val_loss: 0.0250 - val_acc: 0.9864\n",
      "Epoch 572/580\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0258 - acc: 0.9835 - val_loss: 0.0309 - val_acc: 0.9828\n",
      "Epoch 573/580\n",
      "8564/8564 [==============================] - 4s 437us/step - loss: 0.0241 - acc: 0.9827 - val_loss: 0.0230 - val_acc: 0.9846\n",
      "Epoch 574/580\n",
      "8564/8564 [==============================] - 4s 437us/step - loss: 0.0246 - acc: 0.9841 - val_loss: 0.0295 - val_acc: 0.9870\n",
      "Epoch 575/580\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0253 - acc: 0.9826 - val_loss: 0.0200 - val_acc: 0.9884\n",
      "Epoch 576/580\n",
      "8564/8564 [==============================] - 4s 434us/step - loss: 0.0241 - acc: 0.9839 - val_loss: 0.0213 - val_acc: 0.9840\n",
      "Epoch 577/580\n",
      "8564/8564 [==============================] - 4s 435us/step - loss: 0.0243 - acc: 0.9832 - val_loss: 0.0209 - val_acc: 0.9863\n",
      "Epoch 578/580\n",
      "8564/8564 [==============================] - 4s 442us/step - loss: 0.0238 - acc: 0.9838 - val_loss: 0.0214 - val_acc: 0.9880\n",
      "Epoch 579/580\n",
      "8564/8564 [==============================] - 4s 437us/step - loss: 0.0251 - acc: 0.9829 - val_loss: 0.0221 - val_acc: 0.9855\n",
      "Epoch 580/580\n",
      "8564/8564 [==============================] - 4s 435us/step - loss: 0.0244 - acc: 0.9827 - val_loss: 0.0279 - val_acc: 0.9858\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 561/580\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0287 - acc: 0.9355 - val_loss: 0.0266 - val_acc: 0.9407\n",
      "Epoch 562/580\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0276 - acc: 0.9392 - val_loss: 0.0269 - val_acc: 0.9538\n",
      "Epoch 563/580\n",
      "8564/8564 [==============================] - 4s 442us/step - loss: 0.0282 - acc: 0.9370 - val_loss: 0.0262 - val_acc: 0.9440\n",
      "Epoch 564/580\n",
      "8564/8564 [==============================] - 4s 444us/step - loss: 0.0299 - acc: 0.9353 - val_loss: 0.0259 - val_acc: 0.9545\n",
      "Epoch 565/580\n",
      "8564/8564 [==============================] - 4s 440us/step - loss: 0.0299 - acc: 0.9362 - val_loss: 0.0296 - val_acc: 0.9392\n",
      "Epoch 566/580\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0277 - acc: 0.9394 - val_loss: 0.0261 - val_acc: 0.9424\n",
      "Epoch 567/580\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0286 - acc: 0.9362 - val_loss: 0.0329 - val_acc: 0.9179\n",
      "Epoch 568/580\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0292 - acc: 0.9372 - val_loss: 0.0295 - val_acc: 0.9306\n",
      "Epoch 569/580\n",
      "8564/8564 [==============================] - 4s 447us/step - loss: 0.0295 - acc: 0.9360 - val_loss: 0.0282 - val_acc: 0.9349\n",
      "Epoch 570/580\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0287 - acc: 0.9370 - val_loss: 0.0288 - val_acc: 0.9383\n",
      "Epoch 571/580\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0283 - acc: 0.9371 - val_loss: 0.0241 - val_acc: 0.9521\n",
      "Epoch 572/580\n",
      "8564/8564 [==============================] - 4s 481us/step - loss: 0.0278 - acc: 0.9396 - val_loss: 0.0291 - val_acc: 0.9422\n",
      "Epoch 573/580\n",
      "8564/8564 [==============================] - 4s 485us/step - loss: 0.0286 - acc: 0.9376 - val_loss: 0.0249 - val_acc: 0.9362\n",
      "Epoch 574/580\n",
      "8564/8564 [==============================] - 4s 483us/step - loss: 0.0272 - acc: 0.9389 - val_loss: 0.0275 - val_acc: 0.9528\n",
      "Epoch 575/580\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0277 - acc: 0.9386 - val_loss: 0.0264 - val_acc: 0.9393\n",
      "Epoch 576/580\n",
      "8564/8564 [==============================] - 4s 444us/step - loss: 0.0293 - acc: 0.9345 - val_loss: 0.0240 - val_acc: 0.9498\n",
      "Epoch 577/580\n",
      "8564/8564 [==============================] - 4s 447us/step - loss: 0.0282 - acc: 0.9374 - val_loss: 0.0254 - val_acc: 0.9429\n",
      "Epoch 578/580\n",
      "8564/8564 [==============================] - 4s 440us/step - loss: 0.0290 - acc: 0.9382 - val_loss: 0.0256 - val_acc: 0.9520\n",
      "Epoch 579/580\n",
      "8564/8564 [==============================] - 4s 434us/step - loss: 0.0285 - acc: 0.9391 - val_loss: 0.0259 - val_acc: 0.9516\n",
      "Epoch 580/580\n",
      "8564/8564 [==============================] - 4s 437us/step - loss: 0.0286 - acc: 0.9368 - val_loss: 0.0225 - val_acc: 0.9548\n",
      "start training round 29\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 581/600\n",
      "8564/8564 [==============================] - 4s 434us/step - loss: 0.0262 - acc: 0.6736 - val_loss: 0.0261 - val_acc: 0.7004\n",
      "Epoch 582/600\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0262 - acc: 0.7320 - val_loss: 0.0258 - val_acc: 0.7330\n",
      "Epoch 583/600\n",
      "8564/8564 [==============================] - 4s 431us/step - loss: 0.0263 - acc: 0.6607 - val_loss: 0.0263 - val_acc: 0.6625\n",
      "Epoch 584/600\n",
      "8564/8564 [==============================] - 4s 435us/step - loss: 0.0261 - acc: 0.6859 - val_loss: 0.0262 - val_acc: 0.7322\n",
      "Epoch 585/600\n",
      "8564/8564 [==============================] - 4s 434us/step - loss: 0.0263 - acc: 0.7354 - val_loss: 0.0260 - val_acc: 0.7353\n",
      "Epoch 586/600\n",
      "8564/8564 [==============================] - 4s 427us/step - loss: 0.0261 - acc: 0.7037 - val_loss: 0.0262 - val_acc: 0.6836\n",
      "Epoch 587/600\n",
      "8564/8564 [==============================] - 4s 435us/step - loss: 0.0266 - acc: 0.6523 - val_loss: 0.0269 - val_acc: 0.5646\n",
      "Epoch 588/600\n",
      "8564/8564 [==============================] - 4s 426us/step - loss: 0.0265 - acc: 0.6463 - val_loss: 0.0275 - val_acc: 0.5540\n",
      "Epoch 589/600\n",
      "8564/8564 [==============================] - 4s 433us/step - loss: 0.0264 - acc: 0.6533 - val_loss: 0.0260 - val_acc: 0.6420\n",
      "Epoch 590/600\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0264 - acc: 0.6561 - val_loss: 0.0282 - val_acc: 0.5486\n",
      "Epoch 591/600\n",
      "8564/8564 [==============================] - 4s 429us/step - loss: 0.0266 - acc: 0.6514 - val_loss: 0.0271 - val_acc: 0.5730\n",
      "Epoch 592/600\n",
      "8564/8564 [==============================] - 4s 423us/step - loss: 0.0268 - acc: 0.6444 - val_loss: 0.0263 - val_acc: 0.7008\n",
      "Epoch 593/600\n",
      "8564/8564 [==============================] - 4s 428us/step - loss: 0.0259 - acc: 0.6743 - val_loss: 0.0257 - val_acc: 0.7330\n",
      "Epoch 594/600\n",
      "8564/8564 [==============================] - 4s 433us/step - loss: 0.0262 - acc: 0.6691 - val_loss: 0.0273 - val_acc: 0.5484\n",
      "Epoch 595/600\n",
      "8564/8564 [==============================] - 4s 429us/step - loss: 0.0265 - acc: 0.6516 - val_loss: 0.0265 - val_acc: 0.5846\n",
      "Epoch 596/600\n",
      "8564/8564 [==============================] - 4s 431us/step - loss: 0.0263 - acc: 0.6573 - val_loss: 0.0282 - val_acc: 0.5438\n",
      "Epoch 597/600\n",
      "8564/8564 [==============================] - 4s 430us/step - loss: 0.0260 - acc: 0.7273 - val_loss: 0.0273 - val_acc: 0.7363\n",
      "Epoch 598/600\n",
      "8564/8564 [==============================] - 4s 432us/step - loss: 0.0263 - acc: 0.7347 - val_loss: 0.0268 - val_acc: 0.7363\n",
      "Epoch 599/600\n",
      "8564/8564 [==============================] - 4s 429us/step - loss: 0.0265 - acc: 0.7344 - val_loss: 0.0266 - val_acc: 0.7368\n",
      "Epoch 600/600\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0264 - acc: 0.7373 - val_loss: 0.0263 - val_acc: 0.7318\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 581/600\n",
      "8564/8564 [==============================] - 4s 431us/step - loss: 0.0238 - acc: 0.8774 - val_loss: 0.0243 - val_acc: 0.8800\n",
      "Epoch 582/600\n",
      "8564/8564 [==============================] - 4s 430us/step - loss: 0.0240 - acc: 0.8764 - val_loss: 0.0232 - val_acc: 0.8824\n",
      "Epoch 583/600\n",
      "8564/8564 [==============================] - 4s 423us/step - loss: 0.0237 - acc: 0.8774 - val_loss: 0.0241 - val_acc: 0.8796\n",
      "Epoch 584/600\n",
      "8564/8564 [==============================] - 4s 430us/step - loss: 0.0239 - acc: 0.8752 - val_loss: 0.0249 - val_acc: 0.8753\n",
      "Epoch 585/600\n",
      "8564/8564 [==============================] - 4s 429us/step - loss: 0.0239 - acc: 0.8762 - val_loss: 0.0234 - val_acc: 0.8826\n",
      "Epoch 586/600\n",
      "8564/8564 [==============================] - 4s 431us/step - loss: 0.0239 - acc: 0.8763 - val_loss: 0.0237 - val_acc: 0.8806\n",
      "Epoch 587/600\n",
      "8564/8564 [==============================] - 4s 428us/step - loss: 0.0240 - acc: 0.8768 - val_loss: 0.0238 - val_acc: 0.8805\n",
      "Epoch 588/600\n",
      "8564/8564 [==============================] - 4s 433us/step - loss: 0.0238 - acc: 0.8771 - val_loss: 0.0243 - val_acc: 0.8789\n",
      "Epoch 589/600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 4s 431us/step - loss: 0.0238 - acc: 0.8766 - val_loss: 0.0243 - val_acc: 0.8782\n",
      "Epoch 590/600\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0237 - acc: 0.8792 - val_loss: 0.0242 - val_acc: 0.8853\n",
      "Epoch 591/600\n",
      "8564/8564 [==============================] - 4s 429us/step - loss: 0.0239 - acc: 0.8852 - val_loss: 0.0236 - val_acc: 0.8854\n",
      "Epoch 592/600\n",
      "8564/8564 [==============================] - 4s 429us/step - loss: 0.0237 - acc: 0.8839 - val_loss: 0.0241 - val_acc: 0.8827\n",
      "Epoch 593/600\n",
      "8564/8564 [==============================] - 4s 431us/step - loss: 0.0235 - acc: 0.8849 - val_loss: 0.0239 - val_acc: 0.8853\n",
      "Epoch 594/600\n",
      "8564/8564 [==============================] - 4s 428us/step - loss: 0.0237 - acc: 0.8854 - val_loss: 0.0246 - val_acc: 0.8851\n",
      "Epoch 595/600\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0240 - acc: 0.8851 - val_loss: 0.0233 - val_acc: 0.8839\n",
      "Epoch 596/600\n",
      "8564/8564 [==============================] - 4s 422us/step - loss: 0.0238 - acc: 0.8847 - val_loss: 0.0245 - val_acc: 0.8854\n",
      "Epoch 597/600\n",
      "8564/8564 [==============================] - 4s 428us/step - loss: 0.0237 - acc: 0.8846 - val_loss: 0.0243 - val_acc: 0.8844\n",
      "Epoch 598/600\n",
      "8564/8564 [==============================] - 4s 433us/step - loss: 0.0238 - acc: 0.8847 - val_loss: 0.0247 - val_acc: 0.8856\n",
      "Epoch 599/600\n",
      "8564/8564 [==============================] - 4s 426us/step - loss: 0.0238 - acc: 0.8849 - val_loss: 0.0242 - val_acc: 0.8857\n",
      "Epoch 600/600\n",
      "8564/8564 [==============================] - 4s 430us/step - loss: 0.0238 - acc: 0.8846 - val_loss: 0.0248 - val_acc: 0.8845\n",
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 581/600\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0249 - acc: 0.9839 - val_loss: 0.0267 - val_acc: 0.9835\n",
      "Epoch 582/600\n",
      "8564/8564 [==============================] - 4s 436us/step - loss: 0.0242 - acc: 0.9830 - val_loss: 0.0231 - val_acc: 0.9884\n",
      "Epoch 583/600\n",
      "8564/8564 [==============================] - 4s 433us/step - loss: 0.0254 - acc: 0.9839 - val_loss: 0.0188 - val_acc: 0.9889\n",
      "Epoch 584/600\n",
      "8564/8564 [==============================] - 4s 436us/step - loss: 0.0244 - acc: 0.9833 - val_loss: 0.0291 - val_acc: 0.9827\n",
      "Epoch 585/600\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0258 - acc: 0.9838 - val_loss: 0.0230 - val_acc: 0.9882\n",
      "Epoch 586/600\n",
      "8564/8564 [==============================] - 4s 435us/step - loss: 0.0244 - acc: 0.9839 - val_loss: 0.0223 - val_acc: 0.9865\n",
      "Epoch 587/600\n",
      "8564/8564 [==============================] - 4s 430us/step - loss: 0.0259 - acc: 0.9827 - val_loss: 0.0250 - val_acc: 0.9862\n",
      "Epoch 588/600\n",
      "8564/8564 [==============================] - 4s 426us/step - loss: 0.0265 - acc: 0.9838 - val_loss: 0.0248 - val_acc: 0.9829\n",
      "Epoch 589/600\n",
      "8564/8564 [==============================] - 4s 431us/step - loss: 0.0234 - acc: 0.9835 - val_loss: 0.0270 - val_acc: 0.9872\n",
      "Epoch 590/600\n",
      "8564/8564 [==============================] - 4s 429us/step - loss: 0.0253 - acc: 0.9830 - val_loss: 0.0214 - val_acc: 0.9881\n",
      "Epoch 591/600\n",
      "8564/8564 [==============================] - 4s 433us/step - loss: 0.0241 - acc: 0.9826 - val_loss: 0.0216 - val_acc: 0.9872\n",
      "Epoch 592/600\n",
      "8564/8564 [==============================] - 4s 442us/step - loss: 0.0239 - acc: 0.9843 - val_loss: 0.0224 - val_acc: 0.9869\n",
      "Epoch 593/600\n",
      "8564/8564 [==============================] - 4s 437us/step - loss: 0.0252 - acc: 0.9841 - val_loss: 0.0227 - val_acc: 0.9884\n",
      "Epoch 594/600\n",
      "8564/8564 [==============================] - 4s 436us/step - loss: 0.0233 - acc: 0.9838 - val_loss: 0.0251 - val_acc: 0.9811\n",
      "Epoch 595/600\n",
      "8564/8564 [==============================] - 4s 433us/step - loss: 0.0240 - acc: 0.9842 - val_loss: 0.0243 - val_acc: 0.9848\n",
      "Epoch 596/600\n",
      "8564/8564 [==============================] - 4s 437us/step - loss: 0.0251 - acc: 0.9842 - val_loss: 0.0264 - val_acc: 0.9880\n",
      "Epoch 597/600\n",
      "8564/8564 [==============================] - 4s 434us/step - loss: 0.0263 - acc: 0.9835 - val_loss: 0.0243 - val_acc: 0.9890\n",
      "Epoch 598/600\n",
      "8564/8564 [==============================] - 4s 431us/step - loss: 0.0257 - acc: 0.9824 - val_loss: 0.0251 - val_acc: 0.9883\n",
      "Epoch 599/600\n",
      "8564/8564 [==============================] - 4s 434us/step - loss: 0.0245 - acc: 0.9828 - val_loss: 0.0245 - val_acc: 0.9794\n",
      "Epoch 600/600\n",
      "8564/8564 [==============================] - 4s 429us/step - loss: 0.0260 - acc: 0.9798 - val_loss: 0.0227 - val_acc: 0.9852\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 581/600\n",
      "8564/8564 [==============================] - 4s 444us/step - loss: 0.0278 - acc: 0.9408 - val_loss: 0.0270 - val_acc: 0.9467\n",
      "Epoch 582/600\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0287 - acc: 0.9364 - val_loss: 0.0253 - val_acc: 0.9436\n",
      "Epoch 583/600\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0278 - acc: 0.9388 - val_loss: 0.0249 - val_acc: 0.9508\n",
      "Epoch 584/600\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0284 - acc: 0.9370 - val_loss: 0.0276 - val_acc: 0.9305\n",
      "Epoch 585/600\n",
      "8564/8564 [==============================] - 4s 444us/step - loss: 0.0292 - acc: 0.9371 - val_loss: 0.0267 - val_acc: 0.9461\n",
      "Epoch 586/600\n",
      "8564/8564 [==============================] - 4s 440us/step - loss: 0.0290 - acc: 0.9362 - val_loss: 0.0270 - val_acc: 0.9327\n",
      "Epoch 587/600\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0284 - acc: 0.9368 - val_loss: 0.0251 - val_acc: 0.9481\n",
      "Epoch 588/600\n",
      "8564/8564 [==============================] - 4s 436us/step - loss: 0.0278 - acc: 0.9401 - val_loss: 0.0286 - val_acc: 0.9485\n",
      "Epoch 589/600\n",
      "8564/8564 [==============================] - 4s 436us/step - loss: 0.0286 - acc: 0.9361 - val_loss: 0.0269 - val_acc: 0.9493\n",
      "Epoch 590/600\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0286 - acc: 0.9368 - val_loss: 0.0236 - val_acc: 0.9519\n",
      "Epoch 591/600\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0284 - acc: 0.9394 - val_loss: 0.0261 - val_acc: 0.9480\n",
      "Epoch 592/600\n",
      "8564/8564 [==============================] - 4s 442us/step - loss: 0.0286 - acc: 0.9389 - val_loss: 0.0343 - val_acc: 0.9307\n",
      "Epoch 593/600\n",
      "8564/8564 [==============================] - 4s 439us/step - loss: 0.0284 - acc: 0.9380 - val_loss: 0.0287 - val_acc: 0.9382\n",
      "Epoch 594/600\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0283 - acc: 0.9381 - val_loss: 0.0312 - val_acc: 0.9249\n",
      "Epoch 595/600\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0275 - acc: 0.9389 - val_loss: 0.0256 - val_acc: 0.9434\n",
      "Epoch 596/600\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0284 - acc: 0.9375 - val_loss: 0.0243 - val_acc: 0.9542\n",
      "Epoch 597/600\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0293 - acc: 0.9342 - val_loss: 0.0344 - val_acc: 0.9253\n",
      "Epoch 598/600\n",
      "8564/8564 [==============================] - 4s 437us/step - loss: 0.0286 - acc: 0.9390 - val_loss: 0.0249 - val_acc: 0.9334\n",
      "Epoch 599/600\n",
      "8564/8564 [==============================] - 4s 440us/step - loss: 0.0283 - acc: 0.9375 - val_loss: 0.0253 - val_acc: 0.9517\n",
      "Epoch 600/600\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0261 - acc: 0.9428 - val_loss: 0.0241 - val_acc: 0.9431\n",
      "start training round 30\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 601/620\n",
      "8564/8564 [==============================] - 4s 434us/step - loss: 0.0261 - acc: 0.6951 - val_loss: 0.0279 - val_acc: 0.7254\n",
      "Epoch 602/620\n",
      "8564/8564 [==============================] - 4s 432us/step - loss: 0.0265 - acc: 0.7298 - val_loss: 0.0269 - val_acc: 0.7371\n",
      "Epoch 603/620\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0260 - acc: 0.7270 - val_loss: 0.0264 - val_acc: 0.7359\n",
      "Epoch 604/620\n",
      "8564/8564 [==============================] - 4s 431us/step - loss: 0.0261 - acc: 0.7352 - val_loss: 0.0267 - val_acc: 0.7383\n",
      "Epoch 605/620\n",
      "8564/8564 [==============================] - 4s 430us/step - loss: 0.0258 - acc: 0.6851 - val_loss: 0.0277 - val_acc: 0.5485\n",
      "Epoch 606/620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 4s 434us/step - loss: 0.0262 - acc: 0.6632 - val_loss: 0.0258 - val_acc: 0.7177\n",
      "Epoch 607/620\n",
      "8564/8564 [==============================] - 4s 431us/step - loss: 0.0258 - acc: 0.7316 - val_loss: 0.0260 - val_acc: 0.7242\n",
      "Epoch 608/620\n",
      "8564/8564 [==============================] - 4s 437us/step - loss: 0.0262 - acc: 0.6542 - val_loss: 0.0264 - val_acc: 0.6508\n",
      "Epoch 609/620\n",
      "8564/8564 [==============================] - 4s 435us/step - loss: 0.0259 - acc: 0.6804 - val_loss: 0.0268 - val_acc: 0.7291\n",
      "Epoch 610/620\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0261 - acc: 0.7379 - val_loss: 0.0265 - val_acc: 0.7372\n",
      "Epoch 611/620\n",
      "8564/8564 [==============================] - 4s 467us/step - loss: 0.0265 - acc: 0.7383 - val_loss: 0.0258 - val_acc: 0.7387\n",
      "Epoch 612/620\n",
      "8564/8564 [==============================] - 4s 464us/step - loss: 0.0258 - acc: 0.6807 - val_loss: 0.0266 - val_acc: 0.7281\n",
      "Epoch 613/620\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0260 - acc: 0.7219 - val_loss: 0.0267 - val_acc: 0.7368\n",
      "Epoch 614/620\n",
      "8564/8564 [==============================] - 4s 444us/step - loss: 0.0261 - acc: 0.7148 - val_loss: 0.0256 - val_acc: 0.7200\n",
      "Epoch 615/620\n",
      "8564/8564 [==============================] - 4s 433us/step - loss: 0.0258 - acc: 0.7062 - val_loss: 0.0267 - val_acc: 0.7381\n",
      "Epoch 616/620\n",
      "8564/8564 [==============================] - 4s 429us/step - loss: 0.0261 - acc: 0.6812 - val_loss: 0.0264 - val_acc: 0.7311\n",
      "Epoch 617/620\n",
      "8564/8564 [==============================] - 4s 430us/step - loss: 0.0263 - acc: 0.7338 - val_loss: 0.0257 - val_acc: 0.7365\n",
      "Epoch 618/620\n",
      "8564/8564 [==============================] - 4s 431us/step - loss: 0.0257 - acc: 0.6861 - val_loss: 0.0258 - val_acc: 0.7354\n",
      "Epoch 619/620\n",
      "8564/8564 [==============================] - 4s 433us/step - loss: 0.0259 - acc: 0.7078 - val_loss: 0.0264 - val_acc: 0.7288\n",
      "Epoch 620/620\n",
      "8564/8564 [==============================] - 4s 436us/step - loss: 0.0262 - acc: 0.6850 - val_loss: 0.0261 - val_acc: 0.5959\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 601/620\n",
      "8564/8564 [==============================] - 4s 431us/step - loss: 0.0237 - acc: 0.8850 - val_loss: 0.0236 - val_acc: 0.8834\n",
      "Epoch 602/620\n",
      "8564/8564 [==============================] - 4s 429us/step - loss: 0.0236 - acc: 0.8848 - val_loss: 0.0242 - val_acc: 0.8847\n",
      "Epoch 603/620\n",
      "8564/8564 [==============================] - 4s 430us/step - loss: 0.0237 - acc: 0.8850 - val_loss: 0.0241 - val_acc: 0.8821\n",
      "Epoch 604/620\n",
      "8564/8564 [==============================] - 4s 427us/step - loss: 0.0237 - acc: 0.8855 - val_loss: 0.0243 - val_acc: 0.8832\n",
      "Epoch 605/620\n",
      "8564/8564 [==============================] - 4s 433us/step - loss: 0.0241 - acc: 0.8835 - val_loss: 0.0247 - val_acc: 0.8782\n",
      "Epoch 606/620\n",
      "8564/8564 [==============================] - 4s 430us/step - loss: 0.0240 - acc: 0.8766 - val_loss: 0.0241 - val_acc: 0.8727\n",
      "Epoch 607/620\n",
      "8564/8564 [==============================] - 4s 440us/step - loss: 0.0238 - acc: 0.8766 - val_loss: 0.0240 - val_acc: 0.8740\n",
      "Epoch 608/620\n",
      "8564/8564 [==============================] - 4s 433us/step - loss: 0.0238 - acc: 0.8772 - val_loss: 0.0232 - val_acc: 0.8826\n",
      "Epoch 609/620\n",
      "8564/8564 [==============================] - 4s 437us/step - loss: 0.0236 - acc: 0.8782 - val_loss: 0.0252 - val_acc: 0.8624\n",
      "Epoch 610/620\n",
      "8564/8564 [==============================] - 4s 430us/step - loss: 0.0238 - acc: 0.8764 - val_loss: 0.0239 - val_acc: 0.8768\n",
      "Epoch 611/620\n",
      "8564/8564 [==============================] - 4s 431us/step - loss: 0.0238 - acc: 0.8771 - val_loss: 0.0235 - val_acc: 0.8788\n",
      "Epoch 612/620\n",
      "8564/8564 [==============================] - 4s 429us/step - loss: 0.0238 - acc: 0.8777 - val_loss: 0.0244 - val_acc: 0.8657\n",
      "Epoch 613/620\n",
      "8564/8564 [==============================] - 4s 425us/step - loss: 0.0238 - acc: 0.8767 - val_loss: 0.0238 - val_acc: 0.8723\n",
      "Epoch 614/620\n",
      "8564/8564 [==============================] - 4s 424us/step - loss: 0.0239 - acc: 0.8753 - val_loss: 0.0229 - val_acc: 0.8822\n",
      "Epoch 615/620\n",
      "8564/8564 [==============================] - 4s 426us/step - loss: 0.0236 - acc: 0.8788 - val_loss: 0.0241 - val_acc: 0.8746\n",
      "Epoch 616/620\n",
      "8564/8564 [==============================] - 4s 429us/step - loss: 0.0237 - acc: 0.8781 - val_loss: 0.0237 - val_acc: 0.8782\n",
      "Epoch 617/620\n",
      "8564/8564 [==============================] - 4s 436us/step - loss: 0.0234 - acc: 0.8796 - val_loss: 0.0235 - val_acc: 0.8801\n",
      "Epoch 618/620\n",
      "8564/8564 [==============================] - 4s 432us/step - loss: 0.0235 - acc: 0.8834 - val_loss: 0.0246 - val_acc: 0.8865\n",
      "Epoch 619/620\n",
      "8564/8564 [==============================] - 4s 427us/step - loss: 0.0235 - acc: 0.8859 - val_loss: 0.0233 - val_acc: 0.8856\n",
      "Epoch 620/620\n",
      "8564/8564 [==============================] - 4s 432us/step - loss: 0.0236 - acc: 0.8825 - val_loss: 0.0247 - val_acc: 0.8651\n",
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 601/620\n",
      "8564/8564 [==============================] - 4s 436us/step - loss: 0.0248 - acc: 0.9818 - val_loss: 0.0226 - val_acc: 0.9886\n",
      "Epoch 602/620\n",
      "8564/8564 [==============================] - 4s 426us/step - loss: 0.0229 - acc: 0.9841 - val_loss: 0.0259 - val_acc: 0.9856\n",
      "Epoch 603/620\n",
      "8564/8564 [==============================] - 4s 430us/step - loss: 0.0236 - acc: 0.9832 - val_loss: 0.0231 - val_acc: 0.9818\n",
      "Epoch 604/620\n",
      "8564/8564 [==============================] - 4s 428us/step - loss: 0.0236 - acc: 0.9841 - val_loss: 0.0258 - val_acc: 0.9837\n",
      "Epoch 605/620\n",
      "8564/8564 [==============================] - 4s 428us/step - loss: 0.0238 - acc: 0.9825 - val_loss: 0.0197 - val_acc: 0.9877\n",
      "Epoch 606/620\n",
      "8564/8564 [==============================] - 4s 432us/step - loss: 0.0256 - acc: 0.9833 - val_loss: 0.0267 - val_acc: 0.9872\n",
      "Epoch 607/620\n",
      "8564/8564 [==============================] - 4s 431us/step - loss: 0.0256 - acc: 0.9815 - val_loss: 0.0229 - val_acc: 0.9869\n",
      "Epoch 608/620\n",
      "8564/8564 [==============================] - 4s 425us/step - loss: 0.0245 - acc: 0.9845 - val_loss: 0.0238 - val_acc: 0.9790\n",
      "Epoch 609/620\n",
      "8564/8564 [==============================] - 4s 430us/step - loss: 0.0261 - acc: 0.9832 - val_loss: 0.0239 - val_acc: 0.9875\n",
      "Epoch 610/620\n",
      "8564/8564 [==============================] - 4s 434us/step - loss: 0.0252 - acc: 0.9822 - val_loss: 0.0271 - val_acc: 0.9856\n",
      "Epoch 611/620\n",
      "8564/8564 [==============================] - 4s 428us/step - loss: 0.0265 - acc: 0.9842 - val_loss: 0.0244 - val_acc: 0.9863\n",
      "Epoch 612/620\n",
      "8564/8564 [==============================] - 4s 430us/step - loss: 0.0256 - acc: 0.9844 - val_loss: 0.0233 - val_acc: 0.9862\n",
      "Epoch 613/620\n",
      "8564/8564 [==============================] - 4s 432us/step - loss: 0.0236 - acc: 0.9846 - val_loss: 0.0230 - val_acc: 0.9882\n",
      "Epoch 614/620\n",
      "8564/8564 [==============================] - 4s 433us/step - loss: 0.0237 - acc: 0.9830 - val_loss: 0.0244 - val_acc: 0.9809\n",
      "Epoch 615/620\n",
      "8564/8564 [==============================] - 4s 432us/step - loss: 0.0261 - acc: 0.9828 - val_loss: 0.0196 - val_acc: 0.9868\n",
      "Epoch 616/620\n",
      "8564/8564 [==============================] - 4s 444us/step - loss: 0.0247 - acc: 0.9839 - val_loss: 0.0244 - val_acc: 0.9828\n",
      "Epoch 617/620\n",
      "8564/8564 [==============================] - 4s 444us/step - loss: 0.0251 - acc: 0.9815 - val_loss: 0.0296 - val_acc: 0.9835\n",
      "Epoch 618/620\n",
      "8564/8564 [==============================] - 4s 474us/step - loss: 0.0248 - acc: 0.9836 - val_loss: 0.0193 - val_acc: 0.9883\n",
      "Epoch 619/620\n",
      "8564/8564 [==============================] - 4s 472us/step - loss: 0.0247 - acc: 0.9826 - val_loss: 0.0221 - val_acc: 0.9872\n",
      "Epoch 620/620\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0264 - acc: 0.9823 - val_loss: 0.0226 - val_acc: 0.9863\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 601/620\n",
      "8564/8564 [==============================] - 4s 467us/step - loss: 0.0279 - acc: 0.9376 - val_loss: 0.0287 - val_acc: 0.9354\n",
      "Epoch 602/620\n",
      "8564/8564 [==============================] - 4s 437us/step - loss: 0.0276 - acc: 0.9401 - val_loss: 0.0240 - val_acc: 0.9476\n",
      "Epoch 603/620\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 4s 440us/step - loss: 0.0282 - acc: 0.9358 - val_loss: 0.0307 - val_acc: 0.9244\n",
      "Epoch 604/620\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0272 - acc: 0.9372 - val_loss: 0.0224 - val_acc: 0.9506\n",
      "Epoch 605/620\n",
      "8564/8564 [==============================] - 4s 436us/step - loss: 0.0280 - acc: 0.9381 - val_loss: 0.0231 - val_acc: 0.9512\n",
      "Epoch 606/620\n",
      "8564/8564 [==============================] - 4s 435us/step - loss: 0.0281 - acc: 0.9409 - val_loss: 0.0237 - val_acc: 0.9542\n",
      "Epoch 607/620\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0280 - acc: 0.9386 - val_loss: 0.0255 - val_acc: 0.9536\n",
      "Epoch 608/620\n",
      "8564/8564 [==============================] - 4s 433us/step - loss: 0.0282 - acc: 0.9387 - val_loss: 0.0242 - val_acc: 0.9517\n",
      "Epoch 609/620\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0281 - acc: 0.9408 - val_loss: 0.0269 - val_acc: 0.9449\n",
      "Epoch 610/620\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0286 - acc: 0.9381 - val_loss: 0.0259 - val_acc: 0.9464\n",
      "Epoch 611/620\n",
      "8564/8564 [==============================] - 4s 435us/step - loss: 0.0280 - acc: 0.9388 - val_loss: 0.0250 - val_acc: 0.9501\n",
      "Epoch 612/620\n",
      "8564/8564 [==============================] - 4s 434us/step - loss: 0.0274 - acc: 0.9379 - val_loss: 0.0266 - val_acc: 0.9458\n",
      "Epoch 613/620\n",
      "8564/8564 [==============================] - 4s 444us/step - loss: 0.0286 - acc: 0.9399 - val_loss: 0.0241 - val_acc: 0.9505\n",
      "Epoch 614/620\n",
      "8564/8564 [==============================] - 4s 434us/step - loss: 0.0272 - acc: 0.9406 - val_loss: 0.0236 - val_acc: 0.9522\n",
      "Epoch 615/620\n",
      "8564/8564 [==============================] - 4s 440us/step - loss: 0.0282 - acc: 0.9376 - val_loss: 0.0232 - val_acc: 0.9497\n",
      "Epoch 616/620\n",
      "8564/8564 [==============================] - 4s 435us/step - loss: 0.0281 - acc: 0.9378 - val_loss: 0.0276 - val_acc: 0.9318\n",
      "Epoch 617/620\n",
      "8564/8564 [==============================] - 4s 440us/step - loss: 0.0282 - acc: 0.9368 - val_loss: 0.0300 - val_acc: 0.9494\n",
      "Epoch 618/620\n",
      "8564/8564 [==============================] - 4s 442us/step - loss: 0.0280 - acc: 0.9383 - val_loss: 0.0264 - val_acc: 0.9519\n",
      "Epoch 619/620\n",
      "8564/8564 [==============================] - 4s 440us/step - loss: 0.0278 - acc: 0.9418 - val_loss: 0.0250 - val_acc: 0.9387\n",
      "Epoch 620/620\n",
      "8564/8564 [==============================] - 4s 437us/step - loss: 0.0288 - acc: 0.9369 - val_loss: 0.0267 - val_acc: 0.9472\n",
      "start training round 31\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 621/640\n",
      "8564/8564 [==============================] - 4s 432us/step - loss: 0.0260 - acc: 0.6590 - val_loss: 0.0257 - val_acc: 0.7259\n",
      "Epoch 622/640\n",
      "8564/8564 [==============================] - 4s 430us/step - loss: 0.0262 - acc: 0.6613 - val_loss: 0.0262 - val_acc: 0.6080\n",
      "Epoch 623/640\n",
      "8564/8564 [==============================] - 4s 433us/step - loss: 0.0260 - acc: 0.6506 - val_loss: 0.0263 - val_acc: 0.5905\n",
      "Epoch 624/640\n",
      "8564/8564 [==============================] - 4s 430us/step - loss: 0.0261 - acc: 0.6609 - val_loss: 0.0268 - val_acc: 0.5645\n",
      "Epoch 625/640\n",
      "8564/8564 [==============================] - 4s 430us/step - loss: 0.0262 - acc: 0.6496 - val_loss: 0.0269 - val_acc: 0.5644\n",
      "Epoch 626/640\n",
      "8564/8564 [==============================] - 4s 431us/step - loss: 0.0263 - acc: 0.6559 - val_loss: 0.0279 - val_acc: 0.5666\n",
      "Epoch 627/640\n",
      "8564/8564 [==============================] - 4s 431us/step - loss: 0.0261 - acc: 0.6561 - val_loss: 0.0274 - val_acc: 0.5630\n",
      "Epoch 628/640\n",
      "8564/8564 [==============================] - 4s 437us/step - loss: 0.0260 - acc: 0.6549 - val_loss: 0.0282 - val_acc: 0.5456\n",
      "Epoch 629/640\n",
      "8564/8564 [==============================] - 4s 436us/step - loss: 0.0258 - acc: 0.6646 - val_loss: 0.0263 - val_acc: 0.6456\n",
      "Epoch 630/640\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0258 - acc: 0.7000 - val_loss: 0.0271 - val_acc: 0.5531\n",
      "Epoch 631/640\n",
      "8564/8564 [==============================] - 4s 462us/step - loss: 0.0262 - acc: 0.6530 - val_loss: 0.0261 - val_acc: 0.5851\n",
      "Epoch 632/640\n",
      "8564/8564 [==============================] - 4s 435us/step - loss: 0.0257 - acc: 0.6786 - val_loss: 0.0260 - val_acc: 0.7371\n",
      "Epoch 633/640\n",
      "8564/8564 [==============================] - 4s 427us/step - loss: 0.0258 - acc: 0.7292 - val_loss: 0.0256 - val_acc: 0.7367\n",
      "Epoch 634/640\n",
      "8564/8564 [==============================] - 4s 429us/step - loss: 0.0257 - acc: 0.6804 - val_loss: 0.0260 - val_acc: 0.6244\n",
      "Epoch 635/640\n",
      "8564/8564 [==============================] - 4s 432us/step - loss: 0.0258 - acc: 0.6713 - val_loss: 0.0257 - val_acc: 0.7380\n",
      "Epoch 636/640\n",
      "8564/8564 [==============================] - 4s 431us/step - loss: 0.0257 - acc: 0.6720 - val_loss: 0.0277 - val_acc: 0.5435\n",
      "Epoch 637/640\n",
      "8564/8564 [==============================] - 4s 433us/step - loss: 0.0258 - acc: 0.6565 - val_loss: 0.0256 - val_acc: 0.6838\n",
      "Epoch 638/640\n",
      "8564/8564 [==============================] - 4s 433us/step - loss: 0.0255 - acc: 0.6646 - val_loss: 0.0266 - val_acc: 0.5814\n",
      "Epoch 639/640\n",
      "8564/8564 [==============================] - 4s 430us/step - loss: 0.0258 - acc: 0.6610 - val_loss: 0.0265 - val_acc: 0.5853\n",
      "Epoch 640/640\n",
      "8564/8564 [==============================] - 4s 426us/step - loss: 0.0258 - acc: 0.6689 - val_loss: 0.0270 - val_acc: 0.7290\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 621/640\n",
      "8564/8564 [==============================] - 4s 435us/step - loss: 0.0241 - acc: 0.8754 - val_loss: 0.0234 - val_acc: 0.8856\n",
      "Epoch 622/640\n",
      "8564/8564 [==============================] - 4s 434us/step - loss: 0.0239 - acc: 0.8855 - val_loss: 0.0235 - val_acc: 0.8840\n",
      "Epoch 623/640\n",
      "8564/8564 [==============================] - 4s 436us/step - loss: 0.0236 - acc: 0.8798 - val_loss: 0.0234 - val_acc: 0.8835\n",
      "Epoch 624/640\n",
      "8564/8564 [==============================] - 4s 429us/step - loss: 0.0236 - acc: 0.8782 - val_loss: 0.0242 - val_acc: 0.8771\n",
      "Epoch 625/640\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0237 - acc: 0.8784 - val_loss: 0.0238 - val_acc: 0.8829\n",
      "Epoch 626/640\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0235 - acc: 0.8858 - val_loss: 0.0240 - val_acc: 0.8869\n",
      "Epoch 627/640\n",
      "8564/8564 [==============================] - 4s 444us/step - loss: 0.0236 - acc: 0.8861 - val_loss: 0.0236 - val_acc: 0.8861\n",
      "Epoch 628/640\n",
      "8564/8564 [==============================] - 4s 442us/step - loss: 0.0237 - acc: 0.8859 - val_loss: 0.0242 - val_acc: 0.8869\n",
      "Epoch 629/640\n",
      "8564/8564 [==============================] - 4s 436us/step - loss: 0.0236 - acc: 0.8861 - val_loss: 0.0242 - val_acc: 0.8863\n",
      "Epoch 630/640\n",
      "8564/8564 [==============================] - 4s 431us/step - loss: 0.0237 - acc: 0.8861 - val_loss: 0.0241 - val_acc: 0.8867\n",
      "Epoch 631/640\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0235 - acc: 0.8857 - val_loss: 0.0248 - val_acc: 0.8867\n",
      "Epoch 632/640\n",
      "8564/8564 [==============================] - 4s 435us/step - loss: 0.0238 - acc: 0.8862 - val_loss: 0.0234 - val_acc: 0.8856\n",
      "Epoch 633/640\n",
      "8564/8564 [==============================] - 4s 430us/step - loss: 0.0236 - acc: 0.8844 - val_loss: 0.0235 - val_acc: 0.8805\n",
      "Epoch 634/640\n",
      "8564/8564 [==============================] - 4s 430us/step - loss: 0.0237 - acc: 0.8774 - val_loss: 0.0234 - val_acc: 0.8797\n",
      "Epoch 635/640\n",
      "8564/8564 [==============================] - 4s 435us/step - loss: 0.0237 - acc: 0.8780 - val_loss: 0.0240 - val_acc: 0.8724\n",
      "Epoch 636/640\n",
      "8564/8564 [==============================] - 4s 437us/step - loss: 0.0237 - acc: 0.8773 - val_loss: 0.0237 - val_acc: 0.8806\n",
      "Epoch 637/640\n",
      "8564/8564 [==============================] - 4s 430us/step - loss: 0.0235 - acc: 0.8773 - val_loss: 0.0231 - val_acc: 0.8794\n",
      "Epoch 638/640\n",
      "8564/8564 [==============================] - 4s 436us/step - loss: 0.0235 - acc: 0.8787 - val_loss: 0.0233 - val_acc: 0.8817\n",
      "Epoch 639/640\n",
      "8564/8564 [==============================] - 4s 433us/step - loss: 0.0235 - acc: 0.8791 - val_loss: 0.0237 - val_acc: 0.8763\n",
      "Epoch 640/640\n",
      "8564/8564 [==============================] - 4s 434us/step - loss: 0.0235 - acc: 0.8780 - val_loss: 0.0238 - val_acc: 0.8726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 621/640\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0235 - acc: 0.9840 - val_loss: 0.0232 - val_acc: 0.9867\n",
      "Epoch 622/640\n",
      "8564/8564 [==============================] - 4s 429us/step - loss: 0.0260 - acc: 0.9840 - val_loss: 0.0250 - val_acc: 0.9848\n",
      "Epoch 623/640\n",
      "8564/8564 [==============================] - 4s 434us/step - loss: 0.0240 - acc: 0.9810 - val_loss: 0.0223 - val_acc: 0.9849\n",
      "Epoch 624/640\n",
      "8564/8564 [==============================] - 4s 440us/step - loss: 0.0241 - acc: 0.9843 - val_loss: 0.0177 - val_acc: 0.9888\n",
      "Epoch 625/640\n",
      "8564/8564 [==============================] - 4s 437us/step - loss: 0.0250 - acc: 0.9819 - val_loss: 0.0226 - val_acc: 0.9867\n",
      "Epoch 626/640\n",
      "8564/8564 [==============================] - 4s 431us/step - loss: 0.0252 - acc: 0.9834 - val_loss: 0.0275 - val_acc: 0.9869\n",
      "Epoch 627/640\n",
      "8564/8564 [==============================] - 4s 433us/step - loss: 0.0233 - acc: 0.9843 - val_loss: 0.0230 - val_acc: 0.9842\n",
      "Epoch 628/640\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0255 - acc: 0.9834 - val_loss: 0.0268 - val_acc: 0.9857\n",
      "Epoch 629/640\n",
      "8564/8564 [==============================] - 4s 432us/step - loss: 0.0254 - acc: 0.9832 - val_loss: 0.0253 - val_acc: 0.9858\n",
      "Epoch 630/640\n",
      "8564/8564 [==============================] - 4s 429us/step - loss: 0.0238 - acc: 0.9852 - val_loss: 0.0218 - val_acc: 0.9810\n",
      "Epoch 631/640\n",
      "8564/8564 [==============================] - 4s 430us/step - loss: 0.0241 - acc: 0.9841 - val_loss: 0.0225 - val_acc: 0.9871\n",
      "Epoch 632/640\n",
      "8564/8564 [==============================] - 4s 427us/step - loss: 0.0238 - acc: 0.9841 - val_loss: 0.0205 - val_acc: 0.9885\n",
      "Epoch 633/640\n",
      "8564/8564 [==============================] - 4s 427us/step - loss: 0.0253 - acc: 0.9832 - val_loss: 0.0265 - val_acc: 0.9855\n",
      "Epoch 634/640\n",
      "8564/8564 [==============================] - 4s 432us/step - loss: 0.0239 - acc: 0.9844 - val_loss: 0.0216 - val_acc: 0.9857\n",
      "Epoch 635/640\n",
      "8564/8564 [==============================] - 4s 436us/step - loss: 0.0255 - acc: 0.9837 - val_loss: 0.0236 - val_acc: 0.9849\n",
      "Epoch 636/640\n",
      "8564/8564 [==============================] - 4s 431us/step - loss: 0.0226 - acc: 0.9847 - val_loss: 0.0224 - val_acc: 0.9828\n",
      "Epoch 637/640\n",
      "8564/8564 [==============================] - 4s 429us/step - loss: 0.0250 - acc: 0.9831 - val_loss: 0.0200 - val_acc: 0.9864\n",
      "Epoch 638/640\n",
      "8564/8564 [==============================] - 4s 435us/step - loss: 0.0243 - acc: 0.9850 - val_loss: 0.0241 - val_acc: 0.9843\n",
      "Epoch 639/640\n",
      "8564/8564 [==============================] - 4s 429us/step - loss: 0.0256 - acc: 0.9827 - val_loss: 0.0188 - val_acc: 0.9875\n",
      "Epoch 640/640\n",
      "8564/8564 [==============================] - 4s 431us/step - loss: 0.0242 - acc: 0.9840 - val_loss: 0.0269 - val_acc: 0.9842\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 621/640\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0275 - acc: 0.9392 - val_loss: 0.0296 - val_acc: 0.9286\n",
      "Epoch 622/640\n",
      "8564/8564 [==============================] - 4s 436us/step - loss: 0.0294 - acc: 0.9372 - val_loss: 0.0229 - val_acc: 0.9466\n",
      "Epoch 623/640\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0279 - acc: 0.9397 - val_loss: 0.0234 - val_acc: 0.9513\n",
      "Epoch 624/640\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0284 - acc: 0.9398 - val_loss: 0.0268 - val_acc: 0.9354\n",
      "Epoch 625/640\n",
      "8564/8564 [==============================] - 4s 437us/step - loss: 0.0280 - acc: 0.9379 - val_loss: 0.0278 - val_acc: 0.9435\n",
      "Epoch 626/640\n",
      "8564/8564 [==============================] - 4s 436us/step - loss: 0.0268 - acc: 0.9394 - val_loss: 0.0288 - val_acc: 0.9387\n",
      "Epoch 627/640\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0280 - acc: 0.9397 - val_loss: 0.0230 - val_acc: 0.9440\n",
      "Epoch 628/640\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0288 - acc: 0.9383 - val_loss: 0.0271 - val_acc: 0.9432\n",
      "Epoch 629/640\n",
      "8564/8564 [==============================] - 4s 434us/step - loss: 0.0304 - acc: 0.9325 - val_loss: 0.0247 - val_acc: 0.9487\n",
      "Epoch 630/640\n",
      "8564/8564 [==============================] - 4s 437us/step - loss: 0.0276 - acc: 0.9394 - val_loss: 0.0268 - val_acc: 0.9475\n",
      "Epoch 631/640\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0279 - acc: 0.9395 - val_loss: 0.0284 - val_acc: 0.9494\n",
      "Epoch 632/640\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0277 - acc: 0.9412 - val_loss: 0.0228 - val_acc: 0.9452\n",
      "Epoch 633/640\n",
      "8564/8564 [==============================] - 4s 440us/step - loss: 0.0277 - acc: 0.9395 - val_loss: 0.0255 - val_acc: 0.9424\n",
      "Epoch 634/640\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0283 - acc: 0.9383 - val_loss: 0.0217 - val_acc: 0.9490\n",
      "Epoch 635/640\n",
      "8564/8564 [==============================] - 4s 447us/step - loss: 0.0279 - acc: 0.9376 - val_loss: 0.0260 - val_acc: 0.9526\n",
      "Epoch 636/640\n",
      "8564/8564 [==============================] - 4s 430us/step - loss: 0.0276 - acc: 0.9381 - val_loss: 0.0235 - val_acc: 0.9549\n",
      "Epoch 637/640\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0272 - acc: 0.9429 - val_loss: 0.0271 - val_acc: 0.9509\n",
      "Epoch 638/640\n",
      "8564/8564 [==============================] - 4s 439us/step - loss: 0.0272 - acc: 0.9418 - val_loss: 0.0298 - val_acc: 0.9458\n",
      "Epoch 639/640\n",
      "8564/8564 [==============================] - 4s 439us/step - loss: 0.0280 - acc: 0.9401 - val_loss: 0.0239 - val_acc: 0.9487\n",
      "Epoch 640/640\n",
      "8564/8564 [==============================] - 4s 442us/step - loss: 0.0275 - acc: 0.9384 - val_loss: 0.0254 - val_acc: 0.9414\n",
      "start training round 32\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 641/660\n",
      "8564/8564 [==============================] - 4s 430us/step - loss: 0.0258 - acc: 0.7425 - val_loss: 0.0269 - val_acc: 0.7379\n",
      "Epoch 642/660\n",
      "8564/8564 [==============================] - 4s 428us/step - loss: 0.0257 - acc: 0.7407 - val_loss: 0.0256 - val_acc: 0.6600\n",
      "Epoch 643/660\n",
      "8564/8564 [==============================] - 4s 431us/step - loss: 0.0259 - acc: 0.7421 - val_loss: 0.0280 - val_acc: 0.7387\n",
      "Epoch 644/660\n",
      "8564/8564 [==============================] - 4s 437us/step - loss: 0.0262 - acc: 0.7387 - val_loss: 0.0262 - val_acc: 0.7412\n",
      "Epoch 645/660\n",
      "8564/8564 [==============================] - 4s 435us/step - loss: 0.0257 - acc: 0.7332 - val_loss: 0.0254 - val_acc: 0.7367\n",
      "Epoch 646/660\n",
      "8564/8564 [==============================] - 4s 435us/step - loss: 0.0257 - acc: 0.6780 - val_loss: 0.0257 - val_acc: 0.6627\n",
      "Epoch 647/660\n",
      "8564/8564 [==============================] - 4s 427us/step - loss: 0.0260 - acc: 0.6586 - val_loss: 0.0259 - val_acc: 0.6361\n",
      "Epoch 648/660\n",
      "8564/8564 [==============================] - 4s 427us/step - loss: 0.0256 - acc: 0.6686 - val_loss: 0.0260 - val_acc: 0.6636\n",
      "Epoch 649/660\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0258 - acc: 0.6593 - val_loss: 0.0267 - val_acc: 0.5900\n",
      "Epoch 650/660\n",
      "8564/8564 [==============================] - 4s 435us/step - loss: 0.0262 - acc: 0.6556 - val_loss: 0.0263 - val_acc: 0.6055\n",
      "Epoch 651/660\n",
      "8564/8564 [==============================] - 4s 434us/step - loss: 0.0256 - acc: 0.6662 - val_loss: 0.0258 - val_acc: 0.6094\n",
      "Epoch 652/660\n",
      "8564/8564 [==============================] - 4s 436us/step - loss: 0.0257 - acc: 0.6634 - val_loss: 0.0259 - val_acc: 0.6015\n",
      "Epoch 653/660\n",
      "8564/8564 [==============================] - 4s 433us/step - loss: 0.0258 - acc: 0.6582 - val_loss: 0.0273 - val_acc: 0.5706\n",
      "Epoch 654/660\n",
      "8564/8564 [==============================] - 4s 431us/step - loss: 0.0255 - acc: 0.6614 - val_loss: 0.0258 - val_acc: 0.6046\n",
      "Epoch 655/660\n",
      "8564/8564 [==============================] - 4s 431us/step - loss: 0.0256 - acc: 0.6574 - val_loss: 0.0258 - val_acc: 0.5918\n",
      "Epoch 656/660\n",
      "8564/8564 [==============================] - 4s 437us/step - loss: 0.0254 - acc: 0.6741 - val_loss: 0.0278 - val_acc: 0.7392\n",
      "Epoch 657/660\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0256 - acc: 0.7389 - val_loss: 0.0269 - val_acc: 0.7409\n",
      "Epoch 658/660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0256 - acc: 0.7436 - val_loss: 0.0266 - val_acc: 0.7390\n",
      "Epoch 659/660\n",
      "8564/8564 [==============================] - 4s 435us/step - loss: 0.0254 - acc: 0.6703 - val_loss: 0.0261 - val_acc: 0.5820\n",
      "Epoch 660/660\n",
      "8564/8564 [==============================] - 4s 433us/step - loss: 0.0255 - acc: 0.6897 - val_loss: 0.0262 - val_acc: 0.5806\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 641/660\n",
      "8564/8564 [==============================] - 4s 435us/step - loss: 0.0235 - acc: 0.8775 - val_loss: 0.0237 - val_acc: 0.8773\n",
      "Epoch 642/660\n",
      "8564/8564 [==============================] - 4s 431us/step - loss: 0.0237 - acc: 0.8779 - val_loss: 0.0241 - val_acc: 0.8680\n",
      "Epoch 643/660\n",
      "8564/8564 [==============================] - 4s 433us/step - loss: 0.0236 - acc: 0.8769 - val_loss: 0.0232 - val_acc: 0.8793\n",
      "Epoch 644/660\n",
      "8564/8564 [==============================] - 4s 431us/step - loss: 0.0235 - acc: 0.8782 - val_loss: 0.0247 - val_acc: 0.8682\n",
      "Epoch 645/660\n",
      "8564/8564 [==============================] - 4s 432us/step - loss: 0.0237 - acc: 0.8767 - val_loss: 0.0239 - val_acc: 0.8729\n",
      "Epoch 646/660\n",
      "8564/8564 [==============================] - 4s 432us/step - loss: 0.0236 - acc: 0.8770 - val_loss: 0.0235 - val_acc: 0.8804\n",
      "Epoch 647/660\n",
      "8564/8564 [==============================] - 4s 433us/step - loss: 0.0235 - acc: 0.8798 - val_loss: 0.0237 - val_acc: 0.8797\n",
      "Epoch 648/660\n",
      "8564/8564 [==============================] - 4s 433us/step - loss: 0.0235 - acc: 0.8805 - val_loss: 0.0233 - val_acc: 0.8800\n",
      "Epoch 649/660\n",
      "8564/8564 [==============================] - 4s 435us/step - loss: 0.0234 - acc: 0.8798 - val_loss: 0.0234 - val_acc: 0.8853\n",
      "Epoch 650/660\n",
      "8564/8564 [==============================] - 4s 429us/step - loss: 0.0235 - acc: 0.8800 - val_loss: 0.0246 - val_acc: 0.8649\n",
      "Epoch 651/660\n",
      "8564/8564 [==============================] - 4s 428us/step - loss: 0.0235 - acc: 0.8787 - val_loss: 0.0240 - val_acc: 0.8693\n",
      "Epoch 652/660\n",
      "8564/8564 [==============================] - 4s 434us/step - loss: 0.0236 - acc: 0.8785 - val_loss: 0.0239 - val_acc: 0.8717\n",
      "Epoch 653/660\n",
      "8564/8564 [==============================] - 4s 432us/step - loss: 0.0236 - acc: 0.8778 - val_loss: 0.0239 - val_acc: 0.8753\n",
      "Epoch 654/660\n",
      "8564/8564 [==============================] - 4s 432us/step - loss: 0.0235 - acc: 0.8791 - val_loss: 0.0238 - val_acc: 0.8730\n",
      "Epoch 655/660\n",
      "8564/8564 [==============================] - 4s 432us/step - loss: 0.0234 - acc: 0.8789 - val_loss: 0.0238 - val_acc: 0.8791\n",
      "Epoch 656/660\n",
      "8564/8564 [==============================] - 4s 431us/step - loss: 0.0234 - acc: 0.8797 - val_loss: 0.0231 - val_acc: 0.8839\n",
      "Epoch 657/660\n",
      "8564/8564 [==============================] - 4s 428us/step - loss: 0.0234 - acc: 0.8792 - val_loss: 0.0237 - val_acc: 0.8766\n",
      "Epoch 658/660\n",
      "8564/8564 [==============================] - 4s 425us/step - loss: 0.0235 - acc: 0.8786 - val_loss: 0.0234 - val_acc: 0.8816\n",
      "Epoch 659/660\n",
      "8564/8564 [==============================] - 4s 431us/step - loss: 0.0234 - acc: 0.8780 - val_loss: 0.0247 - val_acc: 0.8695\n",
      "Epoch 660/660\n",
      "8564/8564 [==============================] - 4s 426us/step - loss: 0.0235 - acc: 0.8783 - val_loss: 0.0241 - val_acc: 0.8684\n",
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 641/660\n",
      "8564/8564 [==============================] - 4s 428us/step - loss: 0.0229 - acc: 0.9837 - val_loss: 0.0229 - val_acc: 0.9860\n",
      "Epoch 642/660\n",
      "8564/8564 [==============================] - 4s 428us/step - loss: 0.0246 - acc: 0.9828 - val_loss: 0.0243 - val_acc: 0.9885\n",
      "Epoch 643/660\n",
      "8564/8564 [==============================] - 4s 430us/step - loss: 0.0258 - acc: 0.9823 - val_loss: 0.0236 - val_acc: 0.9849\n",
      "Epoch 644/660\n",
      "8564/8564 [==============================] - 4s 429us/step - loss: 0.0241 - acc: 0.9853 - val_loss: 0.0186 - val_acc: 0.9893\n",
      "Epoch 645/660\n",
      "8564/8564 [==============================] - 4s 430us/step - loss: 0.0259 - acc: 0.9814 - val_loss: 0.0226 - val_acc: 0.9859\n",
      "Epoch 646/660\n",
      "8564/8564 [==============================] - 4s 432us/step - loss: 0.0247 - acc: 0.9838 - val_loss: 0.0224 - val_acc: 0.9889\n",
      "Epoch 647/660\n",
      "8564/8564 [==============================] - 4s 434us/step - loss: 0.0259 - acc: 0.9843 - val_loss: 0.0236 - val_acc: 0.9859\n",
      "Epoch 648/660\n",
      "8564/8564 [==============================] - 4s 432us/step - loss: 0.0246 - acc: 0.9834 - val_loss: 0.0210 - val_acc: 0.9858\n",
      "Epoch 649/660\n",
      "8564/8564 [==============================] - 4s 432us/step - loss: 0.0266 - acc: 0.9843 - val_loss: 0.0258 - val_acc: 0.9855\n",
      "Epoch 650/660\n",
      "8564/8564 [==============================] - 4s 436us/step - loss: 0.0257 - acc: 0.9826 - val_loss: 0.0285 - val_acc: 0.9879\n",
      "Epoch 651/660\n",
      "8564/8564 [==============================] - 4s 430us/step - loss: 0.0246 - acc: 0.9841 - val_loss: 0.0231 - val_acc: 0.9878\n",
      "Epoch 652/660\n",
      "8564/8564 [==============================] - 4s 431us/step - loss: 0.0238 - acc: 0.9830 - val_loss: 0.0264 - val_acc: 0.9882\n",
      "Epoch 653/660\n",
      "8564/8564 [==============================] - 4s 430us/step - loss: 0.0256 - acc: 0.9840 - val_loss: 0.0238 - val_acc: 0.9845\n",
      "Epoch 654/660\n",
      "8564/8564 [==============================] - 4s 424us/step - loss: 0.0248 - acc: 0.9828 - val_loss: 0.0231 - val_acc: 0.9881\n",
      "Epoch 655/660\n",
      "8564/8564 [==============================] - 4s 430us/step - loss: 0.0228 - acc: 0.9844 - val_loss: 0.0223 - val_acc: 0.9883\n",
      "Epoch 656/660\n",
      "8564/8564 [==============================] - 4s 428us/step - loss: 0.0273 - acc: 0.9815 - val_loss: 0.0291 - val_acc: 0.9872\n",
      "Epoch 657/660\n",
      "8564/8564 [==============================] - 4s 428us/step - loss: 0.0237 - acc: 0.9853 - val_loss: 0.0209 - val_acc: 0.9891\n",
      "Epoch 658/660\n",
      "8564/8564 [==============================] - 4s 427us/step - loss: 0.0238 - acc: 0.9836 - val_loss: 0.0195 - val_acc: 0.9897\n",
      "Epoch 659/660\n",
      "8564/8564 [==============================] - 4s 428us/step - loss: 0.0251 - acc: 0.9853 - val_loss: 0.0240 - val_acc: 0.9857\n",
      "Epoch 660/660\n",
      "8564/8564 [==============================] - 4s 429us/step - loss: 0.0235 - acc: 0.9847 - val_loss: 0.0244 - val_acc: 0.9867\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 641/660\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0271 - acc: 0.9410 - val_loss: 0.0218 - val_acc: 0.9520\n",
      "Epoch 642/660\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0283 - acc: 0.9386 - val_loss: 0.0232 - val_acc: 0.9436\n",
      "Epoch 643/660\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0276 - acc: 0.9394 - val_loss: 0.0267 - val_acc: 0.9511\n",
      "Epoch 644/660\n",
      "8564/8564 [==============================] - 4s 434us/step - loss: 0.0286 - acc: 0.9382 - val_loss: 0.0313 - val_acc: 0.9388\n",
      "Epoch 645/660\n",
      "8564/8564 [==============================] - 4s 439us/step - loss: 0.0285 - acc: 0.9399 - val_loss: 0.0271 - val_acc: 0.9405\n",
      "Epoch 646/660\n",
      "8564/8564 [==============================] - 4s 436us/step - loss: 0.0296 - acc: 0.9379 - val_loss: 0.0273 - val_acc: 0.9420\n",
      "Epoch 647/660\n",
      "8564/8564 [==============================] - 4s 436us/step - loss: 0.0288 - acc: 0.9373 - val_loss: 0.0218 - val_acc: 0.9489\n",
      "Epoch 648/660\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0271 - acc: 0.9410 - val_loss: 0.0292 - val_acc: 0.9422\n",
      "Epoch 649/660\n",
      "8564/8564 [==============================] - 4s 433us/step - loss: 0.0275 - acc: 0.9420 - val_loss: 0.0249 - val_acc: 0.9484\n",
      "Epoch 650/660\n",
      "8564/8564 [==============================] - 4s 439us/step - loss: 0.0274 - acc: 0.9412 - val_loss: 0.0308 - val_acc: 0.9534\n",
      "Epoch 651/660\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0280 - acc: 0.9383 - val_loss: 0.0291 - val_acc: 0.9374\n",
      "Epoch 652/660\n",
      "8564/8564 [==============================] - 4s 434us/step - loss: 0.0280 - acc: 0.9402 - val_loss: 0.0259 - val_acc: 0.9576\n",
      "Epoch 653/660\n",
      "8564/8564 [==============================] - 4s 435us/step - loss: 0.0269 - acc: 0.9401 - val_loss: 0.0317 - val_acc: 0.9329\n",
      "Epoch 654/660\n",
      "8564/8564 [==============================] - 4s 442us/step - loss: 0.0284 - acc: 0.9377 - val_loss: 0.0233 - val_acc: 0.9525\n",
      "Epoch 655/660\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 4s 435us/step - loss: 0.0281 - acc: 0.9391 - val_loss: 0.0327 - val_acc: 0.9402\n",
      "Epoch 656/660\n",
      "8564/8564 [==============================] - 4s 440us/step - loss: 0.0295 - acc: 0.9375 - val_loss: 0.0257 - val_acc: 0.9454\n",
      "Epoch 657/660\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0275 - acc: 0.9391 - val_loss: 0.0245 - val_acc: 0.9324\n",
      "Epoch 658/660\n",
      "8564/8564 [==============================] - 4s 435us/step - loss: 0.0270 - acc: 0.9395 - val_loss: 0.0264 - val_acc: 0.9518\n",
      "Epoch 659/660\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0279 - acc: 0.9392 - val_loss: 0.0219 - val_acc: 0.9513\n",
      "Epoch 660/660\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0291 - acc: 0.9377 - val_loss: 0.0234 - val_acc: 0.9506\n",
      "start training round 33\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 661/680\n",
      "8564/8564 [==============================] - 4s 432us/step - loss: 0.0255 - acc: 0.6702 - val_loss: 0.0267 - val_acc: 0.5620\n",
      "Epoch 662/680\n",
      "8564/8564 [==============================] - 4s 436us/step - loss: 0.0257 - acc: 0.6667 - val_loss: 0.0262 - val_acc: 0.7405\n",
      "Epoch 663/680\n",
      "8564/8564 [==============================] - 4s 434us/step - loss: 0.0255 - acc: 0.7420 - val_loss: 0.0270 - val_acc: 0.7385\n",
      "Epoch 664/680\n",
      "8564/8564 [==============================] - 4s 434us/step - loss: 0.0260 - acc: 0.7397 - val_loss: 0.0256 - val_acc: 0.7392\n",
      "Epoch 665/680\n",
      "8564/8564 [==============================] - 4s 435us/step - loss: 0.0257 - acc: 0.7160 - val_loss: 0.0258 - val_acc: 0.7319\n",
      "Epoch 666/680\n",
      "8564/8564 [==============================] - 4s 434us/step - loss: 0.0259 - acc: 0.7050 - val_loss: 0.0253 - val_acc: 0.7394\n",
      "Epoch 667/680\n",
      "8564/8564 [==============================] - 4s 426us/step - loss: 0.0255 - acc: 0.6892 - val_loss: 0.0264 - val_acc: 0.7321\n",
      "Epoch 668/680\n",
      "8564/8564 [==============================] - 4s 432us/step - loss: 0.0256 - acc: 0.6848 - val_loss: 0.0266 - val_acc: 0.7404\n",
      "Epoch 669/680\n",
      "8564/8564 [==============================] - 4s 426us/step - loss: 0.0256 - acc: 0.7422 - val_loss: 0.0262 - val_acc: 0.7410\n",
      "Epoch 670/680\n",
      "8564/8564 [==============================] - 4s 431us/step - loss: 0.0259 - acc: 0.7381 - val_loss: 0.0263 - val_acc: 0.7426\n",
      "Epoch 671/680\n",
      "8564/8564 [==============================] - 4s 426us/step - loss: 0.0261 - acc: 0.7366 - val_loss: 0.0256 - val_acc: 0.7438\n",
      "Epoch 672/680\n",
      "8564/8564 [==============================] - 4s 435us/step - loss: 0.0255 - acc: 0.7225 - val_loss: 0.0256 - val_acc: 0.7377\n",
      "Epoch 673/680\n",
      "8564/8564 [==============================] - 4s 446us/step - loss: 0.0255 - acc: 0.6646 - val_loss: 0.0270 - val_acc: 0.7288\n",
      "Epoch 674/680\n",
      "8564/8564 [==============================] - 4s 432us/step - loss: 0.0255 - acc: 0.6828 - val_loss: 0.0252 - val_acc: 0.7301\n",
      "Epoch 675/680\n",
      "8564/8564 [==============================] - 4s 449us/step - loss: 0.0254 - acc: 0.6748 - val_loss: 0.0252 - val_acc: 0.7353\n",
      "Epoch 676/680\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0255 - acc: 0.7396 - val_loss: 0.0260 - val_acc: 0.7411\n",
      "Epoch 677/680\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0256 - acc: 0.7300 - val_loss: 0.0268 - val_acc: 0.7411\n",
      "Epoch 678/680\n",
      "8564/8564 [==============================] - 4s 440us/step - loss: 0.0259 - acc: 0.7389 - val_loss: 0.0264 - val_acc: 0.7435\n",
      "Epoch 679/680\n",
      "8564/8564 [==============================] - 4s 444us/step - loss: 0.0252 - acc: 0.7082 - val_loss: 0.0254 - val_acc: 0.7390\n",
      "Epoch 680/680\n",
      "8564/8564 [==============================] - 4s 432us/step - loss: 0.0253 - acc: 0.6849 - val_loss: 0.0264 - val_acc: 0.7310\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 661/680\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0234 - acc: 0.8783 - val_loss: 0.0232 - val_acc: 0.8785\n",
      "Epoch 662/680\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0234 - acc: 0.8791 - val_loss: 0.0239 - val_acc: 0.8699\n",
      "Epoch 663/680\n",
      "8564/8564 [==============================] - 4s 439us/step - loss: 0.0234 - acc: 0.8772 - val_loss: 0.0237 - val_acc: 0.8809\n",
      "Epoch 664/680\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0233 - acc: 0.8795 - val_loss: 0.0235 - val_acc: 0.8790\n",
      "Epoch 665/680\n",
      "8564/8564 [==============================] - 4s 431us/step - loss: 0.0238 - acc: 0.8762 - val_loss: 0.0239 - val_acc: 0.8805\n",
      "Epoch 666/680\n",
      "8564/8564 [==============================] - 4s 432us/step - loss: 0.0235 - acc: 0.8786 - val_loss: 0.0234 - val_acc: 0.8839\n",
      "Epoch 667/680\n",
      "8564/8564 [==============================] - 4s 435us/step - loss: 0.0235 - acc: 0.8786 - val_loss: 0.0243 - val_acc: 0.8804\n",
      "Epoch 668/680\n",
      "8564/8564 [==============================] - 4s 431us/step - loss: 0.0235 - acc: 0.8786 - val_loss: 0.0249 - val_acc: 0.8765\n",
      "Epoch 669/680\n",
      "8564/8564 [==============================] - 4s 427us/step - loss: 0.0236 - acc: 0.8778 - val_loss: 0.0233 - val_acc: 0.8873\n",
      "Epoch 670/680\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0234 - acc: 0.8864 - val_loss: 0.0239 - val_acc: 0.8871\n",
      "Epoch 671/680\n",
      "8564/8564 [==============================] - 4s 461us/step - loss: 0.0235 - acc: 0.8873 - val_loss: 0.0235 - val_acc: 0.8846\n",
      "Epoch 672/680\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0236 - acc: 0.8865 - val_loss: 0.0245 - val_acc: 0.8863\n",
      "Epoch 673/680\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0235 - acc: 0.8870 - val_loss: 0.0236 - val_acc: 0.8830\n",
      "Epoch 674/680\n",
      "8564/8564 [==============================] - 4s 434us/step - loss: 0.0233 - acc: 0.8794 - val_loss: 0.0229 - val_acc: 0.8865\n",
      "Epoch 675/680\n",
      "8564/8564 [==============================] - 4s 428us/step - loss: 0.0235 - acc: 0.8868 - val_loss: 0.0240 - val_acc: 0.8869\n",
      "Epoch 676/680\n",
      "8564/8564 [==============================] - 4s 433us/step - loss: 0.0235 - acc: 0.8867 - val_loss: 0.0238 - val_acc: 0.8867\n",
      "Epoch 677/680\n",
      "8564/8564 [==============================] - 4s 429us/step - loss: 0.0234 - acc: 0.8869 - val_loss: 0.0234 - val_acc: 0.8865\n",
      "Epoch 678/680\n",
      "8564/8564 [==============================] - 4s 431us/step - loss: 0.0234 - acc: 0.8869 - val_loss: 0.0238 - val_acc: 0.8869\n",
      "Epoch 679/680\n",
      "8564/8564 [==============================] - 4s 424us/step - loss: 0.0233 - acc: 0.8806 - val_loss: 0.0242 - val_acc: 0.8805\n",
      "Epoch 680/680\n",
      "8564/8564 [==============================] - 4s 432us/step - loss: 0.0235 - acc: 0.8782 - val_loss: 0.0243 - val_acc: 0.8798\n",
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 661/680\n",
      "8564/8564 [==============================] - 4s 424us/step - loss: 0.0257 - acc: 0.9835 - val_loss: 0.0277 - val_acc: 0.9863\n",
      "Epoch 662/680\n",
      "8564/8564 [==============================] - 4s 427us/step - loss: 0.0248 - acc: 0.9841 - val_loss: 0.0229 - val_acc: 0.9843\n",
      "Epoch 663/680\n",
      "8564/8564 [==============================] - 4s 430us/step - loss: 0.0250 - acc: 0.9847 - val_loss: 0.0224 - val_acc: 0.9864\n",
      "Epoch 664/680\n",
      "8564/8564 [==============================] - 4s 436us/step - loss: 0.0248 - acc: 0.9832 - val_loss: 0.0237 - val_acc: 0.9843\n",
      "Epoch 665/680\n",
      "8564/8564 [==============================] - 4s 428us/step - loss: 0.0234 - acc: 0.9853 - val_loss: 0.0223 - val_acc: 0.9868\n",
      "Epoch 666/680\n",
      "8564/8564 [==============================] - 4s 428us/step - loss: 0.0243 - acc: 0.9830 - val_loss: 0.0296 - val_acc: 0.9833\n",
      "Epoch 667/680\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0241 - acc: 0.9845 - val_loss: 0.0237 - val_acc: 0.9847\n",
      "Epoch 668/680\n",
      "8564/8564 [==============================] - 4s 434us/step - loss: 0.0244 - acc: 0.9837 - val_loss: 0.0207 - val_acc: 0.9876\n",
      "Epoch 669/680\n",
      "8564/8564 [==============================] - 4s 431us/step - loss: 0.0238 - acc: 0.9852 - val_loss: 0.0230 - val_acc: 0.9854\n",
      "Epoch 670/680\n",
      "8564/8564 [==============================] - 4s 429us/step - loss: 0.0256 - acc: 0.9834 - val_loss: 0.0266 - val_acc: 0.9878\n",
      "Epoch 671/680\n",
      "8564/8564 [==============================] - 4s 433us/step - loss: 0.0229 - acc: 0.9849 - val_loss: 0.0194 - val_acc: 0.9894\n",
      "Epoch 672/680\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 4s 433us/step - loss: 0.0244 - acc: 0.9844 - val_loss: 0.0200 - val_acc: 0.9888\n",
      "Epoch 673/680\n",
      "8564/8564 [==============================] - 4s 437us/step - loss: 0.0232 - acc: 0.9837 - val_loss: 0.0255 - val_acc: 0.9821\n",
      "Epoch 674/680\n",
      "8564/8564 [==============================] - 4s 426us/step - loss: 0.0246 - acc: 0.9829 - val_loss: 0.0224 - val_acc: 0.9882\n",
      "Epoch 675/680\n",
      "8564/8564 [==============================] - 4s 437us/step - loss: 0.0238 - acc: 0.9833 - val_loss: 0.0236 - val_acc: 0.9880\n",
      "Epoch 676/680\n",
      "8564/8564 [==============================] - 4s 435us/step - loss: 0.0229 - acc: 0.9846 - val_loss: 0.0299 - val_acc: 0.9870\n",
      "Epoch 677/680\n",
      "8564/8564 [==============================] - 4s 429us/step - loss: 0.0257 - acc: 0.9840 - val_loss: 0.0281 - val_acc: 0.9873\n",
      "Epoch 678/680\n",
      "8564/8564 [==============================] - 4s 432us/step - loss: 0.0227 - acc: 0.9857 - val_loss: 0.0221 - val_acc: 0.9876\n",
      "Epoch 679/680\n",
      "8564/8564 [==============================] - 4s 432us/step - loss: 0.0238 - acc: 0.9836 - val_loss: 0.0214 - val_acc: 0.9864\n",
      "Epoch 680/680\n",
      "8564/8564 [==============================] - 4s 436us/step - loss: 0.0236 - acc: 0.9844 - val_loss: 0.0222 - val_acc: 0.9872\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 661/680\n",
      "8564/8564 [==============================] - 4s 440us/step - loss: 0.0281 - acc: 0.9383 - val_loss: 0.0321 - val_acc: 0.9245\n",
      "Epoch 662/680\n",
      "8564/8564 [==============================] - 4s 434us/step - loss: 0.0280 - acc: 0.9414 - val_loss: 0.0275 - val_acc: 0.9412\n",
      "Epoch 663/680\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0284 - acc: 0.9382 - val_loss: 0.0252 - val_acc: 0.9353\n",
      "Epoch 664/680\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0263 - acc: 0.9423 - val_loss: 0.0278 - val_acc: 0.9302\n",
      "Epoch 665/680\n",
      "8564/8564 [==============================] - 4s 432us/step - loss: 0.0270 - acc: 0.9406 - val_loss: 0.0227 - val_acc: 0.9510\n",
      "Epoch 666/680\n",
      "8564/8564 [==============================] - 4s 446us/step - loss: 0.0269 - acc: 0.9408 - val_loss: 0.0255 - val_acc: 0.9459\n",
      "Epoch 667/680\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0264 - acc: 0.9422 - val_loss: 0.0276 - val_acc: 0.9499\n",
      "Epoch 668/680\n",
      "8564/8564 [==============================] - 4s 435us/step - loss: 0.0267 - acc: 0.9420 - val_loss: 0.0245 - val_acc: 0.9397\n",
      "Epoch 669/680\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0273 - acc: 0.9403 - val_loss: 0.0297 - val_acc: 0.9266\n",
      "Epoch 670/680\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0273 - acc: 0.9387 - val_loss: 0.0243 - val_acc: 0.9540\n",
      "Epoch 671/680\n",
      "8564/8564 [==============================] - 4s 433us/step - loss: 0.0291 - acc: 0.9392 - val_loss: 0.0251 - val_acc: 0.9530\n",
      "Epoch 672/680\n",
      "8564/8564 [==============================] - 4s 436us/step - loss: 0.0281 - acc: 0.9388 - val_loss: 0.0242 - val_acc: 0.9552\n",
      "Epoch 673/680\n",
      "8564/8564 [==============================] - 4s 439us/step - loss: 0.0271 - acc: 0.9417 - val_loss: 0.0233 - val_acc: 0.9429\n",
      "Epoch 674/680\n",
      "8564/8564 [==============================] - 4s 446us/step - loss: 0.0263 - acc: 0.9432 - val_loss: 0.0242 - val_acc: 0.9434\n",
      "Epoch 675/680\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0283 - acc: 0.9397 - val_loss: 0.0263 - val_acc: 0.9503\n",
      "Epoch 676/680\n",
      "8564/8564 [==============================] - 4s 447us/step - loss: 0.0284 - acc: 0.9377 - val_loss: 0.0257 - val_acc: 0.9474\n",
      "Epoch 677/680\n",
      "8564/8564 [==============================] - 4s 437us/step - loss: 0.0277 - acc: 0.9389 - val_loss: 0.0249 - val_acc: 0.9537\n",
      "Epoch 678/680\n",
      "8564/8564 [==============================] - 4s 435us/step - loss: 0.0276 - acc: 0.9393 - val_loss: 0.0268 - val_acc: 0.9482\n",
      "Epoch 679/680\n",
      "8564/8564 [==============================] - 4s 440us/step - loss: 0.0278 - acc: 0.9393 - val_loss: 0.0287 - val_acc: 0.9420\n",
      "Epoch 680/680\n",
      "8564/8564 [==============================] - 4s 437us/step - loss: 0.0283 - acc: 0.9377 - val_loss: 0.0247 - val_acc: 0.9469\n",
      "start training round 34\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 681/700\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0255 - acc: 0.6707 - val_loss: 0.0264 - val_acc: 0.7311\n",
      "Epoch 682/700\n",
      "8564/8564 [==============================] - 4s 439us/step - loss: 0.0253 - acc: 0.7024 - val_loss: 0.0255 - val_acc: 0.6095\n",
      "Epoch 683/700\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0253 - acc: 0.6822 - val_loss: 0.0259 - val_acc: 0.7372\n",
      "Epoch 684/700\n",
      "8564/8564 [==============================] - 4s 446us/step - loss: 0.0257 - acc: 0.7425 - val_loss: 0.0261 - val_acc: 0.7437\n",
      "Epoch 685/700\n",
      "8564/8564 [==============================] - 4s 446us/step - loss: 0.0256 - acc: 0.7436 - val_loss: 0.0263 - val_acc: 0.7415\n",
      "Epoch 686/700\n",
      "8564/8564 [==============================] - 4s 444us/step - loss: 0.0259 - acc: 0.7432 - val_loss: 0.0263 - val_acc: 0.7427\n",
      "Epoch 687/700\n",
      "8564/8564 [==============================] - 4s 442us/step - loss: 0.0256 - acc: 0.7159 - val_loss: 0.0261 - val_acc: 0.7425\n",
      "Epoch 688/700\n",
      "8564/8564 [==============================] - 4s 442us/step - loss: 0.0255 - acc: 0.6747 - val_loss: 0.0270 - val_acc: 0.5671\n",
      "Epoch 689/700\n",
      "8564/8564 [==============================] - 4s 440us/step - loss: 0.0256 - acc: 0.6631 - val_loss: 0.0259 - val_acc: 0.5886\n",
      "Epoch 690/700\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0255 - acc: 0.6722 - val_loss: 0.0265 - val_acc: 0.7366\n",
      "Epoch 691/700\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0254 - acc: 0.7378 - val_loss: 0.0257 - val_acc: 0.7420\n",
      "Epoch 692/700\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0257 - acc: 0.7395 - val_loss: 0.0265 - val_acc: 0.7429\n",
      "Epoch 693/700\n",
      "8564/8564 [==============================] - 4s 442us/step - loss: 0.0256 - acc: 0.7409 - val_loss: 0.0251 - val_acc: 0.7359\n",
      "Epoch 694/700\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0251 - acc: 0.6978 - val_loss: 0.0254 - val_acc: 0.7384\n",
      "Epoch 695/700\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0255 - acc: 0.7173 - val_loss: 0.0258 - val_acc: 0.5941\n",
      "Epoch 696/700\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0253 - acc: 0.6661 - val_loss: 0.0252 - val_acc: 0.7068\n",
      "Epoch 697/700\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0253 - acc: 0.6775 - val_loss: 0.0258 - val_acc: 0.5833\n",
      "Epoch 698/700\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0256 - acc: 0.6558 - val_loss: 0.0266 - val_acc: 0.5762\n",
      "Epoch 699/700\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0256 - acc: 0.6572 - val_loss: 0.0264 - val_acc: 0.5683\n",
      "Epoch 700/700\n",
      "8564/8564 [==============================] - 4s 442us/step - loss: 0.0254 - acc: 0.6676 - val_loss: 0.0270 - val_acc: 0.5584\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 681/700\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0234 - acc: 0.8787 - val_loss: 0.0249 - val_acc: 0.8772\n",
      "Epoch 682/700\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0232 - acc: 0.8827 - val_loss: 0.0235 - val_acc: 0.8854\n",
      "Epoch 683/700\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0236 - acc: 0.8869 - val_loss: 0.0240 - val_acc: 0.8858\n",
      "Epoch 684/700\n",
      "8564/8564 [==============================] - 4s 437us/step - loss: 0.0235 - acc: 0.8869 - val_loss: 0.0244 - val_acc: 0.8867\n",
      "Epoch 685/700\n",
      "8564/8564 [==============================] - 4s 435us/step - loss: 0.0234 - acc: 0.8870 - val_loss: 0.0237 - val_acc: 0.8870\n",
      "Epoch 686/700\n",
      "8564/8564 [==============================] - 4s 432us/step - loss: 0.0236 - acc: 0.8873 - val_loss: 0.0239 - val_acc: 0.8864\n",
      "Epoch 687/700\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0233 - acc: 0.8867 - val_loss: 0.0229 - val_acc: 0.8870\n",
      "Epoch 688/700\n",
      "8564/8564 [==============================] - 4s 442us/step - loss: 0.0234 - acc: 0.8866 - val_loss: 0.0232 - val_acc: 0.8872\n",
      "Epoch 689/700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 4s 442us/step - loss: 0.0233 - acc: 0.8865 - val_loss: 0.0228 - val_acc: 0.8872\n",
      "Epoch 690/700\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0231 - acc: 0.8814 - val_loss: 0.0236 - val_acc: 0.8775\n",
      "Epoch 691/700\n",
      "8564/8564 [==============================] - 4s 440us/step - loss: 0.0234 - acc: 0.8848 - val_loss: 0.0238 - val_acc: 0.8869\n",
      "Epoch 692/700\n",
      "8564/8564 [==============================] - 4s 439us/step - loss: 0.0234 - acc: 0.8872 - val_loss: 0.0234 - val_acc: 0.8871\n",
      "Epoch 693/700\n",
      "8564/8564 [==============================] - 4s 444us/step - loss: 0.0233 - acc: 0.8874 - val_loss: 0.0231 - val_acc: 0.8871\n",
      "Epoch 694/700\n",
      "8564/8564 [==============================] - 4s 440us/step - loss: 0.0232 - acc: 0.8875 - val_loss: 0.0238 - val_acc: 0.8871\n",
      "Epoch 695/700\n",
      "8564/8564 [==============================] - 4s 444us/step - loss: 0.0235 - acc: 0.8866 - val_loss: 0.0242 - val_acc: 0.8870\n",
      "Epoch 696/700\n",
      "8564/8564 [==============================] - 4s 437us/step - loss: 0.0239 - acc: 0.8859 - val_loss: 0.0230 - val_acc: 0.8837\n",
      "Epoch 697/700\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0233 - acc: 0.8869 - val_loss: 0.0239 - val_acc: 0.8861\n",
      "Epoch 698/700\n",
      "8564/8564 [==============================] - 4s 439us/step - loss: 0.0234 - acc: 0.8867 - val_loss: 0.0237 - val_acc: 0.8867\n",
      "Epoch 699/700\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0234 - acc: 0.8874 - val_loss: 0.0235 - val_acc: 0.8874\n",
      "Epoch 700/700\n",
      "8564/8564 [==============================] - 4s 436us/step - loss: 0.0234 - acc: 0.8869 - val_loss: 0.0240 - val_acc: 0.8875\n",
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 681/700\n",
      "8564/8564 [==============================] - 4s 444us/step - loss: 0.0239 - acc: 0.9842 - val_loss: 0.0178 - val_acc: 0.9878\n",
      "Epoch 682/700\n",
      "8564/8564 [==============================] - 4s 437us/step - loss: 0.0239 - acc: 0.9844 - val_loss: 0.0247 - val_acc: 0.9800\n",
      "Epoch 683/700\n",
      "8564/8564 [==============================] - 4s 440us/step - loss: 0.0235 - acc: 0.9842 - val_loss: 0.0204 - val_acc: 0.9891\n",
      "Epoch 684/700\n",
      "8564/8564 [==============================] - 4s 439us/step - loss: 0.0252 - acc: 0.9829 - val_loss: 0.0261 - val_acc: 0.9884\n",
      "Epoch 685/700\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0254 - acc: 0.9854 - val_loss: 0.0170 - val_acc: 0.9896\n",
      "Epoch 686/700\n",
      "8564/8564 [==============================] - 4s 435us/step - loss: 0.0248 - acc: 0.9836 - val_loss: 0.0211 - val_acc: 0.9883\n",
      "Epoch 687/700\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0234 - acc: 0.9835 - val_loss: 0.0222 - val_acc: 0.9887\n",
      "Epoch 688/700\n",
      "8564/8564 [==============================] - 4s 439us/step - loss: 0.0254 - acc: 0.9844 - val_loss: 0.0200 - val_acc: 0.9879\n",
      "Epoch 689/700\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0242 - acc: 0.9855 - val_loss: 0.0193 - val_acc: 0.9868\n",
      "Epoch 690/700\n",
      "8564/8564 [==============================] - 4s 440us/step - loss: 0.0241 - acc: 0.9831 - val_loss: 0.0171 - val_acc: 0.9897\n",
      "Epoch 691/700\n",
      "8564/8564 [==============================] - 4s 449us/step - loss: 0.0250 - acc: 0.9843 - val_loss: 0.0191 - val_acc: 0.9894\n",
      "Epoch 692/700\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0233 - acc: 0.9850 - val_loss: 0.0224 - val_acc: 0.9843\n",
      "Epoch 693/700\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0232 - acc: 0.9838 - val_loss: 0.0187 - val_acc: 0.9887\n",
      "Epoch 694/700\n",
      "8564/8564 [==============================] - 4s 446us/step - loss: 0.0221 - acc: 0.9841 - val_loss: 0.0250 - val_acc: 0.9858\n",
      "Epoch 695/700\n",
      "8564/8564 [==============================] - 4s 437us/step - loss: 0.0230 - acc: 0.9833 - val_loss: 0.0253 - val_acc: 0.9847\n",
      "Epoch 696/700\n",
      "8564/8564 [==============================] - 4s 447us/step - loss: 0.0241 - acc: 0.9839 - val_loss: 0.0209 - val_acc: 0.9887\n",
      "Epoch 697/700\n",
      "8564/8564 [==============================] - 4s 435us/step - loss: 0.0236 - acc: 0.9862 - val_loss: 0.0194 - val_acc: 0.9880\n",
      "Epoch 698/700\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0255 - acc: 0.9838 - val_loss: 0.0229 - val_acc: 0.9877\n",
      "Epoch 699/700\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0245 - acc: 0.9838 - val_loss: 0.0278 - val_acc: 0.9829\n",
      "Epoch 700/700\n",
      "8564/8564 [==============================] - 4s 447us/step - loss: 0.0230 - acc: 0.9842 - val_loss: 0.0209 - val_acc: 0.9895\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 681/700\n",
      "8564/8564 [==============================] - 4s 439us/step - loss: 0.0276 - acc: 0.9382 - val_loss: 0.0310 - val_acc: 0.9235\n",
      "Epoch 682/700\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0272 - acc: 0.9388 - val_loss: 0.0267 - val_acc: 0.9471\n",
      "Epoch 683/700\n",
      "8564/8564 [==============================] - 4s 442us/step - loss: 0.0277 - acc: 0.9398 - val_loss: 0.0265 - val_acc: 0.9429\n",
      "Epoch 684/700\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0274 - acc: 0.9419 - val_loss: 0.0230 - val_acc: 0.9511\n",
      "Epoch 685/700\n",
      "8564/8564 [==============================] - 4s 439us/step - loss: 0.0281 - acc: 0.9361 - val_loss: 0.0262 - val_acc: 0.9424\n",
      "Epoch 686/700\n",
      "8564/8564 [==============================] - 4s 437us/step - loss: 0.0279 - acc: 0.9406 - val_loss: 0.0250 - val_acc: 0.9462\n",
      "Epoch 687/700\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0280 - acc: 0.9415 - val_loss: 0.0245 - val_acc: 0.9585\n",
      "Epoch 688/700\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0277 - acc: 0.9425 - val_loss: 0.0212 - val_acc: 0.9554\n",
      "Epoch 689/700\n",
      "8564/8564 [==============================] - 4s 442us/step - loss: 0.0277 - acc: 0.9398 - val_loss: 0.0294 - val_acc: 0.9429\n",
      "Epoch 690/700\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0279 - acc: 0.9378 - val_loss: 0.0245 - val_acc: 0.9531\n",
      "Epoch 691/700\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0276 - acc: 0.9404 - val_loss: 0.0285 - val_acc: 0.9422\n",
      "Epoch 692/700\n",
      "8564/8564 [==============================] - 4s 437us/step - loss: 0.0279 - acc: 0.9394 - val_loss: 0.0233 - val_acc: 0.9450\n",
      "Epoch 693/700\n",
      "8564/8564 [==============================] - 4s 436us/step - loss: 0.0296 - acc: 0.9348 - val_loss: 0.0233 - val_acc: 0.9475\n",
      "Epoch 694/700\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0282 - acc: 0.9405 - val_loss: 0.0238 - val_acc: 0.9503\n",
      "Epoch 695/700\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0271 - acc: 0.9425 - val_loss: 0.0238 - val_acc: 0.9400\n",
      "Epoch 696/700\n",
      "8564/8564 [==============================] - 4s 440us/step - loss: 0.0286 - acc: 0.9356 - val_loss: 0.0227 - val_acc: 0.9441\n",
      "Epoch 697/700\n",
      "8564/8564 [==============================] - 4s 442us/step - loss: 0.0269 - acc: 0.9415 - val_loss: 0.0272 - val_acc: 0.9463\n",
      "Epoch 698/700\n",
      "8564/8564 [==============================] - 4s 439us/step - loss: 0.0277 - acc: 0.9404 - val_loss: 0.0275 - val_acc: 0.9409\n",
      "Epoch 699/700\n",
      "8564/8564 [==============================] - 4s 442us/step - loss: 0.0271 - acc: 0.9409 - val_loss: 0.0266 - val_acc: 0.9343\n",
      "Epoch 700/700\n",
      "8564/8564 [==============================] - 4s 439us/step - loss: 0.0281 - acc: 0.9398 - val_loss: 0.0204 - val_acc: 0.9554\n",
      "start training round 35\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 701/720\n",
      "8564/8564 [==============================] - 4s 442us/step - loss: 0.0257 - acc: 0.6551 - val_loss: 0.0259 - val_acc: 0.5940\n",
      "Epoch 702/720\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0254 - acc: 0.6619 - val_loss: 0.0264 - val_acc: 0.5705\n",
      "Epoch 703/720\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0252 - acc: 0.6617 - val_loss: 0.0256 - val_acc: 0.5940\n",
      "Epoch 704/720\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0251 - acc: 0.6883 - val_loss: 0.0254 - val_acc: 0.7276\n",
      "Epoch 705/720\n",
      "8564/8564 [==============================] - 4s 447us/step - loss: 0.0258 - acc: 0.6594 - val_loss: 0.0251 - val_acc: 0.7339\n",
      "Epoch 706/720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 4s 442us/step - loss: 0.0256 - acc: 0.7415 - val_loss: 0.0262 - val_acc: 0.7443\n",
      "Epoch 707/720\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0250 - acc: 0.7099 - val_loss: 0.0254 - val_acc: 0.6612\n",
      "Epoch 708/720\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0251 - acc: 0.6743 - val_loss: 0.0258 - val_acc: 0.5958\n",
      "Epoch 709/720\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0252 - acc: 0.6685 - val_loss: 0.0256 - val_acc: 0.5867\n",
      "Epoch 710/720\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0252 - acc: 0.6634 - val_loss: 0.0256 - val_acc: 0.5935\n",
      "Epoch 711/720\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0252 - acc: 0.6686 - val_loss: 0.0261 - val_acc: 0.5753\n",
      "Epoch 712/720\n",
      "8564/8564 [==============================] - 4s 439us/step - loss: 0.0255 - acc: 0.6675 - val_loss: 0.0252 - val_acc: 0.6202\n",
      "Epoch 713/720\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0255 - acc: 0.6625 - val_loss: 0.0273 - val_acc: 0.5496\n",
      "Epoch 714/720\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0255 - acc: 0.6551 - val_loss: 0.0258 - val_acc: 0.5835\n",
      "Epoch 715/720\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0252 - acc: 0.6659 - val_loss: 0.0261 - val_acc: 0.5724\n",
      "Epoch 716/720\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0251 - acc: 0.7264 - val_loss: 0.0255 - val_acc: 0.7460\n",
      "Epoch 717/720\n",
      "8564/8564 [==============================] - 4s 446us/step - loss: 0.0257 - acc: 0.7454 - val_loss: 0.0256 - val_acc: 0.7452\n",
      "Epoch 718/720\n",
      "8564/8564 [==============================] - 4s 444us/step - loss: 0.0253 - acc: 0.7470 - val_loss: 0.0249 - val_acc: 0.7393\n",
      "Epoch 719/720\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0251 - acc: 0.6844 - val_loss: 0.0259 - val_acc: 0.5702\n",
      "Epoch 720/720\n",
      "8564/8564 [==============================] - 4s 440us/step - loss: 0.0252 - acc: 0.6700 - val_loss: 0.0260 - val_acc: 0.5788\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 701/720\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0234 - acc: 0.8876 - val_loss: 0.0240 - val_acc: 0.8862\n",
      "Epoch 702/720\n",
      "8564/8564 [==============================] - 4s 446us/step - loss: 0.0235 - acc: 0.8871 - val_loss: 0.0235 - val_acc: 0.8870\n",
      "Epoch 703/720\n",
      "8564/8564 [==============================] - 4s 442us/step - loss: 0.0234 - acc: 0.8865 - val_loss: 0.0241 - val_acc: 0.8877\n",
      "Epoch 704/720\n",
      "8564/8564 [==============================] - 4s 442us/step - loss: 0.0236 - acc: 0.8857 - val_loss: 0.0234 - val_acc: 0.8874\n",
      "Epoch 705/720\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0232 - acc: 0.8874 - val_loss: 0.0234 - val_acc: 0.8874\n",
      "Epoch 706/720\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0233 - acc: 0.8869 - val_loss: 0.0240 - val_acc: 0.8877\n",
      "Epoch 707/720\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0233 - acc: 0.8876 - val_loss: 0.0240 - val_acc: 0.8875\n",
      "Epoch 708/720\n",
      "8564/8564 [==============================] - 4s 442us/step - loss: 0.0234 - acc: 0.8876 - val_loss: 0.0239 - val_acc: 0.8875\n",
      "Epoch 709/720\n",
      "8564/8564 [==============================] - 4s 446us/step - loss: 0.0234 - acc: 0.8878 - val_loss: 0.0235 - val_acc: 0.8876\n",
      "Epoch 710/720\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0231 - acc: 0.8874 - val_loss: 0.0238 - val_acc: 0.8876\n",
      "Epoch 711/720\n",
      "8564/8564 [==============================] - 4s 440us/step - loss: 0.0235 - acc: 0.8855 - val_loss: 0.0243 - val_acc: 0.8713\n",
      "Epoch 712/720\n",
      "8564/8564 [==============================] - 4s 442us/step - loss: 0.0233 - acc: 0.8802 - val_loss: 0.0245 - val_acc: 0.8665\n",
      "Epoch 713/720\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0232 - acc: 0.8801 - val_loss: 0.0238 - val_acc: 0.8793\n",
      "Epoch 714/720\n",
      "8564/8564 [==============================] - 4s 444us/step - loss: 0.0235 - acc: 0.8782 - val_loss: 0.0235 - val_acc: 0.8744\n",
      "Epoch 715/720\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0233 - acc: 0.8794 - val_loss: 0.0242 - val_acc: 0.8687\n",
      "Epoch 716/720\n",
      "8564/8564 [==============================] - 4s 437us/step - loss: 0.0234 - acc: 0.8784 - val_loss: 0.0239 - val_acc: 0.8740\n",
      "Epoch 717/720\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0234 - acc: 0.8788 - val_loss: 0.0232 - val_acc: 0.8804\n",
      "Epoch 718/720\n",
      "8564/8564 [==============================] - 4s 447us/step - loss: 0.0235 - acc: 0.8779 - val_loss: 0.0232 - val_acc: 0.8821\n",
      "Epoch 719/720\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0233 - acc: 0.8797 - val_loss: 0.0235 - val_acc: 0.8795\n",
      "Epoch 720/720\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0234 - acc: 0.8790 - val_loss: 0.0231 - val_acc: 0.8859\n",
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 701/720\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0232 - acc: 0.9855 - val_loss: 0.0184 - val_acc: 0.9885\n",
      "Epoch 702/720\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0247 - acc: 0.9844 - val_loss: 0.0285 - val_acc: 0.9803\n",
      "Epoch 703/720\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0236 - acc: 0.9836 - val_loss: 0.0195 - val_acc: 0.9880\n",
      "Epoch 704/720\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0248 - acc: 0.9854 - val_loss: 0.0257 - val_acc: 0.9848\n",
      "Epoch 705/720\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0239 - acc: 0.9841 - val_loss: 0.0201 - val_acc: 0.9901\n",
      "Epoch 706/720\n",
      "8564/8564 [==============================] - 4s 446us/step - loss: 0.0261 - acc: 0.9839 - val_loss: 0.0207 - val_acc: 0.9868\n",
      "Epoch 707/720\n",
      "8564/8564 [==============================] - 4s 442us/step - loss: 0.0244 - acc: 0.9830 - val_loss: 0.0205 - val_acc: 0.9848\n",
      "Epoch 708/720\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0231 - acc: 0.9841 - val_loss: 0.0260 - val_acc: 0.9861\n",
      "Epoch 709/720\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0242 - acc: 0.9841 - val_loss: 0.0265 - val_acc: 0.9883\n",
      "Epoch 710/720\n",
      "8564/8564 [==============================] - 4s 437us/step - loss: 0.0238 - acc: 0.9837 - val_loss: 0.0225 - val_acc: 0.9883\n",
      "Epoch 711/720\n",
      "8564/8564 [==============================] - 4s 440us/step - loss: 0.0258 - acc: 0.9820 - val_loss: 0.0272 - val_acc: 0.9901\n",
      "Epoch 712/720\n",
      "8564/8564 [==============================] - 4s 446us/step - loss: 0.0239 - acc: 0.9849 - val_loss: 0.0222 - val_acc: 0.9883\n",
      "Epoch 713/720\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0241 - acc: 0.9842 - val_loss: 0.0199 - val_acc: 0.9865\n",
      "Epoch 714/720\n",
      "8564/8564 [==============================] - 4s 444us/step - loss: 0.0227 - acc: 0.9837 - val_loss: 0.0261 - val_acc: 0.9889\n",
      "Epoch 715/720\n",
      "8564/8564 [==============================] - 4s 442us/step - loss: 0.0272 - acc: 0.9845 - val_loss: 0.0220 - val_acc: 0.9832\n",
      "Epoch 716/720\n",
      "8564/8564 [==============================] - 4s 437us/step - loss: 0.0238 - acc: 0.9840 - val_loss: 0.0202 - val_acc: 0.9870\n",
      "Epoch 717/720\n",
      "8564/8564 [==============================] - 4s 444us/step - loss: 0.0227 - acc: 0.9850 - val_loss: 0.0211 - val_acc: 0.9874\n",
      "Epoch 718/720\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0241 - acc: 0.9821 - val_loss: 0.0225 - val_acc: 0.9876\n",
      "Epoch 719/720\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0248 - acc: 0.9858 - val_loss: 0.0247 - val_acc: 0.9886\n",
      "Epoch 720/720\n",
      "8564/8564 [==============================] - 4s 442us/step - loss: 0.0244 - acc: 0.9864 - val_loss: 0.0197 - val_acc: 0.9896\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 701/720\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0273 - acc: 0.9397 - val_loss: 0.0258 - val_acc: 0.9470\n",
      "Epoch 702/720\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0273 - acc: 0.9389 - val_loss: 0.0264 - val_acc: 0.9367\n",
      "Epoch 703/720\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0275 - acc: 0.9416 - val_loss: 0.0233 - val_acc: 0.9465\n",
      "Epoch 704/720\n",
      "8564/8564 [==============================] - 4s 446us/step - loss: 0.0273 - acc: 0.9432 - val_loss: 0.0243 - val_acc: 0.9431\n",
      "Epoch 705/720\n",
      "8564/8564 [==============================] - 4s 444us/step - loss: 0.0266 - acc: 0.9416 - val_loss: 0.0235 - val_acc: 0.9580\n",
      "Epoch 706/720\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0278 - acc: 0.9408 - val_loss: 0.0232 - val_acc: 0.9548\n",
      "Epoch 707/720\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0272 - acc: 0.9409 - val_loss: 0.0249 - val_acc: 0.9540\n",
      "Epoch 708/720\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0267 - acc: 0.9423 - val_loss: 0.0250 - val_acc: 0.9450\n",
      "Epoch 709/720\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0282 - acc: 0.9392 - val_loss: 0.0240 - val_acc: 0.9564\n",
      "Epoch 710/720\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0278 - acc: 0.9406 - val_loss: 0.0233 - val_acc: 0.9455\n",
      "Epoch 711/720\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0277 - acc: 0.9385 - val_loss: 0.0263 - val_acc: 0.9467\n",
      "Epoch 712/720\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0279 - acc: 0.9419 - val_loss: 0.0257 - val_acc: 0.9418\n",
      "Epoch 713/720\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0281 - acc: 0.9380 - val_loss: 0.0241 - val_acc: 0.9483\n",
      "Epoch 714/720\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0277 - acc: 0.9392 - val_loss: 0.0244 - val_acc: 0.9544\n",
      "Epoch 715/720\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0277 - acc: 0.9381 - val_loss: 0.0244 - val_acc: 0.9456\n",
      "Epoch 716/720\n",
      "8564/8564 [==============================] - 4s 446us/step - loss: 0.0268 - acc: 0.9427 - val_loss: 0.0224 - val_acc: 0.9557\n",
      "Epoch 717/720\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0266 - acc: 0.9434 - val_loss: 0.0238 - val_acc: 0.9523\n",
      "Epoch 718/720\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0272 - acc: 0.9408 - val_loss: 0.0220 - val_acc: 0.9482\n",
      "Epoch 719/720\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0270 - acc: 0.9415 - val_loss: 0.0247 - val_acc: 0.9435\n",
      "Epoch 720/720\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0273 - acc: 0.9399 - val_loss: 0.0243 - val_acc: 0.9369\n",
      "start training round 36\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 721/740\n",
      "8564/8564 [==============================] - 4s 447us/step - loss: 0.0253 - acc: 0.6637 - val_loss: 0.0259 - val_acc: 0.5736\n",
      "Epoch 722/740\n",
      "8564/8564 [==============================] - 4s 447us/step - loss: 0.0253 - acc: 0.6641 - val_loss: 0.0251 - val_acc: 0.7260\n",
      "Epoch 723/740\n",
      "8564/8564 [==============================] - 4s 444us/step - loss: 0.0252 - acc: 0.6744 - val_loss: 0.0258 - val_acc: 0.5810\n",
      "Epoch 724/740\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0252 - acc: 0.6558 - val_loss: 0.0258 - val_acc: 0.5837\n",
      "Epoch 725/740\n",
      "8564/8564 [==============================] - 4s 449us/step - loss: 0.0251 - acc: 0.6622 - val_loss: 0.0265 - val_acc: 0.5600\n",
      "Epoch 726/740\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0254 - acc: 0.6503 - val_loss: 0.0248 - val_acc: 0.7194\n",
      "Epoch 727/740\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0252 - acc: 0.6610 - val_loss: 0.0265 - val_acc: 0.5589\n",
      "Epoch 728/740\n",
      "8564/8564 [==============================] - 4s 444us/step - loss: 0.0251 - acc: 0.7275 - val_loss: 0.0266 - val_acc: 0.7434\n",
      "Epoch 729/740\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0254 - acc: 0.7394 - val_loss: 0.0259 - val_acc: 0.7453\n",
      "Epoch 730/740\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0253 - acc: 0.7442 - val_loss: 0.0260 - val_acc: 0.7463\n",
      "Epoch 731/740\n",
      "8564/8564 [==============================] - 4s 447us/step - loss: 0.0252 - acc: 0.7456 - val_loss: 0.0273 - val_acc: 0.7434\n",
      "Epoch 732/740\n",
      "8564/8564 [==============================] - 4s 444us/step - loss: 0.0252 - acc: 0.7482 - val_loss: 0.0260 - val_acc: 0.7462\n",
      "Epoch 733/740\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0253 - acc: 0.7499 - val_loss: 0.0248 - val_acc: 0.7455\n",
      "Epoch 734/740\n",
      "8564/8564 [==============================] - 4s 444us/step - loss: 0.0250 - acc: 0.6821 - val_loss: 0.0260 - val_acc: 0.5834\n",
      "Epoch 735/740\n",
      "8564/8564 [==============================] - 4s 447us/step - loss: 0.0253 - acc: 0.6586 - val_loss: 0.0250 - val_acc: 0.6218\n",
      "Epoch 736/740\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0252 - acc: 0.6701 - val_loss: 0.0256 - val_acc: 0.5931\n",
      "Epoch 737/740\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0252 - acc: 0.6678 - val_loss: 0.0264 - val_acc: 0.7406\n",
      "Epoch 738/740\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0252 - acc: 0.7401 - val_loss: 0.0253 - val_acc: 0.7433\n",
      "Epoch 739/740\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0248 - acc: 0.7071 - val_loss: 0.0261 - val_acc: 0.7324\n",
      "Epoch 740/740\n",
      "8564/8564 [==============================] - 4s 439us/step - loss: 0.0252 - acc: 0.6592 - val_loss: 0.0263 - val_acc: 0.7352\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 721/740\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0231 - acc: 0.8814 - val_loss: 0.0237 - val_acc: 0.8755\n",
      "Epoch 722/740\n",
      "8564/8564 [==============================] - 4s 449us/step - loss: 0.0233 - acc: 0.8795 - val_loss: 0.0229 - val_acc: 0.8814\n",
      "Epoch 723/740\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0233 - acc: 0.8797 - val_loss: 0.0232 - val_acc: 0.8818\n",
      "Epoch 724/740\n",
      "8564/8564 [==============================] - 4s 449us/step - loss: 0.0233 - acc: 0.8800 - val_loss: 0.0235 - val_acc: 0.8794\n",
      "Epoch 725/740\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0232 - acc: 0.8804 - val_loss: 0.0235 - val_acc: 0.8760\n",
      "Epoch 726/740\n",
      "8564/8564 [==============================] - 4s 439us/step - loss: 0.0234 - acc: 0.8786 - val_loss: 0.0244 - val_acc: 0.8715\n",
      "Epoch 727/740\n",
      "8564/8564 [==============================] - 4s 440us/step - loss: 0.0234 - acc: 0.8786 - val_loss: 0.0238 - val_acc: 0.8710\n",
      "Epoch 728/740\n",
      "8564/8564 [==============================] - 4s 440us/step - loss: 0.0232 - acc: 0.8803 - val_loss: 0.0232 - val_acc: 0.8821\n",
      "Epoch 729/740\n",
      "8564/8564 [==============================] - 4s 449us/step - loss: 0.0232 - acc: 0.8796 - val_loss: 0.0231 - val_acc: 0.8790\n",
      "Epoch 730/740\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0232 - acc: 0.8797 - val_loss: 0.0240 - val_acc: 0.8706\n",
      "Epoch 731/740\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0234 - acc: 0.8798 - val_loss: 0.0230 - val_acc: 0.8814\n",
      "Epoch 732/740\n",
      "8564/8564 [==============================] - 4s 447us/step - loss: 0.0233 - acc: 0.8875 - val_loss: 0.0233 - val_acc: 0.8881\n",
      "Epoch 733/740\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0233 - acc: 0.8874 - val_loss: 0.0239 - val_acc: 0.8878\n",
      "Epoch 734/740\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0233 - acc: 0.8868 - val_loss: 0.0233 - val_acc: 0.8878\n",
      "Epoch 735/740\n",
      "8564/8564 [==============================] - 4s 446us/step - loss: 0.0232 - acc: 0.8876 - val_loss: 0.0229 - val_acc: 0.8879\n",
      "Epoch 736/740\n",
      "8564/8564 [==============================] - 4s 447us/step - loss: 0.0231 - acc: 0.8879 - val_loss: 0.0237 - val_acc: 0.8880\n",
      "Epoch 737/740\n",
      "8564/8564 [==============================] - 4s 446us/step - loss: 0.0233 - acc: 0.8881 - val_loss: 0.0233 - val_acc: 0.8876\n",
      "Epoch 738/740\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0234 - acc: 0.8878 - val_loss: 0.0234 - val_acc: 0.8878\n",
      "Epoch 739/740\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0233 - acc: 0.8881 - val_loss: 0.0238 - val_acc: 0.8880\n",
      "Epoch 740/740\n",
      "8564/8564 [==============================] - 4s 440us/step - loss: 0.0233 - acc: 0.8877 - val_loss: 0.0235 - val_acc: 0.8882\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 721/740\n",
      "8564/8564 [==============================] - 4s 438us/step - loss: 0.0239 - acc: 0.9842 - val_loss: 0.0296 - val_acc: 0.9875\n",
      "Epoch 722/740\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0245 - acc: 0.9840 - val_loss: 0.0168 - val_acc: 0.9880\n",
      "Epoch 723/740\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0239 - acc: 0.9839 - val_loss: 0.0199 - val_acc: 0.9871\n",
      "Epoch 724/740\n",
      "8564/8564 [==============================] - 4s 442us/step - loss: 0.0244 - acc: 0.9837 - val_loss: 0.0213 - val_acc: 0.9862\n",
      "Epoch 725/740\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0235 - acc: 0.9834 - val_loss: 0.0230 - val_acc: 0.9861\n",
      "Epoch 726/740\n",
      "8564/8564 [==============================] - 4s 447us/step - loss: 0.0229 - acc: 0.9833 - val_loss: 0.0233 - val_acc: 0.9870\n",
      "Epoch 727/740\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0237 - acc: 0.9839 - val_loss: 0.0174 - val_acc: 0.9882\n",
      "Epoch 728/740\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0232 - acc: 0.9831 - val_loss: 0.0217 - val_acc: 0.9874\n",
      "Epoch 729/740\n",
      "8564/8564 [==============================] - 4s 447us/step - loss: 0.0239 - acc: 0.9848 - val_loss: 0.0248 - val_acc: 0.9878\n",
      "Epoch 730/740\n",
      "8564/8564 [==============================] - 4s 464us/step - loss: 0.0233 - acc: 0.9858 - val_loss: 0.0228 - val_acc: 0.9889\n",
      "Epoch 731/740\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0251 - acc: 0.9848 - val_loss: 0.0246 - val_acc: 0.9851\n",
      "Epoch 732/740\n",
      "8564/8564 [==============================] - 4s 446us/step - loss: 0.0255 - acc: 0.9824 - val_loss: 0.0251 - val_acc: 0.9876\n",
      "Epoch 733/740\n",
      "8564/8564 [==============================] - 4s 446us/step - loss: 0.0232 - acc: 0.9851 - val_loss: 0.0202 - val_acc: 0.9870\n",
      "Epoch 734/740\n",
      "8564/8564 [==============================] - 4s 447us/step - loss: 0.0231 - acc: 0.9847 - val_loss: 0.0214 - val_acc: 0.9879\n",
      "Epoch 735/740\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0235 - acc: 0.9855 - val_loss: 0.0235 - val_acc: 0.9860\n",
      "Epoch 736/740\n",
      "8564/8564 [==============================] - 4s 444us/step - loss: 0.0220 - acc: 0.9850 - val_loss: 0.0209 - val_acc: 0.9869\n",
      "Epoch 737/740\n",
      "8564/8564 [==============================] - 4s 463us/step - loss: 0.0250 - acc: 0.9855 - val_loss: 0.0243 - val_acc: 0.9878\n",
      "Epoch 738/740\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0253 - acc: 0.9851 - val_loss: 0.0287 - val_acc: 0.9887\n",
      "Epoch 739/740\n",
      "8564/8564 [==============================] - 4s 444us/step - loss: 0.0225 - acc: 0.9851 - val_loss: 0.0234 - val_acc: 0.9889\n",
      "Epoch 740/740\n",
      "8564/8564 [==============================] - 4s 483us/step - loss: 0.0239 - acc: 0.9848 - val_loss: 0.0205 - val_acc: 0.9869\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 721/740\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0278 - acc: 0.9386 - val_loss: 0.0216 - val_acc: 0.9472\n",
      "Epoch 722/740\n",
      "8564/8564 [==============================] - 4s 464us/step - loss: 0.0278 - acc: 0.9375 - val_loss: 0.0263 - val_acc: 0.9436\n",
      "Epoch 723/740\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0274 - acc: 0.9398 - val_loss: 0.0219 - val_acc: 0.9566\n",
      "Epoch 724/740\n",
      "8564/8564 [==============================] - 4s 447us/step - loss: 0.0273 - acc: 0.9399 - val_loss: 0.0236 - val_acc: 0.9487\n",
      "Epoch 725/740\n",
      "8564/8564 [==============================] - 4s 444us/step - loss: 0.0282 - acc: 0.9382 - val_loss: 0.0244 - val_acc: 0.9497\n",
      "Epoch 726/740\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0282 - acc: 0.9368 - val_loss: 0.0260 - val_acc: 0.9410\n",
      "Epoch 727/740\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0278 - acc: 0.9391 - val_loss: 0.0251 - val_acc: 0.9530\n",
      "Epoch 728/740\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0256 - acc: 0.9424 - val_loss: 0.0252 - val_acc: 0.9446\n",
      "Epoch 729/740\n",
      "8564/8564 [==============================] - 4s 476us/step - loss: 0.0285 - acc: 0.9368 - val_loss: 0.0263 - val_acc: 0.9332\n",
      "Epoch 730/740\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0265 - acc: 0.9408 - val_loss: 0.0228 - val_acc: 0.9524\n",
      "Epoch 731/740\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0270 - acc: 0.9408 - val_loss: 0.0250 - val_acc: 0.9525\n",
      "Epoch 732/740\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0268 - acc: 0.9395 - val_loss: 0.0239 - val_acc: 0.9536\n",
      "Epoch 733/740\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0285 - acc: 0.9384 - val_loss: 0.0298 - val_acc: 0.9473\n",
      "Epoch 734/740\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0267 - acc: 0.9437 - val_loss: 0.0204 - val_acc: 0.9517\n",
      "Epoch 735/740\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0272 - acc: 0.9415 - val_loss: 0.0228 - val_acc: 0.9468\n",
      "Epoch 736/740\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0272 - acc: 0.9398 - val_loss: 0.0255 - val_acc: 0.9438\n",
      "Epoch 737/740\n",
      "8564/8564 [==============================] - 4s 449us/step - loss: 0.0273 - acc: 0.9399 - val_loss: 0.0372 - val_acc: 0.9181\n",
      "Epoch 738/740\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0265 - acc: 0.9392 - val_loss: 0.0222 - val_acc: 0.9526\n",
      "Epoch 739/740\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0264 - acc: 0.9424 - val_loss: 0.0249 - val_acc: 0.9529\n",
      "Epoch 740/740\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0280 - acc: 0.9409 - val_loss: 0.0256 - val_acc: 0.9508\n",
      "start training round 37\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 741/760\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0249 - acc: 0.6834 - val_loss: 0.0247 - val_acc: 0.7430\n",
      "Epoch 742/760\n",
      "8564/8564 [==============================] - 4s 446us/step - loss: 0.0249 - acc: 0.6824 - val_loss: 0.0268 - val_acc: 0.5701\n",
      "Epoch 743/760\n",
      "8564/8564 [==============================] - 4s 446us/step - loss: 0.0255 - acc: 0.6594 - val_loss: 0.0259 - val_acc: 0.5879\n",
      "Epoch 744/760\n",
      "8564/8564 [==============================] - 4s 442us/step - loss: 0.0247 - acc: 0.6796 - val_loss: 0.0256 - val_acc: 0.5847\n",
      "Epoch 745/760\n",
      "8564/8564 [==============================] - 4s 442us/step - loss: 0.0254 - acc: 0.6554 - val_loss: 0.0257 - val_acc: 0.5755\n",
      "Epoch 746/760\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0250 - acc: 0.6615 - val_loss: 0.0262 - val_acc: 0.5634\n",
      "Epoch 747/760\n",
      "8564/8564 [==============================] - 4s 440us/step - loss: 0.0253 - acc: 0.7306 - val_loss: 0.0261 - val_acc: 0.7450\n",
      "Epoch 748/760\n",
      "8564/8564 [==============================] - 4s 444us/step - loss: 0.0253 - acc: 0.7471 - val_loss: 0.0266 - val_acc: 0.7451\n",
      "Epoch 749/760\n",
      "8564/8564 [==============================] - 4s 449us/step - loss: 0.0253 - acc: 0.6801 - val_loss: 0.0248 - val_acc: 0.7410\n",
      "Epoch 750/760\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0251 - acc: 0.6680 - val_loss: 0.0255 - val_acc: 0.5967\n",
      "Epoch 751/760\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0250 - acc: 0.6698 - val_loss: 0.0258 - val_acc: 0.5755\n",
      "Epoch 752/760\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0250 - acc: 0.6651 - val_loss: 0.0259 - val_acc: 0.5682\n",
      "Epoch 753/760\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0249 - acc: 0.6758 - val_loss: 0.0249 - val_acc: 0.7443\n",
      "Epoch 754/760\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0253 - acc: 0.7475 - val_loss: 0.0251 - val_acc: 0.7456\n",
      "Epoch 755/760\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0252 - acc: 0.7263 - val_loss: 0.0257 - val_acc: 0.7463\n",
      "Epoch 756/760\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0253 - acc: 0.7484 - val_loss: 0.0249 - val_acc: 0.7473\n",
      "Epoch 757/760\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0253 - acc: 0.7474 - val_loss: 0.0257 - val_acc: 0.7461\n",
      "Epoch 758/760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0253 - acc: 0.7427 - val_loss: 0.0251 - val_acc: 0.7459\n",
      "Epoch 759/760\n",
      "8564/8564 [==============================] - 4s 447us/step - loss: 0.0249 - acc: 0.6793 - val_loss: 0.0258 - val_acc: 0.5762\n",
      "Epoch 760/760\n",
      "8564/8564 [==============================] - 4s 447us/step - loss: 0.0248 - acc: 0.6755 - val_loss: 0.0253 - val_acc: 0.7453\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 741/760\n",
      "8564/8564 [==============================] - 4s 446us/step - loss: 0.0231 - acc: 0.8883 - val_loss: 0.0232 - val_acc: 0.8878\n",
      "Epoch 742/760\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0232 - acc: 0.8874 - val_loss: 0.0233 - val_acc: 0.8884\n",
      "Epoch 743/760\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0232 - acc: 0.8882 - val_loss: 0.0232 - val_acc: 0.8881\n",
      "Epoch 744/760\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0229 - acc: 0.8866 - val_loss: 0.0232 - val_acc: 0.8783\n",
      "Epoch 745/760\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0231 - acc: 0.8808 - val_loss: 0.0230 - val_acc: 0.8819\n",
      "Epoch 746/760\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0232 - acc: 0.8797 - val_loss: 0.0236 - val_acc: 0.8815\n",
      "Epoch 747/760\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0231 - acc: 0.8809 - val_loss: 0.0236 - val_acc: 0.8816\n",
      "Epoch 748/760\n",
      "8564/8564 [==============================] - 4s 447us/step - loss: 0.0231 - acc: 0.8803 - val_loss: 0.0236 - val_acc: 0.8777\n",
      "Epoch 749/760\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0232 - acc: 0.8799 - val_loss: 0.0231 - val_acc: 0.8811\n",
      "Epoch 750/760\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0232 - acc: 0.8799 - val_loss: 0.0242 - val_acc: 0.8728\n",
      "Epoch 751/760\n",
      "8564/8564 [==============================] - 4s 444us/step - loss: 0.0232 - acc: 0.8799 - val_loss: 0.0237 - val_acc: 0.8753\n",
      "Epoch 752/760\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0232 - acc: 0.8802 - val_loss: 0.0236 - val_acc: 0.8812\n",
      "Epoch 753/760\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0232 - acc: 0.8809 - val_loss: 0.0233 - val_acc: 0.8752\n",
      "Epoch 754/760\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0232 - acc: 0.8805 - val_loss: 0.0234 - val_acc: 0.8838\n",
      "Epoch 755/760\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0232 - acc: 0.8809 - val_loss: 0.0240 - val_acc: 0.8678\n",
      "Epoch 756/760\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0233 - acc: 0.8790 - val_loss: 0.0233 - val_acc: 0.8793\n",
      "Epoch 757/760\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0230 - acc: 0.8820 - val_loss: 0.0227 - val_acc: 0.8849\n",
      "Epoch 758/760\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0235 - acc: 0.8784 - val_loss: 0.0224 - val_acc: 0.8868\n",
      "Epoch 759/760\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0230 - acc: 0.8818 - val_loss: 0.0230 - val_acc: 0.8798\n",
      "Epoch 760/760\n",
      "8564/8564 [==============================] - 4s 466us/step - loss: 0.0233 - acc: 0.8795 - val_loss: 0.0233 - val_acc: 0.8829\n",
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 741/760\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0243 - acc: 0.9845 - val_loss: 0.0263 - val_acc: 0.9878\n",
      "Epoch 742/760\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0248 - acc: 0.9846 - val_loss: 0.0269 - val_acc: 0.9881\n",
      "Epoch 743/760\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0240 - acc: 0.9834 - val_loss: 0.0237 - val_acc: 0.9848\n",
      "Epoch 744/760\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0246 - acc: 0.9844 - val_loss: 0.0222 - val_acc: 0.9827\n",
      "Epoch 745/760\n",
      "8564/8564 [==============================] - 4s 446us/step - loss: 0.0244 - acc: 0.9858 - val_loss: 0.0162 - val_acc: 0.9902\n",
      "Epoch 746/760\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0238 - acc: 0.9838 - val_loss: 0.0179 - val_acc: 0.9880\n",
      "Epoch 747/760\n",
      "8564/8564 [==============================] - 4s 447us/step - loss: 0.0250 - acc: 0.9832 - val_loss: 0.0219 - val_acc: 0.9852\n",
      "Epoch 748/760\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0231 - acc: 0.9849 - val_loss: 0.0245 - val_acc: 0.9857\n",
      "Epoch 749/760\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0241 - acc: 0.9825 - val_loss: 0.0218 - val_acc: 0.9880\n",
      "Epoch 750/760\n",
      "8564/8564 [==============================] - 4s 449us/step - loss: 0.0253 - acc: 0.9831 - val_loss: 0.0199 - val_acc: 0.9882\n",
      "Epoch 751/760\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0223 - acc: 0.9842 - val_loss: 0.0261 - val_acc: 0.9866\n",
      "Epoch 752/760\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0245 - acc: 0.9839 - val_loss: 0.0213 - val_acc: 0.9878\n",
      "Epoch 753/760\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0240 - acc: 0.9847 - val_loss: 0.0233 - val_acc: 0.9850\n",
      "Epoch 754/760\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0244 - acc: 0.9846 - val_loss: 0.0230 - val_acc: 0.9884\n",
      "Epoch 755/760\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0267 - acc: 0.9817 - val_loss: 0.0227 - val_acc: 0.9873\n",
      "Epoch 756/760\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0249 - acc: 0.9836 - val_loss: 0.0174 - val_acc: 0.9899\n",
      "Epoch 757/760\n",
      "8564/8564 [==============================] - 4s 449us/step - loss: 0.0245 - acc: 0.9833 - val_loss: 0.0197 - val_acc: 0.9879\n",
      "Epoch 758/760\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0241 - acc: 0.9856 - val_loss: 0.0304 - val_acc: 0.9894\n",
      "Epoch 759/760\n",
      "8564/8564 [==============================] - 4s 447us/step - loss: 0.0241 - acc: 0.9849 - val_loss: 0.0231 - val_acc: 0.9880\n",
      "Epoch 760/760\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0241 - acc: 0.9847 - val_loss: 0.0245 - val_acc: 0.9864\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 741/760\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0268 - acc: 0.9416 - val_loss: 0.0238 - val_acc: 0.9525\n",
      "Epoch 742/760\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0271 - acc: 0.9401 - val_loss: 0.0257 - val_acc: 0.9466\n",
      "Epoch 743/760\n",
      "8564/8564 [==============================] - 4s 462us/step - loss: 0.0266 - acc: 0.9424 - val_loss: 0.0237 - val_acc: 0.9493\n",
      "Epoch 744/760\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0260 - acc: 0.9424 - val_loss: 0.0257 - val_acc: 0.9507\n",
      "Epoch 745/760\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0275 - acc: 0.9421 - val_loss: 0.0297 - val_acc: 0.9324\n",
      "Epoch 746/760\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0270 - acc: 0.9392 - val_loss: 0.0232 - val_acc: 0.9483\n",
      "Epoch 747/760\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0260 - acc: 0.9429 - val_loss: 0.0237 - val_acc: 0.9564\n",
      "Epoch 748/760\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0284 - acc: 0.9373 - val_loss: 0.0223 - val_acc: 0.9517\n",
      "Epoch 749/760\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0263 - acc: 0.9433 - val_loss: 0.0270 - val_acc: 0.9389\n",
      "Epoch 750/760\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0273 - acc: 0.9414 - val_loss: 0.0226 - val_acc: 0.9465\n",
      "Epoch 751/760\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0266 - acc: 0.9436 - val_loss: 0.0243 - val_acc: 0.9532\n",
      "Epoch 752/760\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0268 - acc: 0.9425 - val_loss: 0.0251 - val_acc: 0.9529\n",
      "Epoch 753/760\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0270 - acc: 0.9413 - val_loss: 0.0272 - val_acc: 0.9427\n",
      "Epoch 754/760\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0269 - acc: 0.9392 - val_loss: 0.0230 - val_acc: 0.9512\n",
      "Epoch 755/760\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0266 - acc: 0.9416 - val_loss: 0.0286 - val_acc: 0.9406\n",
      "Epoch 756/760\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0270 - acc: 0.9393 - val_loss: 0.0297 - val_acc: 0.9474\n",
      "Epoch 757/760\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0274 - acc: 0.9407 - val_loss: 0.0242 - val_acc: 0.9461\n",
      "Epoch 758/760\n",
      "8564/8564 [==============================] - 4s 449us/step - loss: 0.0261 - acc: 0.9411 - val_loss: 0.0300 - val_acc: 0.9396\n",
      "Epoch 759/760\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0276 - acc: 0.9414 - val_loss: 0.0244 - val_acc: 0.9507\n",
      "Epoch 760/760\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0275 - acc: 0.9399 - val_loss: 0.0264 - val_acc: 0.9408\n",
      "start training round 38\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 761/780\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0255 - acc: 0.7383 - val_loss: 0.0271 - val_acc: 0.7444\n",
      "Epoch 762/780\n",
      "8564/8564 [==============================] - 4s 441us/step - loss: 0.0253 - acc: 0.7456 - val_loss: 0.0250 - val_acc: 0.7475\n",
      "Epoch 763/780\n",
      "8564/8564 [==============================] - 4s 442us/step - loss: 0.0251 - acc: 0.7438 - val_loss: 0.0253 - val_acc: 0.7458\n",
      "Epoch 764/780\n",
      "8564/8564 [==============================] - 4s 446us/step - loss: 0.0248 - acc: 0.6947 - val_loss: 0.0248 - val_acc: 0.7457\n",
      "Epoch 765/780\n",
      "8564/8564 [==============================] - 4s 446us/step - loss: 0.0249 - acc: 0.6848 - val_loss: 0.0250 - val_acc: 0.7465\n",
      "Epoch 766/780\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0253 - acc: 0.7452 - val_loss: 0.0261 - val_acc: 0.7491\n",
      "Epoch 767/780\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0249 - acc: 0.7451 - val_loss: 0.0255 - val_acc: 0.7379\n",
      "Epoch 768/780\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0249 - acc: 0.7214 - val_loss: 0.0253 - val_acc: 0.7482\n",
      "Epoch 769/780\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0247 - acc: 0.7511 - val_loss: 0.0246 - val_acc: 0.7465\n",
      "Epoch 770/780\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0248 - acc: 0.6808 - val_loss: 0.0250 - val_acc: 0.6702\n",
      "Epoch 771/780\n",
      "8564/8564 [==============================] - 4s 447us/step - loss: 0.0251 - acc: 0.6678 - val_loss: 0.0263 - val_acc: 0.5754\n",
      "Epoch 772/780\n",
      "8564/8564 [==============================] - 4s 447us/step - loss: 0.0249 - acc: 0.6773 - val_loss: 0.0263 - val_acc: 0.5674\n",
      "Epoch 773/780\n",
      "8564/8564 [==============================] - 4s 446us/step - loss: 0.0252 - acc: 0.6605 - val_loss: 0.0264 - val_acc: 0.5550\n",
      "Epoch 774/780\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0251 - acc: 0.6597 - val_loss: 0.0246 - val_acc: 0.7478\n",
      "Epoch 775/780\n",
      "8564/8564 [==============================] - 4s 449us/step - loss: 0.0248 - acc: 0.6786 - val_loss: 0.0257 - val_acc: 0.7418\n",
      "Epoch 776/780\n",
      "8564/8564 [==============================] - 4s 449us/step - loss: 0.0249 - acc: 0.6745 - val_loss: 0.0253 - val_acc: 0.7431\n",
      "Epoch 777/780\n",
      "8564/8564 [==============================] - 4s 449us/step - loss: 0.0248 - acc: 0.7189 - val_loss: 0.0249 - val_acc: 0.7458\n",
      "Epoch 778/780\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0248 - acc: 0.6853 - val_loss: 0.0249 - val_acc: 0.7470\n",
      "Epoch 779/780\n",
      "8564/8564 [==============================] - 4s 446us/step - loss: 0.0249 - acc: 0.7386 - val_loss: 0.0259 - val_acc: 0.7479\n",
      "Epoch 780/780\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0247 - acc: 0.7487 - val_loss: 0.0269 - val_acc: 0.7480\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 761/780\n",
      "8564/8564 [==============================] - 4s 470us/step - loss: 0.0230 - acc: 0.8882 - val_loss: 0.0237 - val_acc: 0.8881\n",
      "Epoch 762/780\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0231 - acc: 0.8883 - val_loss: 0.0234 - val_acc: 0.8879\n",
      "Epoch 763/780\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0231 - acc: 0.8881 - val_loss: 0.0239 - val_acc: 0.8882\n",
      "Epoch 764/780\n",
      "8564/8564 [==============================] - 4s 446us/step - loss: 0.0236 - acc: 0.8849 - val_loss: 0.0240 - val_acc: 0.8774\n",
      "Epoch 765/780\n",
      "8564/8564 [==============================] - 4s 442us/step - loss: 0.0231 - acc: 0.8848 - val_loss: 0.0236 - val_acc: 0.8876\n",
      "Epoch 766/780\n",
      "8564/8564 [==============================] - 4s 440us/step - loss: 0.0232 - acc: 0.8883 - val_loss: 0.0229 - val_acc: 0.8881\n",
      "Epoch 767/780\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0231 - acc: 0.8888 - val_loss: 0.0232 - val_acc: 0.8884\n",
      "Epoch 768/780\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0230 - acc: 0.8888 - val_loss: 0.0237 - val_acc: 0.8879\n",
      "Epoch 769/780\n",
      "8564/8564 [==============================] - 4s 446us/step - loss: 0.0232 - acc: 0.8884 - val_loss: 0.0238 - val_acc: 0.8861\n",
      "Epoch 770/780\n",
      "8564/8564 [==============================] - 4s 447us/step - loss: 0.0234 - acc: 0.8880 - val_loss: 0.0234 - val_acc: 0.8881\n",
      "Epoch 771/780\n",
      "8564/8564 [==============================] - 4s 442us/step - loss: 0.0232 - acc: 0.8884 - val_loss: 0.0234 - val_acc: 0.8881\n",
      "Epoch 772/780\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0233 - acc: 0.8888 - val_loss: 0.0233 - val_acc: 0.8882\n",
      "Epoch 773/780\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0232 - acc: 0.8880 - val_loss: 0.0227 - val_acc: 0.8879\n",
      "Epoch 774/780\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0230 - acc: 0.8887 - val_loss: 0.0238 - val_acc: 0.8882\n",
      "Epoch 775/780\n",
      "8564/8564 [==============================] - 4s 446us/step - loss: 0.0230 - acc: 0.8886 - val_loss: 0.0233 - val_acc: 0.8841\n",
      "Epoch 776/780\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0231 - acc: 0.8800 - val_loss: 0.0242 - val_acc: 0.8817\n",
      "Epoch 777/780\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0231 - acc: 0.8808 - val_loss: 0.0244 - val_acc: 0.8775\n",
      "Epoch 778/780\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0230 - acc: 0.8814 - val_loss: 0.0231 - val_acc: 0.8847\n",
      "Epoch 779/780\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0232 - acc: 0.8796 - val_loss: 0.0239 - val_acc: 0.8821\n",
      "Epoch 780/780\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0231 - acc: 0.8807 - val_loss: 0.0227 - val_acc: 0.8863\n",
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 761/780\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0243 - acc: 0.9852 - val_loss: 0.0187 - val_acc: 0.9880\n",
      "Epoch 762/780\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0250 - acc: 0.9849 - val_loss: 0.0307 - val_acc: 0.9806\n",
      "Epoch 763/780\n",
      "8564/8564 [==============================] - 4s 447us/step - loss: 0.0232 - acc: 0.9845 - val_loss: 0.0216 - val_acc: 0.9879\n",
      "Epoch 764/780\n",
      "8564/8564 [==============================] - 4s 446us/step - loss: 0.0233 - acc: 0.9848 - val_loss: 0.0255 - val_acc: 0.9898\n",
      "Epoch 765/780\n",
      "8564/8564 [==============================] - 4s 444us/step - loss: 0.0250 - acc: 0.9840 - val_loss: 0.0198 - val_acc: 0.9882\n",
      "Epoch 766/780\n",
      "8564/8564 [==============================] - 4s 444us/step - loss: 0.0240 - acc: 0.9851 - val_loss: 0.0201 - val_acc: 0.9883\n",
      "Epoch 767/780\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0252 - acc: 0.9836 - val_loss: 0.0267 - val_acc: 0.9834\n",
      "Epoch 768/780\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0251 - acc: 0.9831 - val_loss: 0.0269 - val_acc: 0.9846\n",
      "Epoch 769/780\n",
      "8564/8564 [==============================] - 4s 437us/step - loss: 0.0249 - acc: 0.9843 - val_loss: 0.0222 - val_acc: 0.9875\n",
      "Epoch 770/780\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0248 - acc: 0.9847 - val_loss: 0.0237 - val_acc: 0.9822\n",
      "Epoch 771/780\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0232 - acc: 0.9841 - val_loss: 0.0206 - val_acc: 0.9887\n",
      "Epoch 772/780\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0230 - acc: 0.9844 - val_loss: 0.0185 - val_acc: 0.9873\n",
      "Epoch 773/780\n",
      "8564/8564 [==============================] - 4s 447us/step - loss: 0.0232 - acc: 0.9847 - val_loss: 0.0216 - val_acc: 0.9867\n",
      "Epoch 774/780\n",
      "8564/8564 [==============================] - 4s 444us/step - loss: 0.0230 - acc: 0.9857 - val_loss: 0.0288 - val_acc: 0.9903\n",
      "Epoch 775/780\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0235 - acc: 0.9862 - val_loss: 0.0174 - val_acc: 0.9894\n",
      "Epoch 776/780\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0244 - acc: 0.9842 - val_loss: 0.0226 - val_acc: 0.9889\n",
      "Epoch 777/780\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0230 - acc: 0.9848 - val_loss: 0.0208 - val_acc: 0.9889\n",
      "Epoch 778/780\n",
      "8564/8564 [==============================] - 4s 439us/step - loss: 0.0234 - acc: 0.9859 - val_loss: 0.0250 - val_acc: 0.9886\n",
      "Epoch 779/780\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0241 - acc: 0.9842 - val_loss: 0.0283 - val_acc: 0.9880\n",
      "Epoch 780/780\n",
      "8564/8564 [==============================] - 4s 446us/step - loss: 0.0231 - acc: 0.9849 - val_loss: 0.0230 - val_acc: 0.9885\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 761/780\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0257 - acc: 0.9448 - val_loss: 0.0253 - val_acc: 0.9484\n",
      "Epoch 762/780\n",
      "8564/8564 [==============================] - 4s 463us/step - loss: 0.0285 - acc: 0.9380 - val_loss: 0.0282 - val_acc: 0.9386\n",
      "Epoch 763/780\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0272 - acc: 0.9422 - val_loss: 0.0326 - val_acc: 0.9204\n",
      "Epoch 764/780\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0259 - acc: 0.9420 - val_loss: 0.0206 - val_acc: 0.9547\n",
      "Epoch 765/780\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0260 - acc: 0.9420 - val_loss: 0.0349 - val_acc: 0.9285\n",
      "Epoch 766/780\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0271 - acc: 0.9413 - val_loss: 0.0292 - val_acc: 0.9440\n",
      "Epoch 767/780\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0264 - acc: 0.9421 - val_loss: 0.0241 - val_acc: 0.9552\n",
      "Epoch 768/780\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0275 - acc: 0.9411 - val_loss: 0.0329 - val_acc: 0.9193\n",
      "Epoch 769/780\n",
      "8564/8564 [==============================] - 4s 449us/step - loss: 0.0267 - acc: 0.9438 - val_loss: 0.0268 - val_acc: 0.9473\n",
      "Epoch 770/780\n",
      "8564/8564 [==============================] - 4s 464us/step - loss: 0.0274 - acc: 0.9405 - val_loss: 0.0219 - val_acc: 0.9575\n",
      "Epoch 771/780\n",
      "8564/8564 [==============================] - 4s 472us/step - loss: 0.0265 - acc: 0.9432 - val_loss: 0.0212 - val_acc: 0.9572\n",
      "Epoch 772/780\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0284 - acc: 0.9399 - val_loss: 0.0251 - val_acc: 0.9473\n",
      "Epoch 773/780\n",
      "8564/8564 [==============================] - 4s 468us/step - loss: 0.0267 - acc: 0.9401 - val_loss: 0.0234 - val_acc: 0.9562\n",
      "Epoch 774/780\n",
      "8564/8564 [==============================] - 4s 485us/step - loss: 0.0276 - acc: 0.9415 - val_loss: 0.0256 - val_acc: 0.9414\n",
      "Epoch 775/780\n",
      "8564/8564 [==============================] - 4s 493us/step - loss: 0.0279 - acc: 0.9376 - val_loss: 0.0229 - val_acc: 0.9534\n",
      "Epoch 776/780\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0259 - acc: 0.9450 - val_loss: 0.0231 - val_acc: 0.9549\n",
      "Epoch 777/780\n",
      "8564/8564 [==============================] - 4s 469us/step - loss: 0.0270 - acc: 0.9419 - val_loss: 0.0243 - val_acc: 0.9552\n",
      "Epoch 778/780\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0260 - acc: 0.9425 - val_loss: 0.0265 - val_acc: 0.9421\n",
      "Epoch 779/780\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0276 - acc: 0.9402 - val_loss: 0.0244 - val_acc: 0.9497\n",
      "Epoch 780/780\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0267 - acc: 0.9426 - val_loss: 0.0216 - val_acc: 0.9569\n",
      "start training round 39\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 781/800\n",
      "8564/8564 [==============================] - 4s 483us/step - loss: 0.0249 - acc: 0.7516 - val_loss: 0.0253 - val_acc: 0.7490\n",
      "Epoch 782/800\n",
      "8564/8564 [==============================] - 4s 489us/step - loss: 0.0252 - acc: 0.7426 - val_loss: 0.0254 - val_acc: 0.7494\n",
      "Epoch 783/800\n",
      "8564/8564 [==============================] - 4s 467us/step - loss: 0.0252 - acc: 0.7424 - val_loss: 0.0244 - val_acc: 0.6887\n",
      "Epoch 784/800\n",
      "8564/8564 [==============================] - 4s 472us/step - loss: 0.0248 - acc: 0.6705 - val_loss: 0.0261 - val_acc: 0.5715\n",
      "Epoch 785/800\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0251 - acc: 0.6557 - val_loss: 0.0249 - val_acc: 0.5917\n",
      "Epoch 786/800\n",
      "8564/8564 [==============================] - 4s 469us/step - loss: 0.0248 - acc: 0.6855 - val_loss: 0.0247 - val_acc: 0.7491\n",
      "Epoch 787/800\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0250 - acc: 0.7466 - val_loss: 0.0254 - val_acc: 0.7482\n",
      "Epoch 788/800\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0248 - acc: 0.7522 - val_loss: 0.0257 - val_acc: 0.7501\n",
      "Epoch 789/800\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0250 - acc: 0.7492 - val_loss: 0.0256 - val_acc: 0.7508\n",
      "Epoch 790/800\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0249 - acc: 0.7315 - val_loss: 0.0261 - val_acc: 0.7424\n",
      "Epoch 791/800\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0247 - acc: 0.6879 - val_loss: 0.0258 - val_acc: 0.7454\n",
      "Epoch 792/800\n",
      "8564/8564 [==============================] - 4s 461us/step - loss: 0.0249 - acc: 0.7438 - val_loss: 0.0246 - val_acc: 0.7461\n",
      "Epoch 793/800\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0250 - acc: 0.7496 - val_loss: 0.0259 - val_acc: 0.7477\n",
      "Epoch 794/800\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0247 - acc: 0.7478 - val_loss: 0.0252 - val_acc: 0.7471\n",
      "Epoch 795/800\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0250 - acc: 0.7480 - val_loss: 0.0261 - val_acc: 0.7473\n",
      "Epoch 796/800\n",
      "8564/8564 [==============================] - 4s 466us/step - loss: 0.0251 - acc: 0.7471 - val_loss: 0.0252 - val_acc: 0.7488\n",
      "Epoch 797/800\n",
      "8564/8564 [==============================] - 4s 461us/step - loss: 0.0246 - acc: 0.7305 - val_loss: 0.0255 - val_acc: 0.7376\n",
      "Epoch 798/800\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0248 - acc: 0.7255 - val_loss: 0.0256 - val_acc: 0.7446\n",
      "Epoch 799/800\n",
      "8564/8564 [==============================] - 4s 462us/step - loss: 0.0250 - acc: 0.7515 - val_loss: 0.0265 - val_acc: 0.7466\n",
      "Epoch 800/800\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0249 - acc: 0.7512 - val_loss: 0.0257 - val_acc: 0.7487\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 781/800\n",
      "8564/8564 [==============================] - 4s 462us/step - loss: 0.0230 - acc: 0.8840 - val_loss: 0.0228 - val_acc: 0.8882\n",
      "Epoch 782/800\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0232 - acc: 0.8889 - val_loss: 0.0232 - val_acc: 0.8878\n",
      "Epoch 783/800\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0230 - acc: 0.8887 - val_loss: 0.0231 - val_acc: 0.8873\n",
      "Epoch 784/800\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0231 - acc: 0.8888 - val_loss: 0.0235 - val_acc: 0.8882\n",
      "Epoch 785/800\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0231 - acc: 0.8888 - val_loss: 0.0233 - val_acc: 0.8883\n",
      "Epoch 786/800\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0228 - acc: 0.8887 - val_loss: 0.0234 - val_acc: 0.8869\n",
      "Epoch 787/800\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0231 - acc: 0.8886 - val_loss: 0.0235 - val_acc: 0.8870\n",
      "Epoch 788/800\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0229 - acc: 0.8890 - val_loss: 0.0236 - val_acc: 0.8881\n",
      "Epoch 789/800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0233 - acc: 0.8886 - val_loss: 0.0239 - val_acc: 0.8883\n",
      "Epoch 790/800\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0231 - acc: 0.8891 - val_loss: 0.0236 - val_acc: 0.8878\n",
      "Epoch 791/800\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0230 - acc: 0.8887 - val_loss: 0.0235 - val_acc: 0.8882\n",
      "Epoch 792/800\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0231 - acc: 0.8886 - val_loss: 0.0224 - val_acc: 0.8882\n",
      "Epoch 793/800\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0230 - acc: 0.8883 - val_loss: 0.0240 - val_acc: 0.8881\n",
      "Epoch 794/800\n",
      "8564/8564 [==============================] - 4s 447us/step - loss: 0.0234 - acc: 0.8808 - val_loss: 0.0230 - val_acc: 0.8865\n",
      "Epoch 795/800\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0230 - acc: 0.8807 - val_loss: 0.0229 - val_acc: 0.8859\n",
      "Epoch 796/800\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0229 - acc: 0.8820 - val_loss: 0.0231 - val_acc: 0.8847\n",
      "Epoch 797/800\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0229 - acc: 0.8806 - val_loss: 0.0227 - val_acc: 0.8881\n",
      "Epoch 798/800\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0229 - acc: 0.8823 - val_loss: 0.0229 - val_acc: 0.8872\n",
      "Epoch 799/800\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0231 - acc: 0.8820 - val_loss: 0.0235 - val_acc: 0.8832\n",
      "Epoch 800/800\n",
      "8564/8564 [==============================] - 4s 444us/step - loss: 0.0229 - acc: 0.8819 - val_loss: 0.0229 - val_acc: 0.8855\n",
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 781/800\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0240 - acc: 0.9850 - val_loss: 0.0185 - val_acc: 0.9900\n",
      "Epoch 782/800\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0255 - acc: 0.9846 - val_loss: 0.0256 - val_acc: 0.9880\n",
      "Epoch 783/800\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0259 - acc: 0.9836 - val_loss: 0.0179 - val_acc: 0.9892\n",
      "Epoch 784/800\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0234 - acc: 0.9838 - val_loss: 0.0189 - val_acc: 0.9863\n",
      "Epoch 785/800\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0234 - acc: 0.9831 - val_loss: 0.0238 - val_acc: 0.9880\n",
      "Epoch 786/800\n",
      "8564/8564 [==============================] - 4s 449us/step - loss: 0.0228 - acc: 0.9865 - val_loss: 0.0264 - val_acc: 0.9893\n",
      "Epoch 787/800\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0254 - acc: 0.9851 - val_loss: 0.0218 - val_acc: 0.9873\n",
      "Epoch 788/800\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0236 - acc: 0.9844 - val_loss: 0.0209 - val_acc: 0.9881\n",
      "Epoch 789/800\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0236 - acc: 0.9837 - val_loss: 0.0204 - val_acc: 0.9875\n",
      "Epoch 790/800\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0246 - acc: 0.9835 - val_loss: 0.0179 - val_acc: 0.9894\n",
      "Epoch 791/800\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0238 - acc: 0.9843 - val_loss: 0.0210 - val_acc: 0.9862\n",
      "Epoch 792/800\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0238 - acc: 0.9840 - val_loss: 0.0273 - val_acc: 0.9886\n",
      "Epoch 793/800\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0227 - acc: 0.9845 - val_loss: 0.0231 - val_acc: 0.9900\n",
      "Epoch 794/800\n",
      "8564/8564 [==============================] - 4s 446us/step - loss: 0.0246 - acc: 0.9850 - val_loss: 0.0242 - val_acc: 0.9885\n",
      "Epoch 795/800\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0243 - acc: 0.9852 - val_loss: 0.0242 - val_acc: 0.9823\n",
      "Epoch 796/800\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0244 - acc: 0.9848 - val_loss: 0.0217 - val_acc: 0.9867\n",
      "Epoch 797/800\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0236 - acc: 0.9839 - val_loss: 0.0182 - val_acc: 0.9871\n",
      "Epoch 798/800\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0241 - acc: 0.9845 - val_loss: 0.0221 - val_acc: 0.9870\n",
      "Epoch 799/800\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0236 - acc: 0.9855 - val_loss: 0.0310 - val_acc: 0.9900\n",
      "Epoch 800/800\n",
      "8564/8564 [==============================] - 4s 449us/step - loss: 0.0238 - acc: 0.9852 - val_loss: 0.0239 - val_acc: 0.9887\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 781/800\n",
      "8564/8564 [==============================] - 4s 463us/step - loss: 0.0268 - acc: 0.9434 - val_loss: 0.0229 - val_acc: 0.9529\n",
      "Epoch 782/800\n",
      "8564/8564 [==============================] - 4s 476us/step - loss: 0.0268 - acc: 0.9414 - val_loss: 0.0222 - val_acc: 0.9548\n",
      "Epoch 783/800\n",
      "8564/8564 [==============================] - 4s 493us/step - loss: 0.0271 - acc: 0.9412 - val_loss: 0.0268 - val_acc: 0.9472\n",
      "Epoch 784/800\n",
      "8564/8564 [==============================] - 4s 490us/step - loss: 0.0264 - acc: 0.9392 - val_loss: 0.0227 - val_acc: 0.9416\n",
      "Epoch 785/800\n",
      "8564/8564 [==============================] - 4s 474us/step - loss: 0.0273 - acc: 0.9407 - val_loss: 0.0240 - val_acc: 0.9450\n",
      "Epoch 786/800\n",
      "8564/8564 [==============================] - 4s 466us/step - loss: 0.0264 - acc: 0.9418 - val_loss: 0.0266 - val_acc: 0.9460\n",
      "Epoch 787/800\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0266 - acc: 0.9425 - val_loss: 0.0318 - val_acc: 0.9528\n",
      "Epoch 788/800\n",
      "8564/8564 [==============================] - 4s 470us/step - loss: 0.0271 - acc: 0.9424 - val_loss: 0.0252 - val_acc: 0.9404\n",
      "Epoch 789/800\n",
      "8564/8564 [==============================] - 4s 484us/step - loss: 0.0261 - acc: 0.9437 - val_loss: 0.0266 - val_acc: 0.9405\n",
      "Epoch 790/800\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0277 - acc: 0.9398 - val_loss: 0.0267 - val_acc: 0.9407\n",
      "Epoch 791/800\n",
      "8564/8564 [==============================] - 4s 467us/step - loss: 0.0265 - acc: 0.9425 - val_loss: 0.0346 - val_acc: 0.9247\n",
      "Epoch 792/800\n",
      "8564/8564 [==============================] - 4s 468us/step - loss: 0.0281 - acc: 0.9399 - val_loss: 0.0218 - val_acc: 0.9425\n",
      "Epoch 793/800\n",
      "8564/8564 [==============================] - 4s 479us/step - loss: 0.0257 - acc: 0.9452 - val_loss: 0.0230 - val_acc: 0.9486\n",
      "Epoch 794/800\n",
      "8564/8564 [==============================] - 4s 465us/step - loss: 0.0269 - acc: 0.9408 - val_loss: 0.0261 - val_acc: 0.9458\n",
      "Epoch 795/800\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0261 - acc: 0.9430 - val_loss: 0.0220 - val_acc: 0.9557\n",
      "Epoch 796/800\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0273 - acc: 0.9405 - val_loss: 0.0218 - val_acc: 0.9586\n",
      "Epoch 797/800\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0259 - acc: 0.9441 - val_loss: 0.0280 - val_acc: 0.9377\n",
      "Epoch 798/800\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0268 - acc: 0.9416 - val_loss: 0.0255 - val_acc: 0.9430\n",
      "Epoch 799/800\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0272 - acc: 0.9420 - val_loss: 0.0214 - val_acc: 0.9448\n",
      "Epoch 800/800\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0260 - acc: 0.9432 - val_loss: 0.0296 - val_acc: 0.9312\n",
      "start training round 40\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 801/820\n",
      "8564/8564 [==============================] - 4s 449us/step - loss: 0.0247 - acc: 0.7190 - val_loss: 0.0256 - val_acc: 0.5764\n",
      "Epoch 802/820\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0245 - acc: 0.6770 - val_loss: 0.0256 - val_acc: 0.5760\n",
      "Epoch 803/820\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0246 - acc: 0.6669 - val_loss: 0.0243 - val_acc: 0.6978\n",
      "Epoch 804/820\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0246 - acc: 0.6729 - val_loss: 0.0247 - val_acc: 0.6025\n",
      "Epoch 805/820\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0247 - acc: 0.6640 - val_loss: 0.0247 - val_acc: 0.6282\n",
      "Epoch 806/820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0245 - acc: 0.7110 - val_loss: 0.0260 - val_acc: 0.7507\n",
      "Epoch 807/820\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0249 - acc: 0.7043 - val_loss: 0.0246 - val_acc: 0.7511\n",
      "Epoch 808/820\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0244 - acc: 0.6856 - val_loss: 0.0250 - val_acc: 0.5909\n",
      "Epoch 809/820\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0244 - acc: 0.7425 - val_loss: 0.0261 - val_acc: 0.7504\n",
      "Epoch 810/820\n",
      "8564/8564 [==============================] - 4s 467us/step - loss: 0.0251 - acc: 0.7462 - val_loss: 0.0252 - val_acc: 0.7498\n",
      "Epoch 811/820\n",
      "8564/8564 [==============================] - 4s 481us/step - loss: 0.0250 - acc: 0.7438 - val_loss: 0.0248 - val_acc: 0.7477\n",
      "Epoch 812/820\n",
      "8564/8564 [==============================] - 4s 466us/step - loss: 0.0247 - acc: 0.6985 - val_loss: 0.0252 - val_acc: 0.7437\n",
      "Epoch 813/820\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0250 - acc: 0.7453 - val_loss: 0.0257 - val_acc: 0.7484\n",
      "Epoch 814/820\n",
      "8564/8564 [==============================] - 4s 461us/step - loss: 0.0249 - acc: 0.7467 - val_loss: 0.0255 - val_acc: 0.7514\n",
      "Epoch 815/820\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0246 - acc: 0.7423 - val_loss: 0.0245 - val_acc: 0.7494\n",
      "Epoch 816/820\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0246 - acc: 0.6692 - val_loss: 0.0249 - val_acc: 0.5941\n",
      "Epoch 817/820\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0245 - acc: 0.6671 - val_loss: 0.0256 - val_acc: 0.5778\n",
      "Epoch 818/820\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0248 - acc: 0.6728 - val_loss: 0.0239 - val_acc: 0.7509\n",
      "Epoch 819/820\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0243 - acc: 0.6941 - val_loss: 0.0244 - val_acc: 0.7497\n",
      "Epoch 820/820\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0245 - acc: 0.6770 - val_loss: 0.0251 - val_acc: 0.7447\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 801/820\n",
      "8564/8564 [==============================] - 4s 497us/step - loss: 0.0230 - acc: 0.8806 - val_loss: 0.0237 - val_acc: 0.8817\n",
      "Epoch 802/820\n",
      "8564/8564 [==============================] - 4s 479us/step - loss: 0.0232 - acc: 0.8800 - val_loss: 0.0233 - val_acc: 0.8838\n",
      "Epoch 803/820\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0230 - acc: 0.8824 - val_loss: 0.0236 - val_acc: 0.8817\n",
      "Epoch 804/820\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0231 - acc: 0.8806 - val_loss: 0.0231 - val_acc: 0.8830\n",
      "Epoch 805/820\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0231 - acc: 0.8803 - val_loss: 0.0232 - val_acc: 0.8840\n",
      "Epoch 806/820\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0229 - acc: 0.8817 - val_loss: 0.0229 - val_acc: 0.8873\n",
      "Epoch 807/820\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0231 - acc: 0.8806 - val_loss: 0.0234 - val_acc: 0.8826\n",
      "Epoch 808/820\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0230 - acc: 0.8811 - val_loss: 0.0228 - val_acc: 0.8859\n",
      "Epoch 809/820\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0231 - acc: 0.8807 - val_loss: 0.0229 - val_acc: 0.8883\n",
      "Epoch 810/820\n",
      "8564/8564 [==============================] - 4s 444us/step - loss: 0.0228 - acc: 0.8885 - val_loss: 0.0235 - val_acc: 0.8890\n",
      "Epoch 811/820\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0230 - acc: 0.8894 - val_loss: 0.0233 - val_acc: 0.8885\n",
      "Epoch 812/820\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0231 - acc: 0.8885 - val_loss: 0.0237 - val_acc: 0.8889\n",
      "Epoch 813/820\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0231 - acc: 0.8891 - val_loss: 0.0231 - val_acc: 0.8886\n",
      "Epoch 814/820\n",
      "8564/8564 [==============================] - 4s 449us/step - loss: 0.0229 - acc: 0.8894 - val_loss: 0.0236 - val_acc: 0.8886\n",
      "Epoch 815/820\n",
      "8564/8564 [==============================] - 4s 446us/step - loss: 0.0232 - acc: 0.8892 - val_loss: 0.0226 - val_acc: 0.8886\n",
      "Epoch 816/820\n",
      "8564/8564 [==============================] - 4s 447us/step - loss: 0.0229 - acc: 0.8896 - val_loss: 0.0232 - val_acc: 0.8888\n",
      "Epoch 817/820\n",
      "8564/8564 [==============================] - 4s 449us/step - loss: 0.0230 - acc: 0.8893 - val_loss: 0.0231 - val_acc: 0.8890\n",
      "Epoch 818/820\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0229 - acc: 0.8891 - val_loss: 0.0233 - val_acc: 0.8887\n",
      "Epoch 819/820\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0231 - acc: 0.8888 - val_loss: 0.0226 - val_acc: 0.8876\n",
      "Epoch 820/820\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0229 - acc: 0.8892 - val_loss: 0.0233 - val_acc: 0.8879\n",
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 801/820\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0229 - acc: 0.9845 - val_loss: 0.0249 - val_acc: 0.9877\n",
      "Epoch 802/820\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0232 - acc: 0.9840 - val_loss: 0.0288 - val_acc: 0.9881\n",
      "Epoch 803/820\n",
      "8564/8564 [==============================] - 4s 447us/step - loss: 0.0222 - acc: 0.9865 - val_loss: 0.0198 - val_acc: 0.9887\n",
      "Epoch 804/820\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0218 - acc: 0.9853 - val_loss: 0.0208 - val_acc: 0.9887\n",
      "Epoch 805/820\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0246 - acc: 0.9840 - val_loss: 0.0200 - val_acc: 0.9877\n",
      "Epoch 806/820\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0246 - acc: 0.9846 - val_loss: 0.0228 - val_acc: 0.9813\n",
      "Epoch 807/820\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0235 - acc: 0.9852 - val_loss: 0.0197 - val_acc: 0.9873\n",
      "Epoch 808/820\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0245 - acc: 0.9855 - val_loss: 0.0210 - val_acc: 0.9893\n",
      "Epoch 809/820\n",
      "8564/8564 [==============================] - 4s 449us/step - loss: 0.0233 - acc: 0.9843 - val_loss: 0.0243 - val_acc: 0.9879\n",
      "Epoch 810/820\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0250 - acc: 0.9850 - val_loss: 0.0205 - val_acc: 0.9890\n",
      "Epoch 811/820\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0237 - acc: 0.9842 - val_loss: 0.0231 - val_acc: 0.9892\n",
      "Epoch 812/820\n",
      "8564/8564 [==============================] - 4s 449us/step - loss: 0.0237 - acc: 0.9848 - val_loss: 0.0170 - val_acc: 0.9894\n",
      "Epoch 813/820\n",
      "8564/8564 [==============================] - 4s 447us/step - loss: 0.0236 - acc: 0.9837 - val_loss: 0.0205 - val_acc: 0.9903\n",
      "Epoch 814/820\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0216 - acc: 0.9864 - val_loss: 0.0220 - val_acc: 0.9900\n",
      "Epoch 815/820\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0256 - acc: 0.9859 - val_loss: 0.0217 - val_acc: 0.9869\n",
      "Epoch 816/820\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0237 - acc: 0.9853 - val_loss: 0.0172 - val_acc: 0.9887\n",
      "Epoch 817/820\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0231 - acc: 0.9850 - val_loss: 0.0209 - val_acc: 0.9866\n",
      "Epoch 818/820\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0246 - acc: 0.9845 - val_loss: 0.0236 - val_acc: 0.9843\n",
      "Epoch 819/820\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0219 - acc: 0.9863 - val_loss: 0.0254 - val_acc: 0.9894\n",
      "Epoch 820/820\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0243 - acc: 0.9837 - val_loss: 0.0189 - val_acc: 0.9874\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 801/820\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0266 - acc: 0.9434 - val_loss: 0.0215 - val_acc: 0.9553\n",
      "Epoch 802/820\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0259 - acc: 0.9432 - val_loss: 0.0237 - val_acc: 0.9481\n",
      "Epoch 803/820\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0272 - acc: 0.9414 - val_loss: 0.0227 - val_acc: 0.9513\n",
      "Epoch 804/820\n",
      "8564/8564 [==============================] - 4s 461us/step - loss: 0.0251 - acc: 0.9435 - val_loss: 0.0238 - val_acc: 0.9519\n",
      "Epoch 805/820\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0266 - acc: 0.9428 - val_loss: 0.0212 - val_acc: 0.9540\n",
      "Epoch 806/820\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0269 - acc: 0.9418 - val_loss: 0.0238 - val_acc: 0.9531\n",
      "Epoch 807/820\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0262 - acc: 0.9419 - val_loss: 0.0239 - val_acc: 0.9451\n",
      "Epoch 808/820\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0262 - acc: 0.9428 - val_loss: 0.0268 - val_acc: 0.9458\n",
      "Epoch 809/820\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0260 - acc: 0.9417 - val_loss: 0.0263 - val_acc: 0.9414\n",
      "Epoch 810/820\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0272 - acc: 0.9416 - val_loss: 0.0233 - val_acc: 0.9461\n",
      "Epoch 811/820\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0258 - acc: 0.9445 - val_loss: 0.0246 - val_acc: 0.9499\n",
      "Epoch 812/820\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0260 - acc: 0.9424 - val_loss: 0.0254 - val_acc: 0.9474\n",
      "Epoch 813/820\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0266 - acc: 0.9415 - val_loss: 0.0212 - val_acc: 0.9493\n",
      "Epoch 814/820\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0274 - acc: 0.9416 - val_loss: 0.0274 - val_acc: 0.9445\n",
      "Epoch 815/820\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0272 - acc: 0.9415 - val_loss: 0.0240 - val_acc: 0.9556\n",
      "Epoch 816/820\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0249 - acc: 0.9462 - val_loss: 0.0254 - val_acc: 0.9484\n",
      "Epoch 817/820\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0263 - acc: 0.9423 - val_loss: 0.0226 - val_acc: 0.9533\n",
      "Epoch 818/820\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0269 - acc: 0.9414 - val_loss: 0.0241 - val_acc: 0.9496\n",
      "Epoch 819/820\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0263 - acc: 0.9446 - val_loss: 0.0219 - val_acc: 0.9539\n",
      "Epoch 820/820\n",
      "8564/8564 [==============================] - 4s 462us/step - loss: 0.0257 - acc: 0.9441 - val_loss: 0.0245 - val_acc: 0.9491\n",
      "start training round 41\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 821/840\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0243 - acc: 0.7432 - val_loss: 0.0252 - val_acc: 0.7459\n",
      "Epoch 822/840\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0248 - acc: 0.7438 - val_loss: 0.0245 - val_acc: 0.7429\n",
      "Epoch 823/840\n",
      "8564/8564 [==============================] - 4s 472us/step - loss: 0.0241 - acc: 0.6915 - val_loss: 0.0246 - val_acc: 0.6046\n",
      "Epoch 824/840\n",
      "8564/8564 [==============================] - 4s 466us/step - loss: 0.0243 - acc: 0.6825 - val_loss: 0.0250 - val_acc: 0.7523\n",
      "Epoch 825/840\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0249 - acc: 0.7465 - val_loss: 0.0268 - val_acc: 0.7484\n",
      "Epoch 826/840\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0248 - acc: 0.7524 - val_loss: 0.0245 - val_acc: 0.7511\n",
      "Epoch 827/840\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0243 - acc: 0.7400 - val_loss: 0.0251 - val_acc: 0.7486\n",
      "Epoch 828/840\n",
      "8564/8564 [==============================] - 4s 470us/step - loss: 0.0248 - acc: 0.7493 - val_loss: 0.0254 - val_acc: 0.7516\n",
      "Epoch 829/840\n",
      "8564/8564 [==============================] - 4s 465us/step - loss: 0.0247 - acc: 0.7513 - val_loss: 0.0251 - val_acc: 0.7506\n",
      "Epoch 830/840\n",
      "8564/8564 [==============================] - 4s 467us/step - loss: 0.0244 - acc: 0.7037 - val_loss: 0.0246 - val_acc: 0.7438\n",
      "Epoch 831/840\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0245 - acc: 0.6741 - val_loss: 0.0241 - val_acc: 0.7360\n",
      "Epoch 832/840\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0247 - acc: 0.6694 - val_loss: 0.0254 - val_acc: 0.5730\n",
      "Epoch 833/840\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0245 - acc: 0.6690 - val_loss: 0.0245 - val_acc: 0.6349\n",
      "Epoch 834/840\n",
      "8564/8564 [==============================] - 4s 465us/step - loss: 0.0247 - acc: 0.6719 - val_loss: 0.0248 - val_acc: 0.5929\n",
      "Epoch 835/840\n",
      "8564/8564 [==============================] - 4s 495us/step - loss: 0.0243 - acc: 0.6774 - val_loss: 0.0245 - val_acc: 0.6241\n",
      "Epoch 836/840\n",
      "8564/8564 [==============================] - 4s 498us/step - loss: 0.0245 - acc: 0.6712 - val_loss: 0.0249 - val_acc: 0.6626\n",
      "Epoch 837/840\n",
      "8564/8564 [==============================] - 4s 490us/step - loss: 0.0245 - acc: 0.6686 - val_loss: 0.0244 - val_acc: 0.6206\n",
      "Epoch 838/840\n",
      "8564/8564 [==============================] - 4s 468us/step - loss: 0.0248 - acc: 0.6619 - val_loss: 0.0255 - val_acc: 0.5735\n",
      "Epoch 839/840\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0247 - acc: 0.6703 - val_loss: 0.0254 - val_acc: 0.5773\n",
      "Epoch 840/840\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0247 - acc: 0.6631 - val_loss: 0.0253 - val_acc: 0.5843\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 821/840\n",
      "8564/8564 [==============================] - 4s 447us/step - loss: 0.0229 - acc: 0.8894 - val_loss: 0.0240 - val_acc: 0.8885\n",
      "Epoch 822/840\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0230 - acc: 0.8887 - val_loss: 0.0231 - val_acc: 0.8885\n",
      "Epoch 823/840\n",
      "8564/8564 [==============================] - 4s 449us/step - loss: 0.0231 - acc: 0.8892 - val_loss: 0.0231 - val_acc: 0.8891\n",
      "Epoch 824/840\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0230 - acc: 0.8888 - val_loss: 0.0240 - val_acc: 0.8887\n",
      "Epoch 825/840\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0229 - acc: 0.8889 - val_loss: 0.0230 - val_acc: 0.8888\n",
      "Epoch 826/840\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0229 - acc: 0.8895 - val_loss: 0.0228 - val_acc: 0.8889\n",
      "Epoch 827/840\n",
      "8564/8564 [==============================] - 4s 446us/step - loss: 0.0229 - acc: 0.8893 - val_loss: 0.0230 - val_acc: 0.8887\n",
      "Epoch 828/840\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0230 - acc: 0.8892 - val_loss: 0.0232 - val_acc: 0.8889\n",
      "Epoch 829/840\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0229 - acc: 0.8895 - val_loss: 0.0234 - val_acc: 0.8881\n",
      "Epoch 830/840\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0232 - acc: 0.8887 - val_loss: 0.0237 - val_acc: 0.8890\n",
      "Epoch 831/840\n",
      "8564/8564 [==============================] - 4s 449us/step - loss: 0.0229 - acc: 0.8888 - val_loss: 0.0223 - val_acc: 0.8887\n",
      "Epoch 832/840\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0230 - acc: 0.8892 - val_loss: 0.0223 - val_acc: 0.8888\n",
      "Epoch 833/840\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0230 - acc: 0.8892 - val_loss: 0.0237 - val_acc: 0.8720\n",
      "Epoch 834/840\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0230 - acc: 0.8810 - val_loss: 0.0234 - val_acc: 0.8747\n",
      "Epoch 835/840\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0231 - acc: 0.8811 - val_loss: 0.0232 - val_acc: 0.8782\n",
      "Epoch 836/840\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0231 - acc: 0.8801 - val_loss: 0.0227 - val_acc: 0.8863\n",
      "Epoch 837/840\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0231 - acc: 0.8805 - val_loss: 0.0227 - val_acc: 0.8819\n",
      "Epoch 838/840\n",
      "8564/8564 [==============================] - 4s 446us/step - loss: 0.0229 - acc: 0.8813 - val_loss: 0.0231 - val_acc: 0.8818\n",
      "Epoch 839/840\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0230 - acc: 0.8813 - val_loss: 0.0236 - val_acc: 0.8743\n",
      "Epoch 840/840\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0230 - acc: 0.8811 - val_loss: 0.0236 - val_acc: 0.8749\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 821/840\n",
      "8564/8564 [==============================] - 4s 449us/step - loss: 0.0241 - acc: 0.9854 - val_loss: 0.0286 - val_acc: 0.9862\n",
      "Epoch 822/840\n",
      "8564/8564 [==============================] - 4s 449us/step - loss: 0.0236 - acc: 0.9863 - val_loss: 0.0210 - val_acc: 0.9880\n",
      "Epoch 823/840\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0246 - acc: 0.9857 - val_loss: 0.0225 - val_acc: 0.9845\n",
      "Epoch 824/840\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0253 - acc: 0.9858 - val_loss: 0.0201 - val_acc: 0.9902\n",
      "Epoch 825/840\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0240 - acc: 0.9858 - val_loss: 0.0232 - val_acc: 0.9886\n",
      "Epoch 826/840\n",
      "8564/8564 [==============================] - 4s 449us/step - loss: 0.0250 - acc: 0.9844 - val_loss: 0.0247 - val_acc: 0.9894\n",
      "Epoch 827/840\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0245 - acc: 0.9855 - val_loss: 0.0180 - val_acc: 0.9884\n",
      "Epoch 828/840\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0236 - acc: 0.9863 - val_loss: 0.0215 - val_acc: 0.9885\n",
      "Epoch 829/840\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0233 - acc: 0.9841 - val_loss: 0.0202 - val_acc: 0.9905\n",
      "Epoch 830/840\n",
      "8564/8564 [==============================] - 4s 443us/step - loss: 0.0221 - acc: 0.9845 - val_loss: 0.0222 - val_acc: 0.9877\n",
      "Epoch 831/840\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0222 - acc: 0.9863 - val_loss: 0.0259 - val_acc: 0.9834\n",
      "Epoch 832/840\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0244 - acc: 0.9836 - val_loss: 0.0226 - val_acc: 0.9864\n",
      "Epoch 833/840\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0238 - acc: 0.9836 - val_loss: 0.0250 - val_acc: 0.9884\n",
      "Epoch 834/840\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0243 - acc: 0.9863 - val_loss: 0.0236 - val_acc: 0.9889\n",
      "Epoch 835/840\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0243 - acc: 0.9861 - val_loss: 0.0182 - val_acc: 0.9886\n",
      "Epoch 836/840\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0244 - acc: 0.9836 - val_loss: 0.0233 - val_acc: 0.9871\n",
      "Epoch 837/840\n",
      "8564/8564 [==============================] - 4s 462us/step - loss: 0.0237 - acc: 0.9842 - val_loss: 0.0199 - val_acc: 0.9896\n",
      "Epoch 838/840\n",
      "8564/8564 [==============================] - 4s 482us/step - loss: 0.0220 - acc: 0.9863 - val_loss: 0.0214 - val_acc: 0.9866\n",
      "Epoch 839/840\n",
      "8564/8564 [==============================] - 4s 468us/step - loss: 0.0237 - acc: 0.9842 - val_loss: 0.0241 - val_acc: 0.9868\n",
      "Epoch 840/840\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0235 - acc: 0.9855 - val_loss: 0.0190 - val_acc: 0.9865\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 821/840\n",
      "8564/8564 [==============================] - 4s 468us/step - loss: 0.0264 - acc: 0.9454 - val_loss: 0.0213 - val_acc: 0.9577\n",
      "Epoch 822/840\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0259 - acc: 0.9430 - val_loss: 0.0246 - val_acc: 0.9410\n",
      "Epoch 823/840\n",
      "8564/8564 [==============================] - 4s 473us/step - loss: 0.0271 - acc: 0.9422 - val_loss: 0.0254 - val_acc: 0.9587\n",
      "Epoch 824/840\n",
      "8564/8564 [==============================] - 4s 486us/step - loss: 0.0268 - acc: 0.9423 - val_loss: 0.0276 - val_acc: 0.9503\n",
      "Epoch 825/840\n",
      "8564/8564 [==============================] - 4s 466us/step - loss: 0.0264 - acc: 0.9419 - val_loss: 0.0260 - val_acc: 0.9519\n",
      "Epoch 826/840\n",
      "8564/8564 [==============================] - 4s 468us/step - loss: 0.0268 - acc: 0.9427 - val_loss: 0.0217 - val_acc: 0.9549\n",
      "Epoch 827/840\n",
      "8564/8564 [==============================] - 4s 464us/step - loss: 0.0262 - acc: 0.9417 - val_loss: 0.0256 - val_acc: 0.9379\n",
      "Epoch 828/840\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0263 - acc: 0.9414 - val_loss: 0.0248 - val_acc: 0.9504\n",
      "Epoch 829/840\n",
      "8564/8564 [==============================] - 4s 462us/step - loss: 0.0257 - acc: 0.9451 - val_loss: 0.0231 - val_acc: 0.9568\n",
      "Epoch 830/840\n",
      "8564/8564 [==============================] - 4s 488us/step - loss: 0.0274 - acc: 0.9403 - val_loss: 0.0242 - val_acc: 0.9431\n",
      "Epoch 831/840\n",
      "8564/8564 [==============================] - 4s 468us/step - loss: 0.0259 - acc: 0.9434 - val_loss: 0.0231 - val_acc: 0.9445\n",
      "Epoch 832/840\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0263 - acc: 0.9417 - val_loss: 0.0249 - val_acc: 0.9417\n",
      "Epoch 833/840\n",
      "8564/8564 [==============================] - 4s 461us/step - loss: 0.0255 - acc: 0.9445 - val_loss: 0.0221 - val_acc: 0.9548\n",
      "Epoch 834/840\n",
      "8564/8564 [==============================] - 4s 462us/step - loss: 0.0261 - acc: 0.9431 - val_loss: 0.0241 - val_acc: 0.9463\n",
      "Epoch 835/840\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0270 - acc: 0.9436 - val_loss: 0.0299 - val_acc: 0.9431\n",
      "Epoch 836/840\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0274 - acc: 0.9408 - val_loss: 0.0211 - val_acc: 0.9482\n",
      "Epoch 837/840\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0265 - acc: 0.9412 - val_loss: 0.0278 - val_acc: 0.9388\n",
      "Epoch 838/840\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0273 - acc: 0.9408 - val_loss: 0.0240 - val_acc: 0.9443\n",
      "Epoch 839/840\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0258 - acc: 0.9432 - val_loss: 0.0244 - val_acc: 0.9546\n",
      "Epoch 840/840\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0257 - acc: 0.9440 - val_loss: 0.0231 - val_acc: 0.9533\n",
      "start training round 42\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 841/860\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0243 - acc: 0.6842 - val_loss: 0.0251 - val_acc: 0.7450\n",
      "Epoch 842/860\n",
      "8564/8564 [==============================] - 4s 465us/step - loss: 0.0247 - acc: 0.6723 - val_loss: 0.0245 - val_acc: 0.6560\n",
      "Epoch 843/860\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0243 - acc: 0.6724 - val_loss: 0.0261 - val_acc: 0.5737\n",
      "Epoch 844/860\n",
      "8564/8564 [==============================] - 4s 462us/step - loss: 0.0246 - acc: 0.6685 - val_loss: 0.0259 - val_acc: 0.5627\n",
      "Epoch 845/860\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0246 - acc: 0.6635 - val_loss: 0.0241 - val_acc: 0.6411\n",
      "Epoch 846/860\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0247 - acc: 0.6728 - val_loss: 0.0251 - val_acc: 0.7522\n",
      "Epoch 847/860\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0246 - acc: 0.7472 - val_loss: 0.0241 - val_acc: 0.7523\n",
      "Epoch 848/860\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0243 - acc: 0.7543 - val_loss: 0.0242 - val_acc: 0.7522\n",
      "Epoch 849/860\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0244 - acc: 0.7216 - val_loss: 0.0249 - val_acc: 0.7541\n",
      "Epoch 850/860\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0242 - acc: 0.7074 - val_loss: 0.0252 - val_acc: 0.7424\n",
      "Epoch 851/860\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0240 - acc: 0.6992 - val_loss: 0.0242 - val_acc: 0.6514\n",
      "Epoch 852/860\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0248 - acc: 0.7468 - val_loss: 0.0243 - val_acc: 0.7524\n",
      "Epoch 853/860\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0244 - acc: 0.7262 - val_loss: 0.0247 - val_acc: 0.5959\n",
      "Epoch 854/860\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0245 - acc: 0.6622 - val_loss: 0.0252 - val_acc: 0.5892\n",
      "Epoch 855/860\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0246 - acc: 0.6743 - val_loss: 0.0251 - val_acc: 0.7427\n",
      "Epoch 856/860\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0245 - acc: 0.6831 - val_loss: 0.0248 - val_acc: 0.7510\n",
      "Epoch 857/860\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0246 - acc: 0.7495 - val_loss: 0.0239 - val_acc: 0.7501\n",
      "Epoch 858/860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 4s 469us/step - loss: 0.0241 - acc: 0.6742 - val_loss: 0.0256 - val_acc: 0.5767\n",
      "Epoch 859/860\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0243 - acc: 0.6744 - val_loss: 0.0242 - val_acc: 0.6189\n",
      "Epoch 860/860\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0240 - acc: 0.6904 - val_loss: 0.0239 - val_acc: 0.6913\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 841/860\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0229 - acc: 0.8806 - val_loss: 0.0238 - val_acc: 0.8729\n",
      "Epoch 842/860\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0230 - acc: 0.8808 - val_loss: 0.0226 - val_acc: 0.8868\n",
      "Epoch 843/860\n",
      "8564/8564 [==============================] - 4s 462us/step - loss: 0.0227 - acc: 0.8897 - val_loss: 0.0235 - val_acc: 0.8887\n",
      "Epoch 844/860\n",
      "8564/8564 [==============================] - 4s 464us/step - loss: 0.0226 - acc: 0.8897 - val_loss: 0.0238 - val_acc: 0.8885\n",
      "Epoch 845/860\n",
      "8564/8564 [==============================] - 4s 474us/step - loss: 0.0229 - acc: 0.8895 - val_loss: 0.0235 - val_acc: 0.8889\n",
      "Epoch 846/860\n",
      "8564/8564 [==============================] - 4s 461us/step - loss: 0.0230 - acc: 0.8899 - val_loss: 0.0235 - val_acc: 0.8893\n",
      "Epoch 847/860\n",
      "8564/8564 [==============================] - 4s 473us/step - loss: 0.0230 - acc: 0.8895 - val_loss: 0.0237 - val_acc: 0.8891\n",
      "Epoch 848/860\n",
      "8564/8564 [==============================] - 4s 469us/step - loss: 0.0231 - acc: 0.8895 - val_loss: 0.0227 - val_acc: 0.8894\n",
      "Epoch 849/860\n",
      "8564/8564 [==============================] - 4s 470us/step - loss: 0.0228 - acc: 0.8894 - val_loss: 0.0227 - val_acc: 0.8868\n",
      "Epoch 850/860\n",
      "8564/8564 [==============================] - 4s 461us/step - loss: 0.0230 - acc: 0.8892 - val_loss: 0.0236 - val_acc: 0.8893\n",
      "Epoch 851/860\n",
      "8564/8564 [==============================] - 4s 486us/step - loss: 0.0227 - acc: 0.8895 - val_loss: 0.0236 - val_acc: 0.8890\n",
      "Epoch 852/860\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0229 - acc: 0.8898 - val_loss: 0.0230 - val_acc: 0.8888\n",
      "Epoch 853/860\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0229 - acc: 0.8901 - val_loss: 0.0230 - val_acc: 0.8896\n",
      "Epoch 854/860\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0228 - acc: 0.8894 - val_loss: 0.0239 - val_acc: 0.8895\n",
      "Epoch 855/860\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0229 - acc: 0.8897 - val_loss: 0.0229 - val_acc: 0.8893\n",
      "Epoch 856/860\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0229 - acc: 0.8896 - val_loss: 0.0225 - val_acc: 0.8890\n",
      "Epoch 857/860\n",
      "8564/8564 [==============================] - 4s 471us/step - loss: 0.0228 - acc: 0.8894 - val_loss: 0.0226 - val_acc: 0.8894\n",
      "Epoch 858/860\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0228 - acc: 0.8896 - val_loss: 0.0230 - val_acc: 0.8878\n",
      "Epoch 859/860\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0229 - acc: 0.8827 - val_loss: 0.0240 - val_acc: 0.8807\n",
      "Epoch 860/860\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0230 - acc: 0.8810 - val_loss: 0.0227 - val_acc: 0.8880\n",
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 841/860\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0226 - acc: 0.9847 - val_loss: 0.0238 - val_acc: 0.9883\n",
      "Epoch 842/860\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0235 - acc: 0.9837 - val_loss: 0.0212 - val_acc: 0.9898\n",
      "Epoch 843/860\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0231 - acc: 0.9859 - val_loss: 0.0246 - val_acc: 0.9874\n",
      "Epoch 844/860\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0243 - acc: 0.9847 - val_loss: 0.0228 - val_acc: 0.9882\n",
      "Epoch 845/860\n",
      "8564/8564 [==============================] - 4s 463us/step - loss: 0.0245 - acc: 0.9853 - val_loss: 0.0266 - val_acc: 0.9877\n",
      "Epoch 846/860\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0238 - acc: 0.9845 - val_loss: 0.0209 - val_acc: 0.9847\n",
      "Epoch 847/860\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0249 - acc: 0.9847 - val_loss: 0.0231 - val_acc: 0.9872\n",
      "Epoch 848/860\n",
      "8564/8564 [==============================] - 4s 447us/step - loss: 0.0241 - acc: 0.9831 - val_loss: 0.0232 - val_acc: 0.9887\n",
      "Epoch 849/860\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0240 - acc: 0.9861 - val_loss: 0.0238 - val_acc: 0.9869\n",
      "Epoch 850/860\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0240 - acc: 0.9847 - val_loss: 0.0238 - val_acc: 0.9891\n",
      "Epoch 851/860\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0232 - acc: 0.9836 - val_loss: 0.0252 - val_acc: 0.9876\n",
      "Epoch 852/860\n",
      "8564/8564 [==============================] - 4s 463us/step - loss: 0.0232 - acc: 0.9851 - val_loss: 0.0237 - val_acc: 0.9866\n",
      "Epoch 853/860\n",
      "8564/8564 [==============================] - 4s 466us/step - loss: 0.0238 - acc: 0.9854 - val_loss: 0.0192 - val_acc: 0.9892\n",
      "Epoch 854/860\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0225 - acc: 0.9855 - val_loss: 0.0245 - val_acc: 0.9898\n",
      "Epoch 855/860\n",
      "8564/8564 [==============================] - 4s 472us/step - loss: 0.0246 - acc: 0.9836 - val_loss: 0.0190 - val_acc: 0.9896\n",
      "Epoch 856/860\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0236 - acc: 0.9846 - val_loss: 0.0219 - val_acc: 0.9902\n",
      "Epoch 857/860\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0260 - acc: 0.9863 - val_loss: 0.0198 - val_acc: 0.9884\n",
      "Epoch 858/860\n",
      "8564/8564 [==============================] - 4s 463us/step - loss: 0.0220 - acc: 0.9868 - val_loss: 0.0245 - val_acc: 0.9892\n",
      "Epoch 859/860\n",
      "8564/8564 [==============================] - 4s 494us/step - loss: 0.0242 - acc: 0.9839 - val_loss: 0.0255 - val_acc: 0.9846\n",
      "Epoch 860/860\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0239 - acc: 0.9848 - val_loss: 0.0213 - val_acc: 0.9863\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 841/860\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0257 - acc: 0.9442 - val_loss: 0.0284 - val_acc: 0.9324\n",
      "Epoch 842/860\n",
      "8564/8564 [==============================] - 4s 468us/step - loss: 0.0266 - acc: 0.9399 - val_loss: 0.0264 - val_acc: 0.9389\n",
      "Epoch 843/860\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0275 - acc: 0.9418 - val_loss: 0.0286 - val_acc: 0.9454\n",
      "Epoch 844/860\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0261 - acc: 0.9454 - val_loss: 0.0253 - val_acc: 0.9576\n",
      "Epoch 845/860\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0258 - acc: 0.9451 - val_loss: 0.0214 - val_acc: 0.9499\n",
      "Epoch 846/860\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0266 - acc: 0.9426 - val_loss: 0.0224 - val_acc: 0.9489\n",
      "Epoch 847/860\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0264 - acc: 0.9437 - val_loss: 0.0240 - val_acc: 0.9485\n",
      "Epoch 848/860\n",
      "8564/8564 [==============================] - 4s 462us/step - loss: 0.0267 - acc: 0.9414 - val_loss: 0.0271 - val_acc: 0.9495\n",
      "Epoch 849/860\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0276 - acc: 0.9403 - val_loss: 0.0260 - val_acc: 0.9553\n",
      "Epoch 850/860\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0265 - acc: 0.9439 - val_loss: 0.0253 - val_acc: 0.9425\n",
      "Epoch 851/860\n",
      "8564/8564 [==============================] - 4s 462us/step - loss: 0.0254 - acc: 0.9454 - val_loss: 0.0267 - val_acc: 0.9358\n",
      "Epoch 852/860\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0254 - acc: 0.9438 - val_loss: 0.0254 - val_acc: 0.9405\n",
      "Epoch 853/860\n",
      "8564/8564 [==============================] - 4s 466us/step - loss: 0.0266 - acc: 0.9434 - val_loss: 0.0212 - val_acc: 0.9520\n",
      "Epoch 854/860\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0271 - acc: 0.9425 - val_loss: 0.0234 - val_acc: 0.9591\n",
      "Epoch 855/860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0260 - acc: 0.9440 - val_loss: 0.0248 - val_acc: 0.9568\n",
      "Epoch 856/860\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0255 - acc: 0.9448 - val_loss: 0.0223 - val_acc: 0.9552\n",
      "Epoch 857/860\n",
      "8564/8564 [==============================] - 4s 464us/step - loss: 0.0257 - acc: 0.9454 - val_loss: 0.0237 - val_acc: 0.9492\n",
      "Epoch 858/860\n",
      "8564/8564 [==============================] - 4s 472us/step - loss: 0.0260 - acc: 0.9438 - val_loss: 0.0275 - val_acc: 0.9379\n",
      "Epoch 859/860\n",
      "8564/8564 [==============================] - 4s 475us/step - loss: 0.0253 - acc: 0.9432 - val_loss: 0.0262 - val_acc: 0.9367\n",
      "Epoch 860/860\n",
      "8564/8564 [==============================] - 4s 473us/step - loss: 0.0266 - acc: 0.9441 - val_loss: 0.0211 - val_acc: 0.9499\n",
      "start training round 43\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 861/880\n",
      "8564/8564 [==============================] - 4s 464us/step - loss: 0.0243 - acc: 0.6639 - val_loss: 0.0252 - val_acc: 0.5826\n",
      "Epoch 862/880\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0244 - acc: 0.6661 - val_loss: 0.0246 - val_acc: 0.6101\n",
      "Epoch 863/880\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0244 - acc: 0.6650 - val_loss: 0.0256 - val_acc: 0.5810\n",
      "Epoch 864/880\n",
      "8564/8564 [==============================] - 4s 474us/step - loss: 0.0241 - acc: 0.6903 - val_loss: 0.0237 - val_acc: 0.7542\n",
      "Epoch 865/880\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0244 - acc: 0.7418 - val_loss: 0.0240 - val_acc: 0.7546\n",
      "Epoch 866/880\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0246 - acc: 0.7498 - val_loss: 0.0245 - val_acc: 0.7549\n",
      "Epoch 867/880\n",
      "8564/8564 [==============================] - 4s 465us/step - loss: 0.0245 - acc: 0.7514 - val_loss: 0.0248 - val_acc: 0.7544\n",
      "Epoch 868/880\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0243 - acc: 0.7312 - val_loss: 0.0242 - val_acc: 0.7495\n",
      "Epoch 869/880\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0242 - acc: 0.7312 - val_loss: 0.0250 - val_acc: 0.7439\n",
      "Epoch 870/880\n",
      "8564/8564 [==============================] - 4s 465us/step - loss: 0.0244 - acc: 0.6871 - val_loss: 0.0247 - val_acc: 0.7548\n",
      "Epoch 871/880\n",
      "8564/8564 [==============================] - 4s 467us/step - loss: 0.0242 - acc: 0.7056 - val_loss: 0.0248 - val_acc: 0.5996\n",
      "Epoch 872/880\n",
      "8564/8564 [==============================] - 4s 469us/step - loss: 0.0241 - acc: 0.7340 - val_loss: 0.0250 - val_acc: 0.5830\n",
      "Epoch 873/880\n",
      "8564/8564 [==============================] - 4s 466us/step - loss: 0.0246 - acc: 0.6642 - val_loss: 0.0248 - val_acc: 0.6035\n",
      "Epoch 874/880\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0247 - acc: 0.6608 - val_loss: 0.0252 - val_acc: 0.5924\n",
      "Epoch 875/880\n",
      "8564/8564 [==============================] - 4s 446us/step - loss: 0.0244 - acc: 0.6707 - val_loss: 0.0242 - val_acc: 0.6324\n",
      "Epoch 876/880\n",
      "8564/8564 [==============================] - 4s 465us/step - loss: 0.0243 - acc: 0.6768 - val_loss: 0.0245 - val_acc: 0.7474\n",
      "Epoch 877/880\n",
      "8564/8564 [==============================] - 4s 475us/step - loss: 0.0244 - acc: 0.6950 - val_loss: 0.0246 - val_acc: 0.7516\n",
      "Epoch 878/880\n",
      "8564/8564 [==============================] - 4s 469us/step - loss: 0.0246 - acc: 0.7536 - val_loss: 0.0242 - val_acc: 0.7545\n",
      "Epoch 879/880\n",
      "8564/8564 [==============================] - 4s 464us/step - loss: 0.0241 - acc: 0.7028 - val_loss: 0.0249 - val_acc: 0.7437\n",
      "Epoch 880/880\n",
      "8564/8564 [==============================] - 4s 464us/step - loss: 0.0243 - acc: 0.6846 - val_loss: 0.0250 - val_acc: 0.7455\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 861/880\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0229 - acc: 0.8818 - val_loss: 0.0233 - val_acc: 0.8851\n",
      "Epoch 862/880\n",
      "8564/8564 [==============================] - 4s 462us/step - loss: 0.0230 - acc: 0.8819 - val_loss: 0.0231 - val_acc: 0.8889\n",
      "Epoch 863/880\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0229 - acc: 0.8901 - val_loss: 0.0228 - val_acc: 0.8891\n",
      "Epoch 864/880\n",
      "8564/8564 [==============================] - 4s 465us/step - loss: 0.0229 - acc: 0.8898 - val_loss: 0.0227 - val_acc: 0.8891\n",
      "Epoch 865/880\n",
      "8564/8564 [==============================] - 4s 462us/step - loss: 0.0227 - acc: 0.8899 - val_loss: 0.0233 - val_acc: 0.8896\n",
      "Epoch 866/880\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0228 - acc: 0.8896 - val_loss: 0.0226 - val_acc: 0.8892\n",
      "Epoch 867/880\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0228 - acc: 0.8894 - val_loss: 0.0225 - val_acc: 0.8893\n",
      "Epoch 868/880\n",
      "8564/8564 [==============================] - 4s 449us/step - loss: 0.0229 - acc: 0.8897 - val_loss: 0.0235 - val_acc: 0.8900\n",
      "Epoch 869/880\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0229 - acc: 0.8896 - val_loss: 0.0234 - val_acc: 0.8892\n",
      "Epoch 870/880\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0226 - acc: 0.8871 - val_loss: 0.0226 - val_acc: 0.8822\n",
      "Epoch 871/880\n",
      "8564/8564 [==============================] - 4s 444us/step - loss: 0.0228 - acc: 0.8821 - val_loss: 0.0231 - val_acc: 0.8836\n",
      "Epoch 872/880\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0227 - acc: 0.8842 - val_loss: 0.0234 - val_acc: 0.8751\n",
      "Epoch 873/880\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0230 - acc: 0.8802 - val_loss: 0.0236 - val_acc: 0.8711\n",
      "Epoch 874/880\n",
      "8564/8564 [==============================] - 4s 484us/step - loss: 0.0228 - acc: 0.8823 - val_loss: 0.0233 - val_acc: 0.8820\n",
      "Epoch 875/880\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0230 - acc: 0.8810 - val_loss: 0.0230 - val_acc: 0.8844\n",
      "Epoch 876/880\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0229 - acc: 0.8821 - val_loss: 0.0228 - val_acc: 0.8856\n",
      "Epoch 877/880\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0229 - acc: 0.8822 - val_loss: 0.0233 - val_acc: 0.8801\n",
      "Epoch 878/880\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0230 - acc: 0.8805 - val_loss: 0.0232 - val_acc: 0.8811\n",
      "Epoch 879/880\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0229 - acc: 0.8821 - val_loss: 0.0238 - val_acc: 0.8714\n",
      "Epoch 880/880\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0229 - acc: 0.8823 - val_loss: 0.0227 - val_acc: 0.8792\n",
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 861/880\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0214 - acc: 0.9863 - val_loss: 0.0229 - val_acc: 0.9878\n",
      "Epoch 862/880\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0218 - acc: 0.9859 - val_loss: 0.0250 - val_acc: 0.9891\n",
      "Epoch 863/880\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0229 - acc: 0.9858 - val_loss: 0.0206 - val_acc: 0.9889\n",
      "Epoch 864/880\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0226 - acc: 0.9859 - val_loss: 0.0208 - val_acc: 0.9878\n",
      "Epoch 865/880\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0252 - acc: 0.9858 - val_loss: 0.0243 - val_acc: 0.9891\n",
      "Epoch 866/880\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0233 - acc: 0.9857 - val_loss: 0.0172 - val_acc: 0.9882\n",
      "Epoch 867/880\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0244 - acc: 0.9865 - val_loss: 0.0172 - val_acc: 0.9883\n",
      "Epoch 868/880\n",
      "8564/8564 [==============================] - 4s 449us/step - loss: 0.0238 - acc: 0.9833 - val_loss: 0.0169 - val_acc: 0.9893\n",
      "Epoch 869/880\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0241 - acc: 0.9839 - val_loss: 0.0181 - val_acc: 0.9881\n",
      "Epoch 870/880\n",
      "8564/8564 [==============================] - 4s 447us/step - loss: 0.0246 - acc: 0.9854 - val_loss: 0.0258 - val_acc: 0.9905\n",
      "Epoch 871/880\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0243 - acc: 0.9844 - val_loss: 0.0245 - val_acc: 0.9873\n",
      "Epoch 872/880\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0239 - acc: 0.9851 - val_loss: 0.0258 - val_acc: 0.9843\n",
      "Epoch 873/880\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0222 - acc: 0.9857 - val_loss: 0.0204 - val_acc: 0.9887\n",
      "Epoch 874/880\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0244 - acc: 0.9828 - val_loss: 0.0226 - val_acc: 0.9872\n",
      "Epoch 875/880\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0231 - acc: 0.9862 - val_loss: 0.0259 - val_acc: 0.9900\n",
      "Epoch 876/880\n",
      "8564/8564 [==============================] - 4s 461us/step - loss: 0.0233 - acc: 0.9871 - val_loss: 0.0231 - val_acc: 0.9904\n",
      "Epoch 877/880\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0227 - acc: 0.9853 - val_loss: 0.0166 - val_acc: 0.9898\n",
      "Epoch 878/880\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0232 - acc: 0.9854 - val_loss: 0.0221 - val_acc: 0.9877\n",
      "Epoch 879/880\n",
      "8564/8564 [==============================] - 4s 445us/step - loss: 0.0246 - acc: 0.9836 - val_loss: 0.0189 - val_acc: 0.9874\n",
      "Epoch 880/880\n",
      "8564/8564 [==============================] - -3s -358us/step - loss: 0.0242 - acc: 0.9853 - val_loss: 0.0179 - val_acc: 0.9896\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 861/880\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0257 - acc: 0.9425 - val_loss: 0.0226 - val_acc: 0.9560\n",
      "Epoch 862/880\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0276 - acc: 0.9401 - val_loss: 0.0283 - val_acc: 0.9430\n",
      "Epoch 863/880\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0261 - acc: 0.9418 - val_loss: 0.0256 - val_acc: 0.9495\n",
      "Epoch 864/880\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0250 - acc: 0.9456 - val_loss: 0.0243 - val_acc: 0.9439\n",
      "Epoch 865/880\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0263 - acc: 0.9429 - val_loss: 0.0227 - val_acc: 0.9531\n",
      "Epoch 866/880\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0262 - acc: 0.9438 - val_loss: 0.0239 - val_acc: 0.9444\n",
      "Epoch 867/880\n",
      "8564/8564 [==============================] - 4s 462us/step - loss: 0.0259 - acc: 0.9432 - val_loss: 0.0209 - val_acc: 0.9525\n",
      "Epoch 868/880\n",
      "8564/8564 [==============================] - 4s 467us/step - loss: 0.0258 - acc: 0.9453 - val_loss: 0.0239 - val_acc: 0.9586\n",
      "Epoch 869/880\n",
      "8564/8564 [==============================] - 4s 475us/step - loss: 0.0262 - acc: 0.9432 - val_loss: 0.0249 - val_acc: 0.9535\n",
      "Epoch 870/880\n",
      "8564/8564 [==============================] - 4s 483us/step - loss: 0.0270 - acc: 0.9426 - val_loss: 0.0223 - val_acc: 0.9506\n",
      "Epoch 871/880\n",
      "8564/8564 [==============================] - 4s 478us/step - loss: 0.0269 - acc: 0.9398 - val_loss: 0.0272 - val_acc: 0.9376\n",
      "Epoch 872/880\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0252 - acc: 0.9448 - val_loss: 0.0304 - val_acc: 0.9438\n",
      "Epoch 873/880\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0265 - acc: 0.9422 - val_loss: 0.0282 - val_acc: 0.9392\n",
      "Epoch 874/880\n",
      "8564/8564 [==============================] - 4s 465us/step - loss: 0.0279 - acc: 0.9419 - val_loss: 0.0201 - val_acc: 0.9577\n",
      "Epoch 875/880\n",
      "8564/8564 [==============================] - 4s 465us/step - loss: 0.0257 - acc: 0.9461 - val_loss: 0.0223 - val_acc: 0.9469\n",
      "Epoch 876/880\n",
      "8564/8564 [==============================] - 4s 468us/step - loss: 0.0249 - acc: 0.9470 - val_loss: 0.0274 - val_acc: 0.9573\n",
      "Epoch 877/880\n",
      "8564/8564 [==============================] - 4s 463us/step - loss: 0.0262 - acc: 0.9420 - val_loss: 0.0230 - val_acc: 0.9436\n",
      "Epoch 878/880\n",
      "8564/8564 [==============================] - 4s 465us/step - loss: 0.0276 - acc: 0.9403 - val_loss: 0.0231 - val_acc: 0.9579\n",
      "Epoch 879/880\n",
      "8564/8564 [==============================] - 4s 465us/step - loss: 0.0267 - acc: 0.9430 - val_loss: 0.0260 - val_acc: 0.9448\n",
      "Epoch 880/880\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0270 - acc: 0.9429 - val_loss: 0.0257 - val_acc: 0.9440\n",
      "start training round 44\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 881/900\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0243 - acc: 0.6755 - val_loss: 0.0243 - val_acc: 0.7530\n",
      "Epoch 882/900\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0244 - acc: 0.7335 - val_loss: 0.0237 - val_acc: 0.7560\n",
      "Epoch 883/900\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0242 - acc: 0.7448 - val_loss: 0.0256 - val_acc: 0.7531\n",
      "Epoch 884/900\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0242 - acc: 0.7564 - val_loss: 0.0246 - val_acc: 0.7555\n",
      "Epoch 885/900\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0245 - acc: 0.7556 - val_loss: 0.0247 - val_acc: 0.7561\n",
      "Epoch 886/900\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0245 - acc: 0.7555 - val_loss: 0.0236 - val_acc: 0.7528\n",
      "Epoch 887/900\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0239 - acc: 0.6827 - val_loss: 0.0242 - val_acc: 0.6408\n",
      "Epoch 888/900\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0244 - acc: 0.6680 - val_loss: 0.0245 - val_acc: 0.6040\n",
      "Epoch 889/900\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0240 - acc: 0.6715 - val_loss: 0.0244 - val_acc: 0.6050\n",
      "Epoch 890/900\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0242 - acc: 0.6725 - val_loss: 0.0254 - val_acc: 0.5702\n",
      "Epoch 891/900\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0240 - acc: 0.6777 - val_loss: 0.0249 - val_acc: 0.5822\n",
      "Epoch 892/900\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0243 - acc: 0.7207 - val_loss: 0.0239 - val_acc: 0.7560\n",
      "Epoch 893/900\n",
      "8564/8564 [==============================] - 4s 468us/step - loss: 0.0248 - acc: 0.7543 - val_loss: 0.0236 - val_acc: 0.7539\n",
      "Epoch 894/900\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0242 - acc: 0.7114 - val_loss: 0.0254 - val_acc: 0.7471\n",
      "Epoch 895/900\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0241 - acc: 0.7532 - val_loss: 0.0254 - val_acc: 0.7564\n",
      "Epoch 896/900\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0247 - acc: 0.7503 - val_loss: 0.0253 - val_acc: 0.7545\n",
      "Epoch 897/900\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0244 - acc: 0.7562 - val_loss: 0.0245 - val_acc: 0.7569\n",
      "Epoch 898/900\n",
      "8564/8564 [==============================] - 4s 463us/step - loss: 0.0246 - acc: 0.7539 - val_loss: 0.0239 - val_acc: 0.7491\n",
      "Epoch 899/900\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0242 - acc: 0.7521 - val_loss: 0.0247 - val_acc: 0.7561\n",
      "Epoch 900/900\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0247 - acc: 0.7539 - val_loss: 0.0244 - val_acc: 0.7572\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 881/900\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0230 - acc: 0.8793 - val_loss: 0.0238 - val_acc: 0.8746\n",
      "Epoch 882/900\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0229 - acc: 0.8814 - val_loss: 0.0229 - val_acc: 0.8826\n",
      "Epoch 883/900\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0229 - acc: 0.8840 - val_loss: 0.0236 - val_acc: 0.8898\n",
      "Epoch 884/900\n",
      "8564/8564 [==============================] - 4s 461us/step - loss: 0.0231 - acc: 0.8895 - val_loss: 0.0225 - val_acc: 0.8833\n",
      "Epoch 885/900\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0229 - acc: 0.8824 - val_loss: 0.0234 - val_acc: 0.8765\n",
      "Epoch 886/900\n",
      "8564/8564 [==============================] - 4s 466us/step - loss: 0.0229 - acc: 0.8810 - val_loss: 0.0235 - val_acc: 0.8781\n",
      "Epoch 887/900\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0231 - acc: 0.8801 - val_loss: 0.0234 - val_acc: 0.8755\n",
      "Epoch 888/900\n",
      "8564/8564 [==============================] - 4s 465us/step - loss: 0.0229 - acc: 0.8819 - val_loss: 0.0230 - val_acc: 0.8824\n",
      "Epoch 889/900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0228 - acc: 0.8835 - val_loss: 0.0242 - val_acc: 0.8651\n",
      "Epoch 890/900\n",
      "8564/8564 [==============================] - 4s 462us/step - loss: 0.0228 - acc: 0.8812 - val_loss: 0.0233 - val_acc: 0.8812\n",
      "Epoch 891/900\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0228 - acc: 0.8822 - val_loss: 0.0232 - val_acc: 0.8778\n",
      "Epoch 892/900\n",
      "8564/8564 [==============================] - 4s 463us/step - loss: 0.0230 - acc: 0.8809 - val_loss: 0.0236 - val_acc: 0.8788\n",
      "Epoch 893/900\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0228 - acc: 0.8822 - val_loss: 0.0242 - val_acc: 0.8651\n",
      "Epoch 894/900\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0229 - acc: 0.8801 - val_loss: 0.0234 - val_acc: 0.8806\n",
      "Epoch 895/900\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0228 - acc: 0.8820 - val_loss: 0.0231 - val_acc: 0.8815\n",
      "Epoch 896/900\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0228 - acc: 0.8815 - val_loss: 0.0229 - val_acc: 0.8836\n",
      "Epoch 897/900\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0227 - acc: 0.8893 - val_loss: 0.0231 - val_acc: 0.8898\n",
      "Epoch 898/900\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0228 - acc: 0.8899 - val_loss: 0.0231 - val_acc: 0.8897\n",
      "Epoch 899/900\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0228 - acc: 0.8904 - val_loss: 0.0238 - val_acc: 0.8894\n",
      "Epoch 900/900\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0229 - acc: 0.8899 - val_loss: 0.0228 - val_acc: 0.8898\n",
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 881/900\n",
      "8564/8564 [==============================] - 4s 461us/step - loss: 0.0227 - acc: 0.9854 - val_loss: 0.0183 - val_acc: 0.9911\n",
      "Epoch 882/900\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0229 - acc: 0.9840 - val_loss: 0.0247 - val_acc: 0.9862\n",
      "Epoch 883/900\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0236 - acc: 0.9838 - val_loss: 0.0244 - val_acc: 0.9867\n",
      "Epoch 884/900\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0220 - acc: 0.9855 - val_loss: 0.0216 - val_acc: 0.9894\n",
      "Epoch 885/900\n",
      "8564/8564 [==============================] - 4s 463us/step - loss: 0.0245 - acc: 0.9829 - val_loss: 0.0253 - val_acc: 0.9880\n",
      "Epoch 886/900\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0230 - acc: 0.9850 - val_loss: 0.0184 - val_acc: 0.9893\n",
      "Epoch 887/900\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0253 - acc: 0.9848 - val_loss: 0.0215 - val_acc: 0.9868\n",
      "Epoch 888/900\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0223 - acc: 0.9856 - val_loss: 0.0236 - val_acc: 0.9889\n",
      "Epoch 889/900\n",
      "8564/8564 [==============================] - 4s 467us/step - loss: 0.0226 - acc: 0.9840 - val_loss: 0.0229 - val_acc: 0.9865\n",
      "Epoch 890/900\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0249 - acc: 0.9861 - val_loss: 0.0186 - val_acc: 0.9829\n",
      "Epoch 891/900\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0233 - acc: 0.9851 - val_loss: 0.0195 - val_acc: 0.9870\n",
      "Epoch 892/900\n",
      "8564/8564 [==============================] - 4s 467us/step - loss: 0.0238 - acc: 0.9847 - val_loss: 0.0209 - val_acc: 0.9891\n",
      "Epoch 893/900\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0244 - acc: 0.9858 - val_loss: 0.0225 - val_acc: 0.9822\n",
      "Epoch 894/900\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0240 - acc: 0.9843 - val_loss: 0.0220 - val_acc: 0.9873\n",
      "Epoch 895/900\n",
      "8564/8564 [==============================] - 4s 471us/step - loss: 0.0248 - acc: 0.9845 - val_loss: 0.0225 - val_acc: 0.9824\n",
      "Epoch 896/900\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0237 - acc: 0.9846 - val_loss: 0.0226 - val_acc: 0.9859\n",
      "Epoch 897/900\n",
      "8564/8564 [==============================] - 4s 462us/step - loss: 0.0232 - acc: 0.9844 - val_loss: 0.0216 - val_acc: 0.9886\n",
      "Epoch 898/900\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0236 - acc: 0.9849 - val_loss: 0.0223 - val_acc: 0.9851\n",
      "Epoch 899/900\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0233 - acc: 0.9861 - val_loss: 0.0210 - val_acc: 0.9867\n",
      "Epoch 900/900\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0235 - acc: 0.9856 - val_loss: 0.0251 - val_acc: 0.9881\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 881/900\n",
      "8564/8564 [==============================] - 4s 461us/step - loss: 0.0271 - acc: 0.9411 - val_loss: 0.0256 - val_acc: 0.9440\n",
      "Epoch 882/900\n",
      "8564/8564 [==============================] - 4s 462us/step - loss: 0.0251 - acc: 0.9457 - val_loss: 0.0205 - val_acc: 0.9543\n",
      "Epoch 883/900\n",
      "8564/8564 [==============================] - 4s 462us/step - loss: 0.0263 - acc: 0.9435 - val_loss: 0.0200 - val_acc: 0.9579\n",
      "Epoch 884/900\n",
      "8564/8564 [==============================] - 4s 470us/step - loss: 0.0261 - acc: 0.9446 - val_loss: 0.0216 - val_acc: 0.9494\n",
      "Epoch 885/900\n",
      "8564/8564 [==============================] - 4s 470us/step - loss: 0.0260 - acc: 0.9432 - val_loss: 0.0274 - val_acc: 0.9326\n",
      "Epoch 886/900\n",
      "8564/8564 [==============================] - 4s 462us/step - loss: 0.0251 - acc: 0.9449 - val_loss: 0.0235 - val_acc: 0.9525\n",
      "Epoch 887/900\n",
      "8564/8564 [==============================] - 4s 465us/step - loss: 0.0261 - acc: 0.9435 - val_loss: 0.0235 - val_acc: 0.9467\n",
      "Epoch 888/900\n",
      "8564/8564 [==============================] - 4s 465us/step - loss: 0.0249 - acc: 0.9466 - val_loss: 0.0222 - val_acc: 0.9545\n",
      "Epoch 889/900\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0259 - acc: 0.9452 - val_loss: 0.0241 - val_acc: 0.9372\n",
      "Epoch 890/900\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0256 - acc: 0.9438 - val_loss: 0.0257 - val_acc: 0.9401\n",
      "Epoch 891/900\n",
      "8564/8564 [==============================] - 4s 463us/step - loss: 0.0272 - acc: 0.9391 - val_loss: 0.0250 - val_acc: 0.9438\n",
      "Epoch 892/900\n",
      "8564/8564 [==============================] - 4s 462us/step - loss: 0.0267 - acc: 0.9415 - val_loss: 0.0241 - val_acc: 0.9554\n",
      "Epoch 893/900\n",
      "8564/8564 [==============================] - 4s 461us/step - loss: 0.0261 - acc: 0.9435 - val_loss: 0.0231 - val_acc: 0.9525\n",
      "Epoch 894/900\n",
      "8564/8564 [==============================] - 4s 467us/step - loss: 0.0264 - acc: 0.9440 - val_loss: 0.0215 - val_acc: 0.9532\n",
      "Epoch 895/900\n",
      "8564/8564 [==============================] - 4s 473us/step - loss: 0.0258 - acc: 0.9441 - val_loss: 0.0252 - val_acc: 0.9405\n",
      "Epoch 896/900\n",
      "8564/8564 [==============================] - 4s 466us/step - loss: 0.0256 - acc: 0.9443 - val_loss: 0.0260 - val_acc: 0.9473\n",
      "Epoch 897/900\n",
      "8564/8564 [==============================] - 4s 468us/step - loss: 0.0273 - acc: 0.9429 - val_loss: 0.0247 - val_acc: 0.9458\n",
      "Epoch 898/900\n",
      "8564/8564 [==============================] - 4s 472us/step - loss: 0.0261 - acc: 0.9449 - val_loss: 0.0252 - val_acc: 0.9368\n",
      "Epoch 899/900\n",
      "8564/8564 [==============================] - 4s 486us/step - loss: 0.0253 - acc: 0.9452 - val_loss: 0.0232 - val_acc: 0.9480\n",
      "Epoch 900/900\n",
      "8564/8564 [==============================] - 4s 479us/step - loss: 0.0268 - acc: 0.9424 - val_loss: 0.0267 - val_acc: 0.9455\n",
      "start training round 45\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 901/920\n",
      "8564/8564 [==============================] - 4s 467us/step - loss: 0.0244 - acc: 0.7493 - val_loss: 0.0238 - val_acc: 0.7558\n",
      "Epoch 902/920\n",
      "8564/8564 [==============================] - 4s 471us/step - loss: 0.0241 - acc: 0.7420 - val_loss: 0.0257 - val_acc: 0.7562\n",
      "Epoch 903/920\n",
      "8564/8564 [==============================] - 4s 463us/step - loss: 0.0243 - acc: 0.7545 - val_loss: 0.0242 - val_acc: 0.7557\n",
      "Epoch 904/920\n",
      "8564/8564 [==============================] - 4s 469us/step - loss: 0.0243 - acc: 0.7553 - val_loss: 0.0236 - val_acc: 0.7573\n",
      "Epoch 905/920\n",
      "8564/8564 [==============================] - 4s 465us/step - loss: 0.0241 - acc: 0.6673 - val_loss: 0.0249 - val_acc: 0.5797\n",
      "Epoch 906/920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0240 - acc: 0.6713 - val_loss: 0.0241 - val_acc: 0.6108\n",
      "Epoch 907/920\n",
      "8564/8564 [==============================] - 4s 469us/step - loss: 0.0241 - acc: 0.7017 - val_loss: 0.0245 - val_acc: 0.7558\n",
      "Epoch 908/920\n",
      "8564/8564 [==============================] - 4s 475us/step - loss: 0.0242 - acc: 0.7547 - val_loss: 0.0249 - val_acc: 0.7550\n",
      "Epoch 909/920\n",
      "8564/8564 [==============================] - 4s 462us/step - loss: 0.0243 - acc: 0.7578 - val_loss: 0.0243 - val_acc: 0.7555\n",
      "Epoch 910/920\n",
      "8564/8564 [==============================] - 4s 470us/step - loss: 0.0246 - acc: 0.7541 - val_loss: 0.0245 - val_acc: 0.7539\n",
      "Epoch 911/920\n",
      "8564/8564 [==============================] - 4s 466us/step - loss: 0.0241 - acc: 0.7417 - val_loss: 0.0251 - val_acc: 0.7537\n",
      "Epoch 912/920\n",
      "8564/8564 [==============================] - 4s 471us/step - loss: 0.0242 - acc: 0.7564 - val_loss: 0.0241 - val_acc: 0.7565\n",
      "Epoch 913/920\n",
      "8564/8564 [==============================] - 4s 464us/step - loss: 0.0239 - acc: 0.7086 - val_loss: 0.0254 - val_acc: 0.5728\n",
      "Epoch 914/920\n",
      "8564/8564 [==============================] - 4s 462us/step - loss: 0.0243 - acc: 0.6659 - val_loss: 0.0248 - val_acc: 0.6025\n",
      "Epoch 915/920\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0240 - acc: 0.6734 - val_loss: 0.0236 - val_acc: 0.6802\n",
      "Epoch 916/920\n",
      "8564/8564 [==============================] - 4s 461us/step - loss: 0.0244 - acc: 0.6679 - val_loss: 0.0244 - val_acc: 0.6222\n",
      "Epoch 917/920\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0236 - acc: 0.6872 - val_loss: 0.0247 - val_acc: 0.5829\n",
      "Epoch 918/920\n",
      "8564/8564 [==============================] - 4s 463us/step - loss: 0.0243 - acc: 0.6746 - val_loss: 0.0251 - val_acc: 0.5797\n",
      "Epoch 919/920\n",
      "8564/8564 [==============================] - 4s 468us/step - loss: 0.0243 - acc: 0.6684 - val_loss: 0.0243 - val_acc: 0.6221\n",
      "Epoch 920/920\n",
      "8564/8564 [==============================] - 4s 464us/step - loss: 0.0242 - acc: 0.6666 - val_loss: 0.0243 - val_acc: 0.5981\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 901/920\n",
      "8564/8564 [==============================] - 4s 464us/step - loss: 0.0228 - acc: 0.8904 - val_loss: 0.0225 - val_acc: 0.8899\n",
      "Epoch 902/920\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0228 - acc: 0.8900 - val_loss: 0.0223 - val_acc: 0.8890\n",
      "Epoch 903/920\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0226 - acc: 0.8903 - val_loss: 0.0227 - val_acc: 0.8897\n",
      "Epoch 904/920\n",
      "8564/8564 [==============================] - 4s 469us/step - loss: 0.0228 - acc: 0.8904 - val_loss: 0.0246 - val_acc: 0.8899\n",
      "Epoch 905/920\n",
      "8564/8564 [==============================] - 4s 465us/step - loss: 0.0230 - acc: 0.8903 - val_loss: 0.0232 - val_acc: 0.8901\n",
      "Epoch 906/920\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0228 - acc: 0.8900 - val_loss: 0.0227 - val_acc: 0.8902\n",
      "Epoch 907/920\n",
      "8564/8564 [==============================] - 4s 463us/step - loss: 0.0229 - acc: 0.8903 - val_loss: 0.0221 - val_acc: 0.8893\n",
      "Epoch 908/920\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0226 - acc: 0.8862 - val_loss: 0.0234 - val_acc: 0.8895\n",
      "Epoch 909/920\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0228 - acc: 0.8906 - val_loss: 0.0230 - val_acc: 0.8890\n",
      "Epoch 910/920\n",
      "8564/8564 [==============================] - 4s 464us/step - loss: 0.0229 - acc: 0.8905 - val_loss: 0.0227 - val_acc: 0.8901\n",
      "Epoch 911/920\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0227 - acc: 0.8906 - val_loss: 0.0230 - val_acc: 0.8897\n",
      "Epoch 912/920\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0229 - acc: 0.8901 - val_loss: 0.0233 - val_acc: 0.8898\n",
      "Epoch 913/920\n",
      "8564/8564 [==============================] - 4s 468us/step - loss: 0.0228 - acc: 0.8901 - val_loss: 0.0234 - val_acc: 0.8902\n",
      "Epoch 914/920\n",
      "8564/8564 [==============================] - 4s 468us/step - loss: 0.0231 - acc: 0.8893 - val_loss: 0.0218 - val_acc: 0.8883\n",
      "Epoch 915/920\n",
      "8564/8564 [==============================] - 4s 472us/step - loss: 0.0227 - acc: 0.8823 - val_loss: 0.0223 - val_acc: 0.8853\n",
      "Epoch 916/920\n",
      "8564/8564 [==============================] - 4s 469us/step - loss: 0.0228 - acc: 0.8825 - val_loss: 0.0236 - val_acc: 0.8758\n",
      "Epoch 917/920\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0227 - acc: 0.8829 - val_loss: 0.0223 - val_acc: 0.8858\n",
      "Epoch 918/920\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0229 - acc: 0.8867 - val_loss: 0.0234 - val_acc: 0.8900\n",
      "Epoch 919/920\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0229 - acc: 0.8905 - val_loss: 0.0232 - val_acc: 0.8900\n",
      "Epoch 920/920\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0230 - acc: 0.8897 - val_loss: 0.0225 - val_acc: 0.8897\n",
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 901/920\n",
      "8564/8564 [==============================] - 4s 462us/step - loss: 0.0230 - acc: 0.9865 - val_loss: 0.0246 - val_acc: 0.9854\n",
      "Epoch 902/920\n",
      "8564/8564 [==============================] - 4s 478us/step - loss: 0.0241 - acc: 0.9854 - val_loss: 0.0233 - val_acc: 0.9870\n",
      "Epoch 903/920\n",
      "8564/8564 [==============================] - 4s 495us/step - loss: 0.0229 - acc: 0.9846 - val_loss: 0.0177 - val_acc: 0.9886\n",
      "Epoch 904/920\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0233 - acc: 0.9863 - val_loss: 0.0217 - val_acc: 0.9890\n",
      "Epoch 905/920\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0245 - acc: 0.9859 - val_loss: 0.0197 - val_acc: 0.9878\n",
      "Epoch 906/920\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0236 - acc: 0.9852 - val_loss: 0.0202 - val_acc: 0.9894\n",
      "Epoch 907/920\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0228 - acc: 0.9870 - val_loss: 0.0190 - val_acc: 0.9904\n",
      "Epoch 908/920\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0229 - acc: 0.9848 - val_loss: 0.0232 - val_acc: 0.9896\n",
      "Epoch 909/920\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0237 - acc: 0.9862 - val_loss: 0.0186 - val_acc: 0.9893\n",
      "Epoch 910/920\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0246 - acc: 0.9832 - val_loss: 0.0219 - val_acc: 0.9889\n",
      "Epoch 911/920\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0243 - acc: 0.9858 - val_loss: 0.0207 - val_acc: 0.9885\n",
      "Epoch 912/920\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0230 - acc: 0.9869 - val_loss: 0.0165 - val_acc: 0.9882\n",
      "Epoch 913/920\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0239 - acc: 0.9852 - val_loss: 0.0212 - val_acc: 0.9905\n",
      "Epoch 914/920\n",
      "8564/8564 [==============================] - 4s 449us/step - loss: 0.0229 - acc: 0.9870 - val_loss: 0.0195 - val_acc: 0.9897\n",
      "Epoch 915/920\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0230 - acc: 0.9848 - val_loss: 0.0278 - val_acc: 0.9865\n",
      "Epoch 916/920\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0235 - acc: 0.9857 - val_loss: 0.0166 - val_acc: 0.9895\n",
      "Epoch 917/920\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0234 - acc: 0.9857 - val_loss: 0.0243 - val_acc: 0.9868\n",
      "Epoch 918/920\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0246 - acc: 0.9828 - val_loss: 0.0238 - val_acc: 0.9900\n",
      "Epoch 919/920\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0247 - acc: 0.9839 - val_loss: 0.0284 - val_acc: 0.9858\n",
      "Epoch 920/920\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0241 - acc: 0.9860 - val_loss: 0.0220 - val_acc: 0.9870\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 901/920\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0255 - acc: 0.9464 - val_loss: 0.0308 - val_acc: 0.9375\n",
      "Epoch 902/920\n",
      "8564/8564 [==============================] - 4s 518us/step - loss: 0.0267 - acc: 0.9434 - val_loss: 0.0227 - val_acc: 0.9463\n",
      "Epoch 903/920\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 4s 462us/step - loss: 0.0273 - acc: 0.9406 - val_loss: 0.0267 - val_acc: 0.9267\n",
      "Epoch 904/920\n",
      "8564/8564 [==============================] - 4s 461us/step - loss: 0.0260 - acc: 0.9427 - val_loss: 0.0238 - val_acc: 0.9565\n",
      "Epoch 905/920\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0266 - acc: 0.9435 - val_loss: 0.0250 - val_acc: 0.9558\n",
      "Epoch 906/920\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0265 - acc: 0.9430 - val_loss: 0.0259 - val_acc: 0.9465\n",
      "Epoch 907/920\n",
      "8564/8564 [==============================] - 4s 461us/step - loss: 0.0277 - acc: 0.9407 - val_loss: 0.0263 - val_acc: 0.9534\n",
      "Epoch 908/920\n",
      "8564/8564 [==============================] - 4s 465us/step - loss: 0.0260 - acc: 0.9440 - val_loss: 0.0292 - val_acc: 0.9444\n",
      "Epoch 909/920\n",
      "8564/8564 [==============================] - 4s 471us/step - loss: 0.0262 - acc: 0.9424 - val_loss: 0.0222 - val_acc: 0.9528\n",
      "Epoch 910/920\n",
      "8564/8564 [==============================] - 4s 464us/step - loss: 0.0255 - acc: 0.9470 - val_loss: 0.0214 - val_acc: 0.9550\n",
      "Epoch 911/920\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0258 - acc: 0.9431 - val_loss: 0.0222 - val_acc: 0.9453\n",
      "Epoch 912/920\n",
      "8564/8564 [==============================] - 4s 462us/step - loss: 0.0261 - acc: 0.9443 - val_loss: 0.0224 - val_acc: 0.9503\n",
      "Epoch 913/920\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0255 - acc: 0.9453 - val_loss: 0.0266 - val_acc: 0.9354\n",
      "Epoch 914/920\n",
      "8564/8564 [==============================] - 4s 462us/step - loss: 0.0258 - acc: 0.9449 - val_loss: 0.0245 - val_acc: 0.9524\n",
      "Epoch 915/920\n",
      "8564/8564 [==============================] - 4s 467us/step - loss: 0.0264 - acc: 0.9427 - val_loss: 0.0231 - val_acc: 0.9497\n",
      "Epoch 916/920\n",
      "8564/8564 [==============================] - 4s 461us/step - loss: 0.0260 - acc: 0.9437 - val_loss: 0.0198 - val_acc: 0.9608\n",
      "Epoch 917/920\n",
      "8564/8564 [==============================] - 4s 461us/step - loss: 0.0259 - acc: 0.9433 - val_loss: 0.0211 - val_acc: 0.9606\n",
      "Epoch 918/920\n",
      "8564/8564 [==============================] - 4s 461us/step - loss: 0.0267 - acc: 0.9427 - val_loss: 0.0213 - val_acc: 0.9576\n",
      "Epoch 919/920\n",
      "8564/8564 [==============================] - 4s 464us/step - loss: 0.0255 - acc: 0.9449 - val_loss: 0.0214 - val_acc: 0.9560\n",
      "Epoch 920/920\n",
      "8564/8564 [==============================] - 4s 512us/step - loss: 0.0257 - acc: 0.9450 - val_loss: 0.0280 - val_acc: 0.9436\n",
      "start training round 46\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 921/940\n",
      "8564/8564 [==============================] - 4s 462us/step - loss: 0.0241 - acc: 0.6843 - val_loss: 0.0247 - val_acc: 0.5793\n",
      "Epoch 922/940\n",
      "8564/8564 [==============================] - 4s 465us/step - loss: 0.0239 - acc: 0.6868 - val_loss: 0.0252 - val_acc: 0.5719\n",
      "Epoch 923/940\n",
      "8564/8564 [==============================] - 4s 510us/step - loss: 0.0241 - acc: 0.6701 - val_loss: 0.0248 - val_acc: 0.5846\n",
      "Epoch 924/940\n",
      "8564/8564 [==============================] - 4s 473us/step - loss: 0.0241 - acc: 0.6666 - val_loss: 0.0242 - val_acc: 0.6011\n",
      "Epoch 925/940\n",
      "8564/8564 [==============================] - 4s 461us/step - loss: 0.0239 - acc: 0.6730 - val_loss: 0.0238 - val_acc: 0.6514\n",
      "Epoch 926/940\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0241 - acc: 0.6692 - val_loss: 0.0248 - val_acc: 0.7559\n",
      "Epoch 927/940\n",
      "8564/8564 [==============================] - 4s 466us/step - loss: 0.0240 - acc: 0.6886 - val_loss: 0.0237 - val_acc: 0.7555\n",
      "Epoch 928/940\n",
      "8564/8564 [==============================] - 4s 468us/step - loss: 0.0237 - acc: 0.7011 - val_loss: 0.0247 - val_acc: 0.7442\n",
      "Epoch 929/940\n",
      "8564/8564 [==============================] - 4s 470us/step - loss: 0.0240 - acc: 0.6809 - val_loss: 0.0249 - val_acc: 0.7455\n",
      "Epoch 930/940\n",
      "8564/8564 [==============================] - 4s 465us/step - loss: 0.0244 - acc: 0.7040 - val_loss: 0.0243 - val_acc: 0.7572\n",
      "Epoch 931/940\n",
      "8564/8564 [==============================] - 4s 469us/step - loss: 0.0241 - acc: 0.7443 - val_loss: 0.0246 - val_acc: 0.7569\n",
      "Epoch 932/940\n",
      "8564/8564 [==============================] - 4s 519us/step - loss: 0.0244 - acc: 0.7551 - val_loss: 0.0254 - val_acc: 0.7570\n",
      "Epoch 933/940\n",
      "8564/8564 [==============================] - 4s 466us/step - loss: 0.0241 - acc: 0.7131 - val_loss: 0.0241 - val_acc: 0.7579\n",
      "Epoch 934/940\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0240 - acc: 0.7585 - val_loss: 0.0237 - val_acc: 0.7580\n",
      "Epoch 935/940\n",
      "8564/8564 [==============================] - 4s 464us/step - loss: 0.0239 - acc: 0.7539 - val_loss: 0.0236 - val_acc: 0.7556\n",
      "Epoch 936/940\n",
      "8564/8564 [==============================] - 4s 469us/step - loss: 0.0240 - acc: 0.6809 - val_loss: 0.0236 - val_acc: 0.7558\n",
      "Epoch 937/940\n",
      "8564/8564 [==============================] - 4s 463us/step - loss: 0.0240 - acc: 0.7557 - val_loss: 0.0245 - val_acc: 0.7583\n",
      "Epoch 938/940\n",
      "8564/8564 [==============================] - 4s 519us/step - loss: 0.0239 - acc: 0.7034 - val_loss: 0.0242 - val_acc: 0.7514\n",
      "Epoch 939/940\n",
      "8564/8564 [==============================] - 4s 449us/step - loss: 0.0240 - acc: 0.6670 - val_loss: 0.0246 - val_acc: 0.7436\n",
      "Epoch 940/940\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0238 - acc: 0.6811 - val_loss: 0.0251 - val_acc: 0.7398\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 921/940\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0227 - acc: 0.8904 - val_loss: 0.0232 - val_acc: 0.8902\n",
      "Epoch 922/940\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0229 - acc: 0.8904 - val_loss: 0.0231 - val_acc: 0.8895\n",
      "Epoch 923/940\n",
      "8564/8564 [==============================] - 4s 494us/step - loss: 0.0226 - acc: 0.8905 - val_loss: 0.0232 - val_acc: 0.8892\n",
      "Epoch 924/940\n",
      "8564/8564 [==============================] - 4s 478us/step - loss: 0.0230 - acc: 0.8889 - val_loss: 0.0229 - val_acc: 0.8831\n",
      "Epoch 925/940\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0228 - acc: 0.8820 - val_loss: 0.0238 - val_acc: 0.8702\n",
      "Epoch 926/940\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0229 - acc: 0.8802 - val_loss: 0.0232 - val_acc: 0.8820\n",
      "Epoch 927/940\n",
      "8564/8564 [==============================] - 4s 515us/step - loss: 0.0227 - acc: 0.8838 - val_loss: 0.0234 - val_acc: 0.8743\n",
      "Epoch 928/940\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0227 - acc: 0.8823 - val_loss: 0.0237 - val_acc: 0.8719\n",
      "Epoch 929/940\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0228 - acc: 0.8821 - val_loss: 0.0227 - val_acc: 0.8809\n",
      "Epoch 930/940\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0227 - acc: 0.8820 - val_loss: 0.0239 - val_acc: 0.8661\n",
      "Epoch 931/940\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0228 - acc: 0.8824 - val_loss: 0.0232 - val_acc: 0.8896\n",
      "Epoch 932/940\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0227 - acc: 0.8908 - val_loss: 0.0229 - val_acc: 0.8903\n",
      "Epoch 933/940\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0227 - acc: 0.8911 - val_loss: 0.0229 - val_acc: 0.8899\n",
      "Epoch 934/940\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0228 - acc: 0.8903 - val_loss: 0.0226 - val_acc: 0.8901\n",
      "Epoch 935/940\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0226 - acc: 0.8910 - val_loss: 0.0233 - val_acc: 0.8896\n",
      "Epoch 936/940\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0227 - acc: 0.8906 - val_loss: 0.0228 - val_acc: 0.8898\n",
      "Epoch 937/940\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0227 - acc: 0.8906 - val_loss: 0.0232 - val_acc: 0.8902\n",
      "Epoch 938/940\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0227 - acc: 0.8905 - val_loss: 0.0226 - val_acc: 0.8904\n",
      "Epoch 939/940\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0228 - acc: 0.8906 - val_loss: 0.0225 - val_acc: 0.8897\n",
      "Epoch 940/940\n",
      "8564/8564 [==============================] - 4s 471us/step - loss: 0.0226 - acc: 0.8907 - val_loss: 0.0236 - val_acc: 0.8902\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 921/940\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0213 - acc: 0.9858 - val_loss: 0.0220 - val_acc: 0.9876\n",
      "Epoch 922/940\n",
      "8564/8564 [==============================] - 4s 463us/step - loss: 0.0239 - acc: 0.9846 - val_loss: 0.0163 - val_acc: 0.9896\n",
      "Epoch 923/940\n",
      "8564/8564 [==============================] - 4s 467us/step - loss: 0.0255 - acc: 0.9834 - val_loss: 0.0233 - val_acc: 0.9889\n",
      "Epoch 924/940\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0245 - acc: 0.9865 - val_loss: 0.0268 - val_acc: 0.9867\n",
      "Epoch 925/940\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0232 - acc: 0.9854 - val_loss: 0.0199 - val_acc: 0.9879\n",
      "Epoch 926/940\n",
      "8564/8564 [==============================] - 4s 462us/step - loss: 0.0237 - acc: 0.9861 - val_loss: 0.0233 - val_acc: 0.9878\n",
      "Epoch 927/940\n",
      "8564/8564 [==============================] - 4s 464us/step - loss: 0.0244 - acc: 0.9867 - val_loss: 0.0214 - val_acc: 0.9877\n",
      "Epoch 928/940\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0271 - acc: 0.9852 - val_loss: 0.0229 - val_acc: 0.9852\n",
      "Epoch 929/940\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0236 - acc: 0.9853 - val_loss: 0.0165 - val_acc: 0.9907\n",
      "Epoch 930/940\n",
      "8564/8564 [==============================] - 4s 461us/step - loss: 0.0248 - acc: 0.9860 - val_loss: 0.0199 - val_acc: 0.9886\n",
      "Epoch 931/940\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0252 - acc: 0.9870 - val_loss: 0.0211 - val_acc: 0.9839\n",
      "Epoch 932/940\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0222 - acc: 0.9866 - val_loss: 0.0272 - val_acc: 0.9909\n",
      "Epoch 933/940\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0230 - acc: 0.9854 - val_loss: 0.0266 - val_acc: 0.9885\n",
      "Epoch 934/940\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0241 - acc: 0.9860 - val_loss: 0.0223 - val_acc: 0.9911\n",
      "Epoch 935/940\n",
      "8564/8564 [==============================] - 4s 461us/step - loss: 0.0245 - acc: 0.9857 - val_loss: 0.0181 - val_acc: 0.9911\n",
      "Epoch 936/940\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0231 - acc: 0.9851 - val_loss: 0.0235 - val_acc: 0.9885\n",
      "Epoch 937/940\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0221 - acc: 0.9851 - val_loss: 0.0223 - val_acc: 0.9852\n",
      "Epoch 938/940\n",
      "8564/8564 [==============================] - 4s 470us/step - loss: 0.0240 - acc: 0.9837 - val_loss: 0.0204 - val_acc: 0.9858\n",
      "Epoch 939/940\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0233 - acc: 0.9838 - val_loss: 0.0173 - val_acc: 0.9900\n",
      "Epoch 940/940\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0222 - acc: 0.9872 - val_loss: 0.0173 - val_acc: 0.9891\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 921/940\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0256 - acc: 0.9453 - val_loss: 0.0249 - val_acc: 0.9589\n",
      "Epoch 922/940\n",
      "8564/8564 [==============================] - 4s 469us/step - loss: 0.0261 - acc: 0.9433 - val_loss: 0.0280 - val_acc: 0.9443\n",
      "Epoch 923/940\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0259 - acc: 0.9455 - val_loss: 0.0198 - val_acc: 0.9576\n",
      "Epoch 924/940\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0253 - acc: 0.9438 - val_loss: 0.0273 - val_acc: 0.9513\n",
      "Epoch 925/940\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0256 - acc: 0.9446 - val_loss: 0.0218 - val_acc: 0.9531\n",
      "Epoch 926/940\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0259 - acc: 0.9433 - val_loss: 0.0233 - val_acc: 0.9480\n",
      "Epoch 927/940\n",
      "8564/8564 [==============================] - 4s 463us/step - loss: 0.0247 - acc: 0.9470 - val_loss: 0.0229 - val_acc: 0.9481\n",
      "Epoch 928/940\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0256 - acc: 0.9435 - val_loss: 0.0217 - val_acc: 0.9545\n",
      "Epoch 929/940\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0250 - acc: 0.9453 - val_loss: 0.0242 - val_acc: 0.9422\n",
      "Epoch 930/940\n",
      "8564/8564 [==============================] - 4s 524us/step - loss: 0.0255 - acc: 0.9456 - val_loss: 0.0254 - val_acc: 0.9367\n",
      "Epoch 931/940\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0261 - acc: 0.9438 - val_loss: 0.0223 - val_acc: 0.9562\n",
      "Epoch 932/940\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0258 - acc: 0.9438 - val_loss: 0.0240 - val_acc: 0.9495\n",
      "Epoch 933/940\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0255 - acc: 0.9450 - val_loss: 0.0231 - val_acc: 0.9451\n",
      "Epoch 934/940\n",
      "8564/8564 [==============================] - 4s 461us/step - loss: 0.0264 - acc: 0.9420 - val_loss: 0.0252 - val_acc: 0.9491\n",
      "Epoch 935/940\n",
      "8564/8564 [==============================] - 4s 464us/step - loss: 0.0252 - acc: 0.9463 - val_loss: 0.0220 - val_acc: 0.9591\n",
      "Epoch 936/940\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0259 - acc: 0.9432 - val_loss: 0.0242 - val_acc: 0.9533\n",
      "Epoch 937/940\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0269 - acc: 0.9406 - val_loss: 0.0247 - val_acc: 0.9436\n",
      "Epoch 938/940\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0246 - acc: 0.9478 - val_loss: 0.0243 - val_acc: 0.9506\n",
      "Epoch 939/940\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0257 - acc: 0.9447 - val_loss: 0.0267 - val_acc: 0.9331\n",
      "Epoch 940/940\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0269 - acc: 0.9432 - val_loss: 0.0261 - val_acc: 0.9510\n",
      "start training round 47\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 941/960\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0241 - acc: 0.6661 - val_loss: 0.0237 - val_acc: 0.7558\n",
      "Epoch 942/960\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0242 - acc: 0.7576 - val_loss: 0.0255 - val_acc: 0.6100\n",
      "Epoch 943/960\n",
      "8564/8564 [==============================] - 4s 483us/step - loss: 0.0242 - acc: 0.6957 - val_loss: 0.0242 - val_acc: 0.5910\n",
      "Epoch 944/960\n",
      "8564/8564 [==============================] - 5s 553us/step - loss: 0.0237 - acc: 0.6857 - val_loss: 0.0243 - val_acc: 0.7478\n",
      "Epoch 945/960\n",
      "8564/8564 [==============================] - 4s 492us/step - loss: 0.0243 - acc: 0.6608 - val_loss: 0.0238 - val_acc: 0.7522\n",
      "Epoch 946/960\n",
      "8564/8564 [==============================] - 4s 464us/step - loss: 0.0240 - acc: 0.6641 - val_loss: 0.0252 - val_acc: 0.7397\n",
      "Epoch 947/960\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0240 - acc: 0.6669 - val_loss: 0.0248 - val_acc: 0.7416\n",
      "Epoch 948/960\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0239 - acc: 0.6727 - val_loss: 0.0253 - val_acc: 0.7408\n",
      "Epoch 949/960\n",
      "8564/8564 [==============================] - 4s 446us/step - loss: 0.0240 - acc: 0.6707 - val_loss: 0.0240 - val_acc: 0.7508\n",
      "Epoch 950/960\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0236 - acc: 0.6758 - val_loss: 0.0250 - val_acc: 0.7411\n",
      "Epoch 951/960\n",
      "8564/8564 [==============================] - 4s 465us/step - loss: 0.0239 - acc: 0.6667 - val_loss: 0.0252 - val_acc: 0.7397\n",
      "Epoch 952/960\n",
      "8564/8564 [==============================] - 4s 463us/step - loss: 0.0241 - acc: 0.6631 - val_loss: 0.0243 - val_acc: 0.7513\n",
      "Epoch 953/960\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0237 - acc: 0.6797 - val_loss: 0.0240 - val_acc: 0.7488\n",
      "Epoch 954/960\n",
      "8564/8564 [==============================] - 4s 463us/step - loss: 0.0237 - acc: 0.6720 - val_loss: 0.0244 - val_acc: 0.7455\n",
      "Epoch 955/960\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0240 - acc: 0.7457 - val_loss: 0.0243 - val_acc: 0.7548\n",
      "Epoch 956/960\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0242 - acc: 0.7573 - val_loss: 0.0253 - val_acc: 0.7310\n",
      "Epoch 957/960\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0240 - acc: 0.7556 - val_loss: 0.0234 - val_acc: 0.7557\n",
      "Epoch 958/960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0239 - acc: 0.7066 - val_loss: 0.0241 - val_acc: 0.6316\n",
      "Epoch 959/960\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0240 - acc: 0.7504 - val_loss: 0.0243 - val_acc: 0.7565\n",
      "Epoch 960/960\n",
      "8564/8564 [==============================] - 4s 477us/step - loss: 0.0241 - acc: 0.7082 - val_loss: 0.0240 - val_acc: 0.7516\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 941/960\n",
      "8564/8564 [==============================] - 4s 466us/step - loss: 0.0227 - acc: 0.8906 - val_loss: 0.0229 - val_acc: 0.8904\n",
      "Epoch 942/960\n",
      "8564/8564 [==============================] - 4s 464us/step - loss: 0.0226 - acc: 0.8905 - val_loss: 0.0231 - val_acc: 0.8896\n",
      "Epoch 943/960\n",
      "8564/8564 [==============================] - 4s 469us/step - loss: 0.0229 - acc: 0.8905 - val_loss: 0.0234 - val_acc: 0.8904\n",
      "Epoch 944/960\n",
      "8564/8564 [==============================] - 4s 464us/step - loss: 0.0228 - acc: 0.8903 - val_loss: 0.0217 - val_acc: 0.8894\n",
      "Epoch 945/960\n",
      "8564/8564 [==============================] - 4s 463us/step - loss: 0.0225 - acc: 0.8829 - val_loss: 0.0237 - val_acc: 0.8739\n",
      "Epoch 946/960\n",
      "8564/8564 [==============================] - 4s 472us/step - loss: 0.0227 - acc: 0.8824 - val_loss: 0.0229 - val_acc: 0.8801\n",
      "Epoch 947/960\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0227 - acc: 0.8831 - val_loss: 0.0225 - val_acc: 0.8828\n",
      "Epoch 948/960\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0227 - acc: 0.8823 - val_loss: 0.0226 - val_acc: 0.8857\n",
      "Epoch 949/960\n",
      "8564/8564 [==============================] - 4s 465us/step - loss: 0.0227 - acc: 0.8828 - val_loss: 0.0234 - val_acc: 0.8745\n",
      "Epoch 950/960\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0227 - acc: 0.8829 - val_loss: 0.0231 - val_acc: 0.8802\n",
      "Epoch 951/960\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0228 - acc: 0.8827 - val_loss: 0.0224 - val_acc: 0.8821\n",
      "Epoch 952/960\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0227 - acc: 0.8839 - val_loss: 0.0236 - val_acc: 0.8709\n",
      "Epoch 953/960\n",
      "8564/8564 [==============================] - 4s 479us/step - loss: 0.0228 - acc: 0.8822 - val_loss: 0.0228 - val_acc: 0.8819\n",
      "Epoch 954/960\n",
      "8564/8564 [==============================] - 4s 472us/step - loss: 0.0227 - acc: 0.8838 - val_loss: 0.0227 - val_acc: 0.8835\n",
      "Epoch 955/960\n",
      "8564/8564 [==============================] - 4s 447us/step - loss: 0.0227 - acc: 0.8815 - val_loss: 0.0226 - val_acc: 0.8818\n",
      "Epoch 956/960\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0226 - acc: 0.8869 - val_loss: 0.0228 - val_acc: 0.8906\n",
      "Epoch 957/960\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0226 - acc: 0.8906 - val_loss: 0.0236 - val_acc: 0.8905\n",
      "Epoch 958/960\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0228 - acc: 0.8908 - val_loss: 0.0230 - val_acc: 0.8908\n",
      "Epoch 959/960\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0227 - acc: 0.8906 - val_loss: 0.0230 - val_acc: 0.8897\n",
      "Epoch 960/960\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0227 - acc: 0.8907 - val_loss: 0.0230 - val_acc: 0.8889\n",
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 941/960\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0231 - acc: 0.9859 - val_loss: 0.0239 - val_acc: 0.9890\n",
      "Epoch 942/960\n",
      "8564/8564 [==============================] - 4s 447us/step - loss: 0.0242 - acc: 0.9849 - val_loss: 0.0187 - val_acc: 0.9904\n",
      "Epoch 943/960\n",
      "8564/8564 [==============================] - 4s 464us/step - loss: 0.0243 - acc: 0.9835 - val_loss: 0.0198 - val_acc: 0.9887\n",
      "Epoch 944/960\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0216 - acc: 0.9860 - val_loss: 0.0244 - val_acc: 0.9892\n",
      "Epoch 945/960\n",
      "8564/8564 [==============================] - 4s 449us/step - loss: 0.0235 - acc: 0.9866 - val_loss: 0.0221 - val_acc: 0.9895\n",
      "Epoch 946/960\n",
      "8564/8564 [==============================] - 4s 446us/step - loss: 0.0231 - acc: 0.9838 - val_loss: 0.0228 - val_acc: 0.9886\n",
      "Epoch 947/960\n",
      "8564/8564 [==============================] - 4s 449us/step - loss: 0.0235 - acc: 0.9834 - val_loss: 0.0205 - val_acc: 0.9869\n",
      "Epoch 948/960\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0242 - acc: 0.9858 - val_loss: 0.0191 - val_acc: 0.9881\n",
      "Epoch 949/960\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0242 - acc: 0.9846 - val_loss: 0.0180 - val_acc: 0.9908\n",
      "Epoch 950/960\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0226 - acc: 0.9851 - val_loss: 0.0217 - val_acc: 0.9890\n",
      "Epoch 951/960\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0244 - acc: 0.9838 - val_loss: 0.0214 - val_acc: 0.9880\n",
      "Epoch 952/960\n",
      "8564/8564 [==============================] - 4s 446us/step - loss: 0.0225 - acc: 0.9859 - val_loss: 0.0214 - val_acc: 0.9895\n",
      "Epoch 953/960\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0238 - acc: 0.9851 - val_loss: 0.0224 - val_acc: 0.9852\n",
      "Epoch 954/960\n",
      "8564/8564 [==============================] - 4s 468us/step - loss: 0.0241 - acc: 0.9849 - val_loss: 0.0217 - val_acc: 0.9869\n",
      "Epoch 955/960\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0243 - acc: 0.9854 - val_loss: 0.0231 - val_acc: 0.9855\n",
      "Epoch 956/960\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0229 - acc: 0.9866 - val_loss: 0.0232 - val_acc: 0.9858\n",
      "Epoch 957/960\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0230 - acc: 0.9852 - val_loss: 0.0219 - val_acc: 0.9869\n",
      "Epoch 958/960\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0235 - acc: 0.9857 - val_loss: 0.0176 - val_acc: 0.9909\n",
      "Epoch 959/960\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0244 - acc: 0.9841 - val_loss: 0.0227 - val_acc: 0.9829\n",
      "Epoch 960/960\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0232 - acc: 0.9850 - val_loss: 0.0228 - val_acc: 0.9904\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 941/960\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0260 - acc: 0.9430 - val_loss: 0.0274 - val_acc: 0.9523\n",
      "Epoch 942/960\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0259 - acc: 0.9432 - val_loss: 0.0244 - val_acc: 0.9500\n",
      "Epoch 943/960\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0256 - acc: 0.9432 - val_loss: 0.0223 - val_acc: 0.9570\n",
      "Epoch 944/960\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0258 - acc: 0.9440 - val_loss: 0.0245 - val_acc: 0.9461\n",
      "Epoch 945/960\n",
      "8564/8564 [==============================] - 4s 462us/step - loss: 0.0263 - acc: 0.9432 - val_loss: 0.0239 - val_acc: 0.9474\n",
      "Epoch 946/960\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0256 - acc: 0.9456 - val_loss: 0.0227 - val_acc: 0.9557\n",
      "Epoch 947/960\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0257 - acc: 0.9449 - val_loss: 0.0189 - val_acc: 0.9583\n",
      "Epoch 948/960\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0273 - acc: 0.9402 - val_loss: 0.0245 - val_acc: 0.9589\n",
      "Epoch 949/960\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0254 - acc: 0.9450 - val_loss: 0.0201 - val_acc: 0.9563\n",
      "Epoch 950/960\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0254 - acc: 0.9448 - val_loss: 0.0271 - val_acc: 0.9445\n",
      "Epoch 951/960\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0272 - acc: 0.9416 - val_loss: 0.0227 - val_acc: 0.9511\n",
      "Epoch 952/960\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0258 - acc: 0.9434 - val_loss: 0.0231 - val_acc: 0.9502\n",
      "Epoch 953/960\n",
      "8564/8564 [==============================] - 4s 465us/step - loss: 0.0255 - acc: 0.9426 - val_loss: 0.0258 - val_acc: 0.9343\n",
      "Epoch 954/960\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0253 - acc: 0.9447 - val_loss: 0.0233 - val_acc: 0.9566\n",
      "Epoch 955/960\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0264 - acc: 0.9440 - val_loss: 0.0246 - val_acc: 0.9419\n",
      "Epoch 956/960\n",
      "8564/8564 [==============================] - 4s 463us/step - loss: 0.0255 - acc: 0.9456 - val_loss: 0.0239 - val_acc: 0.9467\n",
      "Epoch 957/960\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0256 - acc: 0.9433 - val_loss: 0.0243 - val_acc: 0.9460\n",
      "Epoch 958/960\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0268 - acc: 0.9414 - val_loss: 0.0241 - val_acc: 0.9477\n",
      "Epoch 959/960\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0259 - acc: 0.9444 - val_loss: 0.0232 - val_acc: 0.9491\n",
      "Epoch 960/960\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0261 - acc: 0.9443 - val_loss: 0.0232 - val_acc: 0.9516\n",
      "start training round 48\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 961/980\n",
      "8564/8564 [==============================] - 4s 467us/step - loss: 0.0239 - acc: 0.6748 - val_loss: 0.0242 - val_acc: 0.7520\n",
      "Epoch 962/980\n",
      "8564/8564 [==============================] - 4s 461us/step - loss: 0.0243 - acc: 0.6671 - val_loss: 0.0238 - val_acc: 0.7524\n",
      "Epoch 963/980\n",
      "8564/8564 [==============================] - 4s 464us/step - loss: 0.0242 - acc: 0.6708 - val_loss: 0.0246 - val_acc: 0.7490\n",
      "Epoch 964/980\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0239 - acc: 0.6749 - val_loss: 0.0235 - val_acc: 0.7546\n",
      "Epoch 965/980\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0240 - acc: 0.6797 - val_loss: 0.0241 - val_acc: 0.7499\n",
      "Epoch 966/980\n",
      "8564/8564 [==============================] - 4s 462us/step - loss: 0.0235 - acc: 0.6813 - val_loss: 0.0239 - val_acc: 0.7519\n",
      "Epoch 967/980\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0236 - acc: 0.7464 - val_loss: 0.0252 - val_acc: 0.5672\n",
      "Epoch 968/980\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0235 - acc: 0.6960 - val_loss: 0.0235 - val_acc: 0.7504\n",
      "Epoch 969/980\n",
      "8564/8564 [==============================] - 4s 446us/step - loss: 0.0238 - acc: 0.6834 - val_loss: 0.0245 - val_acc: 0.7491\n",
      "Epoch 970/980\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0242 - acc: 0.6740 - val_loss: 0.0250 - val_acc: 0.7495\n",
      "Epoch 971/980\n",
      "8564/8564 [==============================] - 4s 470us/step - loss: 0.0240 - acc: 0.6776 - val_loss: 0.0250 - val_acc: 0.7491\n",
      "Epoch 972/980\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0239 - acc: 0.6789 - val_loss: 0.0249 - val_acc: 0.7458\n",
      "Epoch 973/980\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0238 - acc: 0.6764 - val_loss: 0.0247 - val_acc: 0.7456\n",
      "Epoch 974/980\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0236 - acc: 0.6770 - val_loss: 0.0239 - val_acc: 0.7529\n",
      "Epoch 975/980\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0240 - acc: 0.7515 - val_loss: 0.0245 - val_acc: 0.7593\n",
      "Epoch 976/980\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0238 - acc: 0.7406 - val_loss: 0.0250 - val_acc: 0.7434\n",
      "Epoch 977/980\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0239 - acc: 0.6750 - val_loss: 0.0251 - val_acc: 0.7410\n",
      "Epoch 978/980\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0242 - acc: 0.6669 - val_loss: 0.0235 - val_acc: 0.7543\n",
      "Epoch 979/980\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0238 - acc: 0.6673 - val_loss: 0.0246 - val_acc: 0.7431\n",
      "Epoch 980/980\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0237 - acc: 0.6803 - val_loss: 0.0245 - val_acc: 0.5945\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 961/980\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0226 - acc: 0.8904 - val_loss: 0.0232 - val_acc: 0.8895\n",
      "Epoch 962/980\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0226 - acc: 0.8910 - val_loss: 0.0226 - val_acc: 0.8905\n",
      "Epoch 963/980\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0225 - acc: 0.8908 - val_loss: 0.0237 - val_acc: 0.8899\n",
      "Epoch 964/980\n",
      "8564/8564 [==============================] - 4s 461us/step - loss: 0.0227 - acc: 0.8909 - val_loss: 0.0237 - val_acc: 0.8895\n",
      "Epoch 965/980\n",
      "8564/8564 [==============================] - 4s 446us/step - loss: 0.0226 - acc: 0.8909 - val_loss: 0.0225 - val_acc: 0.8902\n",
      "Epoch 966/980\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0226 - acc: 0.8908 - val_loss: 0.0226 - val_acc: 0.8904\n",
      "Epoch 967/980\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0225 - acc: 0.8910 - val_loss: 0.0226 - val_acc: 0.8904\n",
      "Epoch 968/980\n",
      "8564/8564 [==============================] - 4s 447us/step - loss: 0.0226 - acc: 0.8909 - val_loss: 0.0229 - val_acc: 0.8902\n",
      "Epoch 969/980\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0225 - acc: 0.8911 - val_loss: 0.0227 - val_acc: 0.8899\n",
      "Epoch 970/980\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0228 - acc: 0.8898 - val_loss: 0.0219 - val_acc: 0.8888\n",
      "Epoch 971/980\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0226 - acc: 0.8908 - val_loss: 0.0229 - val_acc: 0.8894\n",
      "Epoch 972/980\n",
      "8564/8564 [==============================] - 4s 464us/step - loss: 0.0227 - acc: 0.8899 - val_loss: 0.0219 - val_acc: 0.8871\n",
      "Epoch 973/980\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0225 - acc: 0.8851 - val_loss: 0.0232 - val_acc: 0.8798\n",
      "Epoch 974/980\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0228 - acc: 0.8822 - val_loss: 0.0234 - val_acc: 0.8777\n",
      "Epoch 975/980\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0227 - acc: 0.8830 - val_loss: 0.0231 - val_acc: 0.8786\n",
      "Epoch 976/980\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0226 - acc: 0.8835 - val_loss: 0.0234 - val_acc: 0.8749\n",
      "Epoch 977/980\n",
      "8564/8564 [==============================] - 4s 464us/step - loss: 0.0227 - acc: 0.8818 - val_loss: 0.0229 - val_acc: 0.8852\n",
      "Epoch 978/980\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0229 - acc: 0.8821 - val_loss: 0.0218 - val_acc: 0.8875\n",
      "Epoch 979/980\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0225 - acc: 0.8848 - val_loss: 0.0231 - val_acc: 0.8783\n",
      "Epoch 980/980\n",
      "8564/8564 [==============================] - 4s 448us/step - loss: 0.0230 - acc: 0.8817 - val_loss: 0.0229 - val_acc: 0.8777\n",
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 961/980\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0220 - acc: 0.9865 - val_loss: 0.0285 - val_acc: 0.9832\n",
      "Epoch 962/980\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0236 - acc: 0.9852 - val_loss: 0.0178 - val_acc: 0.9895\n",
      "Epoch 963/980\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0240 - acc: 0.9831 - val_loss: 0.0269 - val_acc: 0.9854\n",
      "Epoch 964/980\n",
      "8564/8564 [==============================] - 4s 464us/step - loss: 0.0253 - acc: 0.9861 - val_loss: 0.0210 - val_acc: 0.9861\n",
      "Epoch 965/980\n",
      "8564/8564 [==============================] - 4s 466us/step - loss: 0.0222 - acc: 0.9854 - val_loss: 0.0208 - val_acc: 0.9883\n",
      "Epoch 966/980\n",
      "8564/8564 [==============================] - 4s 485us/step - loss: 0.0223 - acc: 0.9845 - val_loss: 0.0201 - val_acc: 0.9865\n",
      "Epoch 967/980\n",
      "8564/8564 [==============================] - 4s 498us/step - loss: 0.0241 - acc: 0.9863 - val_loss: 0.0212 - val_acc: 0.9867\n",
      "Epoch 968/980\n",
      "8564/8564 [==============================] - 4s 491us/step - loss: 0.0224 - acc: 0.9857 - val_loss: 0.0245 - val_acc: 0.9874\n",
      "Epoch 969/980\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0229 - acc: 0.9859 - val_loss: 0.0230 - val_acc: 0.9874\n",
      "Epoch 970/980\n",
      "8564/8564 [==============================] - 4s 497us/step - loss: 0.0250 - acc: 0.9860 - val_loss: 0.0196 - val_acc: 0.9889\n",
      "Epoch 971/980\n",
      "8564/8564 [==============================] - 4s 479us/step - loss: 0.0210 - acc: 0.9850 - val_loss: 0.0230 - val_acc: 0.9888\n",
      "Epoch 972/980\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0241 - acc: 0.9837 - val_loss: 0.0225 - val_acc: 0.9879\n",
      "Epoch 973/980\n",
      "8564/8564 [==============================] - 4s 464us/step - loss: 0.0229 - acc: 0.9866 - val_loss: 0.0262 - val_acc: 0.9885\n",
      "Epoch 974/980\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0237 - acc: 0.9838 - val_loss: 0.0222 - val_acc: 0.9863\n",
      "Epoch 975/980\n",
      "8564/8564 [==============================] - 4s 471us/step - loss: 0.0241 - acc: 0.9847 - val_loss: 0.0205 - val_acc: 0.9877\n",
      "Epoch 976/980\n",
      "8564/8564 [==============================] - 4s 466us/step - loss: 0.0258 - acc: 0.9858 - val_loss: 0.0191 - val_acc: 0.9890\n",
      "Epoch 977/980\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0237 - acc: 0.9853 - val_loss: 0.0164 - val_acc: 0.9895\n",
      "Epoch 978/980\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0213 - acc: 0.9855 - val_loss: 0.0245 - val_acc: 0.9880\n",
      "Epoch 979/980\n",
      "8564/8564 [==============================] - 4s 461us/step - loss: 0.0226 - acc: 0.9849 - val_loss: 0.0183 - val_acc: 0.9887\n",
      "Epoch 980/980\n",
      "8564/8564 [==============================] - 4s 466us/step - loss: 0.0248 - acc: 0.9861 - val_loss: 0.0251 - val_acc: 0.9882\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 961/980\n",
      "8564/8564 [==============================] - 4s 493us/step - loss: 0.0264 - acc: 0.9427 - val_loss: 0.0244 - val_acc: 0.9480\n",
      "Epoch 962/980\n",
      "8564/8564 [==============================] - 4s 469us/step - loss: 0.0253 - acc: 0.9452 - val_loss: 0.0265 - val_acc: 0.9501\n",
      "Epoch 963/980\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0272 - acc: 0.9419 - val_loss: 0.0249 - val_acc: 0.9454\n",
      "Epoch 964/980\n",
      "8564/8564 [==============================] - 4s 463us/step - loss: 0.0256 - acc: 0.9443 - val_loss: 0.0225 - val_acc: 0.9465\n",
      "Epoch 965/980\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0268 - acc: 0.9411 - val_loss: 0.0226 - val_acc: 0.9489\n",
      "Epoch 966/980\n",
      "8564/8564 [==============================] - 4s 461us/step - loss: 0.0254 - acc: 0.9457 - val_loss: 0.0230 - val_acc: 0.9573\n",
      "Epoch 967/980\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0253 - acc: 0.9452 - val_loss: 0.0236 - val_acc: 0.9426\n",
      "Epoch 968/980\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0261 - acc: 0.9458 - val_loss: 0.0202 - val_acc: 0.9575\n",
      "Epoch 969/980\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0250 - acc: 0.9474 - val_loss: 0.0230 - val_acc: 0.9511\n",
      "Epoch 970/980\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0261 - acc: 0.9454 - val_loss: 0.0257 - val_acc: 0.9439\n",
      "Epoch 971/980\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0251 - acc: 0.9457 - val_loss: 0.0254 - val_acc: 0.9571\n",
      "Epoch 972/980\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0265 - acc: 0.9435 - val_loss: 0.0259 - val_acc: 0.9473\n",
      "Epoch 973/980\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0265 - acc: 0.9436 - val_loss: 0.0216 - val_acc: 0.9509\n",
      "Epoch 974/980\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0257 - acc: 0.9441 - val_loss: 0.0240 - val_acc: 0.9527\n",
      "Epoch 975/980\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0263 - acc: 0.9421 - val_loss: 0.0276 - val_acc: 0.9539\n",
      "Epoch 976/980\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0268 - acc: 0.9436 - val_loss: 0.0238 - val_acc: 0.9494\n",
      "Epoch 977/980\n",
      "8564/8564 [==============================] - 4s 467us/step - loss: 0.0248 - acc: 0.9475 - val_loss: 0.0211 - val_acc: 0.9519\n",
      "Epoch 978/980\n",
      "8564/8564 [==============================] - 4s 461us/step - loss: 0.0252 - acc: 0.9452 - val_loss: 0.0215 - val_acc: 0.9521\n",
      "Epoch 979/980\n",
      "8564/8564 [==============================] - 4s 461us/step - loss: 0.0261 - acc: 0.9422 - val_loss: 0.0226 - val_acc: 0.9451\n",
      "Epoch 980/980\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0254 - acc: 0.9448 - val_loss: 0.0285 - val_acc: 0.9403\n",
      "start training round 49\n",
      "training gyro model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 981/1000\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0236 - acc: 0.6772 - val_loss: 0.0257 - val_acc: 0.5592\n",
      "Epoch 982/1000\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0240 - acc: 0.6687 - val_loss: 0.0256 - val_acc: 0.6377\n",
      "Epoch 983/1000\n",
      "8564/8564 [==============================] - 4s 461us/step - loss: 0.0239 - acc: 0.7532 - val_loss: 0.0249 - val_acc: 0.7585\n",
      "Epoch 984/1000\n",
      "8564/8564 [==============================] - 4s 464us/step - loss: 0.0240 - acc: 0.7624 - val_loss: 0.0257 - val_acc: 0.7046\n",
      "Epoch 985/1000\n",
      "8564/8564 [==============================] - 4s 461us/step - loss: 0.0243 - acc: 0.7612 - val_loss: 0.0256 - val_acc: 0.7597\n",
      "Epoch 986/1000\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0240 - acc: 0.7595 - val_loss: 0.0246 - val_acc: 0.7490\n",
      "Epoch 987/1000\n",
      "8564/8564 [==============================] - 4s 464us/step - loss: 0.0238 - acc: 0.7613 - val_loss: 0.0241 - val_acc: 0.7601\n",
      "Epoch 988/1000\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0239 - acc: 0.7267 - val_loss: 0.0252 - val_acc: 0.5708\n",
      "Epoch 989/1000\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0238 - acc: 0.7408 - val_loss: 0.0247 - val_acc: 0.7550\n",
      "Epoch 990/1000\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0237 - acc: 0.7628 - val_loss: 0.0249 - val_acc: 0.7607\n",
      "Epoch 991/1000\n",
      "8564/8564 [==============================] - 4s 461us/step - loss: 0.0236 - acc: 0.7612 - val_loss: 0.0242 - val_acc: 0.7548\n",
      "Epoch 992/1000\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0242 - acc: 0.7547 - val_loss: 0.0239 - val_acc: 0.7571\n",
      "Epoch 993/1000\n",
      "8564/8564 [==============================] - 4s 464us/step - loss: 0.0238 - acc: 0.7055 - val_loss: 0.0242 - val_acc: 0.5820\n",
      "Epoch 994/1000\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0239 - acc: 0.7189 - val_loss: 0.0252 - val_acc: 0.7497\n",
      "Epoch 995/1000\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0237 - acc: 0.7567 - val_loss: 0.0245 - val_acc: 0.6018\n",
      "Epoch 996/1000\n",
      "8564/8564 [==============================] - 4s 464us/step - loss: 0.0239 - acc: 0.6805 - val_loss: 0.0244 - val_acc: 0.5910\n",
      "Epoch 997/1000\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0240 - acc: 0.6699 - val_loss: 0.0242 - val_acc: 0.5924\n",
      "Epoch 998/1000\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0237 - acc: 0.6811 - val_loss: 0.0243 - val_acc: 0.5865\n",
      "Epoch 999/1000\n",
      "8564/8564 [==============================] - 4s 479us/step - loss: 0.0238 - acc: 0.7130 - val_loss: 0.0257 - val_acc: 0.7570\n",
      "Epoch 1000/1000\n",
      "8564/8564 [==============================] - 4s 463us/step - loss: 0.0238 - acc: 0.7577 - val_loss: 0.0239 - val_acc: 0.7602\n",
      "training linearAcc model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 981/1000\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0226 - acc: 0.8828 - val_loss: 0.0235 - val_acc: 0.8703\n",
      "Epoch 982/1000\n",
      "8564/8564 [==============================] - 4s 455us/step - loss: 0.0229 - acc: 0.8809 - val_loss: 0.0223 - val_acc: 0.8881\n",
      "Epoch 983/1000\n",
      "8564/8564 [==============================] - 4s 461us/step - loss: 0.0226 - acc: 0.8834 - val_loss: 0.0233 - val_acc: 0.8785\n",
      "Epoch 984/1000\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0225 - acc: 0.8840 - val_loss: 0.0231 - val_acc: 0.8821\n",
      "Epoch 985/1000\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0225 - acc: 0.8832 - val_loss: 0.0228 - val_acc: 0.8815\n",
      "Epoch 986/1000\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0226 - acc: 0.8844 - val_loss: 0.0233 - val_acc: 0.8805\n",
      "Epoch 987/1000\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0227 - acc: 0.8826 - val_loss: 0.0227 - val_acc: 0.8848\n",
      "Epoch 988/1000\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0224 - acc: 0.8848 - val_loss: 0.0228 - val_acc: 0.8825\n",
      "Epoch 989/1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0227 - acc: 0.8830 - val_loss: 0.0227 - val_acc: 0.8816\n",
      "Epoch 990/1000\n",
      "8564/8564 [==============================] - 4s 472us/step - loss: 0.0223 - acc: 0.8858 - val_loss: 0.0231 - val_acc: 0.8867\n",
      "Epoch 991/1000\n",
      "8564/8564 [==============================] - 4s 474us/step - loss: 0.0227 - acc: 0.8826 - val_loss: 0.0231 - val_acc: 0.8858\n",
      "Epoch 992/1000\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0226 - acc: 0.8844 - val_loss: 0.0230 - val_acc: 0.8858\n",
      "Epoch 993/1000\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0227 - acc: 0.8836 - val_loss: 0.0236 - val_acc: 0.8837\n",
      "Epoch 994/1000\n",
      "8564/8564 [==============================] - 4s 453us/step - loss: 0.0227 - acc: 0.8829 - val_loss: 0.0233 - val_acc: 0.8846\n",
      "Epoch 995/1000\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0230 - acc: 0.8815 - val_loss: 0.0224 - val_acc: 0.8838\n",
      "Epoch 996/1000\n",
      "8564/8564 [==============================] - 4s 451us/step - loss: 0.0226 - acc: 0.8834 - val_loss: 0.0232 - val_acc: 0.8776\n",
      "Epoch 997/1000\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0227 - acc: 0.8824 - val_loss: 0.0230 - val_acc: 0.8810\n",
      "Epoch 998/1000\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0227 - acc: 0.8826 - val_loss: 0.0228 - val_acc: 0.8861\n",
      "Epoch 999/1000\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0226 - acc: 0.8830 - val_loss: 0.0231 - val_acc: 0.8806\n",
      "Epoch 1000/1000\n",
      "8564/8564 [==============================] - 4s 468us/step - loss: 0.0226 - acc: 0.8836 - val_loss: 0.0223 - val_acc: 0.8854\n",
      "training gravity model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 981/1000\n",
      "8564/8564 [==============================] - 4s 472us/step - loss: 0.0245 - acc: 0.9848 - val_loss: 0.0277 - val_acc: 0.9910\n",
      "Epoch 982/1000\n",
      "8564/8564 [==============================] - 4s 463us/step - loss: 0.0239 - acc: 0.9857 - val_loss: 0.0213 - val_acc: 0.9882\n",
      "Epoch 983/1000\n",
      "8564/8564 [==============================] - 4s 491us/step - loss: 0.0231 - acc: 0.9873 - val_loss: 0.0219 - val_acc: 0.9899\n",
      "Epoch 984/1000\n",
      "8564/8564 [==============================] - 4s 490us/step - loss: 0.0244 - acc: 0.9836 - val_loss: 0.0178 - val_acc: 0.9873\n",
      "Epoch 985/1000\n",
      "8564/8564 [==============================] - 4s 464us/step - loss: 0.0224 - acc: 0.9864 - val_loss: 0.0241 - val_acc: 0.9887\n",
      "Epoch 986/1000\n",
      "8564/8564 [==============================] - 4s 452us/step - loss: 0.0219 - acc: 0.9845 - val_loss: 0.0264 - val_acc: 0.9852\n",
      "Epoch 987/1000\n",
      "8564/8564 [==============================] - 4s 461us/step - loss: 0.0242 - acc: 0.9822 - val_loss: 0.0184 - val_acc: 0.9906\n",
      "Epoch 988/1000\n",
      "8564/8564 [==============================] - 4s 446us/step - loss: 0.0246 - acc: 0.9839 - val_loss: 0.0177 - val_acc: 0.9895\n",
      "Epoch 989/1000\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0222 - acc: 0.9854 - val_loss: 0.0171 - val_acc: 0.9898\n",
      "Epoch 990/1000\n",
      "8564/8564 [==============================] - 4s 449us/step - loss: 0.0222 - acc: 0.9864 - val_loss: 0.0207 - val_acc: 0.9870\n",
      "Epoch 991/1000\n",
      "8564/8564 [==============================] - 4s 450us/step - loss: 0.0224 - acc: 0.9851 - val_loss: 0.0192 - val_acc: 0.9879\n",
      "Epoch 992/1000\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0232 - acc: 0.9861 - val_loss: 0.0258 - val_acc: 0.9856\n",
      "Epoch 993/1000\n",
      "8564/8564 [==============================] - 4s 449us/step - loss: 0.0239 - acc: 0.9847 - val_loss: 0.0236 - val_acc: 0.9880\n",
      "Epoch 994/1000\n",
      "8564/8564 [==============================] - 4s 447us/step - loss: 0.0227 - acc: 0.9866 - val_loss: 0.0208 - val_acc: 0.9892\n",
      "Epoch 995/1000\n",
      "8564/8564 [==============================] - 4s 454us/step - loss: 0.0225 - acc: 0.9856 - val_loss: 0.0253 - val_acc: 0.9890\n",
      "Epoch 996/1000\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0218 - acc: 0.9868 - val_loss: 0.0252 - val_acc: 0.9850\n",
      "Epoch 997/1000\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0225 - acc: 0.9862 - val_loss: 0.0219 - val_acc: 0.9899\n",
      "Epoch 998/1000\n",
      "8564/8564 [==============================] - 4s 465us/step - loss: 0.0238 - acc: 0.9835 - val_loss: 0.0242 - val_acc: 0.9900\n",
      "Epoch 999/1000\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0257 - acc: 0.9865 - val_loss: 0.0276 - val_acc: 0.9900\n",
      "Epoch 1000/1000\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0228 - acc: 0.9867 - val_loss: 0.0259 - val_acc: 0.9894\n",
      "training gameVec model\n",
      "Train on 8564 samples, validate on 952 samples\n",
      "Epoch 981/1000\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0263 - acc: 0.9456 - val_loss: 0.0298 - val_acc: 0.9460\n",
      "Epoch 982/1000\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0267 - acc: 0.9417 - val_loss: 0.0196 - val_acc: 0.9592\n",
      "Epoch 983/1000\n",
      "8564/8564 [==============================] - 4s 457us/step - loss: 0.0257 - acc: 0.9436 - val_loss: 0.0254 - val_acc: 0.9450\n",
      "Epoch 984/1000\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0250 - acc: 0.9453 - val_loss: 0.0262 - val_acc: 0.9388\n",
      "Epoch 985/1000\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0261 - acc: 0.9446 - val_loss: 0.0235 - val_acc: 0.9457\n",
      "Epoch 986/1000\n",
      "8564/8564 [==============================] - 4s 461us/step - loss: 0.0268 - acc: 0.9425 - val_loss: 0.0240 - val_acc: 0.9567\n",
      "Epoch 987/1000\n",
      "8564/8564 [==============================] - 4s 464us/step - loss: 0.0266 - acc: 0.9423 - val_loss: 0.0249 - val_acc: 0.9436\n",
      "Epoch 988/1000\n",
      "8564/8564 [==============================] - 4s 459us/step - loss: 0.0262 - acc: 0.9436 - val_loss: 0.0256 - val_acc: 0.9480\n",
      "Epoch 989/1000\n",
      "8564/8564 [==============================] - 4s 458us/step - loss: 0.0264 - acc: 0.9462 - val_loss: 0.0222 - val_acc: 0.9465\n",
      "Epoch 990/1000\n",
      "8564/8564 [==============================] - 4s 460us/step - loss: 0.0253 - acc: 0.9450 - val_loss: 0.0209 - val_acc: 0.9595\n",
      "Epoch 991/1000\n",
      "8564/8564 [==============================] - 4s 466us/step - loss: 0.0255 - acc: 0.9458 - val_loss: 0.0231 - val_acc: 0.9437\n",
      "Epoch 992/1000\n",
      "8564/8564 [==============================] - 4s 461us/step - loss: 0.0264 - acc: 0.9434 - val_loss: 0.0203 - val_acc: 0.9553\n",
      "Epoch 993/1000\n",
      "8564/8564 [==============================] - 4s 464us/step - loss: 0.0248 - acc: 0.9458 - val_loss: 0.0244 - val_acc: 0.9495\n",
      "Epoch 994/1000\n",
      "8564/8564 [==============================] - 4s 463us/step - loss: 0.0258 - acc: 0.9461 - val_loss: 0.0270 - val_acc: 0.9387\n",
      "Epoch 995/1000\n",
      "8564/8564 [==============================] - 4s 461us/step - loss: 0.0255 - acc: 0.9449 - val_loss: 0.0241 - val_acc: 0.9506\n",
      "Epoch 996/1000\n",
      "8564/8564 [==============================] - 4s 463us/step - loss: 0.0258 - acc: 0.9449 - val_loss: 0.0244 - val_acc: 0.9518\n",
      "Epoch 997/1000\n",
      "8564/8564 [==============================] - 4s 467us/step - loss: 0.0252 - acc: 0.9467 - val_loss: 0.0283 - val_acc: 0.9535\n",
      "Epoch 998/1000\n",
      "8564/8564 [==============================] - 4s 456us/step - loss: 0.0253 - acc: 0.9454 - val_loss: 0.0217 - val_acc: 0.9565\n",
      "Epoch 999/1000\n",
      "8564/8564 [==============================] - 4s 468us/step - loss: 0.0256 - acc: 0.9456 - val_loss: 0.0238 - val_acc: 0.9450\n",
      "Epoch 1000/1000\n",
      "8564/8564 [==============================] - 4s 464us/step - loss: 0.0254 - acc: 0.9436 - val_loss: 0.0269 - val_acc: 0.9504\n"
     ]
    }
   ],
   "source": [
    "train_round =50\n",
    "epochs_per_round=20\n",
    "for i in range(train_round):\n",
    "    print('start training round '+str(i))\n",
    "    print('training gyro model')\n",
    "    gyroModel.fit(gyro_train,gyro_train,\n",
    "                  batch_size=256,\n",
    "                  epochs=(i+1)*epochs_per_round,\n",
    "                  validation_split=0.1,verbose=1,initial_epoch=i*epochs_per_round)\n",
    "    print('training linearAcc model')\n",
    "    linearAccModel.fit(linearAcc_train,linearAcc_train,\n",
    "                       batch_size=256,\n",
    "                       epochs=(i+1)*epochs_per_round,validation_split=0.1,verbose=1,initial_epoch=i*epochs_per_round)\n",
    "    print('training gravity model')\n",
    "    gravityModel.fit(gravity_train,gravity_train,\n",
    "                     batch_size=256,\n",
    "                     epochs=(i+1)*epochs_per_round,validation_split=0.1,verbose=1,initial_epoch=i*epochs_per_round)\n",
    "    print('training gameVec model')\n",
    "    gameVecModel.fit(gameVec_train,gameVec_train,\n",
    "                     batch_size=256,\n",
    "                     epochs=(i+1)*epochs_per_round,validation_split=0.1,verbose=1,initial_epoch=i*epochs_per_round)\n",
    "    gyroModel.save('gyroNorModel_64.h5')\n",
    "    linearAccModel.save('linearAccNorModel_64.h5')\n",
    "    gravityModel.save('gravityNorModel_64.h5')\n",
    "    gameVecModel.save('gameVecNorModel_64.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1680/1680 [==============================] - 2s 1ms/step\n",
      "gyroModel Test:[0.05136282514958154, 0.5086542038690476]\n",
      "1680/1680 [==============================] - 2s 1ms/step\n",
      "linearAccModel Test: [0.03907829916902951, 0.788285900297619]\n",
      "1680/1680 [==============================] - 2s 1ms/step\n",
      "gravityModel Test: [0.04305966567425501, 0.9718377976190476]\n",
      "1680/1680 [==============================] - 1s 491us/step\n",
      "gameVecModel Test: [0.04490568240483602, 0.8896344866071428]\n"
     ]
    }
   ],
   "source": [
    "print(\"gyroModel Test:\"+str(gyroModel.evaluate(gyro_test,gyro_test)))\n",
    "print(\"linearAccModel Test: \"+str(linearAccModel.evaluate(linearAcc_test,linearAcc_test)))\n",
    "print(\"gravityModel Test: \"+str(gravityModel.evaluate(gravity_test,gravity_test)))\n",
    "print(\"gameVecModel Test: \"+str(gameVecModel.evaluate(gameVec_test,gameVec_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x504 with 3 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1440x504 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample = 32\n",
    "gyroy = gyroModel.predict(gyro_train[sample].reshape(1,128,3))\n",
    "fig=plt.figure(figsize=(20,7))\n",
    "for channel in range(3):\n",
    "    plt.subplot(1,3,channel+1)\n",
    "    plot(gyro_train[sample,:,channel],'b')\n",
    "    plot(gyroy[0,:,channel],'r')\n",
    "linearAccy = linearAccModel.predict(linearAcc_train[sample].reshape(1,128,3))\n",
    "fig=plt.figure(figsize=(20,7))\n",
    "for channel in range(3):\n",
    "    plt.subplot(1,3,channel+1)\n",
    "    plot(linearAcc_train[sample,:,channel],'b')\n",
    "    plot(linearAccy[0,:,channel],'r')\n",
    "gravityy = gravityModel.predict(gravity_train[sample].reshape(1,128,3))\n",
    "fig=plt.figure(figsize=(20,7))\n",
    "for channel in range(3):\n",
    "    plt.subplot(1,3,channel+1)\n",
    "    plot(gravity_train[sample,:,channel],'b')\n",
    "    plot(gravityy[0,:,channel],'r')\n",
    "gameVecy = gameVecModel.predict(gameVec_train[sample].reshape(1,128,4))\n",
    "fig=plt.figure(figsize=(20,7))\n",
    "for channel in range(4):\n",
    "    plt.subplot(1,4,channel+1)\n",
    "    plot(gameVec_train[sample,:,channel],'b')\n",
    "    plot(gameVecy[0,:,channel],'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
