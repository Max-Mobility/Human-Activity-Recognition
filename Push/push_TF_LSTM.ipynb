{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "#import tensorflow.nn.rnn_cell as rnn\n",
    "import numpy as np\n",
    "#import tensorflow.keras as keras\n",
    "#from tensorflow.keras.initializers import Constant\n",
    "import tensorflow.keras.backend as K\n",
    "\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"-1\"\n",
    "re_train=True\n",
    "time_steps=1\n",
    "n_input =6\n",
    "num_units=64\n",
    "n_classes=1\n",
    "lr =0.000001\n",
    "logFile = './retrain/traing.txt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2424.0\n",
      "7.439356435643564\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((18033, 6), (18033,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_x=np.loadtxt('./PSDSData (17)_raw.csv',delimiter=\",\")\n",
    "data_y=np.loadtxt('./push(17).csv',delimiter=\",\")\n",
    "print(data_y.sum())\n",
    "print(len(data_y)/data_y.sum())\n",
    "preY = np.loadtxt('./push(17).csv',delimiter=\",\")\n",
    "data_x=data_x[:,3:9];\n",
    "data_x.shape,data_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18033, 6)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((18033, 6), (18033, 1))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "datay=data_y\n",
    "data_y=data_y.reshape(len(data_y),1)\n",
    "data_x=data_x[:,(6-n_input):6].reshape((len(data_x),n_input))\n",
    "data_state = np.zeros([1,num_units*2],dtype='float')\n",
    "data_x.shape,data_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y [11111] to [10000]\n",
    "def detect(predicts,raw,windows=16,threshold=0.8,raw_threshold=-10):\n",
    "    taps=np.zeros_like(predicts)\n",
    "    windows_count=0\n",
    "    buff_size=25\n",
    "    \n",
    "    for i in np.arange(buff_size,len(taps)):\n",
    "        raw_buff=raw[i-buff_size:i+1,2]# only z acc\n",
    "        #print(raw_buff.min())\n",
    "        diff=abs(raw_buff.max()-raw_buff.min())\n",
    "        buff_sum = 0\n",
    "        for x in raw_buff:\n",
    "            if x <0:\n",
    "                buff_sum+= x*-1\n",
    "        \n",
    "        if predicts[i]>threshold and windows_count==0  and buff_sum>raw_threshold:# and diff>6:# and predicts[i-1]>threshold:\n",
    "            #print(diff)\n",
    "            taps[i]=1\n",
    "            windows_count=windows\n",
    "            #print(buff_sum)\n",
    "        else:\n",
    "            windows_count=max(windows_count-1,0)\n",
    "    return taps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "taps_label = detect(data_y,raw=data_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18033, 1)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "taps_label.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_weight = datay*len(datay)/datay.sum()+1\n",
    "loss_weight = loss_weight.reshape([len(loss_weight),1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x= tf.placeholder('float',[None,n_input],name='input_tensor')\n",
    "y = tf.placeholder('float',[None,n_classes],name='labels')\n",
    "#inputs = tf.unstack(x,time_steps,1,name='input_tensor')\n",
    "#state_h = tf.placeholder('float',[None,num_units],name='input_h')\n",
    "#state_c = tf.placeholder('float',[None,num_units],name='input_c')\n",
    "state_in   = tf.placeholder('float',[None,num_units*2],name='input_state')\n",
    "loss_weight_in = tf.placeholder('float',[None,1],name='loss_weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "wi=tf.Variable(name='wi',initial_value=tf.random_normal([n_input, num_units], stddev=0.02))\n",
    "wf=tf.Variable(name='wf',initial_value=tf.random_normal([n_input, num_units], stddev=0.02))\n",
    "wc=tf.Variable(name='wc',initial_value=tf.random_normal([n_input, num_units], stddev=0.02))\n",
    "wo=tf.Variable(name='wo',initial_value=tf.random_normal([n_input, num_units], stddev=0.02))\n",
    "ui=tf.Variable(name='ui',initial_value=tf.random_normal([num_units, num_units], stddev=0.02))\n",
    "uf=tf.Variable(name='uf',initial_value=tf.random_normal([num_units, num_units], stddev=0.02))\n",
    "uc=tf.Variable(name='uc',initial_value=tf.random_normal([num_units, num_units], stddev=0.02))\n",
    "uo=tf.Variable(name='uo',initial_value=tf.random_normal([num_units, num_units], stddev=0.02))\n",
    "\n",
    "bi=tf.Variable(name='bi',initial_value=tf.random_normal([num_units], stddev=0.02))\n",
    "bf=tf.Variable(name='bf',initial_value=tf.random_normal([num_units], stddev=0.02))\n",
    "bc=tf.Variable(name='bc',initial_value=tf.random_normal([num_units], stddev=0.02))\n",
    "bo=tf.Variable(name='bo',initial_value=tf.random_normal([num_units], stddev=0.02))\n",
    "\n",
    "out_weights1=tf.Variable(name=\"weights1\",initial_value=tf.random_normal([num_units, 16], stddev=0.02))\n",
    "out_weights2=tf.Variable(name=\"weights2\",initial_value=tf.random_normal([16, 1], stddev=0.02))\n",
    "out_bias1=tf.Variable(name=\"bias1\",initial_value=tf.random_normal([16], stddev=0.02))\n",
    "out_bias2=tf.Variable(name=\"bias2\",initial_value=tf.random_normal([1], stddev=0.02))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_h,state_c = tf.split(state_in,num_or_size_splits=2,axis=1)\n",
    "x_i = K.dot(x,wi)\n",
    "x_f = K.dot(x,wf)\n",
    "x_c = K.dot(x,wc)\n",
    "x_o = K.dot(x,wo)\n",
    "\n",
    "i = K.hard_sigmoid(tf.add(tf.add(x_i,bi),K.dot(state_h,ui)))\n",
    "f = K.hard_sigmoid(tf.add(tf.add(x_f,bf),K.dot(state_h,uf)))\n",
    "c = tf.add(tf.multiply(f,state_c),tf.multiply(i,K.tanh(tf.add(tf.add(x_c,bc),K.dot(state_h,uc)))),name='output_c')\n",
    "o = K.hard_sigmoid(tf.add(tf.add(x_o,bo),K.dot(state_h,uo)))\n",
    "h = tf.multiply(o,tf.tanh(c),name='output_h')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0923 09:26:07.066440 139797338949440 deprecation.py:323] From /home/xing/anaconda3/envs/tf_cpu/lib/python3.6/site-packages/tensorflow/python/ops/losses/losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "dense1  = tf.nn.relu(tf.add(tf.matmul(h,out_weights1), out_bias1))\n",
    "prediction=tf.nn.relu(tf.add(tf.matmul(dense1,out_weights2), out_bias2,name='output_forRelu'), name=\"output\")\n",
    "#prediction=tf.nn.relu(tf.add(tf.matmul(h,out_weights), out_bias,name='output_forRelu'), name=\"output\")\n",
    "state_out = tf.concat([h,c],axis=1,name='output_state')\n",
    "prediction_weight=tf.matmul(prediction,loss_weight_in)\n",
    "y_weight=tf.matmul(y,loss_weight_in)\n",
    "\n",
    "loss=tf.losses.mean_squared_error(predictions=prediction_weight,labels=y_weight)\n",
    "opt=tf.train.AdamOptimizer(learning_rate=lr).minimize(loss,name=\"train_op\")\n",
    "init = tf.global_variables_initializer()\n",
    "saver=tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'train_op'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "opt.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify(taps,label_taps):\n",
    "    False_positive= np.zeros_like(taps)\n",
    "    False_negtive = np.zeros_like(taps)\n",
    "    True_positive = np.zeros_like(taps)\n",
    "    window=15\n",
    "    #taps=np.append(np.zeros([1]),taps)\n",
    "    #taps=np.append(taps,np.zeros([1]))\n",
    "    #label_taps=np.append(np.zeros([1]),label_taps)\n",
    "    #label_taps=np.append(label_taps,np.zeros([1]))\n",
    "    for i in range(len(taps)):\n",
    "        if label_taps[i] ==1:\n",
    "            if taps[i-window:i+window].sum()==0:\n",
    "                False_negtive[i]=1\n",
    "            else:\n",
    "                True_positive[i]=1\n",
    "\n",
    "        if taps[i] ==1:\n",
    "            if label_taps[i-window:i+window].sum()==0:\n",
    "                False_positive[i]=1\n",
    "                #raw_buff=raw[i-4:i+1,2]# only z acc\n",
    "                #diff=abs(raw_buff.max()-raw_buff.min())\n",
    "                #print(i,raw_buff,diff)\n",
    "            \n",
    "    return [False_positive,  False_negtive ,    True_positive ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18033/18033 [00:23<00:00, 769.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0923 09:27:20.436162 139797338949440 deprecation.py:323] From <ipython-input-17-f42e7c7bfc87>:50: convert_variables_to_constants (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.convert_variables_to_constants`\n",
      "W0923 09:27:20.436730 139797338949440 deprecation.py:323] From /home/xing/anaconda3/envs/tf_cpu/lib/python3.6/site-packages/tensorflow/python/framework/graph_util_impl.py:270: extract_sub_graph (from tensorflow.python.framework.graph_util_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.compat.v1.graph_util.extract_sub_graph`\n",
      "  0%|          | 14/18033 [00:00<02:09, 138.90it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse:0.12356915429324036\n",
      "mse_weight:8.664564849656294\n",
      "total taps:0.0\n",
      "FP,FN,TP:0.0, 202.0, 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18033/18033 [00:22<00:00, 809.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 28/18033 [00:00<01:04, 279.30it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse:0.11753085250625583\n",
      "mse_weight:7.889939740894786\n",
      "total taps:0.0\n",
      "FP,FN,TP:0.0, 202.0, 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18033/18033 [00:22<00:00, 801.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 43/18033 [00:00<00:41, 428.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse:0.12304629259294927\n",
      "mse_weight:5.758114963826899\n",
      "total taps:0.0\n",
      "FP,FN,TP:0.0, 202.0, 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18033/18033 [00:21<00:00, 827.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 46/18033 [00:00<00:39, 454.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse:0.21413737523791002\n",
      "mse_weight:2.9742176140971694\n",
      "total taps:0.0\n",
      "FP,FN,TP:0.0, 202.0, 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18033/18033 [00:22<00:00, 804.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 32/18033 [00:00<00:56, 318.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse:0.4256850491233473\n",
      "mse_weight:1.23848247773275\n",
      "total taps:202.0\n",
      "FP,FN,TP:125.0, 157.0, 45.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18033/18033 [00:21<00:00, 837.84it/s]\n",
      "W0923 09:29:12.746064 139797338949440 deprecation.py:323] From /home/xing/anaconda3/envs/tf_cpu/lib/python3.6/site-packages/tensorflow/python/training/saver.py:960: remove_checkpoint (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to delete files with this prefix.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 24/18033 [00:00<01:15, 237.27it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse:0.6233307815597298\n",
      "mse_weight:0.764682007198984\n",
      "total taps:941.0\n",
      "FP,FN,TP:580.0, 1.0, 201.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18033/18033 [00:22<00:00, 804.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 26/18033 [00:00<01:10, 255.71it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse:0.6388557437383144\n",
      "mse_weight:0.7323432643839525\n",
      "total taps:961.0\n",
      "FP,FN,TP:612.0, 0.0, 202.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18033/18033 [00:21<00:00, 836.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 31/18033 [00:00<00:58, 309.60it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mse:0.6143877977635938\n",
      "mse_weight:0.7231424295612817\n",
      "total taps:915.0\n",
      "FP,FN,TP:551.0, 2.0, 200.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18033/18033 [00:21<00:00, 857.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "mse:0.5910468294628695\n",
      "mse_weight:0.7217132931284309\n",
      "total taps:844.0\n",
      "FP,FN,TP:496.0, 4.0, 198.0\n"
     ]
    }
   ],
   "source": [
    "iters=1\n",
    "predictY=np.zeros([len(data_x)])\n",
    "while iters<10:\n",
    "    for i in tqdm(range(len(data_x))):\n",
    "        feed={x:data_x[i:i+1],\n",
    "              state_in:data_state,\n",
    "                y:data_y[i:i+1],\n",
    "              loss_weight_in:loss_weight[i:i+1]\n",
    "               }\n",
    "        if re_train:\n",
    "            sess.run(opt,feed_dict=feed)\n",
    "\n",
    "        data_state,_predictY = sess.run([state_out,prediction],feed_dict=feed)\n",
    "        #print(data_h.shape)\n",
    "        #data_c = sess.run(c,feed_dict=feed)\n",
    "        predictY[i]=_predictY[0]\n",
    "        #print(i)\n",
    "    #print(sess.run('bi:0'))\n",
    "    saver.save(sess,'./retrain/test_model'+str(iters-1))\n",
    "    if iters%1==0:\n",
    "        print(iters)\n",
    "        mse = (np.square(predictY-data_y.reshape([len(data_x)]))).mean(axis=0)\n",
    "        loss_weight = loss_weight.reshape([len(loss_weight)])\n",
    "        mse_weight =np.square(np. multiply((predictY-data_y.reshape([len(data_x)])),loss_weight)).mean(axis=0)\n",
    "        loss_weight = loss_weight.reshape([len(loss_weight),1])\n",
    "        taps_predict = detect(predictY,raw=data_x)\n",
    "        FP,FN,TP = verify(taps_predict,taps_label)\n",
    "        file = open(logFile,'a+')\n",
    "        file.write('\\niters:'+str(iters))\n",
    "        file.write('\\nmse:'+str(mse))\n",
    "        file.write('\\nmse_weight:'+str(mse_weight))\n",
    "        file.write('\\ntotal taps:'+str(taps_predict.sum()))\n",
    "        file.write('\\nFP,FN,TP:'+str(FP.sum())+', '+str(FN.sum())+', '+str(TP.sum()))\n",
    "        file.close()\n",
    "        print('mse:'+str(mse))\n",
    "        print('mse_weight:'+str(mse_weight))\n",
    "        print('total taps:'+str(taps_predict.sum()))\n",
    "        print('FP,FN,TP:'+str(FP.sum())+', '+str(FN.sum())+', '+str(TP.sum()))\n",
    "        if iters%10==0:\n",
    "\n",
    "            loss_weight = predictY*len(predictY)/predictY.sum()+datay*len(datay)/datay.sum()+1\n",
    "            loss_weight = loss_weight.reshape([len(loss_weight),1])\n",
    "            print('noneTapWeight'+str(len(predictY)/predictY.sum()))\n",
    "\n",
    "    output_graph='./retrain/'+str(num_units)+'_i'+str(iters)+'_mse'+'{0:.2f}'.format(mse*100)+'_'+'{0:.2f}'.format(mse_weight)+'.pb'\n",
    "    #output_graph='../models/100hz/realLSTM_data_U'+str(num_units)+'_i'+str(iters)+'_mse'+'{0:.2f}'.format(mse*100)+'_'+'{0:.2f}'.format(mse_weight)+'.pb'\n",
    "    output_graph_def = tf.compat.v1.graph_util.convert_variables_to_constants(\n",
    "            sess, # The session is used to retrieve the weights\n",
    "            tf.get_default_graph().as_graph_def(), # The graph_def is used to retrieve the nodes \n",
    "            ['output','output_state'] # The output node names are used to select the usefull nodes\n",
    "        ) \n",
    "    with tf.gfile.GFile(output_graph, \"wb\") as f:\n",
    "            f.write(output_graph_def.SerializeToString())\n",
    "    iters=iters+1\n",
    "    out_preY = output_graph.split('.pb')[0]+'_preY.csv'\n",
    "    np.savetxt(out_preY,predictY)\n",
    "    #saver.save(sess,'./retrain/test_model'+str(iters-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 128)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_state.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Saver' object has no attribute 'max_to_keep'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-b9c2ff4b686b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0msaver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_to_keep\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'Saver' object has no attribute 'max_to_keep'"
     ]
    }
   ],
   "source": [
    "save1=saver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0913 14:08:56.287348 140538466105152 deprecation.py:323] From /home/xing/anaconda3/envs/tf_cpu/lib/python3.6/site-packages/tensorflow/python/training/saver.py:1276: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "  0%|          | 0/18033 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-5.7063447e-03 -1.8465047e-03  5.8838739e-03 -6.1452705e-03\n",
      "  3.9387941e-02  3.4139113e-04 -1.5409730e-02  3.5584066e-03\n",
      "  3.5744114e-03 -9.5328046e-03  9.9528451e-03  2.3483027e-02\n",
      " -3.9534951e-03 -2.4520522e-02  7.1736299e-03 -3.8554464e-02\n",
      "  1.8904410e-02 -9.8872939e-03  1.7602913e-02 -1.1894713e-03\n",
      "  2.3886776e-02  9.2592156e-03  8.2292957e-03  7.3925760e-03\n",
      "  4.1925203e-02 -7.2104442e-03  3.6520809e-02 -2.4253873e-02\n",
      "  4.5465745e-02 -9.7077349e-03  1.5023111e-03  7.5401845e-03\n",
      " -2.1253533e-03  2.1378510e-02 -2.4253979e-02  6.6657639e-03\n",
      " -4.4258814e-03 -1.8435629e-02  7.0998478e-03 -2.6672983e-03\n",
      "  1.4627862e-02  6.3955672e-03  9.3487650e-03  6.3213904e-04\n",
      "  3.8069315e-02  3.9136343e-02  1.4842294e-02  1.4239924e-03\n",
      "  9.3876934e-03 -2.3107868e-02 -1.3689954e-05 -3.5314772e-03\n",
      "  5.8002626e-03 -2.9823719e-02 -9.2941336e-03  2.3730097e-02\n",
      " -6.7457925e-03 -2.5603618e-03  4.0934734e-02  1.3285197e-04\n",
      "  2.1441130e-02  4.0641176e-03  1.5266694e-03  3.6570849e-03]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 18033/18033 [00:24<00:00, 750.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 5.4655288e-04  4.1932571e-03  1.2069216e-02  8.3020983e-05\n",
      "  4.5332953e-02  6.5130554e-03 -9.0284301e-03  9.9591156e-03\n",
      "  9.4893007e-03 -3.2852038e-03  1.6295318e-02  2.9366689e-02\n",
      "  2.3052709e-03 -1.8435707e-02  1.3196019e-02 -3.5097402e-02\n",
      "  2.5160553e-02 -3.9449562e-03  2.4069609e-02  5.1611145e-03\n",
      "  3.0340910e-02  1.5648670e-02  1.4800979e-02  1.3480432e-02\n",
      "  4.8062161e-02 -1.0244285e-03  4.2536229e-02 -1.7964175e-02\n",
      "  5.1538408e-02 -3.3480572e-03  8.1804972e-03  1.3606368e-02\n",
      "  3.8716404e-03  2.7285727e-02 -1.8231349e-02  1.3117496e-02\n",
      "  1.8132577e-03 -1.2090015e-02  1.2996495e-02  3.4741813e-03\n",
      "  2.0607755e-02  1.2896574e-02  1.5563586e-02  6.6715190e-03\n",
      "  4.4332627e-02  4.5326903e-02  2.1079680e-02  7.3527810e-03\n",
      "  1.5505044e-02 -1.6964620e-02  5.9772595e-03  2.6771771e-03\n",
      "  1.2224833e-02 -2.3821309e-02 -3.1242734e-03  2.9864591e-02\n",
      " -5.6090078e-04  3.7588375e-03  4.7067113e-02  5.7254382e-03\n",
      "  2.7678646e-02  1.0284196e-02  7.7630761e-03  1.0018184e-02]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#Load model and train new data\n",
    "\n",
    "with tf.Session() as sess1:\n",
    "    init=tf.global_variables_initializer()\n",
    "    sess1.run(init)\n",
    "    new_saver =tf.train.import_meta_graph('./retrain/test_model1.meta')\n",
    "    new_saver.restore(sess1,('./retrain/test_model1'))\n",
    "    print(sess1.run('bi:0'))\n",
    "    graph=tf.get_default_graph()\n",
    "    #print([n.name for n in tf.get_default_graph().as_graph_def().node])\n",
    "    train_step=graph.get_operation_by_name(\"train_op\")\n",
    "    for i in tqdm(range(len(data_x))):\n",
    "        feed={\"input_tensor:0\":data_x[i:i+1],\n",
    "              \"input_state:0\":data_state,\n",
    "              \"labels:0\":data_y[i:i+1],\n",
    "              \"loss_weight:0\":loss_weight[i:i+1]\n",
    "               }\n",
    "        sess1.run(train_step,feed_dict=feed)\n",
    "        data_state,_predictY = sess1.run([\"output_state:0\",\"output:0\"],feed_dict=feed)\n",
    "    print(sess1.run('bi:0'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
